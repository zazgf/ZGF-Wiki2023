{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to ZGF Wiki \u867d\u53e4\u72b9\u4e0d\u53ca \u5b50\u66f0\uff1a\u201c\u5b66\u800c\u65f6\u4e60\u4e4b\uff0c\u4e0d\u4ea6\u8bf4\u4e4e\uff1f\u6709\u670b\u81ea\u8fdc\u65b9\u6765\uff0c\u4e0d\u4ea6\u4e50\u4e4e\uff1f\u4eba\u4e0d\u77e5\u800c\u4e0d\u6120\uff0c\u4e0d\u4ea6\u541b\u5b50\u4e4e\uff1f\u201d \u66fe\u5b50\u66f0\uff1a\u201c\u543e\u65e5\u4e09\u7701\u543e\u8eab\uff1a\u4e3a\u4eba\u8c0b\u800c\u4e0d\u5fe0\u4e4e\uff1f\u4e0e\u670b\u53cb\u4ea4\u800c\u4e0d\u4fe1\u4e4e\uff1f\u4f20\u4e0d\u4e60\u4e4e\uff1f\u201d \u5b50\u66f0\uff1a\u201c\u6e29\u6545\u800c\u77e5\u65b0\uff0c\u53ef\u4ee5\u4e3a\u5e08\u77e3\u3002\u201d \u5b50\u66f0\uff1a\u201c\u5b66\u800c\u4e0d\u601d\u5219\u7f54\uff0c\u601d\u800c\u4e0d\u5b66\u5219\u6b86\u3002\u201d \u5b50\u66f0\uff1a\u201c\u7531\uff0c\u8bf2\u5973\u77e5\u4e4b\u4e4e\uff01\u77e5\u4e4b\u4e3a\u77e5\u4e4b\uff0c\u4e0d\u77e5\u4e3a\u4e0d\u77e5\uff0c\u662f\u77e5\u4e5f\u3002\u201d \u5b50\u66f0\uff1a\u201c\u89c1\u8d24\u601d\u9f50\u7109\uff0c\u89c1\u4e0d\u8d24\u800c\u5185\u81ea\u7701\u4e5f\u3002\u201d \u5b50\u66f0\uff1a\u201c\u4e09\u4eba\u884c\uff0c\u5fc5\u6709\u6211\u5e08\u7109\u3002\u62e9\u5176\u5584\u8005\u800c\u4ece\u4e4b\uff0c\u5176\u4e0d\u5584\u8005\u800c\u6539\u4e4b\u3002\u201d \u66fe\u5b50\u66f0\uff1a\u201c\u58eb\u4e0d\u53ef\u4ee5\u4e0d\u5f18\u6bc5\uff0c\u4efb\u91cd\u800c\u9053\u8fdc\u3002\u4ec1\u4ee5\u4e3a\u5df1\u4efb\uff0c\u4e0d\u4ea6\u91cd\u4e4e\uff1f\u6b7b\u800c\u540e\u5df2\uff0c\u4e0d\u4ea6\u8fdc\u4e4e\uff1f\u201d \u5b50\u66f0\uff1a\u201c\u5c81\u5bd2\uff0c\u7136\u540e\u77e5\u677e\u67cf\u4e4b\u540e\u51cb\u4e5f\u3002\u201d \u5b50\u8d21\u95ee\u66f0\uff1a\u201c\u6709\u4e00\u8a00\u800c\u53ef\u4ee5\u7ec8\u8eab\u884c\u4e4b\u8005\u4e4e\uff1f\u201d \u5b50\u66f0\uff1a\u201c\u5176\u6055\u4e4e\uff01\u5df1\u6240\u4e0d\u6b32\uff0c\u52ff\u65bd\u4e8e\u4eba\u3002\u201d","title":"Home"},{"location":"#welcome-to-zgf-wiki","text":"\u867d\u53e4\u72b9\u4e0d\u53ca \u5b50\u66f0\uff1a\u201c\u5b66\u800c\u65f6\u4e60\u4e4b\uff0c\u4e0d\u4ea6\u8bf4\u4e4e\uff1f\u6709\u670b\u81ea\u8fdc\u65b9\u6765\uff0c\u4e0d\u4ea6\u4e50\u4e4e\uff1f\u4eba\u4e0d\u77e5\u800c\u4e0d\u6120\uff0c\u4e0d\u4ea6\u541b\u5b50\u4e4e\uff1f\u201d \u66fe\u5b50\u66f0\uff1a\u201c\u543e\u65e5\u4e09\u7701\u543e\u8eab\uff1a\u4e3a\u4eba\u8c0b\u800c\u4e0d\u5fe0\u4e4e\uff1f\u4e0e\u670b\u53cb\u4ea4\u800c\u4e0d\u4fe1\u4e4e\uff1f\u4f20\u4e0d\u4e60\u4e4e\uff1f\u201d \u5b50\u66f0\uff1a\u201c\u6e29\u6545\u800c\u77e5\u65b0\uff0c\u53ef\u4ee5\u4e3a\u5e08\u77e3\u3002\u201d \u5b50\u66f0\uff1a\u201c\u5b66\u800c\u4e0d\u601d\u5219\u7f54\uff0c\u601d\u800c\u4e0d\u5b66\u5219\u6b86\u3002\u201d \u5b50\u66f0\uff1a\u201c\u7531\uff0c\u8bf2\u5973\u77e5\u4e4b\u4e4e\uff01\u77e5\u4e4b\u4e3a\u77e5\u4e4b\uff0c\u4e0d\u77e5\u4e3a\u4e0d\u77e5\uff0c\u662f\u77e5\u4e5f\u3002\u201d \u5b50\u66f0\uff1a\u201c\u89c1\u8d24\u601d\u9f50\u7109\uff0c\u89c1\u4e0d\u8d24\u800c\u5185\u81ea\u7701\u4e5f\u3002\u201d \u5b50\u66f0\uff1a\u201c\u4e09\u4eba\u884c\uff0c\u5fc5\u6709\u6211\u5e08\u7109\u3002\u62e9\u5176\u5584\u8005\u800c\u4ece\u4e4b\uff0c\u5176\u4e0d\u5584\u8005\u800c\u6539\u4e4b\u3002\u201d \u66fe\u5b50\u66f0\uff1a\u201c\u58eb\u4e0d\u53ef\u4ee5\u4e0d\u5f18\u6bc5\uff0c\u4efb\u91cd\u800c\u9053\u8fdc\u3002\u4ec1\u4ee5\u4e3a\u5df1\u4efb\uff0c\u4e0d\u4ea6\u91cd\u4e4e\uff1f\u6b7b\u800c\u540e\u5df2\uff0c\u4e0d\u4ea6\u8fdc\u4e4e\uff1f\u201d \u5b50\u66f0\uff1a\u201c\u5c81\u5bd2\uff0c\u7136\u540e\u77e5\u677e\u67cf\u4e4b\u540e\u51cb\u4e5f\u3002\u201d \u5b50\u8d21\u95ee\u66f0\uff1a\u201c\u6709\u4e00\u8a00\u800c\u53ef\u4ee5\u7ec8\u8eab\u884c\u4e4b\u8005\u4e4e\uff1f\u201d \u5b50\u66f0\uff1a\u201c\u5176\u6055\u4e4e\uff01\u5df1\u6240\u4e0d\u6b32\uff0c\u52ff\u65bd\u4e8e\u4eba\u3002\u201d","title":"Welcome to ZGF Wiki"},{"location":"Atlas200%E9%AA%8C%E6%94%B6%E6%8A%A5%E5%91%8A/","text":"Atlas200\u9a8c\u6536\u62a5\u544a \u5728\u7ebf\u652f\u6301 - \u665f\u817e\u5b66\u9662 - Atlas 200 DK \u6027\u80fd\u7279\u70b9 - \u53ef\u63d0\u4f9b16TOPS (INT8) \u7684\u5cf0\u503c\u8ba1\u7b97\u80fd\u529b\u3002 - \u652f\u6301\u4e24\u8defCamera\u8f93\u5165\uff0c\u4e24\u8defISP\u56fe\u50cf\u5904\u7406\uff0c\u652f\u6301HDR10\u9ad8\u52a8\u6001\u8303\u56f4\u6280\u672f\u6807\u51c6 - \u652f\u63011000M\u4ee5\u592a\u7f51\u5bf9\u5916\u63d0\u4f9b\u9ad8\u901f\u7f51\u7edc\u8fde\u63a5\uff0c\u5339\u914d\u5f3a\u52b2\u8ba1\u7b97\u80fd\u529b - \u901a\u7528\u768440-pin\u6269\u5c55\u63a5\u53e3\uff0c\u65b9\u4fbf\u4ea7\u54c1\u539f\u578b\u8bbe\u8ba1 - \u652f\u63015v~28v\u5bbd\u8303\u56f4\u76f4\u6d41\u7535\u6e90\u8f93\u5165 \u9700\u8981\u914d\u4ef6 - micro sd\u5361 32G - typeC usb \u7ebf - \u7f51\u7ebf - \u8bfb\u5361\u5668 - Ubuntu\u670d\u52a1\u5668 \u5236\u4f5cUsb\u7cfb\u7edf\u542f\u52a8\u76d8 - \u5c06SD\u5361\u653e\u5165\u8bfb\u5361\u5668\uff0c\u5c06\u8bfb\u5361\u5668\u4e0eUbuntu\u670d\u52a1\u5668\u7684USB\u8fde\u63a5\uff0c\u7136\u540e\u901a\u8fc7\u5236\u5361\u811a\u672c\u8fdb\u884cSD\u5361\u7684\u5236\u4f5c - \u8f6f\u4ef6\u5305\u51c6\u5907 - \u5236\u5361\u5165\u53e3\u811a\u672c make_sd_card.py - \u5236\u4f5cSD\u5361\u64cd\u4f5c\u7cfb\u7edf\u811a\u672c make_ubuntu_sd.sh - \u8bf7\u9009\u62e9\u914d\u5957\u7684CANN\u7248\u672c\uff1a20.1\u3002 - \u201c\u4ea7\u54c1\u7cfb\u5217\u201d\u8bf7\u9009\u62e9\u201cAtlas 200 DK\u201d\u3002 - \u201c\u4ea7\u54c1\u578b\u53f7\u201d\u8bf7\u9009\u62e9\u201cAtlas 200 DK \u5f00\u53d1\u8005\u5957\u4ef6\u201d\u3002 - Ubuntu\u64cd\u4f5c\u7cfb\u7edf\u955c\u50cf\u5305 - ubuntu-18.04.xx-server-arm64.iso - \u5f00\u53d1\u8005\u677f\u9a71\u52a8\u5305 - A200dk-npu-driver-{software version}-ubuntu18.04-aarch64-minirc.tar.gz - \u8bf7\u9009\u62e9\u914d\u5957\u7684CANN\u7248\u672c\uff1a20.1\u3002 - \u201c\u4ea7\u54c1\u7cfb\u5217\u201d\u8bf7\u9009\u62e9\u201cAtlas 200 DK\u201d\u3002 - \u201c\u4ea7\u54c1\u578b\u53f7\u201d\u8bf7\u9009\u62e9\u201cAtlas 200 DK \u5f00\u53d1\u8005\u5957\u4ef6\u201d\u3002 - \u5f00\u53d1\u8005\u677f\u8fd0\u884c\u5305 - Ascend-cann-minirc_{software version}_ubuntu18.04-aarch64.zip - \u8bf7\u4ece\u201cCANN\u8f6f\u4ef6\u5305\u201d\u4e2d\u9009\u62e9\u5bf9\u5e94\u7248\u672c\u7684\u8f6f\u4ef6\u5305\u4e0b\u8f7d\u3002 \u64cd\u4f5c\u6b65\u9aa4 1. \u8bf7\u5c06SD\u5361\u653e\u5165\u8bfb\u5361\u5668\uff0c\u5e76\u5c06\u8bfb\u5361\u5668\u4e0eUbuntu\u670d\u52a1\u5668\u7684USB\u63a5\u53e3\u8fde\u63a5\u3002 2. sudo apt-get install qemu-user-static binfmt-support python3-yaml gcc-aarch64-linux-gnu g++-aarch64-linux-gnu 3. \u628a\u51c6\u5907\u597d\u7684\u6587\u4ef6\u5305\u653e\u5728/home/username/mksd\u76ee\u5f55\u4e0b\uff0c cd /home/username/mksd 4. \u6267\u884c fdisk -l \u67e5\u770bsd\u5361\u6240\u5728usb\u8bbe\u5907\u540d\u79f0\u4e3a\u201c/dev/sda\u201d\uff0c\u53ef\u901a\u8fc7\u63d2\u62d4SD\u5361\u7684\u65b9\u5f0f\u786e\u5b9a\u8bbe\u5907\u540d\u79f0\u3002 5. \u8fd0\u884cSD\u5236\u5361\u811a\u672c\u201cmake_sd_card.py\u201d\u3002 1. python3 make_sd_card.py local /dev/sda 2. \u201clocal\u201d\u8868\u793a\u4f7f\u7528\u672c\u5730\u65b9\u5f0f\u5236\u4f5cSD\u5361\u3002 3. \u201c/dev/sda\u201d\u4e3aSD\u5361\u6240\u5728\u7684USB\u8bbe\u5907\u540d\u79f0\u3002 \u9047\u5230\u95ee\u9898 \uff1a\u5236\u4f5csd\u5361\u4e0d\u6210\u529f\uff0c\u7ecf\u8fc7\u53cd\u590d\u534f\u52a9\u534e\u4e3a\u65b9\u6280\u672f\u4eba\u5458\uff0c\u53d1\u73b0\u534e\u4e3a\u5b98\u65b9\u63d0\u4f9b\u7684\u811a\u672c\u6587\u4ef6 make_ubuntu_sd.sh \u6709bug\u3002\u4f7f\u7528\u4f53\u9a8c\u5dee\u3002\u534e\u4e3a\u65b9\u89e3\u51b3\u95ee\u9898\u5468\u671f\u957f\uff0c\u8fd9\u4e2a\u95ee\u9898\u7528\u4e86\u4e24\u5929\u624d\u89e3\u51b3\u3002\u540e\u671f\u5e94\u7528\u8fc7\u7a0b\u4e2d\u534e\u4e3aatlas\u5e73\u53f0\u4e5f\u4f1a\u5b58\u5728\u4e00\u4e9bbug\uff0c\u5f00\u53d1\u901f\u5ea6\u53d7\u9650\u4e8e\u534e\u4e3a\u65b9\u89e3\u51b3atlas\u4ea7\u54c1bug\u7684\u901f\u5ea6\u3002\u9047\u5230Atlas\u5e73\u53f0\u95ee\u9898\uff0c\u53ea\u80fd\u5728\u534e\u4e3a\u4e91\u8bba\u575b\u4e0a\u8fdb\u884c\u63d0\u95ee\uff0c\u89e3\u51b3\u95ee\u9898\u5468\u671f\u957f\u3002 \u83b7\u5f97\u4fee\u6b63\u540e\u7684 make_ubuntu_sd.sh \u811a\u672c\u6587\u4ef6\u5b89\u88c5\u6210\u529f\u3002 Atlas200 \u5f00\u673a\u542f\u52a8\u6210\u529f\uff0c\u8fdb\u5165\u7cfb\u7edf\u3002\\ \u8fdb\u884c\u670d\u52a1\u5668ip\u8bbe\u7f6e \uff1a\\ ip:192.168.0.3\\ netmask:255.255.0.0\\ gateway:192.168.0.1\\ ssh\u767b\u5165Atlas200 :\\ ssh HwHiAiUser@192.168.0.2 \\ \u5bc6\u7801:\\ Mind@123 \\","title":"Atlas200\u9a8c\u6536\u62a5\u544a"},{"location":"Atlas200%E9%AA%8C%E6%94%B6%E6%8A%A5%E5%91%8A/#atlas200","text":"\u5728\u7ebf\u652f\u6301 - \u665f\u817e\u5b66\u9662 - Atlas 200 DK \u6027\u80fd\u7279\u70b9 - \u53ef\u63d0\u4f9b16TOPS (INT8) \u7684\u5cf0\u503c\u8ba1\u7b97\u80fd\u529b\u3002 - \u652f\u6301\u4e24\u8defCamera\u8f93\u5165\uff0c\u4e24\u8defISP\u56fe\u50cf\u5904\u7406\uff0c\u652f\u6301HDR10\u9ad8\u52a8\u6001\u8303\u56f4\u6280\u672f\u6807\u51c6 - \u652f\u63011000M\u4ee5\u592a\u7f51\u5bf9\u5916\u63d0\u4f9b\u9ad8\u901f\u7f51\u7edc\u8fde\u63a5\uff0c\u5339\u914d\u5f3a\u52b2\u8ba1\u7b97\u80fd\u529b - \u901a\u7528\u768440-pin\u6269\u5c55\u63a5\u53e3\uff0c\u65b9\u4fbf\u4ea7\u54c1\u539f\u578b\u8bbe\u8ba1 - \u652f\u63015v~28v\u5bbd\u8303\u56f4\u76f4\u6d41\u7535\u6e90\u8f93\u5165 \u9700\u8981\u914d\u4ef6 - micro sd\u5361 32G - typeC usb \u7ebf - \u7f51\u7ebf - \u8bfb\u5361\u5668 - Ubuntu\u670d\u52a1\u5668 \u5236\u4f5cUsb\u7cfb\u7edf\u542f\u52a8\u76d8 - \u5c06SD\u5361\u653e\u5165\u8bfb\u5361\u5668\uff0c\u5c06\u8bfb\u5361\u5668\u4e0eUbuntu\u670d\u52a1\u5668\u7684USB\u8fde\u63a5\uff0c\u7136\u540e\u901a\u8fc7\u5236\u5361\u811a\u672c\u8fdb\u884cSD\u5361\u7684\u5236\u4f5c - \u8f6f\u4ef6\u5305\u51c6\u5907 - \u5236\u5361\u5165\u53e3\u811a\u672c make_sd_card.py - \u5236\u4f5cSD\u5361\u64cd\u4f5c\u7cfb\u7edf\u811a\u672c make_ubuntu_sd.sh - \u8bf7\u9009\u62e9\u914d\u5957\u7684CANN\u7248\u672c\uff1a20.1\u3002 - \u201c\u4ea7\u54c1\u7cfb\u5217\u201d\u8bf7\u9009\u62e9\u201cAtlas 200 DK\u201d\u3002 - \u201c\u4ea7\u54c1\u578b\u53f7\u201d\u8bf7\u9009\u62e9\u201cAtlas 200 DK \u5f00\u53d1\u8005\u5957\u4ef6\u201d\u3002 - Ubuntu\u64cd\u4f5c\u7cfb\u7edf\u955c\u50cf\u5305 - ubuntu-18.04.xx-server-arm64.iso - \u5f00\u53d1\u8005\u677f\u9a71\u52a8\u5305 - A200dk-npu-driver-{software version}-ubuntu18.04-aarch64-minirc.tar.gz - \u8bf7\u9009\u62e9\u914d\u5957\u7684CANN\u7248\u672c\uff1a20.1\u3002 - \u201c\u4ea7\u54c1\u7cfb\u5217\u201d\u8bf7\u9009\u62e9\u201cAtlas 200 DK\u201d\u3002 - \u201c\u4ea7\u54c1\u578b\u53f7\u201d\u8bf7\u9009\u62e9\u201cAtlas 200 DK \u5f00\u53d1\u8005\u5957\u4ef6\u201d\u3002 - \u5f00\u53d1\u8005\u677f\u8fd0\u884c\u5305 - Ascend-cann-minirc_{software version}_ubuntu18.04-aarch64.zip - \u8bf7\u4ece\u201cCANN\u8f6f\u4ef6\u5305\u201d\u4e2d\u9009\u62e9\u5bf9\u5e94\u7248\u672c\u7684\u8f6f\u4ef6\u5305\u4e0b\u8f7d\u3002 \u64cd\u4f5c\u6b65\u9aa4 1. \u8bf7\u5c06SD\u5361\u653e\u5165\u8bfb\u5361\u5668\uff0c\u5e76\u5c06\u8bfb\u5361\u5668\u4e0eUbuntu\u670d\u52a1\u5668\u7684USB\u63a5\u53e3\u8fde\u63a5\u3002 2. sudo apt-get install qemu-user-static binfmt-support python3-yaml gcc-aarch64-linux-gnu g++-aarch64-linux-gnu 3. \u628a\u51c6\u5907\u597d\u7684\u6587\u4ef6\u5305\u653e\u5728/home/username/mksd\u76ee\u5f55\u4e0b\uff0c cd /home/username/mksd 4. \u6267\u884c fdisk -l \u67e5\u770bsd\u5361\u6240\u5728usb\u8bbe\u5907\u540d\u79f0\u4e3a\u201c/dev/sda\u201d\uff0c\u53ef\u901a\u8fc7\u63d2\u62d4SD\u5361\u7684\u65b9\u5f0f\u786e\u5b9a\u8bbe\u5907\u540d\u79f0\u3002 5. \u8fd0\u884cSD\u5236\u5361\u811a\u672c\u201cmake_sd_card.py\u201d\u3002 1. python3 make_sd_card.py local /dev/sda 2. \u201clocal\u201d\u8868\u793a\u4f7f\u7528\u672c\u5730\u65b9\u5f0f\u5236\u4f5cSD\u5361\u3002 3. \u201c/dev/sda\u201d\u4e3aSD\u5361\u6240\u5728\u7684USB\u8bbe\u5907\u540d\u79f0\u3002 \u9047\u5230\u95ee\u9898 \uff1a\u5236\u4f5csd\u5361\u4e0d\u6210\u529f\uff0c\u7ecf\u8fc7\u53cd\u590d\u534f\u52a9\u534e\u4e3a\u65b9\u6280\u672f\u4eba\u5458\uff0c\u53d1\u73b0\u534e\u4e3a\u5b98\u65b9\u63d0\u4f9b\u7684\u811a\u672c\u6587\u4ef6 make_ubuntu_sd.sh \u6709bug\u3002\u4f7f\u7528\u4f53\u9a8c\u5dee\u3002\u534e\u4e3a\u65b9\u89e3\u51b3\u95ee\u9898\u5468\u671f\u957f\uff0c\u8fd9\u4e2a\u95ee\u9898\u7528\u4e86\u4e24\u5929\u624d\u89e3\u51b3\u3002\u540e\u671f\u5e94\u7528\u8fc7\u7a0b\u4e2d\u534e\u4e3aatlas\u5e73\u53f0\u4e5f\u4f1a\u5b58\u5728\u4e00\u4e9bbug\uff0c\u5f00\u53d1\u901f\u5ea6\u53d7\u9650\u4e8e\u534e\u4e3a\u65b9\u89e3\u51b3atlas\u4ea7\u54c1bug\u7684\u901f\u5ea6\u3002\u9047\u5230Atlas\u5e73\u53f0\u95ee\u9898\uff0c\u53ea\u80fd\u5728\u534e\u4e3a\u4e91\u8bba\u575b\u4e0a\u8fdb\u884c\u63d0\u95ee\uff0c\u89e3\u51b3\u95ee\u9898\u5468\u671f\u957f\u3002 \u83b7\u5f97\u4fee\u6b63\u540e\u7684 make_ubuntu_sd.sh \u811a\u672c\u6587\u4ef6\u5b89\u88c5\u6210\u529f\u3002 Atlas200 \u5f00\u673a\u542f\u52a8\u6210\u529f\uff0c\u8fdb\u5165\u7cfb\u7edf\u3002\\ \u8fdb\u884c\u670d\u52a1\u5668ip\u8bbe\u7f6e \uff1a\\ ip:192.168.0.3\\ netmask:255.255.0.0\\ gateway:192.168.0.1\\ ssh\u767b\u5165Atlas200 :\\ ssh HwHiAiUser@192.168.0.2 \\ \u5bc6\u7801:\\ Mind@123 \\","title":"Atlas200\u9a8c\u6536\u62a5\u544a"},{"location":"Autoware.auto/","text":"Autoware.Auto Install ubuntu 20.04 ROS2_FOXY sudo apt update && sudo apt install locales sudo locale-gen en_US en_US.UTF-8 sudo update-locale LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8 export LANG=en_US.UTF-8 sudo apt install curl sudo apt update && sudo apt install curl gnupg2 lsb-release curl -s https://raw.githubusercontent.com/ros/rosdistro/master/ros.asc | sudo apt-key add - sudo sh -c 'echo \"deb [arch=$(dpkg --print-architecture)] http://packages.ros.org/ros2/ubuntu $(lsb_release -cs) main\" > /etc/apt/sources.list.d/ros2-latest.list' sudo apt update && sudo apt install -y \\ build-essential \\ cmake \\ git \\ libbullet-dev \\ python3-colcon-common-extensions \\ python3-flake8 \\ python3-pip \\ python3-pytest-cov \\ python3-rosdep \\ python3-setuptools \\ python3-vcstool \\ wget # install some pip packages needed for testing python3 -m pip install -U \\ argcomplete \\ flake8-blind-except \\ flake8-builtins \\ flake8-class-newline \\ flake8-comprehensions \\ flake8-deprecated \\ flake8-docstrings \\ flake8-import-order \\ flake8-quotes \\ pytest-repeat \\ pytest-rerunfailures \\ pytest # install Fast-RTPS dependencies sudo apt install --no-install-recommends -y \\ libasio-dev \\ libtinyxml2-dev # install Cyclone DDS dependencies sudo apt install --no-install-recommends -y \\ libcunit1-dev \u83b7\u53d6ros2\u4ee3\u7801 mkdir -p ~/ros2_foxy/src cd ~/ros2_foxy wget https://raw.githubusercontent.com/ros2/ros2/foxy/ros2.repos vcs import src < ros2.repos \u4f7f\u7528rosdep\u5b89\u88c5\u4f9d\u8d56 sudo rosdep init rosdep update rosdep install --from-paths src --ignore-src --rosdistro foxy -y --skip-keys \"console_bridge fastcdr fastrtps rti-connext-dds-5.3.1 urdfdom_headers\" Build cd ~/ros2_foxy colcon build --symlink-install build\u8fc7\u7a0b\u4e2d\u4f1a\u51fa\u73b0\u4e0b\u8f7d\u9519\u8bef\uff0c\u4f7f\u7528hub.fastgit.org\u4ee3\u66ffgithub.com","title":"Autoware.Auto"},{"location":"Autoware.auto/#autowareauto","text":"","title":"Autoware.Auto"},{"location":"Autoware.auto/#install","text":"ubuntu 20.04 ROS2_FOXY sudo apt update && sudo apt install locales sudo locale-gen en_US en_US.UTF-8 sudo update-locale LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8 export LANG=en_US.UTF-8 sudo apt install curl sudo apt update && sudo apt install curl gnupg2 lsb-release curl -s https://raw.githubusercontent.com/ros/rosdistro/master/ros.asc | sudo apt-key add - sudo sh -c 'echo \"deb [arch=$(dpkg --print-architecture)] http://packages.ros.org/ros2/ubuntu $(lsb_release -cs) main\" > /etc/apt/sources.list.d/ros2-latest.list' sudo apt update && sudo apt install -y \\ build-essential \\ cmake \\ git \\ libbullet-dev \\ python3-colcon-common-extensions \\ python3-flake8 \\ python3-pip \\ python3-pytest-cov \\ python3-rosdep \\ python3-setuptools \\ python3-vcstool \\ wget # install some pip packages needed for testing python3 -m pip install -U \\ argcomplete \\ flake8-blind-except \\ flake8-builtins \\ flake8-class-newline \\ flake8-comprehensions \\ flake8-deprecated \\ flake8-docstrings \\ flake8-import-order \\ flake8-quotes \\ pytest-repeat \\ pytest-rerunfailures \\ pytest # install Fast-RTPS dependencies sudo apt install --no-install-recommends -y \\ libasio-dev \\ libtinyxml2-dev # install Cyclone DDS dependencies sudo apt install --no-install-recommends -y \\ libcunit1-dev \u83b7\u53d6ros2\u4ee3\u7801 mkdir -p ~/ros2_foxy/src cd ~/ros2_foxy wget https://raw.githubusercontent.com/ros2/ros2/foxy/ros2.repos vcs import src < ros2.repos \u4f7f\u7528rosdep\u5b89\u88c5\u4f9d\u8d56 sudo rosdep init rosdep update rosdep install --from-paths src --ignore-src --rosdistro foxy -y --skip-keys \"console_bridge fastcdr fastrtps rti-connext-dds-5.3.1 urdfdom_headers\"","title":"Install"},{"location":"Autoware.auto/#build","text":"cd ~/ros2_foxy colcon build --symlink-install build\u8fc7\u7a0b\u4e2d\u4f1a\u51fa\u73b0\u4e0b\u8f7d\u9519\u8bef\uff0c\u4f7f\u7528hub.fastgit.org\u4ee3\u66ffgithub.com","title":"Build"},{"location":"PPO/","text":"Tensorflow \u7b97\u6cd5\u529f\u80fd \u8bb0\u5f55\u4eea\u8bbe\u7f6e \u968f\u673a\u79cd\u5b50\u8bbe\u7f6e \u73af\u5883\u5b9e\u4f8b\u5316 \u4e3a\u8ba1\u7b97\u56fe\u5236\u4f5c\u5360\u4f4d\u7b26 actor_critic \u901a\u8fc7\u4f5c\u4e3a\u53c2\u6570\u4f20\u9012\u7ed9\u7b97\u6cd5\u51fd\u6570\u6765\u6784\u5efa\u53c2\u4e0e\u8005\u8bc4\u8bba\u8ba1\u7b97\u56fe \u5b9e\u4f8b\u5316\u4f53\u9a8c\u7f13\u51b2\u533a \u5efa\u7acb\u7279\u5b9a\u4e8e\u7b97\u6cd5\u7684\u635f\u5931\u51fd\u6570\u548c\u8bca\u65ad\u7684\u8ba1\u7b97\u56fe \u8fdb\u884c\u57f9\u8bad \u8fdb\u884ctf\u4f1a\u8bdd\u5e76\u521d\u59cb\u5316\u53c2\u6570 \u901a\u8fc7\u8bb0\u5f55\u5668\u8bbe\u7f6e\u6a21\u578b\u4fdd\u5b58 \u5b9a\u4e49\u8fd0\u884c\u7b97\u6cd5\u4e3b\u5faa\u73af\u6240\u9700\u7684\u529f\u80fd\uff08\u4f8b\u5982\uff0c\u53d6\u51b3\u4e8e\u7b97\u6cd5\uff0c\u6838\u5fc3\u66f4\u65b0\u529f\u80fd\uff0c\u83b7\u53d6\u64cd\u4f5c\u529f\u80fd\u548c\u6d4b\u8bd5\u4ee3\u7406\u529f\u80fd\uff09 \u8fd0\u884c\u7b97\u6cd5\u7684\u4e3b\u5faa\u73af\uff1a \u5728\u73af\u5883\u4e2d\u8fd0\u884c\u4ee3\u7406 \u6839\u636e\u7b97\u6cd5\u7684\u4e3b\u8981\u65b9\u7a0b\u5f0f\u5b9a\u65f6\u66f4\u65b0\u4ee3\u7406\u53c2\u6570 \u8bb0\u5f55\u5173\u952e\u6027\u80fd\u6307\u6807\u5e76\u4fdd\u5b58\u4ee3\u7406 \u6838\u5fc3\u6587\u4ef6 \u6838\u5fc3\u6587\u4ef6\u4e0d\u50cf\u7b97\u6cd5\u6587\u4ef6\u90a3\u6837\u7d27\u5bc6\u5730\u9644\u7740\u5728\u6a21\u7248\u4e0a\uff0c\u4f46\u5177\u6709\u4e00\u4e9b\u8fd1\u4f3c\u7684\u7ed3\u6784\uff1a \u4ec5tensorflow:\u4e0e\u5236\u4f5c\u548c\u7ba1\u7406\u5360\u4f4d\u7b26\u6709\u5173\u7684\u529f\u80fd \u7528\u4e8e\u5efa\u7acbactor_critic \u7279\u5b9a\u7b97\u6cd5\u7684\u65b9\u6cd5\u76f8\u5173\u7684\u8ba1\u7b97\u56fe\u90e8\u5206\u7684\u529f\u80fd \u4efb\u4f55\u5176\u4ed6\u6709\u7528\u7684\u529f\u80fd \u4e8e\u7b97\u6cd5\u517c\u5bb9\u7684MLP actor_critic \u7684\u5b9e\u73b0\uff0c\u5176\u4e2d\u7b56\u7565\u548c\u503c\u51fd\u6570\u90fd\u7531\u7b80\u5355\u7684MLP\u8868\u793a\u3002 \u4f53\u9a8c\u6df1\u5ea6RL\u7684\u6700\u4f73\u65b9\u6cd5\u4e4b\u4e00\u662f\u8fd0\u884c\u7b97\u6cd5\uff0c\u5e76\u67e5\u770b\u5b83\u4eec\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u7684\u6267\u884c\u60c5\u51b5\u3002Spinning Up\u968f\u9644spinup/run.py,\u8fd9\u662f\u4e00\u4e2a\u65b9\u4fbf\u7684\u5de5\u5177\uff0c\u53ef\u4ee5\u8ba9\u4f60\u4ece\u547d\u4ee4\u884c\u8f7b\u677e\u7684\u542f\u52a8\u5404\u79cd\u597d\u7b97\u6cd5\uff08\u53ef\u4ee5\u9009\u62e9\u8d85\u53c2\u6570\uff09 \u8fd1\u7aef\u7b56\u7565\u4f18\u5316 PPO\u53d7\u5230\u4e0eTRPO\u76f8\u540c\u95ee\u9898\u7684\u6fc0\u52b1\uff1a\u6211\u4eec\u5982\u4f55\u624d\u80fd\u4f7f\u7528\u5f53\u524d\u62e5\u6709\u7684\u6570\u636e\u5728\u7b56\u7565\u4e0a\u91c7\u53d6\u6700\u5927\u53ef\u80fd\u7684\u6539\u8fdb\u6b65\u9aa4\uff0c\u800c\u53c8\u4e0d\u4f1a\u8d70\u7684\u592a\u8fdc\u800c\u610f\u5916\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002TRPO\u5c1d\u8bd5\u4f7f\u7528\u590d\u6742\u7684\u4e8c\u9636\u65b9\u6cd5\u89e3\u51b3\u6b64\u95ee\u9898\u3002PPO\u662f\u4e00\u9636\u65b9\u6cd5\u7cfb\u5217\uff0c\u5b83\u4f7f\u7528\u5176\u4ed6\u4e00\u4e9b\u6280\u5de7\u6765\u4f7f\u65b0\u7b56\u7565\u63a5\u8fd1\u4e8e\u65e7\u7b56\u7565\u3002PPO\u65b9\u6cd5\u660e\u663e\u66f4\u6613\u4e8e\u5b9e\u73b0\uff0c\u5e76\u4e14\u4ece\u7ecf\u9a8c\u4e0a\u770b\uff0c\u5176\u6027\u80fd\u81f3\u5c11\u4e0eTRPO\u76f8\u540c\u3002PPO\u6709\u4e24\u79cd\u4e3b\u8981\u53d8\u4f53\uff1aPPO-Penalty\uff0c PPO-Clip\u3002 PPO-Penalty \u5927\u7ea6\u89e3\u51b3\u4e86\u50cfTRPO\u8fd9\u6837\u53d7KL\u7ea6\u675f\u7684\u66f4\u65b0\u3002\u4f46\u662f\u4f1a\u60e9\u7f5a\u76ee\u6807\u51fd\u6570\u4e2d\u7684KL\u6563\u5ea6\uff0c\u800c\u4e0d\u4f7f\u5176\u6210\u4e3a\u786c\u7ea6\u675f\uff0c\u5e76\u4f1a\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u81ea\u52a8\u8c03\u6574\u60e9\u7f5a\u7cfb\u6570\uff0c\u4ee5\u4f7f\u5176\u9002\u5f53\u7f29\u653e\u3002 PPO-Clip \u5728\u76ee\u6807\u4e2d\u6ca1\u6709KL\u6563\u5ea6\u9879\uff0c \u4e5f\u6ca1\u6709\u4efb\u4f55\u7ea6\u675f\u3002\u53d6\u800c\u4ee3\u4e4b\u7684\u662f\u4f9d\u9760\u5bf9\u76ee\u6807\u51fd\u6570\u7684\u4e13\u95e8\u88c1\u526a\u6765\u6d88\u9664\u65b0\u653f\u7b56\u8fdc\u79bb\u65e7\u653f\u7b56\u7684\u52a8\u673a\u3002 \u8981\u95fb\u901f\u89c8 PPO\u662f\u4e00\u79cd\u57fa\u4e8e\u7b56\u7565\u7684\u7b97\u6cd5 PPO\u53ef\u4ee5\u5e94\u7528\u4e8e\u5177\u6709\u79bb\u6563\u7684\u6216\u8fde\u7eed\u7684\u52a8\u4f5c\u7a7a\u95f4\u7684\u73af\u5883 PPO\u7684Spinningup\u5b9e\u73b0\u652f\u6301\u4e0eMPI\u5e76\u884c\u5316 PPO\u66f4\u65b0\u7b56\u7565\u516c\u5f0f \u901a\u5e38\u91c7\u53d6\u591a\u4e2a\u6b65\u9aa4\uff08\u901a\u5e38\u662f\u5c0f\u6279\u91cf\uff09SGD\u6765\u6700\u5927\u5316\u76ee\u6807 $$ L(s,\\alpha,\\theta_k,\\theta)=min\\left(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)}A^{\\pi\\theta_k}(s,\\alpha),clip(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)},1-\\epsilon,1+\\epsilon)A^{\\pi\\theta_k}(s,\\alpha)\\right) $$ $\\epsilon$\u662f\u4e00\u4e2a\u8d85\u53c2\u6570\u7c97\u7565\u5730\u8bf4\u51fa\u65b0\u653f\u7b56\u88ab\u5141\u8bb8\u4e0e\u65e7\u653f\u7b56\u76f8\u8ddd\u591a\u8fdc \u8fd9\u662f\u4e00\u4e2a\u5f88\u590d\u6742\u7684\u65b9\u7a0b\u5f0f\uff0c\u7b2c\u4e00\u773c\u5f88\u96be\u8bf4\u660e\u767d\u5b83\u5728\u505a\u4ec0\u4e48\uff0c\u6216\u8005\u5b83\u5982\u4f55\u5e2e\u52a9\u65b0\u7b56\u7565\u9760\u8fd1\u65e7\u7b56\u7565\u3002\u4e8b\u5b9e\u8bc1\u660e\u6709\u4e00\u4e2a\u76f8\u5f53\u7b80\u5316\u7684\u7248\u672c\uff0c\u8f83\u5bb9\u6613\u7406\u89e3\uff0c\u4e5f\u662f\u6211\u4eec\u5728\u4ee3\u7801\u4e2d\u5b9e\u73b0\u7684\u7248\u672c $$ L(s,\\alpha,\\theta_k,\\theta) = min\\left(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)}A^{\\pi\\theta_k}(s,\\alpha), g\\left(\\epsilon,A^{\\pi\\theta_k}(s,\\alpha)\\right)\\right) $$ $$ g(\\epsilon,A)=\\left{ \\begin{matrix}(1+\\epsilon)A\\quad A\\geq0&\\\\(1-\\epsilon)A\\quad A<0\\end{matrix} \\right. $$ \u4e3a\u4e86\u5f04\u6e05\u695a\u4ece\u4e2d\u5f97\u5230\u7684\u76f4\u89c9\uff0c\u8ba9\u6211\u770b\u4e00\u4e0b\u5355\u4e2a\u7684 \u72b6\u6001\u2014\u52a8\u4f5c \u5bf9\uff0c\u5e76\u8003\u8651\u6848\u4f8b \u4f18\u52bf\u662f\u79ef\u6781 \u7684\uff0c\u5047\u8bbe\u8be5\u72b6\u6001\u2014\u52a8\u4f5c \u5bf9\u7684\u4f18\u52bf\u4e3a\u6b63\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b83\u5bf9\u76ee\u6807\u7684\u8d21\u732e\u51cf\u5c0f\u4e3a\uff1a $$ L(s,\\alpha,\\theta,\\theta_k)=min\\left(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)},(1+\\epsilon)\\right)A^{\\pi\\theta_k}(s,\\alpha) $$ \u56e0\u4e3a\u4f18\u52bf\u662f\u79ef\u6781\u7684\uff0c\u6240\u4ee5\u5982\u679c\u91c7\u53d6\u884c\u52a8\u7684\u53ef\u80fd\u6027\u66f4\u5927\uff08\u4e5f\u5c31\u662f$\\pi_{\\theta}(\\alpha|s)$\u589e\u52a0\uff09\uff0c\u76ee\u6807\u5c31\u4f1a\u589e\u52a0\u3002\u4f46\u662fterm\u4e2d\u7684min\u9650\u5236\u4e86\u53ef\u4ee5\u589e\u52a0\u591a\u5c11\u76ee\u6807\u3002\u4e00\u65e6$\\pi_\\theta(\\alpha|s)>(1+\\epsilon)\\pi_{\\theta_k}(\\alpha|s)$,\u8fd9\u65f6min\u8e22\u8fdb\uff0cterm\u5230\u8fbe$(1+\\epsilon)A^{\\pi\\theta_k}(s,\\alpha)$,\u56e0\u6b64\u7528\u8fc7\u8fdc\u79bb\u65e7\u653f\u7b56\uff0c\u65b0\u653f\u7b56\u4e0d\u4f1a\u83b7\u76ca\u3002 \u4f18\u52bf\u662f\u8d1f\u9762 \u7684\uff0c\u5047\u8bbe\u8be5\u72b6\u6001\u2014\u52a8\u4f5c \u5bf9\u7684\u4f18\u52bf\u662f\u8d1f\u7684\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b83\u5bf9\u76ee\u6807\u7684\u8d21\u732e\u51cf\u5c0f\u4e3a\uff1a $$ L(s,\\alpha,\\theta,\\theta_k)=max\\left(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)},(1-\\epsilon)\\right)A^{\\pi\\theta_k}(s,\\alpha) $$ \u56e0\u4e3a\u4f18\u52bf\u662f\u8d1f\u7684\uff0c\u6240\u4ee5\u5982\u679c \u91c7\u53d6\u884c\u52a8\u7684\u53ef\u80fd\u6027\u66f4\u5c0f\uff08\u4e5f\u5c31\u662f$\\pi_\\theta(\\alpha|s)$\u51cf\u5c0f\uff09\uff0c\u76ee\u6807\u5c31\u4f1a\u589e\u52a0\u3002\u4f46\u662fterm\u4e2d\u7684max\u9650\u5236\u4e86\u53ef\u4ee5\u589e\u52a0\u591a\u5c11\u76ee\u6807\u3002\u4e00\u65e6$\\pi_\\theta(\\alpha|s)<(1-\\epsilon)\\pi_{\\theta_k}(\\alpha|s)$,\u8fd9\u65f6max\u8e22\u8fdb\uff0cterm\u5230\u8fbe$(1-\\epsilon)A^{\\pi\\theta_k}(s,\\alpha)$,\u56e0\u6b64\uff0c\u518d\u6b21\uff0c\u8fdc\u79bb\u65e7\u653f\u7b56\uff0c\u4e0d\u4f1a\u4f7f\u65b0\u653f\u7b56\u53d7\u76ca\u3002 \u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u901a\u8fc7\u53bb\u9664\u4f7f\u653f\u7b56\u53d1\u751f\u91cd\u5927\u53d8\u5316\u7684\u6fc0\u52b1\uff0c\u88c1\u526a\u5145\u5f53\u7740\u6b63\u5219\u5316\u5668\u3002\u800c\u8d85\u53c2\u6570$\\epsilon$\u5219\u5bf9\u5e94\u4e8e\u65b0\u653f\u7b56\u4e0e\u65e7\u653f\u7b56\u53ef\u4ee5\u6709\u591a\u8fdc\uff0c\u540c\u65f6\u4ecd\u4f7f\u76ee\u6807\u53d7\u76ca\u3002 \u63a2\u7d22\u4e0e\u53d1\u73b0 PPO\u6309\u57fa\u4e8e\u7b56\u7565\u65b9\u5f0f\u8bad\u7ec3\u968f\u673a\u7b56\u7565\uff0c\u8fd9\u610f\u5473\u7740\u5b83\u4f1a\u6839\u636e\u5176\u968f\u673a\u7b56\u7565\u7684\u6700\u65b0\u7248\u672c\u901a\u8fc7\u91c7\u6837\u64cd\u4f5c\u6765\u8fdb\u884c\u63a2\u7d22\u3002\u52a8\u4f5c\u9009\u62e9\u7684\u968f\u673a\u6027\u53d6\u51b3\u4e8e\u521d\u59cb\u6761\u4ef6\u548c\u8bad\u7ec3\u6b65\u9aa4\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u4ee3\u7406\u9010\u6b65\u51cf\u5c11\u968f\u673a\u6027\uff0c\u56e0\u4e3a\u66f4\u65b0\u89c4\u5219\u9f13\u52b1\u5b83\u5229\u7528\u5df2\u7ecf\u53d1\u73b0\u7684\u5956\u52b1\u3002\u8fd9\u53ef\u80fd\u5bfc\u81f4\u7b56\u7565\u9677\u5165\u5c40\u90e8\u6700\u4f18\u72b6\u6001\u3002 \u4f2a\u4ee3\u7801 PPO-CLIP \u8f93\u5165\uff1a\u521d\u59cb\u7b56\u7565\u53c2\u6570$\\theta_0$,\u521d\u59cb\u503c\u529f\u80fd\u53c2\u6570$\\phi_0$ for k = 0, 1, 2, ... do \u5728\u73af\u5883\u4e2d\u901a\u8fc7\u6267\u884c\u7b56\u7565$\\pi_k=\\pi(\\theta_k)$\uff0c\u6536\u96c6\u4e00\u7ec4\u8f68\u8ff9$D_k={\\tau_i}$ \u7b97\u5956\u52b1$\\hat{R}_i$ \u57fa\u4e8e\u5f53\u524d\u7684value function $V_{\\phi k}$,\u8ba1\u7b97advantage estimates $\\hat{A}_t$ \u901a\u8fc7\u6700\u5927\u5316PPO-Clip\u76ee\u6807\uff0c\u66f4\u65b0\u7b56\u7565 $$ \\theta_{k+1}=arg\\ \\underset{\\theta}{max}\\frac{1}{|D_k|T}\\sum_{\\tau\\in D_k}\\sum^T_{t=0}min\\left(\\frac{\\pi_\\theta(\\alpha_t|s_t)}{\\pi_{\\theta_k}(\\alpha_t|s_t)}A^{\\pi\\theta_k}(s_t,\\alpha_t),g(\\epsilon,A^{\\pi\\theta_k}(s_t,\\alpha_t))\\right) $$ \u901a\u5e38\u901a\u8fc7\u4e0eAdam\u4e00\u8d77\u7684\u968f\u673a\u68af\u5ea6\u4e0a\u5347 \u901a\u8fc7\u5747\u65b9\u8bef\u5dee\u56de\u5f52\u62df\u5408value function $$ \\phi_{k+1}=arg\\ \\underset{\\phi}{min}\\frac{1}{|D_k|T}\\sum_{\\tau\\in D_k}\\sum^T_{t=0}(V_\\phi(s_t)-\\hat{R}_t)^2 $$ \u7279\u522b\u662f\u901a\u8fc7\u4e00\u4e9b\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5 end for","title":"PPO"},{"location":"PPO/#tensorflow","text":"\u8bb0\u5f55\u4eea\u8bbe\u7f6e \u968f\u673a\u79cd\u5b50\u8bbe\u7f6e \u73af\u5883\u5b9e\u4f8b\u5316 \u4e3a\u8ba1\u7b97\u56fe\u5236\u4f5c\u5360\u4f4d\u7b26 actor_critic \u901a\u8fc7\u4f5c\u4e3a\u53c2\u6570\u4f20\u9012\u7ed9\u7b97\u6cd5\u51fd\u6570\u6765\u6784\u5efa\u53c2\u4e0e\u8005\u8bc4\u8bba\u8ba1\u7b97\u56fe \u5b9e\u4f8b\u5316\u4f53\u9a8c\u7f13\u51b2\u533a \u5efa\u7acb\u7279\u5b9a\u4e8e\u7b97\u6cd5\u7684\u635f\u5931\u51fd\u6570\u548c\u8bca\u65ad\u7684\u8ba1\u7b97\u56fe \u8fdb\u884c\u57f9\u8bad \u8fdb\u884ctf\u4f1a\u8bdd\u5e76\u521d\u59cb\u5316\u53c2\u6570 \u901a\u8fc7\u8bb0\u5f55\u5668\u8bbe\u7f6e\u6a21\u578b\u4fdd\u5b58 \u5b9a\u4e49\u8fd0\u884c\u7b97\u6cd5\u4e3b\u5faa\u73af\u6240\u9700\u7684\u529f\u80fd\uff08\u4f8b\u5982\uff0c\u53d6\u51b3\u4e8e\u7b97\u6cd5\uff0c\u6838\u5fc3\u66f4\u65b0\u529f\u80fd\uff0c\u83b7\u53d6\u64cd\u4f5c\u529f\u80fd\u548c\u6d4b\u8bd5\u4ee3\u7406\u529f\u80fd\uff09 \u8fd0\u884c\u7b97\u6cd5\u7684\u4e3b\u5faa\u73af\uff1a \u5728\u73af\u5883\u4e2d\u8fd0\u884c\u4ee3\u7406 \u6839\u636e\u7b97\u6cd5\u7684\u4e3b\u8981\u65b9\u7a0b\u5f0f\u5b9a\u65f6\u66f4\u65b0\u4ee3\u7406\u53c2\u6570 \u8bb0\u5f55\u5173\u952e\u6027\u80fd\u6307\u6807\u5e76\u4fdd\u5b58\u4ee3\u7406 \u6838\u5fc3\u6587\u4ef6 \u6838\u5fc3\u6587\u4ef6\u4e0d\u50cf\u7b97\u6cd5\u6587\u4ef6\u90a3\u6837\u7d27\u5bc6\u5730\u9644\u7740\u5728\u6a21\u7248\u4e0a\uff0c\u4f46\u5177\u6709\u4e00\u4e9b\u8fd1\u4f3c\u7684\u7ed3\u6784\uff1a \u4ec5tensorflow:\u4e0e\u5236\u4f5c\u548c\u7ba1\u7406\u5360\u4f4d\u7b26\u6709\u5173\u7684\u529f\u80fd \u7528\u4e8e\u5efa\u7acbactor_critic \u7279\u5b9a\u7b97\u6cd5\u7684\u65b9\u6cd5\u76f8\u5173\u7684\u8ba1\u7b97\u56fe\u90e8\u5206\u7684\u529f\u80fd \u4efb\u4f55\u5176\u4ed6\u6709\u7528\u7684\u529f\u80fd \u4e8e\u7b97\u6cd5\u517c\u5bb9\u7684MLP actor_critic \u7684\u5b9e\u73b0\uff0c\u5176\u4e2d\u7b56\u7565\u548c\u503c\u51fd\u6570\u90fd\u7531\u7b80\u5355\u7684MLP\u8868\u793a\u3002 \u4f53\u9a8c\u6df1\u5ea6RL\u7684\u6700\u4f73\u65b9\u6cd5\u4e4b\u4e00\u662f\u8fd0\u884c\u7b97\u6cd5\uff0c\u5e76\u67e5\u770b\u5b83\u4eec\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u7684\u6267\u884c\u60c5\u51b5\u3002Spinning Up\u968f\u9644spinup/run.py,\u8fd9\u662f\u4e00\u4e2a\u65b9\u4fbf\u7684\u5de5\u5177\uff0c\u53ef\u4ee5\u8ba9\u4f60\u4ece\u547d\u4ee4\u884c\u8f7b\u677e\u7684\u542f\u52a8\u5404\u79cd\u597d\u7b97\u6cd5\uff08\u53ef\u4ee5\u9009\u62e9\u8d85\u53c2\u6570\uff09","title":"Tensorflow \u7b97\u6cd5\u529f\u80fd"},{"location":"PPO/#_1","text":"PPO\u53d7\u5230\u4e0eTRPO\u76f8\u540c\u95ee\u9898\u7684\u6fc0\u52b1\uff1a\u6211\u4eec\u5982\u4f55\u624d\u80fd\u4f7f\u7528\u5f53\u524d\u62e5\u6709\u7684\u6570\u636e\u5728\u7b56\u7565\u4e0a\u91c7\u53d6\u6700\u5927\u53ef\u80fd\u7684\u6539\u8fdb\u6b65\u9aa4\uff0c\u800c\u53c8\u4e0d\u4f1a\u8d70\u7684\u592a\u8fdc\u800c\u610f\u5916\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002TRPO\u5c1d\u8bd5\u4f7f\u7528\u590d\u6742\u7684\u4e8c\u9636\u65b9\u6cd5\u89e3\u51b3\u6b64\u95ee\u9898\u3002PPO\u662f\u4e00\u9636\u65b9\u6cd5\u7cfb\u5217\uff0c\u5b83\u4f7f\u7528\u5176\u4ed6\u4e00\u4e9b\u6280\u5de7\u6765\u4f7f\u65b0\u7b56\u7565\u63a5\u8fd1\u4e8e\u65e7\u7b56\u7565\u3002PPO\u65b9\u6cd5\u660e\u663e\u66f4\u6613\u4e8e\u5b9e\u73b0\uff0c\u5e76\u4e14\u4ece\u7ecf\u9a8c\u4e0a\u770b\uff0c\u5176\u6027\u80fd\u81f3\u5c11\u4e0eTRPO\u76f8\u540c\u3002PPO\u6709\u4e24\u79cd\u4e3b\u8981\u53d8\u4f53\uff1aPPO-Penalty\uff0c PPO-Clip\u3002 PPO-Penalty \u5927\u7ea6\u89e3\u51b3\u4e86\u50cfTRPO\u8fd9\u6837\u53d7KL\u7ea6\u675f\u7684\u66f4\u65b0\u3002\u4f46\u662f\u4f1a\u60e9\u7f5a\u76ee\u6807\u51fd\u6570\u4e2d\u7684KL\u6563\u5ea6\uff0c\u800c\u4e0d\u4f7f\u5176\u6210\u4e3a\u786c\u7ea6\u675f\uff0c\u5e76\u4f1a\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u81ea\u52a8\u8c03\u6574\u60e9\u7f5a\u7cfb\u6570\uff0c\u4ee5\u4f7f\u5176\u9002\u5f53\u7f29\u653e\u3002 PPO-Clip \u5728\u76ee\u6807\u4e2d\u6ca1\u6709KL\u6563\u5ea6\u9879\uff0c \u4e5f\u6ca1\u6709\u4efb\u4f55\u7ea6\u675f\u3002\u53d6\u800c\u4ee3\u4e4b\u7684\u662f\u4f9d\u9760\u5bf9\u76ee\u6807\u51fd\u6570\u7684\u4e13\u95e8\u88c1\u526a\u6765\u6d88\u9664\u65b0\u653f\u7b56\u8fdc\u79bb\u65e7\u653f\u7b56\u7684\u52a8\u673a\u3002","title":"\u8fd1\u7aef\u7b56\u7565\u4f18\u5316"},{"location":"PPO/#_2","text":"PPO\u662f\u4e00\u79cd\u57fa\u4e8e\u7b56\u7565\u7684\u7b97\u6cd5 PPO\u53ef\u4ee5\u5e94\u7528\u4e8e\u5177\u6709\u79bb\u6563\u7684\u6216\u8fde\u7eed\u7684\u52a8\u4f5c\u7a7a\u95f4\u7684\u73af\u5883 PPO\u7684Spinningup\u5b9e\u73b0\u652f\u6301\u4e0eMPI\u5e76\u884c\u5316 PPO\u66f4\u65b0\u7b56\u7565\u516c\u5f0f \u901a\u5e38\u91c7\u53d6\u591a\u4e2a\u6b65\u9aa4\uff08\u901a\u5e38\u662f\u5c0f\u6279\u91cf\uff09SGD\u6765\u6700\u5927\u5316\u76ee\u6807 $$ L(s,\\alpha,\\theta_k,\\theta)=min\\left(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)}A^{\\pi\\theta_k}(s,\\alpha),clip(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)},1-\\epsilon,1+\\epsilon)A^{\\pi\\theta_k}(s,\\alpha)\\right) $$ $\\epsilon$\u662f\u4e00\u4e2a\u8d85\u53c2\u6570\u7c97\u7565\u5730\u8bf4\u51fa\u65b0\u653f\u7b56\u88ab\u5141\u8bb8\u4e0e\u65e7\u653f\u7b56\u76f8\u8ddd\u591a\u8fdc \u8fd9\u662f\u4e00\u4e2a\u5f88\u590d\u6742\u7684\u65b9\u7a0b\u5f0f\uff0c\u7b2c\u4e00\u773c\u5f88\u96be\u8bf4\u660e\u767d\u5b83\u5728\u505a\u4ec0\u4e48\uff0c\u6216\u8005\u5b83\u5982\u4f55\u5e2e\u52a9\u65b0\u7b56\u7565\u9760\u8fd1\u65e7\u7b56\u7565\u3002\u4e8b\u5b9e\u8bc1\u660e\u6709\u4e00\u4e2a\u76f8\u5f53\u7b80\u5316\u7684\u7248\u672c\uff0c\u8f83\u5bb9\u6613\u7406\u89e3\uff0c\u4e5f\u662f\u6211\u4eec\u5728\u4ee3\u7801\u4e2d\u5b9e\u73b0\u7684\u7248\u672c $$ L(s,\\alpha,\\theta_k,\\theta) = min\\left(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)}A^{\\pi\\theta_k}(s,\\alpha), g\\left(\\epsilon,A^{\\pi\\theta_k}(s,\\alpha)\\right)\\right) $$ $$ g(\\epsilon,A)=\\left{ \\begin{matrix}(1+\\epsilon)A\\quad A\\geq0&\\\\(1-\\epsilon)A\\quad A<0\\end{matrix} \\right. $$ \u4e3a\u4e86\u5f04\u6e05\u695a\u4ece\u4e2d\u5f97\u5230\u7684\u76f4\u89c9\uff0c\u8ba9\u6211\u770b\u4e00\u4e0b\u5355\u4e2a\u7684 \u72b6\u6001\u2014\u52a8\u4f5c \u5bf9\uff0c\u5e76\u8003\u8651\u6848\u4f8b \u4f18\u52bf\u662f\u79ef\u6781 \u7684\uff0c\u5047\u8bbe\u8be5\u72b6\u6001\u2014\u52a8\u4f5c \u5bf9\u7684\u4f18\u52bf\u4e3a\u6b63\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b83\u5bf9\u76ee\u6807\u7684\u8d21\u732e\u51cf\u5c0f\u4e3a\uff1a $$ L(s,\\alpha,\\theta,\\theta_k)=min\\left(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)},(1+\\epsilon)\\right)A^{\\pi\\theta_k}(s,\\alpha) $$ \u56e0\u4e3a\u4f18\u52bf\u662f\u79ef\u6781\u7684\uff0c\u6240\u4ee5\u5982\u679c\u91c7\u53d6\u884c\u52a8\u7684\u53ef\u80fd\u6027\u66f4\u5927\uff08\u4e5f\u5c31\u662f$\\pi_{\\theta}(\\alpha|s)$\u589e\u52a0\uff09\uff0c\u76ee\u6807\u5c31\u4f1a\u589e\u52a0\u3002\u4f46\u662fterm\u4e2d\u7684min\u9650\u5236\u4e86\u53ef\u4ee5\u589e\u52a0\u591a\u5c11\u76ee\u6807\u3002\u4e00\u65e6$\\pi_\\theta(\\alpha|s)>(1+\\epsilon)\\pi_{\\theta_k}(\\alpha|s)$,\u8fd9\u65f6min\u8e22\u8fdb\uff0cterm\u5230\u8fbe$(1+\\epsilon)A^{\\pi\\theta_k}(s,\\alpha)$,\u56e0\u6b64\u7528\u8fc7\u8fdc\u79bb\u65e7\u653f\u7b56\uff0c\u65b0\u653f\u7b56\u4e0d\u4f1a\u83b7\u76ca\u3002 \u4f18\u52bf\u662f\u8d1f\u9762 \u7684\uff0c\u5047\u8bbe\u8be5\u72b6\u6001\u2014\u52a8\u4f5c \u5bf9\u7684\u4f18\u52bf\u662f\u8d1f\u7684\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b83\u5bf9\u76ee\u6807\u7684\u8d21\u732e\u51cf\u5c0f\u4e3a\uff1a $$ L(s,\\alpha,\\theta,\\theta_k)=max\\left(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)},(1-\\epsilon)\\right)A^{\\pi\\theta_k}(s,\\alpha) $$ \u56e0\u4e3a\u4f18\u52bf\u662f\u8d1f\u7684\uff0c\u6240\u4ee5\u5982\u679c \u91c7\u53d6\u884c\u52a8\u7684\u53ef\u80fd\u6027\u66f4\u5c0f\uff08\u4e5f\u5c31\u662f$\\pi_\\theta(\\alpha|s)$\u51cf\u5c0f\uff09\uff0c\u76ee\u6807\u5c31\u4f1a\u589e\u52a0\u3002\u4f46\u662fterm\u4e2d\u7684max\u9650\u5236\u4e86\u53ef\u4ee5\u589e\u52a0\u591a\u5c11\u76ee\u6807\u3002\u4e00\u65e6$\\pi_\\theta(\\alpha|s)<(1-\\epsilon)\\pi_{\\theta_k}(\\alpha|s)$,\u8fd9\u65f6max\u8e22\u8fdb\uff0cterm\u5230\u8fbe$(1-\\epsilon)A^{\\pi\\theta_k}(s,\\alpha)$,\u56e0\u6b64\uff0c\u518d\u6b21\uff0c\u8fdc\u79bb\u65e7\u653f\u7b56\uff0c\u4e0d\u4f1a\u4f7f\u65b0\u653f\u7b56\u53d7\u76ca\u3002 \u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u901a\u8fc7\u53bb\u9664\u4f7f\u653f\u7b56\u53d1\u751f\u91cd\u5927\u53d8\u5316\u7684\u6fc0\u52b1\uff0c\u88c1\u526a\u5145\u5f53\u7740\u6b63\u5219\u5316\u5668\u3002\u800c\u8d85\u53c2\u6570$\\epsilon$\u5219\u5bf9\u5e94\u4e8e\u65b0\u653f\u7b56\u4e0e\u65e7\u653f\u7b56\u53ef\u4ee5\u6709\u591a\u8fdc\uff0c\u540c\u65f6\u4ecd\u4f7f\u76ee\u6807\u53d7\u76ca\u3002","title":"\u8981\u95fb\u901f\u89c8"},{"location":"PPO/#_3","text":"PPO\u6309\u57fa\u4e8e\u7b56\u7565\u65b9\u5f0f\u8bad\u7ec3\u968f\u673a\u7b56\u7565\uff0c\u8fd9\u610f\u5473\u7740\u5b83\u4f1a\u6839\u636e\u5176\u968f\u673a\u7b56\u7565\u7684\u6700\u65b0\u7248\u672c\u901a\u8fc7\u91c7\u6837\u64cd\u4f5c\u6765\u8fdb\u884c\u63a2\u7d22\u3002\u52a8\u4f5c\u9009\u62e9\u7684\u968f\u673a\u6027\u53d6\u51b3\u4e8e\u521d\u59cb\u6761\u4ef6\u548c\u8bad\u7ec3\u6b65\u9aa4\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u4ee3\u7406\u9010\u6b65\u51cf\u5c11\u968f\u673a\u6027\uff0c\u56e0\u4e3a\u66f4\u65b0\u89c4\u5219\u9f13\u52b1\u5b83\u5229\u7528\u5df2\u7ecf\u53d1\u73b0\u7684\u5956\u52b1\u3002\u8fd9\u53ef\u80fd\u5bfc\u81f4\u7b56\u7565\u9677\u5165\u5c40\u90e8\u6700\u4f18\u72b6\u6001\u3002","title":"\u63a2\u7d22\u4e0e\u53d1\u73b0"},{"location":"PPO/#_4","text":"PPO-CLIP \u8f93\u5165\uff1a\u521d\u59cb\u7b56\u7565\u53c2\u6570$\\theta_0$,\u521d\u59cb\u503c\u529f\u80fd\u53c2\u6570$\\phi_0$ for k = 0, 1, 2, ... do \u5728\u73af\u5883\u4e2d\u901a\u8fc7\u6267\u884c\u7b56\u7565$\\pi_k=\\pi(\\theta_k)$\uff0c\u6536\u96c6\u4e00\u7ec4\u8f68\u8ff9$D_k={\\tau_i}$ \u7b97\u5956\u52b1$\\hat{R}_i$ \u57fa\u4e8e\u5f53\u524d\u7684value function $V_{\\phi k}$,\u8ba1\u7b97advantage estimates $\\hat{A}_t$ \u901a\u8fc7\u6700\u5927\u5316PPO-Clip\u76ee\u6807\uff0c\u66f4\u65b0\u7b56\u7565 $$ \\theta_{k+1}=arg\\ \\underset{\\theta}{max}\\frac{1}{|D_k|T}\\sum_{\\tau\\in D_k}\\sum^T_{t=0}min\\left(\\frac{\\pi_\\theta(\\alpha_t|s_t)}{\\pi_{\\theta_k}(\\alpha_t|s_t)}A^{\\pi\\theta_k}(s_t,\\alpha_t),g(\\epsilon,A^{\\pi\\theta_k}(s_t,\\alpha_t))\\right) $$ \u901a\u5e38\u901a\u8fc7\u4e0eAdam\u4e00\u8d77\u7684\u968f\u673a\u68af\u5ea6\u4e0a\u5347 \u901a\u8fc7\u5747\u65b9\u8bef\u5dee\u56de\u5f52\u62df\u5408value function $$ \\phi_{k+1}=arg\\ \\underset{\\phi}{min}\\frac{1}{|D_k|T}\\sum_{\\tau\\in D_k}\\sum^T_{t=0}(V_\\phi(s_t)-\\hat{R}_t)^2 $$ \u7279\u522b\u662f\u901a\u8fc7\u4e00\u4e9b\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5 end for","title":"\u4f2a\u4ee3\u7801"},{"location":"RL/","text":"what to learn in model-free RL There are two main approaches to representing and training agents with model-free RL: Policy Optimization Methods in this family represent a policy explicitly as $\\pi_\\theta(\\alpha,s)$.They optimize the parameters $\\theta$ either directly by gradient ascent on the performance objective $J(\\pi_\\theta)$ ,or indirectly, by maximizing local approximation of $J(\\pi_\\theta)$ ,This optimiation is almost always performed on-policy,which means that each update only uses data collected while acting according to the most recent version of the policy.Policy optimization also usually invovles learning an approximator $V_\\phi(s)$ for the on-policy value function $V^\\pi(s)$ ,which gets used in figuring out how to update the policy. A couple of examples of policy optimization methods are: A2C/A3C, which performs gradient ascent to directly maximize performance. A2C/A3C We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input. Deep RL algorithms based on experience replay have achieved unprecedented success in challenging domains such as Atari 2600.However experience replay has serval drawbacks : it uses more memory and computation per real interaction; and it requires off-policy learning algorithms that can update from data generated by an older policy. The best of the proposed methods, asynchronous advantage actor-critic(A3C), also mastered a variety of continuous motor control tasks as well as learned general strategies for exploring 3D mazes purely from visual inputs. Related Work The General Reinforcement Learning Architecture (Gorila) of performs asynchronous training of reinforcement learning agents in a distributed setting.In Gorila, each process contains an actor that acts in its own copy of the environment, a seperate replay memory, and a learner that samples data from the replay memory, and a learner that samples data from the replay memory and computes gradients of the DQN loss with respect to the policy parameters. The gradients are asynchronously sent to a central parameter server which updates a central copy of the model. The updated policy parameters are sent to the actor-learners at fixed intervals.By using 100 separate actor-learner processes and 30 parameter server instances, a total of 130 machines, Gorila was able to significantly outperform DQN over 49 Atari games. On many games Gorila reached the score achieved by DQN over 20 times faster than DQN. We also note that a similar way of parallelizing DQN was proposed by Chavez. In earlier work Li applied the Map Reduce framework to parallelizing batch reinforcement learning methods with linear function approximation.Parallelism was used to speed up large matrix operations but not to paralleize the collection of experience or stabilize learning. Grounds proposed a parallel vision of the Sarsa algorithm that uses multiple separate actor-learners to accelerate training .Each actor-learner learns separately and periodically sends updates to weights that have changed significantly to the other learners using peer-to-peer communication. T studied convergence properties of Q-learning in the asynchronous optimization setting. These results show that Q-learning is still guaranteed to converage when some of the information is outdated as long as outdated information is always eventually discarded and several other technical assumptions are satisfied. Even earlier, studied the related problem of distributed dynamic programming. Another related area of work is in evolutionary methods, which are often straightforward to parallelize by distributing fitness evaluations over multiple machines or threads.Such parallel evolutionary approaches have recently been applied to some visual reinforcement learning tasks.In one example, Koutnik evolved convolutional neural network controllers for the TORCS driving simulator by performing fitness evaluations on 8 CPU cores in parallel. Reinforcement Learning Background We consider the standard reinforcement learning setting where an agent interacts with an environment $\\varepsilon$ over a number of discrete time steps.At each time step $t$ ,the agent receives a state $s_t$ and selects an action $a_t$ from some set of possible actions $A$ according to its policy $\\pi$ ,where $\\pi$ is a mapping from states $s_t$ to actions $a_t$ . In return the agent receives the next state $s_{t+1}$ and receives a scalar reward $\\tau_t$ . The process continues until the agent reaches a terminal state after which the process restarts. The return $R_t=\\sum^{\\infty} {k=0}\\gamma^k\\tau {k+1}$ is the total accumulated return from time step $t$ with discount factor $\\gamma\\in(0,1]$ .The goal of the agent is to maximize the expected return from each state $s_t$. The action value $Q^\\pi(s,a)=\\mathbb{E}[R_t|s_t=s,a]$ is the expected return for selecting action $a$ in the state $s$ and following policy $\\pi$ . The optimal value function $Q^*(s,a)=max_\\pi Q^\\pi(s,a)$ gives the maximum action value for state $s$ and action $a$ achievable by any policy. Similarly, the value of the state $s$ and action $a$ achievable by any policy. Similarly, the value of state $s$ under policy $\\pi$ is defined as $V^\\pi(s)=\\mathbb{E}[R_t|s_t=s]$ and is simply the expected return for following policy $\\pi$ from state $s$. In value-based model-free reinforcement learning methods, the action value function is represented using a function approximator, such as a neural network. Let $Q(s,a;\\theta) $ be an approximate action-value function with parameters $\\theta$ . The updates to $\\theta$ can be derived from a variety of reinforcement learning algorithms.One example of such an algorithm is Q-learning, which aims to directly approximate the optimal action value function: $Q^*(s,a)\\approx Q(s,a,\\theta)$ . In one-step Q-learning, the parameters $\\theta$ of the action value function $Q(s,a,\\theta)$ are learned by iteratively minimizing a sequence of loss functions, where the $i$th loss function defined as $$ L_i(\\theta_i)=\\mathbb{E}\\left(\\tau+\\gamma\\underset{a'}{max}Q(s',a';\\theta_{i-1})-Q(s,a;\\theta_i\\right)^2 $$ where $s'$ is the state encountered after state $s$. We refer to the above method as one-step Q-learning because it updates the action value Q(s,a) toward the onestep return $\\tau+\\gamma\\underset{a'}{max}Q(s',a';\\theta)$.One drawback of using one-step methods is that obtaining a reward $\\tau$ only directly affects the value of the state action pair $s,a$ that led to the reward. The values of other state action pairs are affected only indirectly through the updated value $Q(s,a)$ . This can make the learning process slow since many updates are required the propagate a reward to the relevant preceding states and actions. One way of propagating rewards faster is by using n-step returns.In $n$-step Q-learning, $Q(s,a)$ is updated toward the $n$-step return defined as $\\tau_t+\\gamma\\tau_{t+1}+...+\\gamma^{n-1}\\tau_{t+n-1}+\\underset{a}{max}\\gamma^nQ(s_{t+n},a)$ . This results in a single reward $\\tau$ directly affecting the values of $n$ preceding state action pairs.This makes the process of propagating rewards to relevant state-action pairs potentially much more efficient. In construct to value-based methods, policy-based model-free methods directly parameterize the policy $\\pi(a|s;\\theta)$ and update the parameters $\\theta$ by performing, typically approximate, gradient ascent on $\\mathbb{E}[R_t]$ . One example of such a method is the REINFORCE family of algorithms due to Williams. Standard REINFORCE updates the policy parameters $\\theta$ in the direction $\\triangledown_\\theta log\\pi(a_t|s_t;\\theta)R_t$,which is an unbiased estimate of $\\triangledown_\\theta\\mathbb{E}[R_t]$. It is possible to reduce the variance of this estimate while keeping it unbiased by substracting a learned function of the state $b_t(s_t)$, known as a baseline, from the return. The resulting gradient is $\\triangledown_\\theta log\\pi(a_t|s_t;\\theta)(R_t-b_t(s_t)).$ A learned estimate of the value function is commonly used as the baseline $b_t(s_t)\\approx V^\\pi(s_t)$ leading to a much lower variance estimate of the policy gradient. When an aprroximate value function is used as the baseline, the quantity $R_t-b_t$ used to scale the policy gradient can be seen as an estimate of the advantage of action $a_t$ in state $s_t$, or $A(a_t,s_t)=Q(a_t,s_t)-V(s_t)$ ,because $R_t$ is an estimate of $Q^\\pi(a_t,s_t)$ and $b_t$ is an estimate of $V^\\pi(s_t)$. This approach can be viewed as an actor-critic architecture where the policy $\\pi$ is the actor and the baseline $b_t$ is the critic. Asynchronous RL Framework We now present multi-threaded asynchronous variants of one-step Sarsa, one-step Q-learning, n-step Q-learning, and advantage actor-critic. The aim in designing these methods was to find RL algorithms that can train deep neural network policies reliably and without large resource requirements. While the underlying RL methods are quite different , with actor-critic being an on-policy policy search method and Q-learning being an off-policy value-based method ,we use two main ideas to make all four algorithms practical given our design goal. First, we use asynchronous actor-learners , similarly to the Gorila framework , but instead of using sparate machines and a parameter server , we use multiple CPU threads on a single machine. Keeping the learners on a single machine removes the communication costs of sending gradients and parameters and enables us to use Hogwild! style updates for training. Second ,we make the observation that multiple actors-learners running in parallel are likely to be exploring different parts of the environment. asynchronous one-step Q-learning pseudocode for each actor-learner thread //Assume global shared \u03b8,$\\theta^-$, and counter $T=0$. Initialize thread step counter $t\\leftarrow 0$ Initialize target network weights $\\theta^-\\leftarrow\\theta$ Initialize network gradients $d\\theta\\leftarrow 0$ Get initial state $s$ repeat $\\$Take action","title":"RL"},{"location":"RL/#what-to-learn-in-model-free-rl","text":"There are two main approaches to representing and training agents with model-free RL:","title":"what to learn in model-free RL"},{"location":"RL/#policy-optimization","text":"Methods in this family represent a policy explicitly as $\\pi_\\theta(\\alpha,s)$.They optimize the parameters $\\theta$ either directly by gradient ascent on the performance objective $J(\\pi_\\theta)$ ,or indirectly, by maximizing local approximation of $J(\\pi_\\theta)$ ,This optimiation is almost always performed on-policy,which means that each update only uses data collected while acting according to the most recent version of the policy.Policy optimization also usually invovles learning an approximator $V_\\phi(s)$ for the on-policy value function $V^\\pi(s)$ ,which gets used in figuring out how to update the policy. A couple of examples of policy optimization methods are: A2C/A3C, which performs gradient ascent to directly maximize performance.","title":"Policy Optimization"},{"location":"RL/#a2ca3c","text":"We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input. Deep RL algorithms based on experience replay have achieved unprecedented success in challenging domains such as Atari 2600.However experience replay has serval drawbacks : it uses more memory and computation per real interaction; and it requires off-policy learning algorithms that can update from data generated by an older policy. The best of the proposed methods, asynchronous advantage actor-critic(A3C), also mastered a variety of continuous motor control tasks as well as learned general strategies for exploring 3D mazes purely from visual inputs.","title":"A2C/A3C"},{"location":"RL/#related-work","text":"The General Reinforcement Learning Architecture (Gorila) of performs asynchronous training of reinforcement learning agents in a distributed setting.In Gorila, each process contains an actor that acts in its own copy of the environment, a seperate replay memory, and a learner that samples data from the replay memory, and a learner that samples data from the replay memory and computes gradients of the DQN loss with respect to the policy parameters. The gradients are asynchronously sent to a central parameter server which updates a central copy of the model. The updated policy parameters are sent to the actor-learners at fixed intervals.By using 100 separate actor-learner processes and 30 parameter server instances, a total of 130 machines, Gorila was able to significantly outperform DQN over 49 Atari games. On many games Gorila reached the score achieved by DQN over 20 times faster than DQN. We also note that a similar way of parallelizing DQN was proposed by Chavez. In earlier work Li applied the Map Reduce framework to parallelizing batch reinforcement learning methods with linear function approximation.Parallelism was used to speed up large matrix operations but not to paralleize the collection of experience or stabilize learning. Grounds proposed a parallel vision of the Sarsa algorithm that uses multiple separate actor-learners to accelerate training .Each actor-learner learns separately and periodically sends updates to weights that have changed significantly to the other learners using peer-to-peer communication. T studied convergence properties of Q-learning in the asynchronous optimization setting. These results show that Q-learning is still guaranteed to converage when some of the information is outdated as long as outdated information is always eventually discarded and several other technical assumptions are satisfied. Even earlier, studied the related problem of distributed dynamic programming. Another related area of work is in evolutionary methods, which are often straightforward to parallelize by distributing fitness evaluations over multiple machines or threads.Such parallel evolutionary approaches have recently been applied to some visual reinforcement learning tasks.In one example, Koutnik evolved convolutional neural network controllers for the TORCS driving simulator by performing fitness evaluations on 8 CPU cores in parallel.","title":"Related Work"},{"location":"RL/#reinforcement-learning-background","text":"We consider the standard reinforcement learning setting where an agent interacts with an environment $\\varepsilon$ over a number of discrete time steps.At each time step $t$ ,the agent receives a state $s_t$ and selects an action $a_t$ from some set of possible actions $A$ according to its policy $\\pi$ ,where $\\pi$ is a mapping from states $s_t$ to actions $a_t$ . In return the agent receives the next state $s_{t+1}$ and receives a scalar reward $\\tau_t$ . The process continues until the agent reaches a terminal state after which the process restarts. The return $R_t=\\sum^{\\infty} {k=0}\\gamma^k\\tau {k+1}$ is the total accumulated return from time step $t$ with discount factor $\\gamma\\in(0,1]$ .The goal of the agent is to maximize the expected return from each state $s_t$. The action value $Q^\\pi(s,a)=\\mathbb{E}[R_t|s_t=s,a]$ is the expected return for selecting action $a$ in the state $s$ and following policy $\\pi$ . The optimal value function $Q^*(s,a)=max_\\pi Q^\\pi(s,a)$ gives the maximum action value for state $s$ and action $a$ achievable by any policy. Similarly, the value of the state $s$ and action $a$ achievable by any policy. Similarly, the value of state $s$ under policy $\\pi$ is defined as $V^\\pi(s)=\\mathbb{E}[R_t|s_t=s]$ and is simply the expected return for following policy $\\pi$ from state $s$. In value-based model-free reinforcement learning methods, the action value function is represented using a function approximator, such as a neural network. Let $Q(s,a;\\theta) $ be an approximate action-value function with parameters $\\theta$ . The updates to $\\theta$ can be derived from a variety of reinforcement learning algorithms.One example of such an algorithm is Q-learning, which aims to directly approximate the optimal action value function: $Q^*(s,a)\\approx Q(s,a,\\theta)$ . In one-step Q-learning, the parameters $\\theta$ of the action value function $Q(s,a,\\theta)$ are learned by iteratively minimizing a sequence of loss functions, where the $i$th loss function defined as $$ L_i(\\theta_i)=\\mathbb{E}\\left(\\tau+\\gamma\\underset{a'}{max}Q(s',a';\\theta_{i-1})-Q(s,a;\\theta_i\\right)^2 $$ where $s'$ is the state encountered after state $s$. We refer to the above method as one-step Q-learning because it updates the action value Q(s,a) toward the onestep return $\\tau+\\gamma\\underset{a'}{max}Q(s',a';\\theta)$.One drawback of using one-step methods is that obtaining a reward $\\tau$ only directly affects the value of the state action pair $s,a$ that led to the reward. The values of other state action pairs are affected only indirectly through the updated value $Q(s,a)$ . This can make the learning process slow since many updates are required the propagate a reward to the relevant preceding states and actions. One way of propagating rewards faster is by using n-step returns.In $n$-step Q-learning, $Q(s,a)$ is updated toward the $n$-step return defined as $\\tau_t+\\gamma\\tau_{t+1}+...+\\gamma^{n-1}\\tau_{t+n-1}+\\underset{a}{max}\\gamma^nQ(s_{t+n},a)$ . This results in a single reward $\\tau$ directly affecting the values of $n$ preceding state action pairs.This makes the process of propagating rewards to relevant state-action pairs potentially much more efficient. In construct to value-based methods, policy-based model-free methods directly parameterize the policy $\\pi(a|s;\\theta)$ and update the parameters $\\theta$ by performing, typically approximate, gradient ascent on $\\mathbb{E}[R_t]$ . One example of such a method is the REINFORCE family of algorithms due to Williams. Standard REINFORCE updates the policy parameters $\\theta$ in the direction $\\triangledown_\\theta log\\pi(a_t|s_t;\\theta)R_t$,which is an unbiased estimate of $\\triangledown_\\theta\\mathbb{E}[R_t]$. It is possible to reduce the variance of this estimate while keeping it unbiased by substracting a learned function of the state $b_t(s_t)$, known as a baseline, from the return. The resulting gradient is $\\triangledown_\\theta log\\pi(a_t|s_t;\\theta)(R_t-b_t(s_t)).$ A learned estimate of the value function is commonly used as the baseline $b_t(s_t)\\approx V^\\pi(s_t)$ leading to a much lower variance estimate of the policy gradient. When an aprroximate value function is used as the baseline, the quantity $R_t-b_t$ used to scale the policy gradient can be seen as an estimate of the advantage of action $a_t$ in state $s_t$, or $A(a_t,s_t)=Q(a_t,s_t)-V(s_t)$ ,because $R_t$ is an estimate of $Q^\\pi(a_t,s_t)$ and $b_t$ is an estimate of $V^\\pi(s_t)$. This approach can be viewed as an actor-critic architecture where the policy $\\pi$ is the actor and the baseline $b_t$ is the critic.","title":"Reinforcement Learning Background"},{"location":"RL/#asynchronous-rl-framework","text":"We now present multi-threaded asynchronous variants of one-step Sarsa, one-step Q-learning, n-step Q-learning, and advantage actor-critic. The aim in designing these methods was to find RL algorithms that can train deep neural network policies reliably and without large resource requirements. While the underlying RL methods are quite different , with actor-critic being an on-policy policy search method and Q-learning being an off-policy value-based method ,we use two main ideas to make all four algorithms practical given our design goal. First, we use asynchronous actor-learners , similarly to the Gorila framework , but instead of using sparate machines and a parameter server , we use multiple CPU threads on a single machine. Keeping the learners on a single machine removes the communication costs of sending gradients and parameters and enables us to use Hogwild! style updates for training. Second ,we make the observation that multiple actors-learners running in parallel are likely to be exploring different parts of the environment. asynchronous one-step Q-learning pseudocode for each actor-learner thread //Assume global shared \u03b8,$\\theta^-$, and counter $T=0$. Initialize thread step counter $t\\leftarrow 0$ Initialize target network weights $\\theta^-\\leftarrow\\theta$ Initialize network gradients $d\\theta\\leftarrow 0$ Get initial state $s$ repeat $\\$Take action","title":"Asynchronous RL Framework"},{"location":"Ros/","text":"Ros create msg \u5728node\u8282\u70b9\u6587\u4ef6\u5939\u4e0b\u521b\u5efamsg\u6587\u4ef6\u5939 \u5728msg\u6587\u4ef6\u5939\u4e0b\u521b\u5efa **.msg\u6587\u4ef6 detect.msg bool begin string first_name string last_name uint8 age uint32 score int64 stateLabel int64 CIPV int64 trackID int64 trackFrameNum int64 obstacleType float32 real3DRightX float32 real3DLeftX float32 width float32 real3DLowY float32 real3DUpY float32 height float32 HMW float32 fuzzyCollisionTimeZ float32 longitudinalZ float32 speedOfLongitudinal float32 lateralX float32 speedOfLateral float32 currentSpeed detect_array.msg dectect [] package.xml <build_depend>message_generation</build_depend> <exec_depend>message_runtime</exec_depend> CMakelists.txt find_package(catkin REQUIRED COMPONENTS roscpp rospy std_msgs message_generation ) catkin_package( ... CATKIN_DEPENDS message_runtime ... ...) add_message_files( FILES detect.msg detect_array.msg ) generate_messages( DEPENDENCIES std_msgs ) \u5305\u542b\u5934\u6587\u4ef6 #include \"smart_eye/detect.h\" #include \"smart_eye/detect_array.h\" msg_pub = nh_.advertise<smart_eye::detect_array>(\"detect_array\", 10); smart_eye::detect msg; smart_eye::detect_array msg_array; msg.stateLabel = stateLabel; msg.CIPV = CIPV; msg.trackID = trackID; msg.trackFrameNum = trackFrameNum; msg.obstacleType = obstacleType; msg.real3DRightX = real3DRightX; msg.real3DLeftX = real3DLeftX; msg.width = width; msg.real3DLowY = real3DLowY; msg.real3DUpY = real3DUpY; msg.height = height; msg.HMW = HMW; msg.fuzzyCollisionTimeZ = fuzzyCollisionTimeZ; msg.longitudinalZ = longitudinalZ; msg.speedOfLongitudinal = speedOfLongitudinal; msg.lateralX = lateralX; msg.speedOfLateral = speedOfLateral; msg.currentSpeed = currentSpeed; msg_array.push(msg); msg_pub.publish(msg_array); \u5f15\u7528\u5176\u4ed6\u5305\u7684\uff4d\uff53\uff47 \u9996\u5148cmakelists.txt find_package(catkin REQUIRED COMPONENTS message_generation pcl_ros roscpp rospy std_msgs wjj ) catkin_package( INCLUDE_DIRS include CATKIN_DEPENDS message_runtime pcl_ros roscpp rospy std_msgs wjj ) add_dependencies(teaching ${catkin_EXPORTED_TARGETS} wjj_gencpp) \u518d\u8005package.xml <build_depend>wjj</build_depend> <exec_depend>wjj</exec_depend> \u6700\u540e\u4f7f\u7528 #include <wjj/SaeJ1939.h> sudo rosdep init &rosdep update\u5931\u8d25 \u8be5\u89e3\u51b3\u65b9\u6848\u662f\u9488\u5bf9\u7531\u4e8e\u4ee5\u4e0b\u4e24\u4e2a\u65e0\u6cd5\u6b63\u5e38\u8bbf\u95ee\uff0c\u4f46\u53ef\u4ee5ping\u901a\uff0c\u4e8e\u662f\u4fee\u6539hosts\u6587\u4ef6\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e24\u4e2a\u7f51\u5740\u7684IP\u5730\u5740\u5b9e\u73b0\u8bbf\u95ee\u3002 'sudo gedit /etc/hosts' \u6dfb\u52a0 199.232.28.133 raw.githubusercontent.com 151.101.228.133 raw.github.com \u4fee\u6539\u5b8c\u6210\u540e\uff0c\u5728\u7ec8\u7aef\u6267\u884c sudo rosdep init redep update vscode python ros debug \u9996\u5148 catkin_make -DCMAKE_BUILD_TYPE=DEBUG \u5176\u6b21\u70b9\u51fbdebug\u6309\u626d,\u518d\u70b9\u51fb`create a launch.json file \u9009\u62e9 Show \u9009\u62e9 Add Configuration.. \u518d\u6dfb\u52a0 Ros Attach launch.json { \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"ROS: Attach\", \"type\": \"ros\", \"request\": \"attach\" } ] } \uff44ebug\u65f6\u9047\u5230\u8def\u5f84\u95ee\u9898,\u4f8b\u5982rosmsg\u8def\u5f84\uff0c\u6700\u597d\u5728\u6587\u4ef6\u5c5e\u6027\u91cc\u590d\u5236\u8def\u5f84\uff0c\u4e0d\u7136\u5bb9\u6613\u51fa\u9519 import sys sys.path.append(\"/home/pmjd/Downloads/catkin_ws/devel/lib/python2.7/dist-packages\") Error: package 'teleop_twist_keyboard' not found You need to download the teleop_twist_keyboard from the github to your ~/catkin_ws/src folder. Steps: 1) cd ~/catkin_ws/src 2) git clone https://github.com/ros-teleop/teleop_twist_keyboard Spawn service failed. Exiting. export ROS_MASTER_URI=http://promote-OMEN-by-HP-Laptop-17-cb1xxx:11311/ createQuaternionFromRPY static geometry_msgs::Quaternion createQuaternionFromRPY(double roll, double pitch, double yaw) { geometry_msgs::Quaternion q; double t0 = cos(yaw * 0.5); double t1 = sin(yaw * 0.5); double t2 = cos(roll * 0.5); double t3 = sin(roll * 0.5); double t4 = cos(pitch * 0.5); double t5 = sin(pitch * 0.5); q.w = t0 * t2 * t4 + t1 * t3 * t5; q.x = t0 * t3 * t4 - t1 * t2 * t5; q.y = t0 * t2 * t5 + t1 * t3 * t4; q.z = t1 * t2 * t4 - t0 * t3 * t5; return q; } launch \u542f\u52a8 rviz <launch> <node type=\"rviz\" name=\"rviz\" pkg=\"rviz\" args=\"-d $(find package_name)/rviz/config_file.rviz\" /> </launch> rosdep\u5931\u8d25 \u9996\u5148 sudo rosdep init \u8fd9\u4e00\u6b65\u4f1a\u5728 /etc/ros/rosdep/sources.list.d/ \u76ee\u5f55\u4e0b\u65b0\u5efa 20-default.list # os-specific listings first yaml https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/osx-homebrew.yaml osx # generic yaml https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/base.yaml yaml https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/python.yaml yaml https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/ruby.yaml gbpdistro https://raw.githubusercontent.com/ros/rosdistro/master/releases/fuerte.yaml fuerte # newer distributions (Groovy, Hydro, ...) must not be listed anymore, they are being fetched from the rosdistro index.yaml instead \u6211\u4eec\u9700\u8981\u4f9d\u6b21\u4e0b\u8f7d osx-homebrew.yaml \u7b49\u8fd9\u51e0\u4e2a yaml \u6587\u4ef6, \u4e0b\u8f7d\u5de5\u5177 , \u5b58\u653e\u5728/home/promote/Downloads\u76ee\u5f55\u4e0b \u7136\u540e\u66f4\u6539 20-default.list \u4e3a # os-specific listings first yaml file:///home/promote/Downloads/2021-04-01-14-01-21-master-osx-homebrew.yaml osx # generic yaml file:///home/promote/Downloads/2021-04-01-14-02-25-master-base.yaml yaml file:///home/promote/Downloads/2021-04-01-14-05-35-master-python.yaml yaml file:///home/promote/Downloads/2021-04-01-14-06-50-master-ruby.yaml gbpdistro file:///home/promote/Downloads/2021-04-01-14-07-41-master-fuerte.yaml fuerte # newer distributions (Groovy, Hydro, ...) must not be listed anymore, they are being fetched from the rosdistro index.yaml instead \u8fd8\u9700\u8981\u4f7f\u7528 mate-search-tool \u5728 /usr/lib/ \u76ee\u5f55\u4e0b\u627e\u5230\u5305\u542b DEFAULT_INDEX_URL \u7684 py \u6587\u4ef6\u3002 /usr/lib/python2.7/dist-packages/rosdistro/__init__.py \u627e\u5230\u4ee3\u7801\u884c DEFAULT_INDEX_URL = 'https://raw.githubusercontent.com/ros/rosdistro/master/index-v4.yaml' \u540c\u6837\u6211\u4eec\u4e0b\u8f7d index-v4.yaml \u6587\u4ef6\u81f3 /home/promote/Downloads \uff0c\u628a\u6b64\u884c\u4ee3\u7801\u6539\u4e3a # DEFAULT_INDEX_URL = 'https://raw.githubusercontent.com/ros/rosdistro/master/index-v4.yaml' DEFAULT_INDEX_URL = 'file:/home/promote/Downloads/2021-04-01-14-51-42-rosdistro-index-v4.yaml' \u8fd9\u65f6\u518d\u8fd0\u884c rosdep update \u4f1a\u63d0\u793a No such file or directory: '/home/promote/Downloads/dashing/distribution.yaml' \u6211\u4eec\u9700\u8981\u4e0b\u8f7d https://raw.githubusercontent.com/ros/rosdistro/master/dashing/distribution.yaml \u5728 /home/promote/Downloads \u76ee\u5f55\u4e0b\u65b0\u5efa\u6587\u4ef6\u5939 dashing \uff0c\u5e76\u628a\u4e0b\u8f7d\u7684 yaml \u6587\u4ef6\u653e\u5165 dashing \u6587\u4ef6\u5939\u4e0b\u91cd\u547d\u540d\u4e3a distribution.yaml \u91cd\u590d\u4e0a\u4e00\u6b65\uff0c\u4f9d\u6b21\u4e0b\u8f7d\u5b8c dashing, kinetic, melodic, rolling, noetic, foxy \u7b49 \u518d\u8fd0\u884c rosdep update \u5c31\u6210\u529f\u4e86 ros node \u6253\u5305 \u5b89\u88c5\u4f9d\u8d56 Install bloom : sudo apt-get install python-bloom or (recommended) sudo pip install -U bloom Install fakeroot: sudo apt-get install fakeroot \u51c6\u5907 To make a debian folder structure from the ROS package you must cd into the package to be in the same folder where package.xml file is. \u751f\u6210debian\u5305 bloom-generate rosdebian --os-name ubuntu --os-version trusty --ros-distro indigo ros\u603b\u662f\u94fe\u63a5anaconda\u4e0b\u7684\u5e93\u6587\u4ef6 export LD_LIBRARY_PATH=\"\" Use parent frame_id Display \u5728launch\u6587\u4ef6\u91cc\u6dfb\u52a0 <node pkg=\"tf\" type=\"static_transform_publisher\" name=\"base_to_map\" args=\"1000 0 0 0 0 0 /base_link /map 50\" > <node pkg=\"ros2qt\" type=\"ros2qt\" name=\"ros2qt\" > <rosparam> use_sim_time: true </rosparam> </node> <node pkg=\"ros2qt\" type=\"myclock.py\" name =\"myclock\"/> myclock.py #!/usr/bin/python import rospy from rosgraph_msgs.msg import Clock import time def simtime_talker(): rospy.set_param('use_sim_time',False) pub1 = rospy.Publisher('clock',Clock, queue_size=10) rospy.init_node('talker', anonymous=True) rate = rospy.Rate(10) # 10hz sim_speed_multiplier = 10 sim_clock = Clock() zero_time = rospy.get_time() while not rospy.is_shutdown(): sim_clock.clock = rospy.Time.from_sec(sim_speed_multiplier*(rospy.get_time() - zero_time)) rospy.loginfo(sim_clock) pub1.publish(sim_clock) rate.sleep() if __name__ == '__main__': try: simtime_talker() except rospy.ROSInterruptException: pass \u5728\u7ec8\u7aef\u5efa\u7acb\u7236\u5b50\u5173\u7cfb rosrun tf stat_transform_publisher 0 0 1 0 0 0 1 base_link rslidar 50 \u5373\u53ef\u5728rviz\u91cc\u8bbe\u7f6e fixed frame \u4e3a base_link ,\u89c2\u770b\u70b9\u4e91\u6570\u636e \u5982\u4f55\u53d1\u5e03tf\u8f6c\u6362\u5173\u7cfb Use static_transform_publisher -- this one is a ready to use node, that you can run from command line. rosrun tf stat_transform_publisher 0 0 1 0 0 0 world rslidar 50 Use tf::TransformBroadcaster -- to create transforms in a code. void QNode::biaodingCallback(const PointCloud::ConstPtr& msg) { frame_name = msg->header.frame_id; static tf::TransformBroadcaster br; tf::Transform transform; transform.setOrigin( tf::Vector3(x_pianyi, y_pianyi, z_pianyi) ); tf::Quaternion q; q.setRPY(x_xuanzhuan, y_xuanzhuan, z_xuanzhuan); transform.setRotation(q); br.sendTransform(tf::StampedTransform(transform, ros::Time::now(), \"base_link\", frame_name)); } roslaunch \u5305\u542b roslaunch\u6587\u4ef6 <include file=\"$(find robot_description)/launch/urdf_test.launch\" /> roslaunch \u5305\u542byaml \u6587\u4ef6 <rosparam file=\"$(find ldtest)/config/para.yaml\" command=\"load\" /> para.yaml x_offset: -1.45 y_offset: 0.56 z_offset: 2.95 x_jiaodu: -1.5 y_jiaodu: 44.3 z_jiaodu: 4.0 stone2car_x: -1.25 stone2car_y: 5.35 stone2car_z: -1.0 dipan_init_jiaodu: 91.4 remote: true roslaunch py node <node name=\"LidarTool\" pkg=\"ldtest\" type=\"LidarTool.py\" required=\"false\" output=\"screen\" />","title":"Ros"},{"location":"Ros/#ros-create-msg","text":"\u5728node\u8282\u70b9\u6587\u4ef6\u5939\u4e0b\u521b\u5efamsg\u6587\u4ef6\u5939 \u5728msg\u6587\u4ef6\u5939\u4e0b\u521b\u5efa **.msg\u6587\u4ef6 detect.msg bool begin string first_name string last_name uint8 age uint32 score int64 stateLabel int64 CIPV int64 trackID int64 trackFrameNum int64 obstacleType float32 real3DRightX float32 real3DLeftX float32 width float32 real3DLowY float32 real3DUpY float32 height float32 HMW float32 fuzzyCollisionTimeZ float32 longitudinalZ float32 speedOfLongitudinal float32 lateralX float32 speedOfLateral float32 currentSpeed detect_array.msg dectect [] package.xml <build_depend>message_generation</build_depend> <exec_depend>message_runtime</exec_depend> CMakelists.txt find_package(catkin REQUIRED COMPONENTS roscpp rospy std_msgs message_generation ) catkin_package( ... CATKIN_DEPENDS message_runtime ... ...) add_message_files( FILES detect.msg detect_array.msg ) generate_messages( DEPENDENCIES std_msgs ) \u5305\u542b\u5934\u6587\u4ef6 #include \"smart_eye/detect.h\" #include \"smart_eye/detect_array.h\" msg_pub = nh_.advertise<smart_eye::detect_array>(\"detect_array\", 10); smart_eye::detect msg; smart_eye::detect_array msg_array; msg.stateLabel = stateLabel; msg.CIPV = CIPV; msg.trackID = trackID; msg.trackFrameNum = trackFrameNum; msg.obstacleType = obstacleType; msg.real3DRightX = real3DRightX; msg.real3DLeftX = real3DLeftX; msg.width = width; msg.real3DLowY = real3DLowY; msg.real3DUpY = real3DUpY; msg.height = height; msg.HMW = HMW; msg.fuzzyCollisionTimeZ = fuzzyCollisionTimeZ; msg.longitudinalZ = longitudinalZ; msg.speedOfLongitudinal = speedOfLongitudinal; msg.lateralX = lateralX; msg.speedOfLateral = speedOfLateral; msg.currentSpeed = currentSpeed; msg_array.push(msg); msg_pub.publish(msg_array);","title":"Ros create msg"},{"location":"Ros/#msg","text":"\u9996\u5148cmakelists.txt find_package(catkin REQUIRED COMPONENTS message_generation pcl_ros roscpp rospy std_msgs wjj ) catkin_package( INCLUDE_DIRS include CATKIN_DEPENDS message_runtime pcl_ros roscpp rospy std_msgs wjj ) add_dependencies(teaching ${catkin_EXPORTED_TARGETS} wjj_gencpp) \u518d\u8005package.xml <build_depend>wjj</build_depend> <exec_depend>wjj</exec_depend> \u6700\u540e\u4f7f\u7528 #include <wjj/SaeJ1939.h>","title":"\u5f15\u7528\u5176\u4ed6\u5305\u7684\uff4d\uff53\uff47"},{"location":"Ros/#sudo-rosdep-init-rosdep-update","text":"\u8be5\u89e3\u51b3\u65b9\u6848\u662f\u9488\u5bf9\u7531\u4e8e\u4ee5\u4e0b\u4e24\u4e2a\u65e0\u6cd5\u6b63\u5e38\u8bbf\u95ee\uff0c\u4f46\u53ef\u4ee5ping\u901a\uff0c\u4e8e\u662f\u4fee\u6539hosts\u6587\u4ef6\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e24\u4e2a\u7f51\u5740\u7684IP\u5730\u5740\u5b9e\u73b0\u8bbf\u95ee\u3002 'sudo gedit /etc/hosts' \u6dfb\u52a0 199.232.28.133 raw.githubusercontent.com 151.101.228.133 raw.github.com \u4fee\u6539\u5b8c\u6210\u540e\uff0c\u5728\u7ec8\u7aef\u6267\u884c sudo rosdep init redep update","title":"sudo rosdep init &amp;rosdep update\u5931\u8d25"},{"location":"Ros/#vscode-python-ros-debug","text":"\u9996\u5148 catkin_make -DCMAKE_BUILD_TYPE=DEBUG \u5176\u6b21\u70b9\u51fbdebug\u6309\u626d,\u518d\u70b9\u51fb`create a launch.json file \u9009\u62e9 Show \u9009\u62e9 Add Configuration.. \u518d\u6dfb\u52a0 Ros Attach launch.json { \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"ROS: Attach\", \"type\": \"ros\", \"request\": \"attach\" } ] } \uff44ebug\u65f6\u9047\u5230\u8def\u5f84\u95ee\u9898,\u4f8b\u5982rosmsg\u8def\u5f84\uff0c\u6700\u597d\u5728\u6587\u4ef6\u5c5e\u6027\u91cc\u590d\u5236\u8def\u5f84\uff0c\u4e0d\u7136\u5bb9\u6613\u51fa\u9519 import sys sys.path.append(\"/home/pmjd/Downloads/catkin_ws/devel/lib/python2.7/dist-packages\")","title":"vscode python ros debug"},{"location":"Ros/#error-package-teleop_twist_keyboard-not-found","text":"You need to download the teleop_twist_keyboard from the github to your ~/catkin_ws/src folder. Steps: 1) cd ~/catkin_ws/src 2) git clone https://github.com/ros-teleop/teleop_twist_keyboard","title":"Error: package 'teleop_twist_keyboard' not found"},{"location":"Ros/#spawn-service-failed-exiting","text":"export ROS_MASTER_URI=http://promote-OMEN-by-HP-Laptop-17-cb1xxx:11311/","title":"Spawn service failed. Exiting."},{"location":"Ros/#createquaternionfromrpy","text":"static geometry_msgs::Quaternion createQuaternionFromRPY(double roll, double pitch, double yaw) { geometry_msgs::Quaternion q; double t0 = cos(yaw * 0.5); double t1 = sin(yaw * 0.5); double t2 = cos(roll * 0.5); double t3 = sin(roll * 0.5); double t4 = cos(pitch * 0.5); double t5 = sin(pitch * 0.5); q.w = t0 * t2 * t4 + t1 * t3 * t5; q.x = t0 * t3 * t4 - t1 * t2 * t5; q.y = t0 * t2 * t5 + t1 * t3 * t4; q.z = t1 * t2 * t4 - t0 * t3 * t5; return q; }","title":"createQuaternionFromRPY"},{"location":"Ros/#launch-rviz","text":"<launch> <node type=\"rviz\" name=\"rviz\" pkg=\"rviz\" args=\"-d $(find package_name)/rviz/config_file.rviz\" /> </launch>","title":"launch \u542f\u52a8 rviz"},{"location":"Ros/#rosdep","text":"\u9996\u5148 sudo rosdep init \u8fd9\u4e00\u6b65\u4f1a\u5728 /etc/ros/rosdep/sources.list.d/ \u76ee\u5f55\u4e0b\u65b0\u5efa 20-default.list # os-specific listings first yaml https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/osx-homebrew.yaml osx # generic yaml https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/base.yaml yaml https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/python.yaml yaml https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/ruby.yaml gbpdistro https://raw.githubusercontent.com/ros/rosdistro/master/releases/fuerte.yaml fuerte # newer distributions (Groovy, Hydro, ...) must not be listed anymore, they are being fetched from the rosdistro index.yaml instead \u6211\u4eec\u9700\u8981\u4f9d\u6b21\u4e0b\u8f7d osx-homebrew.yaml \u7b49\u8fd9\u51e0\u4e2a yaml \u6587\u4ef6, \u4e0b\u8f7d\u5de5\u5177 , \u5b58\u653e\u5728/home/promote/Downloads\u76ee\u5f55\u4e0b \u7136\u540e\u66f4\u6539 20-default.list \u4e3a # os-specific listings first yaml file:///home/promote/Downloads/2021-04-01-14-01-21-master-osx-homebrew.yaml osx # generic yaml file:///home/promote/Downloads/2021-04-01-14-02-25-master-base.yaml yaml file:///home/promote/Downloads/2021-04-01-14-05-35-master-python.yaml yaml file:///home/promote/Downloads/2021-04-01-14-06-50-master-ruby.yaml gbpdistro file:///home/promote/Downloads/2021-04-01-14-07-41-master-fuerte.yaml fuerte # newer distributions (Groovy, Hydro, ...) must not be listed anymore, they are being fetched from the rosdistro index.yaml instead \u8fd8\u9700\u8981\u4f7f\u7528 mate-search-tool \u5728 /usr/lib/ \u76ee\u5f55\u4e0b\u627e\u5230\u5305\u542b DEFAULT_INDEX_URL \u7684 py \u6587\u4ef6\u3002 /usr/lib/python2.7/dist-packages/rosdistro/__init__.py \u627e\u5230\u4ee3\u7801\u884c DEFAULT_INDEX_URL = 'https://raw.githubusercontent.com/ros/rosdistro/master/index-v4.yaml' \u540c\u6837\u6211\u4eec\u4e0b\u8f7d index-v4.yaml \u6587\u4ef6\u81f3 /home/promote/Downloads \uff0c\u628a\u6b64\u884c\u4ee3\u7801\u6539\u4e3a # DEFAULT_INDEX_URL = 'https://raw.githubusercontent.com/ros/rosdistro/master/index-v4.yaml' DEFAULT_INDEX_URL = 'file:/home/promote/Downloads/2021-04-01-14-51-42-rosdistro-index-v4.yaml' \u8fd9\u65f6\u518d\u8fd0\u884c rosdep update \u4f1a\u63d0\u793a No such file or directory: '/home/promote/Downloads/dashing/distribution.yaml' \u6211\u4eec\u9700\u8981\u4e0b\u8f7d https://raw.githubusercontent.com/ros/rosdistro/master/dashing/distribution.yaml \u5728 /home/promote/Downloads \u76ee\u5f55\u4e0b\u65b0\u5efa\u6587\u4ef6\u5939 dashing \uff0c\u5e76\u628a\u4e0b\u8f7d\u7684 yaml \u6587\u4ef6\u653e\u5165 dashing \u6587\u4ef6\u5939\u4e0b\u91cd\u547d\u540d\u4e3a distribution.yaml \u91cd\u590d\u4e0a\u4e00\u6b65\uff0c\u4f9d\u6b21\u4e0b\u8f7d\u5b8c dashing, kinetic, melodic, rolling, noetic, foxy \u7b49 \u518d\u8fd0\u884c rosdep update \u5c31\u6210\u529f\u4e86","title":"rosdep\u5931\u8d25"},{"location":"Ros/#ros-node","text":"","title":"ros node \u6253\u5305"},{"location":"Ros/#_1","text":"Install bloom : sudo apt-get install python-bloom or (recommended) sudo pip install -U bloom Install fakeroot: sudo apt-get install fakeroot","title":"\u5b89\u88c5\u4f9d\u8d56"},{"location":"Ros/#_2","text":"To make a debian folder structure from the ROS package you must cd into the package to be in the same folder where package.xml file is.","title":"\u51c6\u5907"},{"location":"Ros/#debian","text":"bloom-generate rosdebian --os-name ubuntu --os-version trusty --ros-distro indigo","title":"\u751f\u6210debian\u5305"},{"location":"Ros/#rosanaconda","text":"export LD_LIBRARY_PATH=\"\"","title":"ros\u603b\u662f\u94fe\u63a5anaconda\u4e0b\u7684\u5e93\u6587\u4ef6"},{"location":"Ros/#use-parent-frame_id-display","text":"\u5728launch\u6587\u4ef6\u91cc\u6dfb\u52a0 <node pkg=\"tf\" type=\"static_transform_publisher\" name=\"base_to_map\" args=\"1000 0 0 0 0 0 /base_link /map 50\" > <node pkg=\"ros2qt\" type=\"ros2qt\" name=\"ros2qt\" > <rosparam> use_sim_time: true </rosparam> </node> <node pkg=\"ros2qt\" type=\"myclock.py\" name =\"myclock\"/> myclock.py #!/usr/bin/python import rospy from rosgraph_msgs.msg import Clock import time def simtime_talker(): rospy.set_param('use_sim_time',False) pub1 = rospy.Publisher('clock',Clock, queue_size=10) rospy.init_node('talker', anonymous=True) rate = rospy.Rate(10) # 10hz sim_speed_multiplier = 10 sim_clock = Clock() zero_time = rospy.get_time() while not rospy.is_shutdown(): sim_clock.clock = rospy.Time.from_sec(sim_speed_multiplier*(rospy.get_time() - zero_time)) rospy.loginfo(sim_clock) pub1.publish(sim_clock) rate.sleep() if __name__ == '__main__': try: simtime_talker() except rospy.ROSInterruptException: pass \u5728\u7ec8\u7aef\u5efa\u7acb\u7236\u5b50\u5173\u7cfb rosrun tf stat_transform_publisher 0 0 1 0 0 0 1 base_link rslidar 50 \u5373\u53ef\u5728rviz\u91cc\u8bbe\u7f6e fixed frame \u4e3a base_link ,\u89c2\u770b\u70b9\u4e91\u6570\u636e","title":"Use parent frame_id Display"},{"location":"Ros/#tf","text":"Use static_transform_publisher -- this one is a ready to use node, that you can run from command line. rosrun tf stat_transform_publisher 0 0 1 0 0 0 world rslidar 50 Use tf::TransformBroadcaster -- to create transforms in a code. void QNode::biaodingCallback(const PointCloud::ConstPtr& msg) { frame_name = msg->header.frame_id; static tf::TransformBroadcaster br; tf::Transform transform; transform.setOrigin( tf::Vector3(x_pianyi, y_pianyi, z_pianyi) ); tf::Quaternion q; q.setRPY(x_xuanzhuan, y_xuanzhuan, z_xuanzhuan); transform.setRotation(q); br.sendTransform(tf::StampedTransform(transform, ros::Time::now(), \"base_link\", frame_name)); }","title":"\u5982\u4f55\u53d1\u5e03tf\u8f6c\u6362\u5173\u7cfb"},{"location":"Ros/#roslaunch-roslaunch","text":"<include file=\"$(find robot_description)/launch/urdf_test.launch\" />","title":"roslaunch \u5305\u542b roslaunch\u6587\u4ef6"},{"location":"Ros/#roslaunch-yaml","text":"<rosparam file=\"$(find ldtest)/config/para.yaml\" command=\"load\" /> para.yaml x_offset: -1.45 y_offset: 0.56 z_offset: 2.95 x_jiaodu: -1.5 y_jiaodu: 44.3 z_jiaodu: 4.0 stone2car_x: -1.25 stone2car_y: 5.35 stone2car_z: -1.0 dipan_init_jiaodu: 91.4 remote: true","title":"roslaunch \u5305\u542byaml \u6587\u4ef6"},{"location":"Ros/#roslaunch-py-node","text":"<node name=\"LidarTool\" pkg=\"ldtest\" type=\"LidarTool.py\" required=\"false\" output=\"screen\" />","title":"roslaunch py node"},{"location":"Ros%E5%BB%BA%E6%A8%A1/","text":"Ros\u5efa\u6a21 robot_model Create your own urdf file description: in this tutorial you start creating your own urdf robot description file. Create the tree structure in this tutorial we will create the URDF description of the \"robot\" shown in the image below. The robot in the image is a tree structure.Let's start very simple,and create a description of that tree structure,without worrying about the dimensions etc.Fire up your favorite text editor,and create a file called my_robot.urdf: 1 <robot name=\"test_robot\"> 2 <link name=\"link1\" /> 3 <link name=\"link2\" /> 4 <link name=\"link3\" /> 5 <link name=\"link4\" /> 6 7 <joint name=\"joint1\" type=\"continuous\"> 8 <parent link=\"link1\"/> 9 <child link=\"link2\"/> 10 </joint> 11 12 <joint name=\"joint2\" type=\"continuous\"> 13 <parent link=\"link1\"/> 14 <child link=\"link3\"/> 15 </joint> 16 17 <joint name=\"joint3\" type=\"continuous\"> 18 <parent link=\"link3\"/> 19 <child link=\"link4\"/> 20 </joint> 21 </robot> So,just creating the structure is very simple.Now let's see if can get this urdf file parsed.There is a simple command line tool that will parse a urdf file for you, and tell you if the syntax is correct. you might need to install, urdfdom as an upstream,ROS independent package:\\ sudo apt-get install liburdfdom-tools Now run the check command: rosmake urdfdom_model check_urdf my_robot.urdf Add the dimensions So now that we have the basic tree structure.let's add the appropriate dimensions.As you notice in the robot image,the reference frame of each link (in green) is located at the bottom of the link.and is identical to the reference frame of the joint.So,to add dimensions to our tree,all we habe to specify is the offset from a link to the joint(s) of its children.To accomplish this,we will add the field to each of the joints. Let's look at the second joint.Joint2 is offset in the Y-direction from link1,a little offset in the negative X-direction from link1, and it is rotated 90 degrees around the Z-axis.So,we need to add the following element: <origin xyz=\"-2 5 0\" rpy=\"0 0 1.57\"/> If you repeat this for all the elements our URDF will look like this: 1 <robot name=\"test_robot\"> 2 <link name=\"link1\" /> 3 <link name=\"link2\" /> 4 <link name=\"link3\" /> 5 <link name=\"link4\" /> 6 7 8 <joint name=\"joint1\" type=\"continuous\"> 9 <parent link=\"link1\"/> 10 <child link=\"link2\"/> 11 <origin xyz=\"5 3 0\" rpy=\"0 0 0\" /> 12 </joint> 13 14 <joint name=\"joint2\" type=\"continuous\"> 15 <parent link=\"link1\"/> 16 <child link=\"link3\"/> 17 <origin xyz=\"-2 5 0\" rpy=\"0 0 1.57\" /> 18 </joint> 19 20 <joint name=\"joint3\" type=\"continuous\"> 21 <parent link=\"link3\"/> 22 <child link=\"link4\"/> 23 <origin xyz=\"5 0 0\" rpy=\"0 0 -1.57\" /> 24 </joint> 25 </robot> Completing the Kinematics What we didn't specify yet is around which axis the points rotate.Once we add that,we actually have a full kinematic model of this robot!All we need to do is add the element to each joint.The axis specifies the rotational axis in the local frame. So, If you look at joint2, you see it rotates around the positive Y-axis.So, simple add the following xml to the joint element: <axis xyz=\"0 1 0\" /> Similarly,joint1 is rotating around the following axis: <axis xyz=\"-0.707 0.707 0\"/> Note that it is a good idea to normalize the axis. If we add this to all the joints of the robot, our URDF looks like this: 1 <robot name=\"test_robot\"> 2 <link name=\"link1\" /> 3 <link name=\"link2\" /> 4 <link name=\"link3\" /> 5 <link name=\"link4\" /> 6 7 <joint name=\"joint1\" type=\"continuous\"> 8 <parent link=\"link1\"/> 9 <child link=\"link2\"/> 10 <origin xyz=\"5 3 0\" rpy=\"0 0 0\" /> 11 <axis xyz=\"-0.9 0.15 0\" /> 12 </joint> 13 14 <joint name=\"joint2\" type=\"continuous\"> 15 <parent link=\"link1\"/> 16 <child link=\"link3\"/> 17 <origin xyz=\"-2 5 0\" rpy=\"0 0 1.57\" /> 18 <axis xyz=\"-0.707 0.707 0\" /> 19 </joint> 20 21 <joint name=\"joint3\" type=\"continuous\"> 22 <parent link=\"link3\"/> 23 <child link=\"link4\"/> 24 <origin xyz=\"5 0 0\" rpy=\"0 0 -1.57\" /> 25 <axis xyz=\"0.707 -0.707 0\" /> 26 </joint> 27 </robot> Update your file my_robot.urdf and run it through the parser. check_urdf my_robot.urdf That's it.you created your first URDF robot description!Now you can try to visualize the URDF using graphiz: urdf_to_graphiz my_robot.urdf and open the generated file with your favorite pdf viewer: evince test_robot.pdf Parse a urdf file Description: This tutorial teachs you how to use the urdf parser Reading a URDF file This tutorial starts off where the previous one ended.You should still habe your my_robot.urdf file with a description of the robot shown before below. Let's first create a package with a dependency on the urdf parser in our snadbox: cd ~/catkin_ws/src catkin_create_pkg robot_description urdf roscpp rospy tf sensor_msgs std_msgs cd robot_description Now create a /urdf folder to store the urdf file we just created: mkdir urdf cd urdf This follows the convention of always storing your robot's URDF file in a ROS package named MYROBOT_description and within a subfolder named /urdf.Other standard subfolders of your robot's description package include /meshes, /media and /cad,like so:\\ /MYROBOT_description package.xml CMakeLists.txt /urdf /meshes /materials /cad Using the robot state publisher on your own robot Description: This tutorial explains how you can publish the state of your robot to tf, using the robot state publisher. When you are working with a robot that has many relevant frames.it becomes quite a task to publish them all to tf.The robot state publisher is a tool that will do this job for you. The robot state publisher helps you to broadcast the state of your robot to the tf transform library.The robot state publisher internally has a kinematic\u8fd0\u52a8\u5b66 model of the robot; so given the joint positions of the robot,the robot state publisher can compute and broadcast the 3D pose of each link in the robot. You can use the robot state publisher as a standalone ROS node or as a library: 1.Running as a ROS node 1.1 robot_state_pubisher The easiest way to run the robot state publisher is as a node.For normal users,this is the recommanded usage.You need two things to run the robot state publisher: a urdf xml robot description loaded on the Parameter Server. A source that publishes the joint positions as a sensor_msgs/JointState 1.1.1 Subscribed topics joint_states(Sensor_msgs/JointState) joint position information 1.1.2 Parameters robot_description(urdf map)tf_prefix(string) Set the tf prefix for namespace-aware publishing of transform publish_frequency(double) Publish frequency of state publisher,default:50Hz 1.2 Example launch file <launch> <param name = \"robot_description\" textfile = \"$(find mypackage)/urdf/robotmodel.xml\"/> <node pkg = \"robot_state_publisher\" type=\"robot_state_publisher\" name = \"rob_st_pub\"> <remap from=\"robot_state_publisher\" to=\"my_robot_description\"/> <remap from=\"joint_states\" to=\"different_joint_states\"/> </node> </launch> 2.Runing as a library Advanced users can also run the robot state publisher as a library,from within their own c++ code.After you include the header: #include <robot_state_publisher/robot_state_publisher.h> all you need is the constructor which takes in a KDL tree RobotStatePublisher(const KDL::Tree& tree); and now, everytime you want to publish the state of your robot, you call the publishTransforms functions: //Publish moving joints void publishTransfroms(const std::map<std::string, double>& joint_positions, const ros::Time& time); //publish fixed joints void publishFixedTransforms(); The first argument is a map with joint names and joint positions, and the second argument is the time at which the joint positions were recorded. It is okay if the map does not contain all the joint names. It is also okay if the map contains some joints names that are not part of the kinematic model. But note if you don't tell the joint state publisher about some of the joints in your kinematic model, then your tf tree will not be complete. Using urdf with robot_state_publisher Create the URDF File Publishing the State cd %TOP_DIR_YOUR_CATKIN_WS%/src Then fire your favourite editor and paste the following code into the src/state_publisher.cpp file: 1 #include <string> 2 #include <ros/ros.h> 3 #include <sensor_msgs/JointState.h> 4 #include <tf/transform_broadcaster.h> 5 6 int main(int argc, char** argv) { 7 ros::init(argc, argv, \"state_publisher\"); 8 ros::NodeHandle n; 9 ros::Publisher joint_pub = n.advertise<sensor_msgs::JointState>(\"joint_states\", 1); 10 tf::TransformBroadcaster broadcaster; 11 ros::Rate loop_rate(30); 12 13 const double degree = M_PI/180; 14 15 // robot state 16 double tilt = 0, tinc = degree, swivel=0, angle=0, height=0, hinc=0.005; 17 18 // message declarations 19 geometry_msgs::TransformStamped odom_trans; 20 sensor_msgs::JointState joint_state; 21 odom_trans.header.frame_id = \"odom\"; 22 odom_trans.child_frame_id = \"axis\"; 23 24 while (ros::ok()) { 25 //update joint_state 26 joint_state.header.stamp = ros::Time::now(); 27 joint_state.name.resize(3); 28 joint_state.position.resize(3); 29 joint_state.name[0] =\"swivel\"; 30 joint_state.position[0] = swivel; 31 joint_state.name[1] =\"tilt\"; 32 joint_state.position[1] = tilt; 33 joint_state.name[2] =\"periscope\"; 34 joint_state.position[2] = height; 35 36 37 // update transform 38 // (moving in a circle with radius=2) 39 odom_trans.header.stamp = ros::Time::now(); 40 odom_trans.transform.translation.x = cos(angle)*2; 41 odom_trans.transform.translation.y = sin(angle)*2; 42 odom_trans.transform.translation.z = .7; 43 odom_trans.transform.rotation = tf::createQuaternionMsgFromYaw(angle+M_PI/2); 44 45 //send the joint state and transform 46 joint_pub.publish(joint_state); 47 broadcaster.sendTransform(odom_trans); 48 49 // Create new robot state 50 tilt += tinc; 51 if (tilt<-.5 || tilt>0) tinc *= -1; 52 height += hinc; 53 if (height>.2 || height<0) hinc *= -1; 54 swivel += degree; 55 angle += degree/4; 56 57 // This will adjust as needed per iteration 58 loop_rate.sleep(); 59 } 60 61 62 return 0; 63 } Launch File This launch file assumes you are using the package name \"r2d2\" and node name \"state_publisher\" and you have saved this urdf to the \"r2d2\" package. 1 <launch> 2 <param name=\"robot_description\" command=\"cat $(find r2d2)/model.xml\" /> 3 <node name=\"robot_state_publisher\" pkg=\"robot_state_publisher\" type=\"robot_state_publisher\" /> 4 <node name=\"state_publisher\" pkg=\"robot_description\" type=\"state_publisher\" /> 5 </launch> Viewing the Reult First we have to edit the CMakeLists.txt in the package where we saved the above source code.Make sure to add the tf dependency in addition to the other dependencies: find_package(catkin REQUIRED COMPONENTS roscpp rospy std_msgs tf) Notice that roscpp is used to parse the code that we wrote and generate the state_publisher node. We also have to add the following to the end of the CMakelists.txt in order to generate the state_publisher node: include_directories(include ${catkin_INCLUDE_DIRS})\\ add_executable(state_publisher src/state_publisher.cpp)\\ target_link_libraries(state_publisher ${catkin_LIBRARIES}) Now we should go to the directory of the workspace and build it using: catkin_make Now launch the package (assuming that our launch file is named display.launch): roslaunch r2d2 display.launch Run rviz in a new terminal using: rosrun rviz rviz Choose odom as your fixed frame (under Global Options). Then choose \"Add Display\" and add a Robot Model Display and a TF Display Building a Visual Robot Model with URDF from Scratch Description:learn how to build a visual model of a robot that you can view in rviz Before continuing,make sure you have the joint_state_publisher package installed. sudo apt-get install ros-melodic-joint-state-publisher 1.One Shape First, we're just going to explore one simple shape.Here's about as simple as a urdf as you can make. <?xml version=\"1.0\"?> <robot name=\"myfirst\"> <link name=\"base_link\"> <visual> <geometry> <cylinder length=\"0.6\" radius=\"0.2\"/> </geometry> </visual> </link> </robot> To translate the XML into English, this is a robot with the name myfirst, that contains only one link (a.k.a. part), whose visual component is just a cylinder 0.6 meters long with a 0.2 meter radius. This may seem like a lot of enclosing tags for a simple \u201chello world\u201d type example, but it will get more complicated, trust me.\\ Gazebo \u5b89\u88c5 sudo apt-get install ros-melodic-gazebo-ros-pkgs ros-melodic-gazebo-ros-control ros link \u63d0\u793a\u6ca1\u6709\u5305\u542b\u5728gazebo \u53ef\u80fd\u6709\u4e24\u4e2a\u539f\u56e0\uff0c\u4e00\u4e2a\u662f\u5b83\u81ea\u8eab\u6ca1\u6709\u7269\u7406\u5c5e\u6027\uff0c\u53e6\u5916\u4e00\u4e2a\u662f\u5b83\u7684\u8fde\u63a5\u5bf9\u8c61\u6ca1\u6709\u7269\u7406\u5c5e\u6027 Denavit-Hartenberg parameters In mechanical engineering, the Denavit-Hartenberg parameters(also called DH parameters) are the four parameters associated with a particular convention for attaching reference frames to the links of a spatial kinematic chain or robot manipulator. Jacques Denavit and Richard Hartenberg introduced this convention in 1955 in order to standardlize the coordinate frames for spatial linkages. Denavit-Hartenberg convention A commonly used convention for selecting frames of reference in robotics applications is Denavit and Hartenberg(D-H) convention. In this convention, coordinate frames are attached to the joints between two links such that one transformation is associated with the joint,[Z], and the second is associated with the link,[X]. The coordinate transformations along a serial robot consisting of n links from the kinematics equations of the robot. where [T] is the transformation locating the end-link. In order to determine the coordinate transformations [Z] and [X], the joints connecting the links are modeled as either hinged or sliding joints, each of which have a unique line S in space that forms the joint axis and define the relative movement of the two links.A typical serial robot is characterized by a sequence of six lines Si,i=1,......6, one for each joint in the robot.For each sequence of lines Si and Si+1, there is a common normal line Ai,j+1.The system of six joint axes Si","title":"Ros\u5efa\u6a21"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#ros","text":"robot_model","title":"Ros\u5efa\u6a21"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#create-your-own-urdf-file","text":"","title":"Create your own urdf file"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#description-in-this-tutorial-you-start-creating-your-own-urdf-robot-description-file","text":"Create the tree structure in this tutorial we will create the URDF description of the \"robot\" shown in the image below. The robot in the image is a tree structure.Let's start very simple,and create a description of that tree structure,without worrying about the dimensions etc.Fire up your favorite text editor,and create a file called my_robot.urdf: 1 <robot name=\"test_robot\"> 2 <link name=\"link1\" /> 3 <link name=\"link2\" /> 4 <link name=\"link3\" /> 5 <link name=\"link4\" /> 6 7 <joint name=\"joint1\" type=\"continuous\"> 8 <parent link=\"link1\"/> 9 <child link=\"link2\"/> 10 </joint> 11 12 <joint name=\"joint2\" type=\"continuous\"> 13 <parent link=\"link1\"/> 14 <child link=\"link3\"/> 15 </joint> 16 17 <joint name=\"joint3\" type=\"continuous\"> 18 <parent link=\"link3\"/> 19 <child link=\"link4\"/> 20 </joint> 21 </robot> So,just creating the structure is very simple.Now let's see if can get this urdf file parsed.There is a simple command line tool that will parse a urdf file for you, and tell you if the syntax is correct. you might need to install, urdfdom as an upstream,ROS independent package:\\ sudo apt-get install liburdfdom-tools Now run the check command: rosmake urdfdom_model check_urdf my_robot.urdf","title":"description: in this tutorial you start creating your own urdf robot description file."},{"location":"Ros%E5%BB%BA%E6%A8%A1/#add-the-dimensions","text":"So now that we have the basic tree structure.let's add the appropriate dimensions.As you notice in the robot image,the reference frame of each link (in green) is located at the bottom of the link.and is identical to the reference frame of the joint.So,to add dimensions to our tree,all we habe to specify is the offset from a link to the joint(s) of its children.To accomplish this,we will add the field to each of the joints. Let's look at the second joint.Joint2 is offset in the Y-direction from link1,a little offset in the negative X-direction from link1, and it is rotated 90 degrees around the Z-axis.So,we need to add the following element: <origin xyz=\"-2 5 0\" rpy=\"0 0 1.57\"/> If you repeat this for all the elements our URDF will look like this: 1 <robot name=\"test_robot\"> 2 <link name=\"link1\" /> 3 <link name=\"link2\" /> 4 <link name=\"link3\" /> 5 <link name=\"link4\" /> 6 7 8 <joint name=\"joint1\" type=\"continuous\"> 9 <parent link=\"link1\"/> 10 <child link=\"link2\"/> 11 <origin xyz=\"5 3 0\" rpy=\"0 0 0\" /> 12 </joint> 13 14 <joint name=\"joint2\" type=\"continuous\"> 15 <parent link=\"link1\"/> 16 <child link=\"link3\"/> 17 <origin xyz=\"-2 5 0\" rpy=\"0 0 1.57\" /> 18 </joint> 19 20 <joint name=\"joint3\" type=\"continuous\"> 21 <parent link=\"link3\"/> 22 <child link=\"link4\"/> 23 <origin xyz=\"5 0 0\" rpy=\"0 0 -1.57\" /> 24 </joint> 25 </robot>","title":"Add the dimensions"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#completing-the-kinematics","text":"What we didn't specify yet is around which axis the points rotate.Once we add that,we actually have a full kinematic model of this robot!All we need to do is add the element to each joint.The axis specifies the rotational axis in the local frame. So, If you look at joint2, you see it rotates around the positive Y-axis.So, simple add the following xml to the joint element: <axis xyz=\"0 1 0\" /> Similarly,joint1 is rotating around the following axis: <axis xyz=\"-0.707 0.707 0\"/> Note that it is a good idea to normalize the axis. If we add this to all the joints of the robot, our URDF looks like this: 1 <robot name=\"test_robot\"> 2 <link name=\"link1\" /> 3 <link name=\"link2\" /> 4 <link name=\"link3\" /> 5 <link name=\"link4\" /> 6 7 <joint name=\"joint1\" type=\"continuous\"> 8 <parent link=\"link1\"/> 9 <child link=\"link2\"/> 10 <origin xyz=\"5 3 0\" rpy=\"0 0 0\" /> 11 <axis xyz=\"-0.9 0.15 0\" /> 12 </joint> 13 14 <joint name=\"joint2\" type=\"continuous\"> 15 <parent link=\"link1\"/> 16 <child link=\"link3\"/> 17 <origin xyz=\"-2 5 0\" rpy=\"0 0 1.57\" /> 18 <axis xyz=\"-0.707 0.707 0\" /> 19 </joint> 20 21 <joint name=\"joint3\" type=\"continuous\"> 22 <parent link=\"link3\"/> 23 <child link=\"link4\"/> 24 <origin xyz=\"5 0 0\" rpy=\"0 0 -1.57\" /> 25 <axis xyz=\"0.707 -0.707 0\" /> 26 </joint> 27 </robot> Update your file my_robot.urdf and run it through the parser. check_urdf my_robot.urdf That's it.you created your first URDF robot description!Now you can try to visualize the URDF using graphiz: urdf_to_graphiz my_robot.urdf and open the generated file with your favorite pdf viewer: evince test_robot.pdf","title":"Completing the Kinematics"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#parse-a-urdf-file","text":"","title":"Parse a urdf file"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#description-this-tutorial-teachs-you-how-to-use-the-urdf-parser","text":"","title":"Description: This tutorial teachs you how to use the urdf parser"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#reading-a-urdf-file","text":"This tutorial starts off where the previous one ended.You should still habe your my_robot.urdf file with a description of the robot shown before below. Let's first create a package with a dependency on the urdf parser in our snadbox: cd ~/catkin_ws/src catkin_create_pkg robot_description urdf roscpp rospy tf sensor_msgs std_msgs cd robot_description Now create a /urdf folder to store the urdf file we just created: mkdir urdf cd urdf This follows the convention of always storing your robot's URDF file in a ROS package named MYROBOT_description and within a subfolder named /urdf.Other standard subfolders of your robot's description package include /meshes, /media and /cad,like so:\\ /MYROBOT_description package.xml CMakeLists.txt /urdf /meshes /materials /cad","title":"Reading a URDF file"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#using-the-robot-state-publisher-on-your-own-robot","text":"","title":"Using the robot state publisher on your own robot"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#description-this-tutorial-explains-how-you-can-publish-the-state-of-your-robot-to-tf-using-the-robot-state-publisher","text":"When you are working with a robot that has many relevant frames.it becomes quite a task to publish them all to tf.The robot state publisher is a tool that will do this job for you. The robot state publisher helps you to broadcast the state of your robot to the tf transform library.The robot state publisher internally has a kinematic\u8fd0\u52a8\u5b66 model of the robot; so given the joint positions of the robot,the robot state publisher can compute and broadcast the 3D pose of each link in the robot. You can use the robot state publisher as a standalone ROS node or as a library:","title":"Description: This tutorial explains how you can publish the state of your robot to tf, using the robot state publisher."},{"location":"Ros%E5%BB%BA%E6%A8%A1/#1running-as-a-ros-node","text":"","title":"1.Running as a ROS node"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#11-robot_state_pubisher","text":"The easiest way to run the robot state publisher is as a node.For normal users,this is the recommanded usage.You need two things to run the robot state publisher: a urdf xml robot description loaded on the Parameter Server. A source that publishes the joint positions as a sensor_msgs/JointState","title":"1.1 robot_state_pubisher"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#111-subscribed-topics","text":"joint_states(Sensor_msgs/JointState) joint position information","title":"1.1.1 Subscribed topics"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#112-parameters","text":"robot_description(urdf map)tf_prefix(string) Set the tf prefix for namespace-aware publishing of transform publish_frequency(double) Publish frequency of state publisher,default:50Hz","title":"1.1.2 Parameters"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#12-example-launch-file","text":"<launch> <param name = \"robot_description\" textfile = \"$(find mypackage)/urdf/robotmodel.xml\"/> <node pkg = \"robot_state_publisher\" type=\"robot_state_publisher\" name = \"rob_st_pub\"> <remap from=\"robot_state_publisher\" to=\"my_robot_description\"/> <remap from=\"joint_states\" to=\"different_joint_states\"/> </node> </launch>","title":"1.2 Example launch file"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#2runing-as-a-library","text":"Advanced users can also run the robot state publisher as a library,from within their own c++ code.After you include the header: #include <robot_state_publisher/robot_state_publisher.h> all you need is the constructor which takes in a KDL tree RobotStatePublisher(const KDL::Tree& tree); and now, everytime you want to publish the state of your robot, you call the publishTransforms functions: //Publish moving joints void publishTransfroms(const std::map<std::string, double>& joint_positions, const ros::Time& time); //publish fixed joints void publishFixedTransforms(); The first argument is a map with joint names and joint positions, and the second argument is the time at which the joint positions were recorded. It is okay if the map does not contain all the joint names. It is also okay if the map contains some joints names that are not part of the kinematic model. But note if you don't tell the joint state publisher about some of the joints in your kinematic model, then your tf tree will not be complete.","title":"2.Runing as a library"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#using-urdf-with-robot_state_publisher","text":"","title":"Using urdf with robot_state_publisher"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#create-the-urdf-file","text":"","title":"Create the URDF File"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#publishing-the-state","text":"cd %TOP_DIR_YOUR_CATKIN_WS%/src Then fire your favourite editor and paste the following code into the src/state_publisher.cpp file: 1 #include <string> 2 #include <ros/ros.h> 3 #include <sensor_msgs/JointState.h> 4 #include <tf/transform_broadcaster.h> 5 6 int main(int argc, char** argv) { 7 ros::init(argc, argv, \"state_publisher\"); 8 ros::NodeHandle n; 9 ros::Publisher joint_pub = n.advertise<sensor_msgs::JointState>(\"joint_states\", 1); 10 tf::TransformBroadcaster broadcaster; 11 ros::Rate loop_rate(30); 12 13 const double degree = M_PI/180; 14 15 // robot state 16 double tilt = 0, tinc = degree, swivel=0, angle=0, height=0, hinc=0.005; 17 18 // message declarations 19 geometry_msgs::TransformStamped odom_trans; 20 sensor_msgs::JointState joint_state; 21 odom_trans.header.frame_id = \"odom\"; 22 odom_trans.child_frame_id = \"axis\"; 23 24 while (ros::ok()) { 25 //update joint_state 26 joint_state.header.stamp = ros::Time::now(); 27 joint_state.name.resize(3); 28 joint_state.position.resize(3); 29 joint_state.name[0] =\"swivel\"; 30 joint_state.position[0] = swivel; 31 joint_state.name[1] =\"tilt\"; 32 joint_state.position[1] = tilt; 33 joint_state.name[2] =\"periscope\"; 34 joint_state.position[2] = height; 35 36 37 // update transform 38 // (moving in a circle with radius=2) 39 odom_trans.header.stamp = ros::Time::now(); 40 odom_trans.transform.translation.x = cos(angle)*2; 41 odom_trans.transform.translation.y = sin(angle)*2; 42 odom_trans.transform.translation.z = .7; 43 odom_trans.transform.rotation = tf::createQuaternionMsgFromYaw(angle+M_PI/2); 44 45 //send the joint state and transform 46 joint_pub.publish(joint_state); 47 broadcaster.sendTransform(odom_trans); 48 49 // Create new robot state 50 tilt += tinc; 51 if (tilt<-.5 || tilt>0) tinc *= -1; 52 height += hinc; 53 if (height>.2 || height<0) hinc *= -1; 54 swivel += degree; 55 angle += degree/4; 56 57 // This will adjust as needed per iteration 58 loop_rate.sleep(); 59 } 60 61 62 return 0; 63 }","title":"Publishing the State"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#launch-file","text":"This launch file assumes you are using the package name \"r2d2\" and node name \"state_publisher\" and you have saved this urdf to the \"r2d2\" package. 1 <launch> 2 <param name=\"robot_description\" command=\"cat $(find r2d2)/model.xml\" /> 3 <node name=\"robot_state_publisher\" pkg=\"robot_state_publisher\" type=\"robot_state_publisher\" /> 4 <node name=\"state_publisher\" pkg=\"robot_description\" type=\"state_publisher\" /> 5 </launch>","title":"Launch File"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#viewing-the-reult","text":"First we have to edit the CMakeLists.txt in the package where we saved the above source code.Make sure to add the tf dependency in addition to the other dependencies: find_package(catkin REQUIRED COMPONENTS roscpp rospy std_msgs tf) Notice that roscpp is used to parse the code that we wrote and generate the state_publisher node. We also have to add the following to the end of the CMakelists.txt in order to generate the state_publisher node: include_directories(include ${catkin_INCLUDE_DIRS})\\ add_executable(state_publisher src/state_publisher.cpp)\\ target_link_libraries(state_publisher ${catkin_LIBRARIES}) Now we should go to the directory of the workspace and build it using: catkin_make Now launch the package (assuming that our launch file is named display.launch): roslaunch r2d2 display.launch Run rviz in a new terminal using: rosrun rviz rviz Choose odom as your fixed frame (under Global Options). Then choose \"Add Display\" and add a Robot Model Display and a TF Display","title":"Viewing the Reult"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#building-a-visual-robot-model-with-urdf-from-scratch","text":"","title":"Building a Visual Robot Model with URDF from Scratch"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#descriptionlearn-how-to-build-a-visual-model-of-a-robot-that-you-can-view-in-rviz","text":"Before continuing,make sure you have the joint_state_publisher package installed. sudo apt-get install ros-melodic-joint-state-publisher","title":"Description:learn how to build a visual model of a robot that you can view in rviz"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#1one-shape","text":"First, we're just going to explore one simple shape.Here's about as simple as a urdf as you can make. <?xml version=\"1.0\"?> <robot name=\"myfirst\"> <link name=\"base_link\"> <visual> <geometry> <cylinder length=\"0.6\" radius=\"0.2\"/> </geometry> </visual> </link> </robot> To translate the XML into English, this is a robot with the name myfirst, that contains only one link (a.k.a. part), whose visual component is just a cylinder 0.6 meters long with a 0.2 meter radius. This may seem like a lot of enclosing tags for a simple \u201chello world\u201d type example, but it will get more complicated, trust me.\\","title":"1.One Shape"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#gazebo","text":"sudo apt-get install ros-melodic-gazebo-ros-pkgs ros-melodic-gazebo-ros-control","title":"Gazebo \u5b89\u88c5"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#ros-link-gazebo","text":"\u53ef\u80fd\u6709\u4e24\u4e2a\u539f\u56e0\uff0c\u4e00\u4e2a\u662f\u5b83\u81ea\u8eab\u6ca1\u6709\u7269\u7406\u5c5e\u6027\uff0c\u53e6\u5916\u4e00\u4e2a\u662f\u5b83\u7684\u8fde\u63a5\u5bf9\u8c61\u6ca1\u6709\u7269\u7406\u5c5e\u6027","title":"ros link \u63d0\u793a\u6ca1\u6709\u5305\u542b\u5728gazebo"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#denavit-hartenberg-parameters","text":"In mechanical engineering, the Denavit-Hartenberg parameters(also called DH parameters) are the four parameters associated with a particular convention for attaching reference frames to the links of a spatial kinematic chain or robot manipulator. Jacques Denavit and Richard Hartenberg introduced this convention in 1955 in order to standardlize the coordinate frames for spatial linkages. Denavit-Hartenberg convention A commonly used convention for selecting frames of reference in robotics applications is Denavit and Hartenberg(D-H) convention. In this convention, coordinate frames are attached to the joints between two links such that one transformation is associated with the joint,[Z], and the second is associated with the link,[X]. The coordinate transformations along a serial robot consisting of n links from the kinematics equations of the robot. where [T] is the transformation locating the end-link. In order to determine the coordinate transformations [Z] and [X], the joints connecting the links are modeled as either hinged or sliding joints, each of which have a unique line S in space that forms the joint axis and define the relative movement of the two links.A typical serial robot is characterized by a sequence of six lines Si,i=1,......6, one for each joint in the robot.For each sequence of lines Si and Si+1, there is a common normal line Ai,j+1.The system of six joint axes Si","title":"Denavit-Hartenberg parameters"},{"location":"c%2B%2B/","text":"string \u524d\u8865\u96f6 #include <iostream> #include <sstream> #include <iomanip> using namespace std; void main() { int num = 1024; stringstream ss; ss << setw(5) << setfill('0') << num ; string str; ss >> str; //\u5c06\u5b57\u7b26\u6d41\u4f20\u7ed9 str //str = ss.str(); //\u4e5f\u53ef\u4ee5 cout << str; } \u5f97\u5230\u5f53\u524d\u65f6\u95f4 #include <iostream> #include <ctime> int main () { time_t rawtime; struct tm * timeinfo; char buffer[80]; time (&rawtime); timeinfo = localtime(&rawtime); strftime(buffer,sizeof(buffer),\"%d-%m-%Y %H:%M:%S\",timeinfo); std::string str(buffer); std::cout << str; return 0; } \u5199\u5165\u6587\u4ef6append #include <fstream> int main() { std::ofstream outfile; outfile.open(\"test.txt\", std::ios_base::app); // append instead of overwrite outfile << \"Data\"; return 0; } float to string to_string(float) string to float stof(str) \u8bfb\u53d6\u6587\u6863\u6bcf\u4e00\u884c\u5185\u5bb9 fstream newfile; newfile.open(learning_file,ios::in); if (newfile.is_open()) { string tp; while(getline(newfile, tp)) { cout << tp << \"\\n\"; } newfile.close(); } \u6587\u4ef6\u6e05\u7a7a fstream newfile; newfile.open(learning_file,ios::out); newfile.close(); \u5b57\u7b26\u4e32\uff53\uff50\uff4c\uff49\uff54 #include <boost/algorithm/string.hpp> std::string text = \"Let me split this into words\"; std::vector<std::string> results; boost::split(results, text, [](char c){return c == ' ';}); CMakeLists.txt \u5f15\u5165\uff42\uff4f\uff4f\uff53\uff54 find_package(Boost COMPONENTS program_options filesystem REQUIRED ) include_directories(${Boost_INCLUDE_DIRS}) link_directories(${Boost_LIBRARY_DIRS}) target_link_libraries(learning ${catkin_LIBRARIES} ${Boost_LIBRARIES}) explicit #include <iostream> using namespace std; class Complex { private: double real; double imag; public: // Default constructor Complex(double r = 0.0, double i = 0.0) : real(r), imag(i) {} // A method to compare two Complex numbers bool operator == (Complex rhs) { return (real == rhs.real && imag == rhs.imag)? true : false; } }; int main() { // a Complex object Complex com1(3.0, 0.0); if (com1 == 3.0) cout << \"Same\"; else cout << \"Not Same\"; return 0; } \u7a0b\u5e8f\u8f93\u51fa\u3000 Same \u5728C ++\u4e2d\uff0c\u5982\u679c\u7c7b\u5177\u6709\u53ef\u4ee5\u7528\u5355\u4e2a\u53c2\u6570\u8c03\u7528\u7684\u6784\u9020\u51fd\u6570\uff0c\u5219\u6b64\u6784\u9020\u51fd\u6570\u5c06\u6210\u4e3a\u8f6c\u6362\u6784\u9020\u51fd\u6570\uff0c\u56e0\u4e3a\u8fd9\u6837\u7684\u6784\u9020\u51fd\u6570\u5141\u8bb8\u5c06\u5355\u4e2a\u53c2\u6570\u8f6c\u6362\u4e3a\u6b63\u5728\u6784\u9020\u7684\u7c7b\u3002 \u6211\u4eec\u53ef\u4ee5\u907f\u514d\u8fd9\u79cd\u9690\u5f0f\u8f6c\u6362\uff0c\u56e0\u4e3a\u5b83\u4eec\u53ef\u80fd\u5bfc\u81f4\u610f\u5916\u7684\u7ed3\u679c\u3002\u6211\u4eec\u53ef\u4ee5\u5728explicit\u5173\u952e\u5b57\u7684\u5e2e\u52a9\u4e0b\u4f7f\u6784\u9020\u51fd\u6570\u663e\u5f0f\u5316\u3002\\ using namespace std; class Complex { private: double real; double imag; public: // Default constructor explicit Complex(double r = 0.0, double i = 0.0) : real(r), imag(i) {} // A method to compare two Complex numbers bool operator== (Complex rhs) { return (real == rhs.real && imag == rhs.imag)? true : false; } }; int main() { // a Complex object Complex com1(3.0, 0.0); if (com1 == (Complex)3.0) cout << \"Same\"; else cout << \"Not Same\"; return 0; } Use Eigen in cmake program ```bash find_package(Eigen3 3.3 REQUIRED) ... include_directories(${EIGEN3_INCLUDE_DIR}) ... target_link_libraries(example Eigen3::Eigen) double to string \u4fdd\u7559\u5c0f\u6570\u70b9\u540e1\u4f4d std::string doubleToString(const double &val) { char* chCode; chCode = new char[20]; sprintf(chCode, \"%.2lf\", val); std::string str(chCode); delete[]chCode; return str; } \u8bfb\u53d6\u7f51\u53e3\u4fe1\u606f #include <arpa/inet.h> #include <sys/socket.h> #include <netdb.h> #include <ifaddrs.h> #include <stdio.h> #include <stdlib.h> #include <unistd.h> #include <linux/if_link.h> #include <iostream> std::string my_get_WIP() { struct ifaddrs *ifaddr, *ifa; int family, s, n; char host[NI_MAXHOST]; if (getifaddrs(&ifaddr) == -1) { perror(\"getifaddrs\"); exit(EXIT_FAILURE); } std::string result; for (ifa = ifaddr, n = 0; ifa != NULL; ifa = ifa->ifa_next, n++) { if (ifa->ifa_addr == NULL) continue; family = ifa->ifa_addr->sa_family; if (family == AF_INET || family == AF_INET6) { s = getnameinfo(ifa->ifa_addr, (family == AF_INET) ? sizeof(struct sockaddr_in) : sizeof(struct sockaddr_in6), host, NI_MAXHOST, NULL, 0, NI_NUMERICHOST); if (s != 0) { printf(\"getnameinfo() failed: %s\\n\", gai_strerror(s)); exit(EXIT_FAILURE); } } else if (family == AF_PACKET && ifa->ifa_data != NULL) { struct rtnl_link_stats *stats = (struct rtnl_link_stats *)ifa->ifa_data; } std::string host_str = std::string(host); if(host_str.find(\"192.168.1.\") != std::string::npos) { result = std::string(ifa->ifa_name) + \":\" + host_str; } } freeifaddrs(ifaddr); return result; exit(EXIT_SUCCESS); } \u5224\u65ad\u5b57\u7b26\u4e32\u662f\u5426\u542b\u6709\u67d0\u5b57\u7b26 std::size_t find = str.find(\":\"); if(find != std::string::npos) { std::cout << \u201cfind\u201d << std::endl; }","title":"C++"},{"location":"c%2B%2B/#string","text":"#include <iostream> #include <sstream> #include <iomanip> using namespace std; void main() { int num = 1024; stringstream ss; ss << setw(5) << setfill('0') << num ; string str; ss >> str; //\u5c06\u5b57\u7b26\u6d41\u4f20\u7ed9 str //str = ss.str(); //\u4e5f\u53ef\u4ee5 cout << str; }","title":"string \u524d\u8865\u96f6"},{"location":"c%2B%2B/#_1","text":"#include <iostream> #include <ctime> int main () { time_t rawtime; struct tm * timeinfo; char buffer[80]; time (&rawtime); timeinfo = localtime(&rawtime); strftime(buffer,sizeof(buffer),\"%d-%m-%Y %H:%M:%S\",timeinfo); std::string str(buffer); std::cout << str; return 0; }","title":"\u5f97\u5230\u5f53\u524d\u65f6\u95f4"},{"location":"c%2B%2B/#append","text":"#include <fstream> int main() { std::ofstream outfile; outfile.open(\"test.txt\", std::ios_base::app); // append instead of overwrite outfile << \"Data\"; return 0; }","title":"\u5199\u5165\u6587\u4ef6append"},{"location":"c%2B%2B/#float-to-string","text":"to_string(float)","title":"float to string"},{"location":"c%2B%2B/#string-to-float","text":"stof(str)","title":"string to float"},{"location":"c%2B%2B/#_2","text":"fstream newfile; newfile.open(learning_file,ios::in); if (newfile.is_open()) { string tp; while(getline(newfile, tp)) { cout << tp << \"\\n\"; } newfile.close(); }","title":"\u8bfb\u53d6\u6587\u6863\u6bcf\u4e00\u884c\u5185\u5bb9"},{"location":"c%2B%2B/#_3","text":"fstream newfile; newfile.open(learning_file,ios::out); newfile.close();","title":"\u6587\u4ef6\u6e05\u7a7a"},{"location":"c%2B%2B/#split","text":"#include <boost/algorithm/string.hpp> std::string text = \"Let me split this into words\"; std::vector<std::string> results; boost::split(results, text, [](char c){return c == ' ';});","title":"\u5b57\u7b26\u4e32\uff53\uff50\uff4c\uff49\uff54"},{"location":"c%2B%2B/#cmakeliststxt-boost","text":"find_package(Boost COMPONENTS program_options filesystem REQUIRED ) include_directories(${Boost_INCLUDE_DIRS}) link_directories(${Boost_LIBRARY_DIRS}) target_link_libraries(learning ${catkin_LIBRARIES} ${Boost_LIBRARIES})","title":"CMakeLists.txt \u5f15\u5165\uff42\uff4f\uff4f\uff53\uff54"},{"location":"c%2B%2B/#explicit","text":"#include <iostream> using namespace std; class Complex { private: double real; double imag; public: // Default constructor Complex(double r = 0.0, double i = 0.0) : real(r), imag(i) {} // A method to compare two Complex numbers bool operator == (Complex rhs) { return (real == rhs.real && imag == rhs.imag)? true : false; } }; int main() { // a Complex object Complex com1(3.0, 0.0); if (com1 == 3.0) cout << \"Same\"; else cout << \"Not Same\"; return 0; } \u7a0b\u5e8f\u8f93\u51fa\u3000 Same \u5728C ++\u4e2d\uff0c\u5982\u679c\u7c7b\u5177\u6709\u53ef\u4ee5\u7528\u5355\u4e2a\u53c2\u6570\u8c03\u7528\u7684\u6784\u9020\u51fd\u6570\uff0c\u5219\u6b64\u6784\u9020\u51fd\u6570\u5c06\u6210\u4e3a\u8f6c\u6362\u6784\u9020\u51fd\u6570\uff0c\u56e0\u4e3a\u8fd9\u6837\u7684\u6784\u9020\u51fd\u6570\u5141\u8bb8\u5c06\u5355\u4e2a\u53c2\u6570\u8f6c\u6362\u4e3a\u6b63\u5728\u6784\u9020\u7684\u7c7b\u3002 \u6211\u4eec\u53ef\u4ee5\u907f\u514d\u8fd9\u79cd\u9690\u5f0f\u8f6c\u6362\uff0c\u56e0\u4e3a\u5b83\u4eec\u53ef\u80fd\u5bfc\u81f4\u610f\u5916\u7684\u7ed3\u679c\u3002\u6211\u4eec\u53ef\u4ee5\u5728explicit\u5173\u952e\u5b57\u7684\u5e2e\u52a9\u4e0b\u4f7f\u6784\u9020\u51fd\u6570\u663e\u5f0f\u5316\u3002\\ using namespace std; class Complex { private: double real; double imag; public: // Default constructor explicit Complex(double r = 0.0, double i = 0.0) : real(r), imag(i) {} // A method to compare two Complex numbers bool operator== (Complex rhs) { return (real == rhs.real && imag == rhs.imag)? true : false; } }; int main() { // a Complex object Complex com1(3.0, 0.0); if (com1 == (Complex)3.0) cout << \"Same\"; else cout << \"Not Same\"; return 0; }","title":"explicit"},{"location":"c%2B%2B/#use-eigen-in-cmake-program","text":"```bash find_package(Eigen3 3.3 REQUIRED) ... include_directories(${EIGEN3_INCLUDE_DIR}) ... target_link_libraries(example Eigen3::Eigen)","title":"Use Eigen in cmake program"},{"location":"c%2B%2B/#double-to-string-1","text":"std::string doubleToString(const double &val) { char* chCode; chCode = new char[20]; sprintf(chCode, \"%.2lf\", val); std::string str(chCode); delete[]chCode; return str; }","title":"double to string \u4fdd\u7559\u5c0f\u6570\u70b9\u540e1\u4f4d"},{"location":"c%2B%2B/#_4","text":"#include <arpa/inet.h> #include <sys/socket.h> #include <netdb.h> #include <ifaddrs.h> #include <stdio.h> #include <stdlib.h> #include <unistd.h> #include <linux/if_link.h> #include <iostream> std::string my_get_WIP() { struct ifaddrs *ifaddr, *ifa; int family, s, n; char host[NI_MAXHOST]; if (getifaddrs(&ifaddr) == -1) { perror(\"getifaddrs\"); exit(EXIT_FAILURE); } std::string result; for (ifa = ifaddr, n = 0; ifa != NULL; ifa = ifa->ifa_next, n++) { if (ifa->ifa_addr == NULL) continue; family = ifa->ifa_addr->sa_family; if (family == AF_INET || family == AF_INET6) { s = getnameinfo(ifa->ifa_addr, (family == AF_INET) ? sizeof(struct sockaddr_in) : sizeof(struct sockaddr_in6), host, NI_MAXHOST, NULL, 0, NI_NUMERICHOST); if (s != 0) { printf(\"getnameinfo() failed: %s\\n\", gai_strerror(s)); exit(EXIT_FAILURE); } } else if (family == AF_PACKET && ifa->ifa_data != NULL) { struct rtnl_link_stats *stats = (struct rtnl_link_stats *)ifa->ifa_data; } std::string host_str = std::string(host); if(host_str.find(\"192.168.1.\") != std::string::npos) { result = std::string(ifa->ifa_name) + \":\" + host_str; } } freeifaddrs(ifaddr); return result; exit(EXIT_SUCCESS); }","title":"\u8bfb\u53d6\u7f51\u53e3\u4fe1\u606f"},{"location":"c%2B%2B/#_5","text":"std::size_t find = str.find(\":\"); if(find != std::string::npos) { std::cout << \u201cfind\u201d << std::endl; }","title":"\u5224\u65ad\u5b57\u7b26\u4e32\u662f\u5426\u542b\u6709\u67d0\u5b57\u7b26"},{"location":"caffe/","text":"caffe paython \u73af\u5883\u914d\u7f6e export PYTHONPATH=$HOME/Downloads/caffe/python:$PYTHONPATH caffe \u5b89\u88c5 git clone https://github.com/BVLC/caffe.git \u5b89\u88c5\u4f9d\u8d56 sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler sudo apt-get install --no-install-recommends libboost-all-dev sudo apt-get install libopenblas-dev sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev Makefile.config ## Refer to http://caffe.berkeleyvision.org/installation.html # Contributions simplifying and improving our build system are welcome! # cuDNN acceleration switch (uncomment to build with cuDNN). # USE_CUDNN := 1 # CPU-only switch (uncomment to build without GPU support). # CPU_ONLY := 1 # uncomment to disable IO dependencies and corresponding data layers # USE_OPENCV := 0 # USE_LEVELDB := 0 # USE_LMDB := 0 # uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary) # You should not set this flag if you will be reading LMDBs with any # possibility of simultaneous read and write # ALLOW_LMDB_NOLOCK := 1 # Uncomment if you're using OpenCV 3 OPENCV_VERSION := 3 # To customize your choice of compiler, uncomment and set the following. # N.B. the default for Linux is g++ and the default for OSX is clang++ # CUSTOM_CXX := g++ # CUDA directory contains bin/ and lib/ directories that we need. CUDA_DIR := /usr/local/cuda # On Ubuntu 14.04, if cuda tools are installed via # \"sudo apt-get install nvidia-cuda-toolkit\" then use this instead: # CUDA_DIR := /usr # CUDA architecture setting: going with all of them. # For CUDA < 6.0, comment the lines after *_35 for compatibility. CUDA_ARCH := -gencode arch=compute_30,code=sm_30 \\ -gencode arch=compute_35,code=sm_35 \\ -gencode arch=compute_50,code=sm_50 \\ -gencode arch=compute_52,code=sm_52 \\ -gencode arch=compute_61,code=sm_61 \\ -gencode arch=compute_61,code=compute_61 # -gencode arch=compute_20,code=sm_20 \\ # -gencode arch=compute_20,code=sm_21 \\ # BLAS choice: # atlas for ATLAS (default) # mkl for MKL # open for OpenBlas # BLAS := atlas BLAS := open # Custom (MKL/ATLAS/OpenBLAS) include and lib directories. # Leave commented to accept the defaults for your choice of BLAS # (which should work)! # BLAS_INCLUDE := /path/to/your/blas # BLAS_LIB := /path/to/your/blas # Homebrew puts openblas in a directory that is not on the standard search path # BLAS_INCLUDE := $(shell brew --prefix openblas)/include # BLAS_LIB := $(shell brew --prefix openblas)/lib # This is required only if you will compile the matlab interface. # MATLAB directory should contain the mex binary in /bin. # MATLAB_DIR := /usr/local # MATLAB_DIR := /Applications/MATLAB_R2012b.app # NOTE: this is required only if you will compile the python interface. # We need to be able to find Python.h and numpy/arrayobject.h. PYTHON_INCLUDE := /usr/include/python2.7 \\ /usr/lib/python2.7/dist-packages/numpy/core/include # Anaconda Python distribution is quite popular. Include path: # Verify anaconda location, sometimes it's in root. # ANACONDA_HOME := $(HOME)/anaconda2 # PYTHON_INCLUDE := $(ANACONDA_HOME)/include \\ $(ANACONDA_HOME)/include/python2.7 \\ $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include \\ # Uncomment to use Python 3 (default is Python 2) # PYTHON_LIBRARIES := boost_python3 python3.5m # PYTHON_INCLUDE := /usr/include/python3.5m \\ # /usr/lib/python3.5/dist-packages/numpy/core/include # We need to be able to find libpythonX.X.so or .dylib. PYTHON_LIB := /usr/lib # PYTHON_LIB := $(ANACONDA_HOME)/lib # Homebrew installs numpy in a non standard path (keg only) # PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include # PYTHON_LIB += $(shell brew --prefix numpy)/lib # Uncomment to support layers written in Python (will link against Python libs) # WITH_PYTHON_LAYER := 1 # Whatever else you find you need goes here. INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial/ LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu/hdf5/serial # If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies # INCLUDE_DIRS += $(shell brew --prefix)/include # LIBRARY_DIRS += $(shell brew --prefix)/lib # Uncomment to use `pkg-config` to specify OpenCV library paths. # (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.) # USE_PKG_CONFIG := 1 # N.B. both build and distribute dirs are cleared on `make clean` BUILD_DIR := build DISTRIBUTE_DIR := distribute # Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171 # DEBUG := 1 # The ID of the GPU that 'make runtest' will use to run unit tests. TEST_GPUID := 0 # enable pretty build (comment to see full commands) Q ?= @ \u5b89\u88c5 sudo make -j8\\ sudo make distribute \u5e94\u7528 CMakeLists.txt set(CAFFE_PATH \"$ENV{HOME}/Disk/caffe/distribute\") if (EXISTS \"${CAFFE_PATH}\") include_directories( include ${catkin_INCLUDE_DIRS} ${OpenCV_INCLUDE_DIRS} ${CAFFE_PATH}/include ) target_link_libraries(lidar_cnn_seg_detect ${catkin_LIBRARIES} ${OpenCV_LIBRARIES} ${CUDA_LIBRARIES} ${CAFFE_PATH}/lib/libcaffe.so glog ) caffe python layer layer tuozhan The models are defined in plaintext protocol buffer schema (prototxt) while the learned models are serialized as binary protocol buffer (binaryproto) .caffemodel files. caffe \u8bad\u7ec3\u9700\u8981\u6570\u636e\u96c6\uff0c\u7f51\u7edc.prototxt \u548c solver.prototxt caffe \u7f51\u7edc\u53ef\u89c6\u5316 python draw_net.py --rankdir TB test.prototxt test.png","title":"Caffe"},{"location":"caffe/#caffe-paython","text":"export PYTHONPATH=$HOME/Downloads/caffe/python:$PYTHONPATH","title":"caffe paython \u73af\u5883\u914d\u7f6e"},{"location":"caffe/#caffe","text":"git clone https://github.com/BVLC/caffe.git","title":"caffe \u5b89\u88c5"},{"location":"caffe/#_1","text":"sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler sudo apt-get install --no-install-recommends libboost-all-dev sudo apt-get install libopenblas-dev sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev","title":"\u5b89\u88c5\u4f9d\u8d56"},{"location":"caffe/#makefileconfig","text":"## Refer to http://caffe.berkeleyvision.org/installation.html # Contributions simplifying and improving our build system are welcome! # cuDNN acceleration switch (uncomment to build with cuDNN). # USE_CUDNN := 1 # CPU-only switch (uncomment to build without GPU support). # CPU_ONLY := 1 # uncomment to disable IO dependencies and corresponding data layers # USE_OPENCV := 0 # USE_LEVELDB := 0 # USE_LMDB := 0 # uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary) # You should not set this flag if you will be reading LMDBs with any # possibility of simultaneous read and write # ALLOW_LMDB_NOLOCK := 1 # Uncomment if you're using OpenCV 3 OPENCV_VERSION := 3 # To customize your choice of compiler, uncomment and set the following. # N.B. the default for Linux is g++ and the default for OSX is clang++ # CUSTOM_CXX := g++ # CUDA directory contains bin/ and lib/ directories that we need. CUDA_DIR := /usr/local/cuda # On Ubuntu 14.04, if cuda tools are installed via # \"sudo apt-get install nvidia-cuda-toolkit\" then use this instead: # CUDA_DIR := /usr # CUDA architecture setting: going with all of them. # For CUDA < 6.0, comment the lines after *_35 for compatibility. CUDA_ARCH := -gencode arch=compute_30,code=sm_30 \\ -gencode arch=compute_35,code=sm_35 \\ -gencode arch=compute_50,code=sm_50 \\ -gencode arch=compute_52,code=sm_52 \\ -gencode arch=compute_61,code=sm_61 \\ -gencode arch=compute_61,code=compute_61 # -gencode arch=compute_20,code=sm_20 \\ # -gencode arch=compute_20,code=sm_21 \\ # BLAS choice: # atlas for ATLAS (default) # mkl for MKL # open for OpenBlas # BLAS := atlas BLAS := open # Custom (MKL/ATLAS/OpenBLAS) include and lib directories. # Leave commented to accept the defaults for your choice of BLAS # (which should work)! # BLAS_INCLUDE := /path/to/your/blas # BLAS_LIB := /path/to/your/blas # Homebrew puts openblas in a directory that is not on the standard search path # BLAS_INCLUDE := $(shell brew --prefix openblas)/include # BLAS_LIB := $(shell brew --prefix openblas)/lib # This is required only if you will compile the matlab interface. # MATLAB directory should contain the mex binary in /bin. # MATLAB_DIR := /usr/local # MATLAB_DIR := /Applications/MATLAB_R2012b.app # NOTE: this is required only if you will compile the python interface. # We need to be able to find Python.h and numpy/arrayobject.h. PYTHON_INCLUDE := /usr/include/python2.7 \\ /usr/lib/python2.7/dist-packages/numpy/core/include # Anaconda Python distribution is quite popular. Include path: # Verify anaconda location, sometimes it's in root. # ANACONDA_HOME := $(HOME)/anaconda2 # PYTHON_INCLUDE := $(ANACONDA_HOME)/include \\ $(ANACONDA_HOME)/include/python2.7 \\ $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include \\ # Uncomment to use Python 3 (default is Python 2) # PYTHON_LIBRARIES := boost_python3 python3.5m # PYTHON_INCLUDE := /usr/include/python3.5m \\ # /usr/lib/python3.5/dist-packages/numpy/core/include # We need to be able to find libpythonX.X.so or .dylib. PYTHON_LIB := /usr/lib # PYTHON_LIB := $(ANACONDA_HOME)/lib # Homebrew installs numpy in a non standard path (keg only) # PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include # PYTHON_LIB += $(shell brew --prefix numpy)/lib # Uncomment to support layers written in Python (will link against Python libs) # WITH_PYTHON_LAYER := 1 # Whatever else you find you need goes here. INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial/ LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu/hdf5/serial # If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies # INCLUDE_DIRS += $(shell brew --prefix)/include # LIBRARY_DIRS += $(shell brew --prefix)/lib # Uncomment to use `pkg-config` to specify OpenCV library paths. # (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.) # USE_PKG_CONFIG := 1 # N.B. both build and distribute dirs are cleared on `make clean` BUILD_DIR := build DISTRIBUTE_DIR := distribute # Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171 # DEBUG := 1 # The ID of the GPU that 'make runtest' will use to run unit tests. TEST_GPUID := 0 # enable pretty build (comment to see full commands) Q ?= @","title":"Makefile.config"},{"location":"caffe/#_2","text":"sudo make -j8\\ sudo make distribute","title":"\u5b89\u88c5"},{"location":"caffe/#_3","text":"CMakeLists.txt set(CAFFE_PATH \"$ENV{HOME}/Disk/caffe/distribute\") if (EXISTS \"${CAFFE_PATH}\") include_directories( include ${catkin_INCLUDE_DIRS} ${OpenCV_INCLUDE_DIRS} ${CAFFE_PATH}/include ) target_link_libraries(lidar_cnn_seg_detect ${catkin_LIBRARIES} ${OpenCV_LIBRARIES} ${CUDA_LIBRARIES} ${CAFFE_PATH}/lib/libcaffe.so glog )","title":"\u5e94\u7528"},{"location":"caffe/#caffe-python-layer","text":"layer tuozhan","title":"caffe python layer"},{"location":"caffe/#the-models-are-defined-in-plaintext-protocol-buffer-schema-prototxt-while-the-learned-models-are-serialized-as-binary-protocol-buffer-binaryproto-caffemodel-files","text":"caffe \u8bad\u7ec3\u9700\u8981\u6570\u636e\u96c6\uff0c\u7f51\u7edc.prototxt \u548c solver.prototxt","title":"The models are defined in plaintext protocol buffer schema (prototxt) while the learned models are serialized as binary protocol buffer (binaryproto) .caffemodel files."},{"location":"caffe/#caffe_1","text":"python draw_net.py --rankdir TB test.prototxt test.png","title":"caffe \u7f51\u7edc\u53ef\u89c6\u5316"},{"location":"deeplearning/","text":"LeNet use convolution to extract spatial features. subsample using spatial average of maps. non-linearity in the form of tanh or sigmoids. multi-layer neural network(MLP) as final classifier sparse connection matrix between layers to avoid large computational cost AlexNet use of rectified linear units(ReLU) as non-linearities. use of dropout technique to selectively ignore single neurons during training , a way to avoid overfitting of the model. overlapping max pooling, avoiding the averaging effects of average pooling. using GPUs to reduce training time xception Computing the mean and std of dataset import tensorflow as tf from PIL import ImageStat class Stats(ImageStat.Stat): def __add__(self, other): return Stats(list(map(add, self.h, other.h))) loader = DataLoader(dataset, batch_size=10, num_workers=5) statistics = None for data in loader: for b in range(data.shape[0]): if statistics is None: statistics = Stats(tf.to_pil_image(data[b])) else: statistics += Stats(tf.to_pil_image(data[b])) print(f'mean:{statistics.mean}, std:{statistics.stddev}') pyx to so python setup.py build_ext --inplace \u5f7b\u5e95\u5220\u9664cuda sudo apt-get purge nvidia* sudo apt-get autoremove sudo apt-get autoclean sudo rm -rf /usr/local/cuda* Steps to Install PyTorch With CUDA 10.0 conda install pytorch torchvision cudatoolkit=10.0 -c pytorch \\ pip install torchsummary conda install matplotlib conda install -c conda-forge matplotlib conda install tensorflow object_detection api conda install -c conda-forge tf_object_detection ModuleNotFoundError: No module named 'deployment' from slim.deployment import model_deploy ModuleNotFoundError: No module named 'slim' conda install -c conda-forge tf-slim ModuleNotFoundError: No module named 'nets' change frome nets to from slim.nets creating trainval.txt import os import random import sys if len(sys.argv) < 2: print(\"no directory specified, please input target directory\") exit() root_path = sys.argv[1] xmlfilepath = root_path + '/Annotations' txtsavepath = root_path + '/ImageSets/Main' if not os.path.exists(root_path): print(\"cannot find such directory: \" + root_path) exit() if not os.path.exists(txtsavepath): os.makedirs(txtsavepath) trainval_percent = 0.9 train_percent = 0.8 total_xml = os.listdir(xmlfilepath) num = len(total_xml) list = range(num) tv = int(num * trainval_percent) tr = int(tv * train_percent) trainval = random.sample(list, tv) train = random.sample(trainval, tr) print(\"train and val size:\", tv) print(\"train size:\", tr) ftrainval = open(txtsavepath + '/trainval.txt', 'w') ftest = open(txtsavepath + '/test.txt', 'w') ftrain = open(txtsavepath + '/train.txt', 'w') fval = open(txtsavepath + '/val.txt', 'w') for i in list: name = total_xml[i][:-4] + '\\n' if i in trainval: ftrainval.write(name) if i in train: ftrain.write(name) else: fval.write(name) else: ftest.write(name) ftrainval.close() ftrain.close() fval.close() ftest.close() creating TFR datasets import hashlib import io import logging import os import random import re from lxml import etree import PIL.Image import tensorflow as tf from object_detection.utils import dataset_util from object_detection.utils import label_map_util flags = tf.app.flags flags.DEFINE_string('data_dir', '', 'Root directory to raw pet dataset.') flags.DEFINE_string('output_dir', '', 'Path to directory to output TFRecords.') flags.DEFINE_string('label_map_path', 'data/pet_label_map.pbtxt', 'Path to label map proto') FLAGS = flags.FLAGS def get_class_name_from_filename(file_name): \"\"\"Gets the class name from a file. Args: file_name: The file name to get the class name from. ie. \"american_pit_bull_terrier_105.jpg\" Returns: A string of the class name. \"\"\" print(file_name) match = re.match(r'([A-Za-z_]+)(_[0-9]+\\.jpg)', file_name, re.I) return match.groups()[0] def dict_to_tf_example(data, label_map_dict, image_subdirectory, ignore_difficult_instances=False): \"\"\"Convert XML derived dict to tf.Example proto. Notice that this function normalizes the bounding box coordinates provided by the raw data. Args: data: dict holding PASCAL XML fields for a single image (obtained by running dataset_util.recursive_parse_xml_to_dict) label_map_dict: A map from string label names to integers ids. image_subdirectory: String specifying subdirectory within the Pascal dataset directory holding the actual image data. ignore_difficult_instances: Whether to skip difficult instances in the dataset (default: False). Returns: example: The converted tf.Example. Raises: ValueError: if the image pointed to by data['filename'] is not a valid JPEG \"\"\" img_path = os.path.join(image_subdirectory, data['filename']) with tf.gfile.GFile(img_path, 'rb') as fid: encoded_jpg = fid.read() encoded_jpg_io = io.BytesIO(encoded_jpg) image = PIL.Image.open(encoded_jpg_io) if image.format != 'JPEG': raise ValueError('Image format not JPEG') key = hashlib.sha256(encoded_jpg).hexdigest() width = int(data['size']['width']) height = int(data['size']['height']) xmin = [] ymin = [] xmax = [] ymax = [] classes = [] classes_text = [] truncated = [] poses = [] difficult_obj = [] for obj in data['object']: difficult = bool(int(obj['difficult'])) if ignore_difficult_instances and difficult: continue difficult_obj.append(int(difficult)) xmin.append(float(obj['bndbox']['xmin']) / width) ymin.append(float(obj['bndbox']['ymin']) / height) xmax.append(float(obj['bndbox']['xmax']) / width) ymax.append(float(obj['bndbox']['ymax']) / height) class_name = get_class_name_from_filename(data['filename']) classes_text.append(class_name.encode('utf8')) classes.append(label_map_dict[class_name]) truncated.append(int(obj['truncated'])) poses.append(obj['pose'].encode('utf8')) example = tf.train.Example(features=tf.train.Features(feature={ 'image/height': dataset_util.int64_feature(height), 'image/width': dataset_util.int64_feature(width), 'image/filename': dataset_util.bytes_feature( data['filename'].encode('utf8')), 'image/source_id': dataset_util.bytes_feature( data['filename'].encode('utf8')), 'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')), 'image/encoded': dataset_util.bytes_feature(encoded_jpg), 'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')), 'image/object/bbox/xmin': dataset_util.float_list_feature(xmin), 'image/object/bbox/xmax': dataset_util.float_list_feature(xmax), 'image/object/bbox/ymin': dataset_util.float_list_feature(ymin), 'image/object/bbox/ymax': dataset_util.float_list_feature(ymax), 'image/object/class/text': dataset_util.bytes_list_feature(classes_text), 'image/object/class/label': dataset_util.int64_list_feature(classes), 'image/object/difficult': dataset_util.int64_list_feature(difficult_obj), 'image/object/truncated': dataset_util.int64_list_feature(truncated), 'image/object/view': dataset_util.bytes_list_feature(poses), })) return example def create_tf_record(output_filename, label_map_dict, annotations_dir, image_dir, examples): \"\"\"Creates a TFRecord file from examples. Args: output_filename: Path to where output file is saved. label_map_dict: The label map dictionary. annotations_dir: Directory where annotation files are stored. image_dir: Directory where image files are stored. examples: Examples to parse and save to tf record. \"\"\" writer = tf.python_io.TFRecordWriter(output_filename) for idx, example in enumerate(examples): if idx % 100 == 0: logging.info('On image %d of %d', idx, len(examples)) # path = os.path.join(annotations_dir, 'xmls', example + '.xml') path = os.path.join(annotations_dir, example + '.xml') if not os.path.exists(path): logging.warning('Could not find %s, ignoring example.', path) continue with tf.gfile.GFile(path, 'r') as fid: xml_str = fid.read() xml = etree.fromstring(xml_str) data = dataset_util.recursive_parse_xml_to_dict(xml)['annotation'] tf_example = dict_to_tf_example(data, label_map_dict, image_dir) writer.write(tf_example.SerializeToString()) writer.close() # TODO: Add test for pet/PASCAL main files. def main(_): data_dir = FLAGS.data_dir label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path) logging.info('Reading from Pet dataset.') image_dir = os.path.join(data_dir, 'images') annotations_dir = os.path.join(data_dir, 'annotations') examples_path = os.path.join(annotations_dir, 'trainval.txt') examples_list = dataset_util.read_examples_list(examples_path) # Test images are not included in the downloaded data set, so we shall perform # our own split. random.seed(42) random.shuffle(examples_list) num_examples = len(examples_list) num_train = int(0.7 * num_examples) train_examples = examples_list[:num_train] val_examples = examples_list[num_train:] logging.info('%d training and %d validation examples.', len(train_examples), len(val_examples)) train_output_path = os.path.join(FLAGS.output_dir, 'pet_train.record') val_output_path = os.path.join(FLAGS.output_dir, 'pet_val.record') create_tf_record(train_output_path, label_map_dict, annotations_dir, image_dir, train_examples) create_tf_record(val_output_path, label_map_dict, annotations_dir, image_dir, val_examples) if __name__ == '__main__': tf.app.run() python object_detection/dataset_tools/create_pet_tf_record.py \\ --label_map_path=object_detection/data/pet_label_map.pbtxt \\ --data_dir=`pwd` \\ --output_dir=`pwd` create model config file # Faster R-CNN with Resnet-101 (v1), configuration for MSCOCO Dataset. # Users should configure the fine_tune_checkpoint field in the train config as # well as the label_map_path and input_path fields in the train_input_reader and # eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that # should be configured. model { faster_rcnn { num_classes: 90 image_resizer { keep_aspect_ratio_resizer { min_dimension: 600 max_dimension: 1024 } } feature_extractor { type: 'faster_rcnn_resnet101' first_stage_features_stride: 16 } first_stage_anchor_generator { grid_anchor_generator { scales: [0.25, 0.5, 1.0, 2.0] aspect_ratios: [0.5, 1.0, 2.0] height_stride: 16 width_stride: 16 } } first_stage_box_predictor_conv_hyperparams { op: CONV regularizer { l2_regularizer { weight: 0.0 } } initializer { truncated_normal_initializer { stddev: 0.01 } } } first_stage_nms_score_threshold: 0.0 first_stage_nms_iou_threshold: 0.7 first_stage_max_proposals: 300 first_stage_localization_loss_weight: 2.0 first_stage_objectness_loss_weight: 1.0 initial_crop_size: 14 maxpool_kernel_size: 2 maxpool_stride: 2 second_stage_box_predictor { mask_rcnn_box_predictor { use_dropout: false dropout_keep_probability: 1.0 fc_hyperparams { op: FC regularizer { l2_regularizer { weight: 0.0 } } initializer { variance_scaling_initializer { factor: 1.0 uniform: true mode: FAN_AVG } } } } } second_stage_post_processing { batch_non_max_suppression { score_threshold: 0.0 iou_threshold: 0.6 max_detections_per_class: 100 max_total_detections: 300 } score_converter: SOFTMAX } second_stage_localization_loss_weight: 2.0 second_stage_classification_loss_weight: 1.0 } } train_config: { batch_size: 1 optimizer { momentum_optimizer: { learning_rate: { manual_step_learning_rate { initial_learning_rate: 0.0003 schedule { step: 900000 learning_rate: .00003 } schedule { step: 1200000 learning_rate: .000003 } } } momentum_optimizer_value: 0.9 } use_moving_average: false } gradient_clipping_by_norm: 10.0 fine_tune_checkpoint: \"PATH_TO_BE_CONFIGURED/model.ckpt\" from_detection_checkpoint: true data_augmentation_options { random_horizontal_flip { } } } train_input_reader: { tf_record_input_reader { input_path: \"PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100\" } label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\" } eval_config: { num_examples: 8000 # Note: The below line limits the evaluation process to 10 evaluations. # Remove the below line to evaluate indefinitely. max_evals: 10 } eval_input_reader: { tf_record_input_reader { input_path: \"PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010\" } label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\" shuffle: false num_readers: 1 } train python train.py --logtostderr --train_dir=/home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/train_dir --pipeline_config_path=/home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/net.config tensorboard --logdir /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/train_dir/ evaluator python new_eval.py --logtostderr --checkpoint_dir /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/train_dir --eval_dir /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/eval_dir --pipeline_config_path /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/net.config tensorboard --logdir /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/eval_dir/ # new_eval.py import functools import os import tensorflow.compat.v1 as tf from tensorflow.python.util.deprecation import deprecated from object_detection.builders import dataset_builder from object_detection.builders import graph_rewriter_builder from object_detection.builders import model_builder from object_detection.legacy import evaluator from object_detection.utils import config_util from object_detection.utils import label_map_util from tensorflow.compat.v1 import ConfigProto from tensorflow.compat.v1 import InteractiveSession import keras.backend as K config = ConfigProto() config.gpu_options.allow_growth = True session = InteractiveSession(config=config) K.set_session(session) tf.logging.set_verbosity(tf.logging.INFO) flags = tf.app.flags flags.DEFINE_boolean('eval_training_data', False, 'If training data should be evaluated for this job.') flags.DEFINE_string( 'checkpoint_dir', '', 'Directory containing checkpoints to evaluate, typically ' 'set to `train_dir` used in the training job.') flags.DEFINE_string('eval_dir', '', 'Directory to write eval summaries to.') flags.DEFINE_string( 'pipeline_config_path', '', 'Path to a pipeline_pb2.TrainEvalPipelineConfig config ' 'file. If provided, other configs are ignored') flags.DEFINE_string('eval_config_path', '', 'Path to an eval_pb2.EvalConfig config file.') flags.DEFINE_string('input_config_path', '', 'Path to an input_reader_pb2.InputReader config file.') flags.DEFINE_string('model_config_path', '', 'Path to a model_pb2.DetectionModel config file.') flags.DEFINE_boolean( 'run_once', False, 'Option to only run a single pass of ' 'evaluation. Overrides the `max_evals` parameter in the ' 'provided config.') FLAGS = flags.FLAGS @deprecated(None, 'Use object_detection/model_main.py.') def main(unused_argv): assert FLAGS.checkpoint_dir, '`checkpoint_dir` is missing.' assert FLAGS.eval_dir, '`eval_dir` is missing.' tf.gfile.MakeDirs(FLAGS.eval_dir) if FLAGS.pipeline_config_path: configs = config_util.get_configs_from_pipeline_file( FLAGS.pipeline_config_path) tf.gfile.Copy( FLAGS.pipeline_config_path, os.path.join(FLAGS.eval_dir, 'pipeline.config'), overwrite=True) else: configs = config_util.get_configs_from_multiple_files( model_config_path=FLAGS.model_config_path, eval_config_path=FLAGS.eval_config_path, eval_input_config_path=FLAGS.input_config_path) for name, config in [('model.config', FLAGS.model_config_path), ('eval.config', FLAGS.eval_config_path), ('input.config', FLAGS.input_config_path)]: tf.gfile.Copy(config, os.path.join(FLAGS.eval_dir, name), overwrite=True) model_config = configs['model'] eval_config = configs['eval_config'] input_config = configs['eval_input_config'] if FLAGS.eval_training_data: input_config = configs['train_input_config'] model_fn = functools.partial( model_builder.build, model_config=model_config, is_training=False) def get_next(config): return dataset_builder.make_initializable_iterator( dataset_builder.build(config)).get_next() create_input_dict_fn = functools.partial(get_next, input_config) categories = label_map_util.create_categories_from_labelmap( input_config.label_map_path) if FLAGS.run_once: eval_config.max_evals = 1 graph_rewriter_fn = None if 'graph_rewriter_config' in configs: graph_rewriter_fn = graph_rewriter_builder.build( configs['graph_rewriter_config'], is_training=False) evaluator.evaluate( create_input_dict_fn, model_fn, eval_config, categories, FLAGS.checkpoint_dir, FLAGS.eval_dir, graph_hook_fn=graph_rewriter_fn) if __name__ == '__main__': tf.app.run() from object_detection import evaluator ImportError: cannot import name 'evaluator' \\ from object_detection.legacy import evaluator control trainning steps num_steps:600 tensorflow freeze # From tensorflow/models/research/ INPUT_TYPE=image_tensor PIPELINE_CONFIG_PATH={path to pipeline config file} TRAINED_CKPT_PREFIX={path to model.ckpt} EXPORT_DIR={path to folder that will be used for export} python object_detection/export_inference_graph.py \\ --input_type=${INPUT_TYPE} \\ --pipeline_config_path=${PIPELINE_CONFIG_PATH} \\ --trained_checkpoint_prefix=${TRAINED_CKPT_PREFIX} \\ --output_directory=${EXPORT_DIR} model:save and load model.fit(x_train, y_train, epochs = 150, batch_size = 32,callbacks=[tensorboard_callback]) model.save('./models/model.h5') model.save_weights('./models/weights.h5') model_path = './models/model.h5' model_weights_path = './models/weights.h5' model = load_model(model_path) model.load_weights(model_weights_path) array = model.predict(point_set) or tf.keras.models.save_model( model, 'models/mymode', overwrite=True, include_optimizer=True ) model = tf.keras.models.load_model('./models/mymode') cudnn\u5931\u8d25 from tensorflow.compat.v1 import ConfigProto from tensorflow.compat.v1 import InteractiveSession import keras.backend as K K.set_image_data_format('channels_last') K.set_learning_phase(1) config = ConfigProto() config.gpu_options.allow_growth = True session = InteractiveSession(config=config) K.set_session(session) pb:save and load def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True): graph = session.graph with graph.as_default(): freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or [])) output_names = output_names or [] output_names += [v.op.name for v in tf.global_variables()] input_graph_def = graph.as_graph_def() if clear_devices: for node in input_graph_def.node: node.device = '' frozen_graph = tf.graph_util.convert_variables_to_constants( session, input_graph_def, output_names, freeze_var_names) return frozen_graph frozen_graph = freeze_session(K.get_session(), output_names=[out.op.name for out in model.outputs]) tf.io.write_graph(frozen_graph, './models', 'xor.pbtxt', as_text=True) tf.io.write_graph(frozen_graph, './models', 'xor.pb', as_text=False) detection_graph = tf.Graph() with detection_graph.as_default(): od_graph_def = tf.GraphDef() with tf.gfile.GFile('xor.pb', 'rb') as fid: serialized_graph = fid.read() od_graph_def.ParseFromString(serialized_graph) tf.import_graph_def(od_graph_def, name='') input = tf.get_default_graph().get_tensor_by_name('input_1:0') output = tf.get_default_graph().get_tensor_by_name('fc2/Softmax:0') with detection_graph.as_default(): with tf.Session() as sess: values =sess.run(output, feed_dict={input: point_set}) print(values) \u67e5\u770b\u6a21\u578b\u7684\u8f93\u5165\u5c42\u548c\u8f93\u51fa\u5c42 print('model.inputs :',model.inputs) print('model.outputs : ',model.outputs) output model.inputs : [<tf.Tensor 'input_1:0' shape=(?, 2600, 3) dtype=float32>] model.outputs : [<tf.Tensor 'fc2/Softmax:0' shape=(?, 2) dtype=float32>] \u6240\u4ee5\u8f93\u5165\u5c42\u662f input_1:0 \u8f93\u51fa\u5c42\u662f fc2/Softmax:0 \u5b89\u88c5\uff54\uff45\uff4e\uff53\uff4f\uff52RT \u4e0b\u8f7d\u4e0e\u60a8\u4f7f\u7528\u7684Ubuntu\u7248\u672c\u548cCPU\u67b6\u6784\u5339\u914d\u7684TensorRT\u672c\u5730repo\u6587\u4ef6\u3002 \u4eceDebian\u672c\u5730repo\u8f6f\u4ef6\u5305\u5b89\u88c5TensorRT\u3002 os=\"ubuntu1x04\" tag=\"cudax.x-trt7.x.x.x-ga-yyyymmdd\" sudo dpkg -i nv-tensorrt-repo-${os}-${tag}_1-1_amd64.deb sudo apt-key add /var/nv-tensorrt-repo-${tag}/7fa2af80.pub sudo apt-get update sudo apt-get install tensorrt \u5982\u679c\u4f7f\u7528Python 2.7\uff1a\\ sudo apt-get install python-libnvinfer-dev \\ \u5c06\u5b89\u88c5\u4ee5\u4e0b\u9644\u52a0\u8f6f\u4ef6\u5305\uff1a\\ python-libnvinfer \\ \u5982\u679c\u4f7f\u7528Python 3.x\uff1a\\ sudo apt-get install python3-libnvinfer-dev \\ \u5c06\u5b89\u88c5\u4ee5\u4e0b\u9644\u52a0\u8f6f\u4ef6\u5305\uff1a\\ python3-libnvinfer \\ \u5982\u679c\u60a8\u6253\u7b97\u5c06TensorRT\u4e0eTensorFlow\u7ed3\u5408\u4f7f\u7528\uff1a\\ sudo apt-get install uff-converter-tf \\ \u5982\u679c\u60a8\u8981\u8fd0\u884c\u9700\u8981ONNX\u7684\u793a\u4f8b \u56fe\u5f62\u5916\u79d1\u533b\u751f \u6216\u5c06Python\u6a21\u5757\u7528\u4e8e\u60a8\u81ea\u5df1\u7684\u9879\u76ee\uff0c\u8fd0\u884c\uff1a\\ sudo apt-get install onnx-graphsurgeon \\ anaconda tensorRT \u4e0b\u8f7dtar\u6587\u4ef6 TensorRT-7.0.0.11.Ubuntu-18.04.x86_64-gnu.cuda-10.0.cudnn7.6.tar.gz (wind1) star@xmatrix:~$ (wind1) star@xmatrix:~$ (wind1) star@xmatrix:~$ cd TensorRT (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ ls TensorRT TensorRT_1 TensorRT-7.0.0.11 TensorRT-7.0.0.11.Ubuntu-16.04.x86_64-gnu.cuda-10.0.cudnn7.6.tar.gz TensorRT-7.0.0(1).tar.gz (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ cd TensorRT-7.0.0.11 (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ ls bin data doc graphsurgeon include lib python samples targets TensorRT-Release-Notes.pdf uff (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ cd python (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ ls tensorrt-7.0.0.11-cp27-none-linux_x86_64.whl tensorrt-7.0.0.11-cp35-none-linux_x86_64.whl tensorrt-7.0.0.11-cp37-none-linux_x86_64.whl tensorrt-7.0.0.11-cp34-none-linux_x86_64.whl tensorrt-7.0.0.11-cp36-none-linux_x86_64.whl (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ pip --version pip 19.3.1 from /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages/pip (python 3.6) (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ pip3 --version pip 19.3.1 from /usr/local/lib/python3.5/dist-packages/pip (python 3.5) (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ python -m pip install tensorrt-7.0.0.11-cp36-none-linux_x86_64.whl Processing ./tensorrt-7.0.0.11-cp36-none-linux_x86_64.whl Installing collected packages: tensorrt Successfully installed tensorrt-7.0.0.11 (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ python Python 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) [GCC 7.3.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> >>> >>> import tensorrt >>> >>> exit(); (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ cd ../ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ ls bin data doc graphsurgeon include lib python samples targets TensorRT-Release-Notes.pdf uff (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ cd uff (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ ls uff-0.6.5-py2.py3-none-any.whl (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ python -m pip install uff-0.6.5-py2.py3-none-any.whl Processing ./uff-0.6.5-py2.py3-none-any.whl Requirement already satisfied: numpy>=1.11.0 in /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages (from uff==0.6.5) (1.16.4) Requirement already satisfied: protobuf>=3.3.0 in /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages (from uff==0.6.5) (3.11.2) Requirement already satisfied: six>=1.9 in /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages (from protobuf>=3.3.0->uff==0.6.5) (1.13.0) Requirement already satisfied: setuptools in /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages (from protobuf>=3.3.0->uff==0.6.5) (42.0.2.post20191203) Installing collected packages: uff Successfully installed uff-0.6.5 (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ python Python 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) [GCC 7.3.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> >>> >>> import tensorrt >>> import uff WARNING:tensorflow:From /home/star/anaconda3/envs/wind1/lib/python3.6 7.0.0.11 (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ Using UFF converter to convert the frozen tensorflow model to a UFF file conda activate wjj \\ pip install nvidia-pyindex \\ pip install uff \\ you need to find your uff installed path. \\ import uff print(uff.__path__) And after locating it , in it\u2019s bin folder there should be a script named as convert_to_uff.py . And now you need to open the terminal and simply type python3 convert_to_uff.py < path to the saved model >\\ In my case-->\\ python3 convert_to_uff.py /home/models/catsAndDogs.pb \\ And it will simply save the converted .uff in your .pb model location. And then this is how the next script should be done. NVIDIA\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6 NVIDIA\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6 Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR Use allow_growth memory option in TensorFlow and Keras, before your code. For Keras import tensorflow as tf config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True config.log_device_placement = True sess = tf.compat.v1.Session(config=config) tf.compat.v1.keras.backend.set_session(sess) For TensorFlow import tensorflow as tf config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True config.log_device_placement = True sess = tf.compat.v1.Session(config=config) \u8fd8\u6709\u4e00\u79cd\u60c5\u51b5,\u51cf\u5c11batch size Anchor boxes One of the hardest concepts to grasp\u628a\u63e1 when learning about Convolutional Neural Networks for object detection is the idea of anchor boxes.It is also one of the most important parameters you can tune for improved performance on your dataset.In fact,if anchor oxes are not tuned correctly,your neural network will never even know that certain\u67d0\u4e9b small,large or irregular\u4e0d\u89c4\u5219 objects exist and will never have a chance to detect them.Luckily, there are some simple steps you can take to make sure you do not fall into this trap\u9677\u9631. what are anchor boxes? \\ when you use a neural network like yolo or ssd to predict multiple objects in a picture, the network is actually making thousands of predictions and only showing the ones that it decided were an object.The multiple predictions are output the following format:\\ Prediction 1: (X,Y,Height,Width),Class\\ ...\\ Prediction ~8000: (X,Y,Height,Width),Class Where the (X,Y,Height,Width) is called the \"bounding box\", or box surrounding the objects.This box and the object class are labelled manually by human annotators. In an extremely simplified example, imagine that we have a model that has two predictions and receives the following image:\\ We need to tell our network if each of its predictions is correct or not in order for it to be able to learn.But what do we tell the neural network it prediction should be? Should the predicted class be:\\ Prediction 1:Pear\\ Prediction 2:Apple Or should it be:\\ Prediction 1:Apple\\ Prediction 2:Pear What if the network predicts:\\ Prediction 1:Apple\\ Prediction 2:Apple We need our network's two predictors to be able to tell whether it is their job to predict the pear or the apple.To do this there are a several tools.Predictors can specialize in certain size objects, objects with a certain aspect ratio(tall vs. wide),or objects in different parts of the image.Most networks use all three criteria\u6807\u51c6.In our example of the pear/apple image,we could have Prediction 1 be for objects on the left and Prediction 2 for objects on the right side of the image.Then we would have our answer for what the network should be predict:\\ Prediction 1:Pear\\ Prediction 2:Apple Anchor Boxes in Practice\\ State of the art\u6700\u5148\u8fdb\u7684 object detection systems currently do the following:\\ 1. Create thousands of \"anchor boxes\" or \"prior boxes\" for each predictor that represent the ideal location, shape and size of the object it specializes\u4e13 in predicting. 2. For each anchor box,calculate which object's bounding box has the highest overlap divided by non-overlap.This is called Intersection Over Union or IOU. 3. If the highest IOU is greater than 50%, tell the anchor box that it should detect the object that gave the highest IOU. 4. Otherwise if the IOU is greater than 40%, tell the neural network that the true detection is ambiguous and not to learn from that example. 5. If the highest IOU is less than 40%,then the anchor box should predict that there is no object. This works well in practice and the thousands of predictors do a very good job of deciding whether their type of object apperars in an image. Using the default anchor box configuration can create predictors that are too specialized and ojects that appear in the image may not achieve an IOU of 50% with any of the anchor boxes.In this case, the neural network will never know these objects existed and will never learn to predict them. We can tweak\u8c03\u6574 our anchor boxes to be much smaller In xx net configuration, the smallest anchor box size is 32x32.This means that many objects smaller than this will go undetected. We can fix this by changing our default anchor box configurations. Reducing the smallest anchor box size, all of the things line up with at least one of our anchor boxes and our neural network can learn to detect them! Improving Anchor Box Configuration \\ As a general rule,you should ask yourself the following questions about your dataset before diving into training your model: 1. What is the smallest size box I want to be able to detect? 2. What is the largest size box I want to be able to detect? 3. What are the shapes the box can take? For example, a car detector might have short and wide anchor boxes as long as there is no chance of the car or the camera being turned on its side. You can get a rough estimate of these by actually calculating the most extreme\u6781\u7aef sizes\u3000and aspect ratios in the dataset.Yolo V3 uses K-means to estimate the ideal bounding boxes.Another option is to learn the anchor box configuration. Once you have thought through these questions you can start designing your anchor boxes. Be sure to test them by encoding your ground truth bounding boxes\u5730\u9762\u771f\u503c\u8fb9\u754c\u6846 and then decoding them as though\u5c31\u50cf they were predictions from your model.You should be able to recover the ground truth bounding boxes. Autonomous self-learning systems Preface This report describes the various processes that are part of the work on the main project at Oslo and Akershus University College(HIOA), department of engineering education,spring 2015. The report deals with the development of a self-learning algorithm and a demonstrator in the form of a robot that will avoid static and dynamic objects in an environment that is constantly changing.The thesis\u8bba\u6587 is given by HIOA.The report addresses the theory behind the most well-known and used self-learning algorithms,and discusses the advantages,disadvantages and uses of theese.It also contains a description of the technical solution for the demonstrator, and the method used in this project. The report is written within a topic that is considered new technical and is therefore assumed to be able to be used for futher research and/or learning within autonomous self-learning system. The reader is expected to have basic knowledge in electronics and information technology. We would like to thank our employer.Oslo and Akershus University College,for the opportunity to carry out the project and for financial support.We would also like to thank supervisor EZ for a good collaboration, as well as important and constructive guidance throughout the project period. Summary In today's society, self-learning systems are an increasingly relevant topic.Systems that are not explicity programmed to perform a specific task, but are even able to adapt,can be very useful. The system described in this thesis is realized with Q-learning by both the table method and the neural network.Q-learning is a learning algorithm based on experience.The algorithm involves an agent exploring his environment, where the environment is represented by a number of states. The agent experiments withs the environment by performing an action, and then observes the consequence of that action.The consequence is given in the form of a positive or negative reward. The goal of the method is to maximize the accumulated reward over time. Autonomous self-learning systems are becoming increasingly relevant because the system is able to adapt to partially or completely unknown situations.It learns from experience and needs less information at start-up as it acquires information along the way.In autonomous self-learning systems and self-propelled robots, avoiding obstacles is a key task.This report addresses a demonstrator of such a system, realized with Q-learning presented later in the reportk, and provides a description of the algorithm and results. Theory Before looking at the structure of a self-learning system, one can advantageouly look at what the concept of learning is.Learning is often defined as a lasting change in behavior as a result of experience(St. Olavs Hospital,undated).The property of organisms\u751f\u7269\u4f53 that is defined as learning is one of the cornerstones of what is called intelligence which, among other things, is defined as an ability to acquire and apply knowledge and skills.Humans and animals are considered intelligent, among other things,based on their ability to learn from experience. machine learning This subchapter is based on the theory of S.M,2009.Machine learning is a form of artifical intelligence that focuses on the development of self-learning algorithms. In most cases,self-learning systems deal with parts of natural intellignece, including memory,adaptation and generalization\u6982\u62ec.Unlike traditional non-learning systems, the method makes it possible to construct a system that is able to expand , adapt to new information and learn a given task without being specifically programmed for this.For machine learning,this system is called an agent.By using menmory,an agent can recognize the last time it was in a similiar situation and what action it took.Based on the outcome from the previous time, it can, if it was correct,choose to repeat the action, or try something new.By generallizing, the agent can recognize similarities in different situations, and thus use experienct from one situation and apply this experience in another. In order to realize\u5b9e\u73b0 this concept, machine learning uses principles\u539f\u7406 from statistics, mathematics, physics, neurology and biology. \uff37hen talking about machine learning and self-learning systems,algorithms are mainly the main product.The actual process in these algorithms can be compared to data mining\u6316\u6398.Data mining is a process that analyzes data from different perspectives and summarizes if into useful information.Both methods go through data to find patterns\u6a21\u5f0f,but instead of extractiong data for human interpretation,the information is used to improve the agent's understanding.For the agent to be able to learn, it must know how to improve , and whether it will get better or not.There are 15 more methods to solve this, which in turn provide servel main categories within machine learning:Supervised learning,unsupervised learning and reinforcement learning. Supervised Learning An agent is given a training set with ,for example,pictures of a face and pictures without a face.The agent then prepared through a training process where it gives a forecast of what the picture is of. Whenever the forecast is incorrect, the agent is corrected.This process continues untill the model achieves a desired level of accuracy. Since the algorithm does not have a specific definition of what is a face and what is not, it must therefore learn this using examples.A good algorithm will eventually be able to estimate whether an image is of a face or not.The learning methods are best explained by examples: Unsupervised Learning When learning without supervision, information is not marked.This is ,the system is not told what is the image of a face and what is not.As a result, there is no correction or reward to indicate a potential solution, but the algorithm tries to identify the similarities between the images, swfor then categorize them and divide them into groups. Reinforcement Learning In reinforcement learning,the algorithm is told when the answer it gives is incorrect, but receives no suggestions on how to correct this.It must explore and try out different solutions untill it finds out how it gets the right answers.This is a kind of middle ground of supervised learning and unsupervised learning.Examples could be learning to play a board game or a robot that is going to learn to walk.Each time an agent performs an action,he or she recieves a reward or a penalty, based on how desirable\u53ef\u53d6\u7684 the outcome of the action is.For example.when an agent is trained to play a board game, he gets a positive reward for winning, and a negative reward for losing.All other cases give no reward.\\ \\ The activity mentioned above can be represent as a sequence of state-action reward:\\ \\ This means that the agent was in state s0, performed action a0,which resulted in it receiving reward r1 and ending up in state s1.Furthermore, it performed action a1, received reward r2, and ended up in state s2, and so on.\\ \uff34his sequence is made up of experiences where experience is given as:\\ \\ Experience shows that the agent was in state s,performed action a,received reward r, and ended up in state s', and represented by (s,a,s',r) In order for the agent to be able to learn from the sequences mentioned and thus call it an experience.It has a table called Q-table which acts as its memory.All data points stored in this table are called Q-values and represent how desirable\u53ef\u53d6\u7684 it is to perform a specific action in a specific state. One experience adds one data point Q(s,a) in the table that represents the agents current estimate\u4f30\u8ba1 of the optimal\u6700\u4f73 Q value. It is this information that the agent uses to learn an optimal pattern of action.The size of the table depends on how many conditions and actions are included in the problem you are trying to solve, where the number of conditions gives the number of rows ,while the number of actions gives the number of colums.For example if you have 20 states and 3 actions, you will get a table of 20x3. Before the agent begins to experiment, it knows nothing else what actions it is capable of performing, and the Q table is consequently empty.This is, all Q values are equal to 0. The key to the method described above is to update the Q value that is applied to the agent when it performs an action in a given state in the current data point when it gains an experience. This value is given by the Q function\\ \\ or more clearly:\\ \u03b1\uff0d\uff0dlearning rate\\ \u0393\uff0d\uff0ddiscount factor\\ r\uff0d\uff0dreward\\ The learning rate dicates\u6307\u793a how much of previouly\u5148\u524d acquired learning should be replaced with new information. By \u03b1=1,previous values are replaced with new information.while \u03b1=0 ,corresponds to no update.In other words,the agent will konw by \u03b1=1,assume that the last reward and resulting state are representative of future values. The discount factor indicates the weight of all futher step rewards.If \u0393=0,the agent will only consider current rewards, while it will think more long-term\u957f\u671f and strive\u52aa\u529b for higher future rewards as \u0393 approaches 1. The reward is defined when you create the program in the form of reward functions\u5956\u52b1\u662f\u5728\u521b\u5efa\u7a0b\u5e8f\u65f6\u4ee5\u5956\u52b1\u51fd\u6570\u7684\u5f62\u5f0f\u5b9a\u4e49\u7684,and can be positive or negative. What this reward value is defined as is not critical.On the other hand, It is important that there is a clear distinction\u533a\u522b between the reward for good deeds\u884c\u4e3a and the reward for bad deeds. When a new experience is added to the algorithm, a new Q value is estimated, and the old value of the Q table for the last experience is updated with the new one. Assume that the agent is a mouse that is placed in a room divded into six states, see below Figure.In state s6 there is a piece of cheese,while in state s5 there is a cat.If the agent finds the piece of cheese, it receives a reward of 10, while it receives a reward of -10 if it moves to the state where the cat is.These two states are called terminal states.The agent explores the environment untill one of the terminal states is reached,before starting a new iteration(attempt)\\ \uff34he agent can perform four actions: up,down,left,right.If it moves in a direction where there is a wall, it gets a reward of -1, and it remains in the same state. All other conditions have a value equal to 0.This is implemented in the algorithm using reward functions.\\ \\ The agent has no information about his surroundings other than that there are six states and four acitons, what state he is in at any given time and any rewards it receives as actions are performed.Nor does it know what a reward or punishment is.The Q table is illustrated in Table 1. By assuming the following sequences of experiences,(s,a,r,s') one can illustrate how a sequence updates the Q-table. For the sake of illustration, the learning rate \u03b1 and the discount factor \u0393 set equal to 1 and all Q-values have an initial value equal to 0.\\ After this sequence of experiences, the Q table with updated values will look like this:\\ \\ opp=up ned=down venstre=left h\u00d8yre=right Each experience will update the value of the state table for the performed state and action combination, and the Q values will converge\u6c47\u805a to optimal values over time.The more experiments the agent conducts\u884c\u4e3a, the better the estimates in the Qtable.In theory, the agent will eventually achieve an optimized Q-table, and will be able to choose the shortest path to the cheese each time, regardless of the starting state. Exploration or Utilize \u52d8\u63a2\u6216\u5f00\u53d1 One of the challenges with the Q-learning algorithm is that it does not directly tell what the agent should do, but rather functions as a tool that shows the agent what the optimal action is in a given condition.This is a challenge because the agent must explore the environment enough times to be able to build a solid foundation\u575a\u5b9e\u7684\u57fa\u7840 that provides a good estimate of the Q values.To address this, an exploration feature is implemented that determines the strategy\u6218\u7565 the agent will use to select actions, either explore or exploit. Exploration:The agent is bold\u80c6\u5927 and does not necessary choose the best course of action, with the intension of establishing a better estimate of the Q values. Utilize: The agent utilizes the experience it has already built up, and selects the optimal action for the condition it is in. That is, the action that gives the highest Q value. It is said that the agent is greedy\u8d2a\u5a6a The purpose of this feature is to establish a relationship between the two strategies. It is important to explore enough so that the agent builds a solid foundation of the Q-values, but it is also important to utilize the already acquired knowledge to ensure the highest possible reward. There are a number of ways to accomplish this, but two of the most popular methods are the greedy feature and the Boltzmann distribution, popularly called \"softmax\": \u03b5-greedy \u03b5-greedy is a strategy based on always choosing the optimal action except for \u03b5 of the aisles and choosing a random action of the aisles.\\ \\ there\\ a* is optimal action with probability 1-\u03b5\\ ar is random action with probability \u03b5\\ where\\ 0< \u03b5 < 1 It is possible to change over time. This allows the agent to choose a more random strategy at the beginning of the learning process, and then become more greedy as it acquires more knowledge. One of the challenges with greed is that it considers all actions except the optimal one as equal.\u5b83\u5c06\u6700\u4f73\u884c\u52a8\u4e4b\u5916\u7684\u6240\u6709\u884c\u52a8\u90fd\u89c6\u4e3a\u5e73\u7b49 If, in addition to an optimal action, there are two good actions as well as additional actions that look less promising,\u8fd8\u6709\u4e24\u4e2a\u597d\u7684\u52a8\u4f5c\u4ee5\u53ca\u770b\u8d77\u6765\u4e0d\u592a\u6709\u524d\u9014\u7684\u5176\u4ed6\u52a8\u4f5c\uff0c it will make sense\u6709\u610f\u4e49 for the agent to choose one of the two better ones instead of spending time exploring the bad actions. Using greed is as likely to explore a bad action as to explore one of the two better ones. One way to solve this is to use the Boltzmann distribution 2.3.2 Boltzmann distribution This strategy is known as \"softmax\" action selection and involves selecting an action with a probability that depends on the Q value. The probability of selecting action a in state s is proportional to\u3000 ,which means that the agent in state s chooses action a with probability \\ The parameter T specifies how random values are to be selected.\u53c2\u6570T\u6307\u5b9a\u5982\u4f55\u9009\u62e9\u968f\u673a\u503c\u3002 When T has a high value, actions are chosen to approximately the same extent.\u9009\u62e9\u7684\u52a8\u4f5c\u7684\u7a0b\u5ea6\u5927\u81f4\u76f8\u540c As T decreases, the actions with the highest Q value are more likely to be selected, while the best action is always selected when T-> 0 One of the advantages of the Q-learning algorithm is that it is exploration insensitive\u5bf9\u63a2\u7d22\u4e0d\u654f\u611f. That is, the Q values will converge to optimal values regardless of the agent's behavior as long as information is collected. This is one of the reasons why Q-learning is one of the most widely used and effective methods in reinforcement learning. On the other hand, it is a prerequisite that all combinations of condition / action are tried out many enough times, which in most cases means that the exploration and exploitation aspect must be addressed as it is not always possible for an agent to experiment infinitely. There are several methods for setting up an artificial neural network.One of the most common,which is also used in this project, is called a feedforward neural network, which means that the information is always fed from the input page and supplied further through the network.In other words,output from one layer + bias is used as input in the next. Fuzzy Logic\u6a21\u7cca\u7406\u8bba The theory of fuzzy logic is based on S.K.J(2007)\\ Fuzzy logic is a decision-making or management model based on diffuse\u3000\u6269\u6563\uff0c\u6563\u53d1,and input data classified as degrees of membership in a diffuse quantity. A diffuse set has no sharp boundaries, and is a set of values that belong to the same definition, where the definition is a diffuse term often described by an adjective\u5f62\u5bb9\u8bcd. For example: big potatoes or tall people.A diffuse set is also called a semantically\u8bed\u4e49\u4e0a variable or set. Fuzzy logic classifies input data according to how large a membership the input data has to a set.The degree of membership is graded\u5206\u7ea7 between 0 and 1,and determined on the basis of a membership function\u6839\u636e\u96b6\u5c5e\u51fd\u6570\u786e\u5b9a.\uff34he member ship function is determined on the basis of knowledge of the field, and can be non-linear.Input data can have membership in servel different sets, but can only have full membership in one set. Fuzzy logic behavior Diffusion\u6269\u6563: input data is graded according to the diffused quantities,and the diffused quantities are sent to subprocess 2 Logical rules:the diffused quantites are tested against rules.The degree of fulfillment \u5c65\u884c from each rule is sent to sub-process 3.The rules can be implented with {and, if, or},or other methods such as an impact\u5f71\u54cd matrix.The rules describe the relationship between the diffused input and output. Implication: the received data from sub-process 2 is tested againist rules that grade the impact on the overall consequence of each of the received data.The ratings are then sent to sub-process 4. Aggregation\u805a\u5408:all the consequence\u540e\u679c quantities are compiled into an aggregate quantity\u603b\u91cf,which is the union of the quantities from subprocess 3. A vdiffusion: The most representative element is extracted from the aggregate amount.This is usually the center of gravity\u91cd\u529b.That value will then be the control signal. The Fuzzy logic method has a number of advantages in that it is able to process imprecise\u4e0d\u7cbe\u786e data and model non-linear relationships between input and output.The principle enables an accurate and realistic\u73b0\u5b9e\u7684 classification of data, as data is rarely\u5f88\u5c11 only true or false. Technical solution Agents Two versions of the agent were developed.The first version,which was later replaced, was built on the basis of drivetrain\u52a8\u529b\u603b\u6210 and mechanics\u673a\u68b0\u5e08 taken from 'WLtoys' model 'A969'. The radio-controlled car in scale 1:18 was delivered\u5df2\u4ea4\u4ed8 to drive with ratio and battery.The car measured 285x140x105mm(LxWxH) and could run for about 10 minutes on a fully charged battery. Since the car was to be equiped with an Xbee module and communicate via bluetooth.the car's original reciever was removed.The steering servo\u8f6c\u5411\u4f3a\u670d was adapted to the car's receiver and was therefore replaced in favour of a PWM-controllable servo.To improve the agent's running time, the original battery(7.4V/1100mAh) was replaced with a 7.4V/5200mAh \"Fire Bull\" type battery, which would thoretically provide approximately five times as long. To make room for all the electronics, a plate 1.5mm aluminum\u94dd was cut out and mounted on top of the car.The motor driver, microcontroller board and Xbee module were placed together with the motor and servo on the classis itself, while the battery, sensors and voltage regulator were mounted\u5b89\u88c5 on the new plate for easier access. After the mentioned changes, the agent had improved battery life, adjustable speed and the correct physical size, but it turned out not to have a good enough turning radius. It had particular problems in situations where it had to react in one direction, and then change direction. The car's turning radius was tested before the conversion was started, and it was measured that the car need 70cm to turn 90 degrees to the left or right(figure 3-4).It thus needed an area of 140cm in diameter to turn 180 degrees.It was therefore assumed that a round arena\u7ade\u6280\u573a with a diameter of about four meters was needed.The area was built,and it was found that four meters was enough to be able to drive around without obstacles,but that it became too samll when car had to avoid objects. The ultrasonic sensors are limited, through discretion\u901a\u8fc7\u659f\u914c,to integer values between 1cm and 120cm, If one is to create a look-up table for the entire spectrum\u8303\u56f4, the table will consist of 120^5 different combinations, and would with three possible actions end up with a look-up table of 120^5x3 number of places. This is a less good solution as the table would be very large(74 billion seats).A large lookup table means that the agent spends much more time learning all the combinations of conditions, and actions for these.In addition, such a solution is far too large to be implemented on a normal computer.This chanllenge is solved by discretizing\u79bb\u6563\u5316 or dividing, the continuous sensor area into smaller and more managerable state spaces. 4.2.1 Condition space model All sensors can measure distances up to 400 cm. To avoid a large state table and to streamline the learning process, the sensor values are discretized down to 144 states. For the project described in this report, this also simplifies the reward function. Each snsor is divided into four zones based on distance in centimeters. The division is shown in Figure 4.4\\ \\ A condition consists of four parameters. Two for the left side and two for the right side of the agent's field of view.A state is a combination of distance to the nearest obstacle and in which sector the obstacle is located .a sector describes the angle at which an object is located, relative to the agent. The four parameter are defined by:\\ k1:zone\u533a left side \u533a\u57df\u5de6\u4fa7\\ k2:zone right side \u533a\u57df\u53f3\u4fa7\\ k3:sector\u90e8\u95e8 left side \u6247\u533a\u5de6\u4fa7\\ k4:sector right side \u6247\u533a\u53f3\u4fa7\\ k5:observation of dynamic or static obstacle left side \u89c2\u5bdf\u52a8\u6001\u6216\u9759\u6001\u969c\u788d\u7269\u5de6\u4fa7\\ k6:observation of dynamic or static obstacle right side \u89c2\u5bdf\u52a8\u6001\u6216\u9759\u6001\u969c\u788d\u7269\u53f3\u4fa7\\ S(state space model) = k1 x k2 x k3 x k4 x (k5 x k6)\\ The agent can perceive\u611f\u77e5 two obstacles simultaneously, but not on the same page.With two obstacles on the same side,the algorithm will prioritize\u4f18\u5148 the obstacle that is in the zone with the lowest numbering. The division of the sectors is illustrated in Figure below\\ \\ sector\u6247\u533a\\ The sectors have four different values, 0,1,2,3.The sensors perceive\u611f\u77e5 an object when the zone value is less than 2.The sectors are symmetrically\u5bf9\u79f0\u5730 distributed\u5206\u5e03.so that the agent's right field of view is discretized in the same way as the left.Sensors 1,2 and 3 on the left side correspond to sensors 3,4 and 5 for the right side. 4.2.11 Discretion for system with dynamic obstacles In the simulation for a system with dynamic obstacles, the state space model is expanded with two additional parameters,k5 and k6. k5 indicates with the value 1 or 0 whether the obstacle detected on the left side is dynamic, while k6 does the same for the right side.The number 1 indicates a dynamic obstacle,while 0 indicates static.The state table has thus benn expanded to 576 states.,An object os classified as a dynamic obstacle in that the agent first, while standing still,performs two measurements in a row. Then the two measurements are compared, if the obstacle moves away from the agent, the first measurement will be positive number, and classified as a static obstacle.If on the other hand, the same calculation operation gives a negative number, the obstacle is classified as a dynamic obstacle and the agent must react accordingly. 4.2.1.2 the state table The state table is structured as a table where all combinations of states and actions are represented as a table point.The state table for the physical model has 144 rows, and three columns that present each of the possible actions(left,right, straightfoward). A section of the condition table is shown in Table 7. 4.2.2 Reward function The reward feature determines the agent's immmediate reward after each experience.When implementing the reward function, a distinction is made between good actions that give a positive reward value,and bad actions that give a negative reward value.The reward function used for Q-learning with the table method is inspired by and shown under equation.\\ The reward function, r1 is defined to give a positive reward value when the agent drives straight ahead, while for turns a small negative reward value is given. The negative value is given to prioritize \u4f18\u5148 driving straight ahead. The parameter r2 gives a positve reward when the total change in the sensor values is positive. That is , the agent moves away from an obstacle. If the agent moves towards an obstacle , it becomes total the change in sensor values is negatie, and the agent recieve a negative reward. The function r3 gives the agent a negative reward value if it first turns to right and then to the left, or first to the left and then to right one after the other. This feature allows the agent to avoid an indecisive\u4f18\u67d4\u5be1\u65ad\u7684 sequence, where it only swings\u6447\u6446 back and forth. \uff34he function r gives the agent a reward value equal to the sum of r1,r2 and r3 if it does not collide, while in the event of a collision it gets a negative reward value of -100. 4.2.3 Exploration function The exploration function determines the agent's strategy for choosing actions. It can either explore the state space or take advantage of the experiences it already has. The function ensures an appropriate balance between these strategies (see chapter 2.3). The simulations for the exploration methods show that softmax gives the best results in the simulated model, and is thus used in the simulation. The parameter T is set equal to 24, and is reduced by 5% of its total value for each individual experiment.In the physical model, epsilon is used because it provides a better overview of how much coincidence there is for the agent to take an arbitrary action, and epsilon is set at a 5% chance of doing a random action. 4.2.4 Implementation The implementation of Q-learning with the table method is based on the same algorithm and reward function in the simulation and for the physical model.The different between the simulation and the physical agent is the exploration function and an additional extension of the state space in the simulation(see the discretization\u79bb\u6563\u5316 chapter).In the simulation, you have the choice between static and dynamic obstacles, and to adapt the agent to dynamic obstacle, the Q table is expanded. 4.2.4.1 The Q-Learning algorithm with the table method The Q-Learning algorithm with the table method At startup: - initialize state space S - initialize the action list A - initialize the Q table Q(s,a)(empty table) based on the state space S and the action list A For each attempt: 1) Observe initial sensor values 2) Discretize the sensor values and set it equal to s(initial mode) 3) Select an action a based on the exploration function and perform the action 4) Observe the agent's sensor values after the operation 5) Discretize the sensor values and set it equal to s'(Next mode) 6) Give the agent the reward r based on the reward function 7) Update the Q table 8) Set s=s'(Now state) 9) Keep last action, prev_a =a (Used in the reward function) 10) Repeat the operation from point iii)(a new step /experience ) If the agent has not reached its terminal state(collision or maximum number of steps). Start a new attempt if the agent has reached its terminal state(Restart from i). 4.2.4.2 Simulated model In the simulation, a user interface is created that shows the user the simulation of the agent, two different graphs, buttons for different functions and information panel.The information panel show the user relevant\u76f8\u5173\u7684 values from the simulation.One graph shows an average of the number of steps for every tenth attempt, while the other shows the agent's accumulated reward.One step in the program corresponds to an experienced situation,whether it has been experienced before or is new.A new attempt is initialiated each time the agent has moved the maximum number of steps for the current attempt, or has collided with an obstacle .The maximum number of steps can be changed, but is normally 400-600.The dimensions of the simulation are scaled 1\uff1a10[cm] The ultrasonic sensors on the physical agent are limited to integer values from 1 to 120cm in the code of the agents microcontrollers.Distance less than 8 cm id defined as a collision. The initial state of the physical agent is an arbitrary position in the environment.The only requirement is that the initial state is not a terminal state(collision). The commands - command 1 : request sensor values - command 2: ask the agent to drive straight ahead - command 3: ask the agent to turn right - command 4: ask the agent to turn left - command 5: ask the agent to stop, sensor values <8 cm(collision) 4.3 Q-Learning with neural network (Linear function approximation) Q-Learning with neural network is based on approximation of one or more linear Q-functions. Unlike Q-table learning.this method stores the Q functions in a neural network. By using this method ,discretization is not necessary, and the entire continuous sensor spectrum can be used. The behavior behind an artifical neuron is described in chapter 2.3.3\\ neural network \\ \\ input layer---------------------------------hidden layer-------------------------------------output layer The figure above shows the neural network as it is implemented in this system. In the network, all the neurous, including the bias from one layer, are connected to each individual neuron in the next layer. The neural network has three layers.Layer One, the input layer, has five neurons (one for each sensor),layer two has fifteen neurons, and layer three has three neurons according to the number of actions in the action list.Layer 1 is the start of the network, and the sensor values are used as input.and these must be in the interval\u95f4\u9694[-1,1] The weight matrix w(1) is used to weight the link between layer 1 and layer 2.means w(2) for layer 2 and layer 3.The matrices have a dimension of (number of neurons in layer L)x(number of neurons in Layer (l-1) + bias) w(1) has a dimension of 15x6 and w(2) dimension of 3x16. In the output layer, there are three neurons, and each individual neuron corresponds to a Q function.The number of neurons in the output layer is equal to the number of actions of the agent, and each individual neuron estimates a Q function with respect to an action in the agent's action list Equation(21)\\ These Q function estimate a Q value that is futher used to determine the optimal actions for the agent, where the optimal action for each condition is therefore defined as a = max(Q(s,a)). It is impossible to say which Q function are suitable for the system, since the neural network estimates the Q function based on the agent's experience. In order for the agent to achieve the highest total reward, it must therefore explore the environment long enough for the network to converge to an optimal function. \\ The figure shows the learning process of agent. The method needs initial values, and these are given by sensor values and the network's weight matrices.The weight matrices are initiallized with random values between 0 and 1, but not zero.If the weight matrices are initialized with a value equal to 0,all the neurons will have the same value,which means that the neural network will not be usable. This algorithm uses the agent's sensor values, as opposed to the table method which uses a discretization of values. The exploration function used is the same as for the table method, but the reward function is different. The purpose of this method is to find optimal Q functions by updating the weight machines after each experiment. 4.3.1 Reward function Since the agent's state space on the neural network is continuous, an equally elaborate reward function is not required as for the table method.The reward function is defined by the agent receiving a positive reward if it drives straight ahead, and a negative reward in the event of a turn and a collision. The reward value r must be in the interval [-1,1], and the function as defined for the system is shown in equation.\\ 4.3.2 Feedforward As described in Chapter 2 Theory, neural network in this project is a forward-connected neural network. The neurons are activated using the function of the hyperbolic forceps\u53cc\u66f2\u7ebf\u94b3 given in equation. \\ The activated values become output values in this layer, and are used as input for the next layer.The three neurons in the output layer correspond to the Q funcitons when all data is fed into the network. \\ \\ The weight matrix w has m number of rows equal to the number of neurons,and columns n equal to the number of inputs.The matrix w(2) and the vector y are multiplied by each other and the product is used in the activation function described ealier in the chapter.The elements in both the weight matrix and the input vector are real numbers,and the value of the inputs is in the interval[-1,1].Equation(26-28) shows the Q functions of the three actions. The calculations in the figures above can be abbreviated\u7f29\u5199 and shown in equation 27 in vector form.\\ \\ Activation of the hidden layer takes place in the same way as for the output layer, and the input to this layer is the sensor values.\\ Install Pytroch-Gpu versison:1.3.0 conda install -c anaconda pytorch-gpu version:1.8.0 conda install -c conda-forge pytorch-gpu \u51cf\u5c11\u5185\u5b58\u5360\u7528\u7684\u4e00\u79cd\u65b9\u6cd5 The projected depth values in the original depth maps are float32 and the unit is meter (m). However, we don't want to save float32 because it took too much storage. A common technique is that we can convert it to uint16 by int(depth * 256). This keeps certain degree of accuracy but takes less storage. That's why we need to divide the value by 256.","title":"deeplearning"},{"location":"deeplearning/#lenet","text":"use convolution to extract spatial features. subsample using spatial average of maps. non-linearity in the form of tanh or sigmoids. multi-layer neural network(MLP) as final classifier sparse connection matrix between layers to avoid large computational cost","title":"LeNet"},{"location":"deeplearning/#alexnet","text":"use of rectified linear units(ReLU) as non-linearities. use of dropout technique to selectively ignore single neurons during training , a way to avoid overfitting of the model. overlapping max pooling, avoiding the averaging effects of average pooling. using GPUs to reduce training time","title":"AlexNet"},{"location":"deeplearning/#xception","text":"","title":"xception"},{"location":"deeplearning/#computing-the-mean-and-std-of-dataset","text":"import tensorflow as tf from PIL import ImageStat class Stats(ImageStat.Stat): def __add__(self, other): return Stats(list(map(add, self.h, other.h))) loader = DataLoader(dataset, batch_size=10, num_workers=5) statistics = None for data in loader: for b in range(data.shape[0]): if statistics is None: statistics = Stats(tf.to_pil_image(data[b])) else: statistics += Stats(tf.to_pil_image(data[b])) print(f'mean:{statistics.mean}, std:{statistics.stddev}')","title":"Computing the mean and std of dataset"},{"location":"deeplearning/#pyx-to-so","text":"python setup.py build_ext --inplace","title":"pyx to so"},{"location":"deeplearning/#cuda","text":"sudo apt-get purge nvidia* sudo apt-get autoremove sudo apt-get autoclean sudo rm -rf /usr/local/cuda*","title":"\u5f7b\u5e95\u5220\u9664cuda"},{"location":"deeplearning/#steps-to-install-pytorch-with-cuda-100","text":"conda install pytorch torchvision cudatoolkit=10.0 -c pytorch \\ pip install torchsummary","title":"Steps to Install PyTorch With CUDA 10.0"},{"location":"deeplearning/#conda-install-matplotlib","text":"conda install -c conda-forge matplotlib","title":"conda install matplotlib"},{"location":"deeplearning/#conda-install-tensorflow-object_detection-api","text":"conda install -c conda-forge tf_object_detection","title":"conda install tensorflow object_detection api"},{"location":"deeplearning/#modulenotfounderror-no-module-named-deployment","text":"from slim.deployment import model_deploy","title":"ModuleNotFoundError: No module named 'deployment'"},{"location":"deeplearning/#modulenotfounderror-no-module-named-slim","text":"conda install -c conda-forge tf-slim","title":"ModuleNotFoundError: No module named 'slim'"},{"location":"deeplearning/#modulenotfounderror-no-module-named-nets","text":"change frome nets to from slim.nets","title":"ModuleNotFoundError: No module named 'nets'"},{"location":"deeplearning/#creating-trainvaltxt","text":"import os import random import sys if len(sys.argv) < 2: print(\"no directory specified, please input target directory\") exit() root_path = sys.argv[1] xmlfilepath = root_path + '/Annotations' txtsavepath = root_path + '/ImageSets/Main' if not os.path.exists(root_path): print(\"cannot find such directory: \" + root_path) exit() if not os.path.exists(txtsavepath): os.makedirs(txtsavepath) trainval_percent = 0.9 train_percent = 0.8 total_xml = os.listdir(xmlfilepath) num = len(total_xml) list = range(num) tv = int(num * trainval_percent) tr = int(tv * train_percent) trainval = random.sample(list, tv) train = random.sample(trainval, tr) print(\"train and val size:\", tv) print(\"train size:\", tr) ftrainval = open(txtsavepath + '/trainval.txt', 'w') ftest = open(txtsavepath + '/test.txt', 'w') ftrain = open(txtsavepath + '/train.txt', 'w') fval = open(txtsavepath + '/val.txt', 'w') for i in list: name = total_xml[i][:-4] + '\\n' if i in trainval: ftrainval.write(name) if i in train: ftrain.write(name) else: fval.write(name) else: ftest.write(name) ftrainval.close() ftrain.close() fval.close() ftest.close()","title":"creating trainval.txt"},{"location":"deeplearning/#creating-tfr-datasets","text":"import hashlib import io import logging import os import random import re from lxml import etree import PIL.Image import tensorflow as tf from object_detection.utils import dataset_util from object_detection.utils import label_map_util flags = tf.app.flags flags.DEFINE_string('data_dir', '', 'Root directory to raw pet dataset.') flags.DEFINE_string('output_dir', '', 'Path to directory to output TFRecords.') flags.DEFINE_string('label_map_path', 'data/pet_label_map.pbtxt', 'Path to label map proto') FLAGS = flags.FLAGS def get_class_name_from_filename(file_name): \"\"\"Gets the class name from a file. Args: file_name: The file name to get the class name from. ie. \"american_pit_bull_terrier_105.jpg\" Returns: A string of the class name. \"\"\" print(file_name) match = re.match(r'([A-Za-z_]+)(_[0-9]+\\.jpg)', file_name, re.I) return match.groups()[0] def dict_to_tf_example(data, label_map_dict, image_subdirectory, ignore_difficult_instances=False): \"\"\"Convert XML derived dict to tf.Example proto. Notice that this function normalizes the bounding box coordinates provided by the raw data. Args: data: dict holding PASCAL XML fields for a single image (obtained by running dataset_util.recursive_parse_xml_to_dict) label_map_dict: A map from string label names to integers ids. image_subdirectory: String specifying subdirectory within the Pascal dataset directory holding the actual image data. ignore_difficult_instances: Whether to skip difficult instances in the dataset (default: False). Returns: example: The converted tf.Example. Raises: ValueError: if the image pointed to by data['filename'] is not a valid JPEG \"\"\" img_path = os.path.join(image_subdirectory, data['filename']) with tf.gfile.GFile(img_path, 'rb') as fid: encoded_jpg = fid.read() encoded_jpg_io = io.BytesIO(encoded_jpg) image = PIL.Image.open(encoded_jpg_io) if image.format != 'JPEG': raise ValueError('Image format not JPEG') key = hashlib.sha256(encoded_jpg).hexdigest() width = int(data['size']['width']) height = int(data['size']['height']) xmin = [] ymin = [] xmax = [] ymax = [] classes = [] classes_text = [] truncated = [] poses = [] difficult_obj = [] for obj in data['object']: difficult = bool(int(obj['difficult'])) if ignore_difficult_instances and difficult: continue difficult_obj.append(int(difficult)) xmin.append(float(obj['bndbox']['xmin']) / width) ymin.append(float(obj['bndbox']['ymin']) / height) xmax.append(float(obj['bndbox']['xmax']) / width) ymax.append(float(obj['bndbox']['ymax']) / height) class_name = get_class_name_from_filename(data['filename']) classes_text.append(class_name.encode('utf8')) classes.append(label_map_dict[class_name]) truncated.append(int(obj['truncated'])) poses.append(obj['pose'].encode('utf8')) example = tf.train.Example(features=tf.train.Features(feature={ 'image/height': dataset_util.int64_feature(height), 'image/width': dataset_util.int64_feature(width), 'image/filename': dataset_util.bytes_feature( data['filename'].encode('utf8')), 'image/source_id': dataset_util.bytes_feature( data['filename'].encode('utf8')), 'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')), 'image/encoded': dataset_util.bytes_feature(encoded_jpg), 'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')), 'image/object/bbox/xmin': dataset_util.float_list_feature(xmin), 'image/object/bbox/xmax': dataset_util.float_list_feature(xmax), 'image/object/bbox/ymin': dataset_util.float_list_feature(ymin), 'image/object/bbox/ymax': dataset_util.float_list_feature(ymax), 'image/object/class/text': dataset_util.bytes_list_feature(classes_text), 'image/object/class/label': dataset_util.int64_list_feature(classes), 'image/object/difficult': dataset_util.int64_list_feature(difficult_obj), 'image/object/truncated': dataset_util.int64_list_feature(truncated), 'image/object/view': dataset_util.bytes_list_feature(poses), })) return example def create_tf_record(output_filename, label_map_dict, annotations_dir, image_dir, examples): \"\"\"Creates a TFRecord file from examples. Args: output_filename: Path to where output file is saved. label_map_dict: The label map dictionary. annotations_dir: Directory where annotation files are stored. image_dir: Directory where image files are stored. examples: Examples to parse and save to tf record. \"\"\" writer = tf.python_io.TFRecordWriter(output_filename) for idx, example in enumerate(examples): if idx % 100 == 0: logging.info('On image %d of %d', idx, len(examples)) # path = os.path.join(annotations_dir, 'xmls', example + '.xml') path = os.path.join(annotations_dir, example + '.xml') if not os.path.exists(path): logging.warning('Could not find %s, ignoring example.', path) continue with tf.gfile.GFile(path, 'r') as fid: xml_str = fid.read() xml = etree.fromstring(xml_str) data = dataset_util.recursive_parse_xml_to_dict(xml)['annotation'] tf_example = dict_to_tf_example(data, label_map_dict, image_dir) writer.write(tf_example.SerializeToString()) writer.close() # TODO: Add test for pet/PASCAL main files. def main(_): data_dir = FLAGS.data_dir label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path) logging.info('Reading from Pet dataset.') image_dir = os.path.join(data_dir, 'images') annotations_dir = os.path.join(data_dir, 'annotations') examples_path = os.path.join(annotations_dir, 'trainval.txt') examples_list = dataset_util.read_examples_list(examples_path) # Test images are not included in the downloaded data set, so we shall perform # our own split. random.seed(42) random.shuffle(examples_list) num_examples = len(examples_list) num_train = int(0.7 * num_examples) train_examples = examples_list[:num_train] val_examples = examples_list[num_train:] logging.info('%d training and %d validation examples.', len(train_examples), len(val_examples)) train_output_path = os.path.join(FLAGS.output_dir, 'pet_train.record') val_output_path = os.path.join(FLAGS.output_dir, 'pet_val.record') create_tf_record(train_output_path, label_map_dict, annotations_dir, image_dir, train_examples) create_tf_record(val_output_path, label_map_dict, annotations_dir, image_dir, val_examples) if __name__ == '__main__': tf.app.run() python object_detection/dataset_tools/create_pet_tf_record.py \\ --label_map_path=object_detection/data/pet_label_map.pbtxt \\ --data_dir=`pwd` \\ --output_dir=`pwd`","title":"creating TFR datasets"},{"location":"deeplearning/#create-model-config-file","text":"# Faster R-CNN with Resnet-101 (v1), configuration for MSCOCO Dataset. # Users should configure the fine_tune_checkpoint field in the train config as # well as the label_map_path and input_path fields in the train_input_reader and # eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that # should be configured. model { faster_rcnn { num_classes: 90 image_resizer { keep_aspect_ratio_resizer { min_dimension: 600 max_dimension: 1024 } } feature_extractor { type: 'faster_rcnn_resnet101' first_stage_features_stride: 16 } first_stage_anchor_generator { grid_anchor_generator { scales: [0.25, 0.5, 1.0, 2.0] aspect_ratios: [0.5, 1.0, 2.0] height_stride: 16 width_stride: 16 } } first_stage_box_predictor_conv_hyperparams { op: CONV regularizer { l2_regularizer { weight: 0.0 } } initializer { truncated_normal_initializer { stddev: 0.01 } } } first_stage_nms_score_threshold: 0.0 first_stage_nms_iou_threshold: 0.7 first_stage_max_proposals: 300 first_stage_localization_loss_weight: 2.0 first_stage_objectness_loss_weight: 1.0 initial_crop_size: 14 maxpool_kernel_size: 2 maxpool_stride: 2 second_stage_box_predictor { mask_rcnn_box_predictor { use_dropout: false dropout_keep_probability: 1.0 fc_hyperparams { op: FC regularizer { l2_regularizer { weight: 0.0 } } initializer { variance_scaling_initializer { factor: 1.0 uniform: true mode: FAN_AVG } } } } } second_stage_post_processing { batch_non_max_suppression { score_threshold: 0.0 iou_threshold: 0.6 max_detections_per_class: 100 max_total_detections: 300 } score_converter: SOFTMAX } second_stage_localization_loss_weight: 2.0 second_stage_classification_loss_weight: 1.0 } } train_config: { batch_size: 1 optimizer { momentum_optimizer: { learning_rate: { manual_step_learning_rate { initial_learning_rate: 0.0003 schedule { step: 900000 learning_rate: .00003 } schedule { step: 1200000 learning_rate: .000003 } } } momentum_optimizer_value: 0.9 } use_moving_average: false } gradient_clipping_by_norm: 10.0 fine_tune_checkpoint: \"PATH_TO_BE_CONFIGURED/model.ckpt\" from_detection_checkpoint: true data_augmentation_options { random_horizontal_flip { } } } train_input_reader: { tf_record_input_reader { input_path: \"PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100\" } label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\" } eval_config: { num_examples: 8000 # Note: The below line limits the evaluation process to 10 evaluations. # Remove the below line to evaluate indefinitely. max_evals: 10 } eval_input_reader: { tf_record_input_reader { input_path: \"PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010\" } label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\" shuffle: false num_readers: 1 }","title":"create model config file"},{"location":"deeplearning/#train","text":"python train.py --logtostderr --train_dir=/home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/train_dir --pipeline_config_path=/home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/net.config tensorboard --logdir /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/train_dir/","title":"train"},{"location":"deeplearning/#evaluator","text":"python new_eval.py --logtostderr --checkpoint_dir /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/train_dir --eval_dir /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/eval_dir --pipeline_config_path /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/net.config tensorboard --logdir /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/eval_dir/ # new_eval.py import functools import os import tensorflow.compat.v1 as tf from tensorflow.python.util.deprecation import deprecated from object_detection.builders import dataset_builder from object_detection.builders import graph_rewriter_builder from object_detection.builders import model_builder from object_detection.legacy import evaluator from object_detection.utils import config_util from object_detection.utils import label_map_util from tensorflow.compat.v1 import ConfigProto from tensorflow.compat.v1 import InteractiveSession import keras.backend as K config = ConfigProto() config.gpu_options.allow_growth = True session = InteractiveSession(config=config) K.set_session(session) tf.logging.set_verbosity(tf.logging.INFO) flags = tf.app.flags flags.DEFINE_boolean('eval_training_data', False, 'If training data should be evaluated for this job.') flags.DEFINE_string( 'checkpoint_dir', '', 'Directory containing checkpoints to evaluate, typically ' 'set to `train_dir` used in the training job.') flags.DEFINE_string('eval_dir', '', 'Directory to write eval summaries to.') flags.DEFINE_string( 'pipeline_config_path', '', 'Path to a pipeline_pb2.TrainEvalPipelineConfig config ' 'file. If provided, other configs are ignored') flags.DEFINE_string('eval_config_path', '', 'Path to an eval_pb2.EvalConfig config file.') flags.DEFINE_string('input_config_path', '', 'Path to an input_reader_pb2.InputReader config file.') flags.DEFINE_string('model_config_path', '', 'Path to a model_pb2.DetectionModel config file.') flags.DEFINE_boolean( 'run_once', False, 'Option to only run a single pass of ' 'evaluation. Overrides the `max_evals` parameter in the ' 'provided config.') FLAGS = flags.FLAGS @deprecated(None, 'Use object_detection/model_main.py.') def main(unused_argv): assert FLAGS.checkpoint_dir, '`checkpoint_dir` is missing.' assert FLAGS.eval_dir, '`eval_dir` is missing.' tf.gfile.MakeDirs(FLAGS.eval_dir) if FLAGS.pipeline_config_path: configs = config_util.get_configs_from_pipeline_file( FLAGS.pipeline_config_path) tf.gfile.Copy( FLAGS.pipeline_config_path, os.path.join(FLAGS.eval_dir, 'pipeline.config'), overwrite=True) else: configs = config_util.get_configs_from_multiple_files( model_config_path=FLAGS.model_config_path, eval_config_path=FLAGS.eval_config_path, eval_input_config_path=FLAGS.input_config_path) for name, config in [('model.config', FLAGS.model_config_path), ('eval.config', FLAGS.eval_config_path), ('input.config', FLAGS.input_config_path)]: tf.gfile.Copy(config, os.path.join(FLAGS.eval_dir, name), overwrite=True) model_config = configs['model'] eval_config = configs['eval_config'] input_config = configs['eval_input_config'] if FLAGS.eval_training_data: input_config = configs['train_input_config'] model_fn = functools.partial( model_builder.build, model_config=model_config, is_training=False) def get_next(config): return dataset_builder.make_initializable_iterator( dataset_builder.build(config)).get_next() create_input_dict_fn = functools.partial(get_next, input_config) categories = label_map_util.create_categories_from_labelmap( input_config.label_map_path) if FLAGS.run_once: eval_config.max_evals = 1 graph_rewriter_fn = None if 'graph_rewriter_config' in configs: graph_rewriter_fn = graph_rewriter_builder.build( configs['graph_rewriter_config'], is_training=False) evaluator.evaluate( create_input_dict_fn, model_fn, eval_config, categories, FLAGS.checkpoint_dir, FLAGS.eval_dir, graph_hook_fn=graph_rewriter_fn) if __name__ == '__main__': tf.app.run()","title":"evaluator"},{"location":"deeplearning/#from-object_detection-import-evaluator","text":"ImportError: cannot import name 'evaluator' \\ from object_detection.legacy import evaluator","title":"from object_detection import evaluator"},{"location":"deeplearning/#control-trainning-steps","text":"num_steps:600","title":"control trainning steps"},{"location":"deeplearning/#tensorflow-freeze","text":"# From tensorflow/models/research/ INPUT_TYPE=image_tensor PIPELINE_CONFIG_PATH={path to pipeline config file} TRAINED_CKPT_PREFIX={path to model.ckpt} EXPORT_DIR={path to folder that will be used for export} python object_detection/export_inference_graph.py \\ --input_type=${INPUT_TYPE} \\ --pipeline_config_path=${PIPELINE_CONFIG_PATH} \\ --trained_checkpoint_prefix=${TRAINED_CKPT_PREFIX} \\ --output_directory=${EXPORT_DIR}","title":"tensorflow freeze"},{"location":"deeplearning/#modelsave-and-load","text":"model.fit(x_train, y_train, epochs = 150, batch_size = 32,callbacks=[tensorboard_callback]) model.save('./models/model.h5') model.save_weights('./models/weights.h5') model_path = './models/model.h5' model_weights_path = './models/weights.h5' model = load_model(model_path) model.load_weights(model_weights_path) array = model.predict(point_set) or tf.keras.models.save_model( model, 'models/mymode', overwrite=True, include_optimizer=True ) model = tf.keras.models.load_model('./models/mymode')","title":"model:save and load"},{"location":"deeplearning/#cudnn","text":"from tensorflow.compat.v1 import ConfigProto from tensorflow.compat.v1 import InteractiveSession import keras.backend as K K.set_image_data_format('channels_last') K.set_learning_phase(1) config = ConfigProto() config.gpu_options.allow_growth = True session = InteractiveSession(config=config) K.set_session(session)","title":"cudnn\u5931\u8d25"},{"location":"deeplearning/#pbsave-and-load","text":"def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True): graph = session.graph with graph.as_default(): freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or [])) output_names = output_names or [] output_names += [v.op.name for v in tf.global_variables()] input_graph_def = graph.as_graph_def() if clear_devices: for node in input_graph_def.node: node.device = '' frozen_graph = tf.graph_util.convert_variables_to_constants( session, input_graph_def, output_names, freeze_var_names) return frozen_graph frozen_graph = freeze_session(K.get_session(), output_names=[out.op.name for out in model.outputs]) tf.io.write_graph(frozen_graph, './models', 'xor.pbtxt', as_text=True) tf.io.write_graph(frozen_graph, './models', 'xor.pb', as_text=False) detection_graph = tf.Graph() with detection_graph.as_default(): od_graph_def = tf.GraphDef() with tf.gfile.GFile('xor.pb', 'rb') as fid: serialized_graph = fid.read() od_graph_def.ParseFromString(serialized_graph) tf.import_graph_def(od_graph_def, name='') input = tf.get_default_graph().get_tensor_by_name('input_1:0') output = tf.get_default_graph().get_tensor_by_name('fc2/Softmax:0') with detection_graph.as_default(): with tf.Session() as sess: values =sess.run(output, feed_dict={input: point_set}) print(values)","title":"pb:save and load"},{"location":"deeplearning/#_1","text":"print('model.inputs :',model.inputs) print('model.outputs : ',model.outputs) output model.inputs : [<tf.Tensor 'input_1:0' shape=(?, 2600, 3) dtype=float32>] model.outputs : [<tf.Tensor 'fc2/Softmax:0' shape=(?, 2) dtype=float32>] \u6240\u4ee5\u8f93\u5165\u5c42\u662f input_1:0 \u8f93\u51fa\u5c42\u662f fc2/Softmax:0","title":"\u67e5\u770b\u6a21\u578b\u7684\u8f93\u5165\u5c42\u548c\u8f93\u51fa\u5c42"},{"location":"deeplearning/#tensorrt","text":"\u4e0b\u8f7d\u4e0e\u60a8\u4f7f\u7528\u7684Ubuntu\u7248\u672c\u548cCPU\u67b6\u6784\u5339\u914d\u7684TensorRT\u672c\u5730repo\u6587\u4ef6\u3002 \u4eceDebian\u672c\u5730repo\u8f6f\u4ef6\u5305\u5b89\u88c5TensorRT\u3002 os=\"ubuntu1x04\" tag=\"cudax.x-trt7.x.x.x-ga-yyyymmdd\" sudo dpkg -i nv-tensorrt-repo-${os}-${tag}_1-1_amd64.deb sudo apt-key add /var/nv-tensorrt-repo-${tag}/7fa2af80.pub sudo apt-get update sudo apt-get install tensorrt \u5982\u679c\u4f7f\u7528Python 2.7\uff1a\\ sudo apt-get install python-libnvinfer-dev \\ \u5c06\u5b89\u88c5\u4ee5\u4e0b\u9644\u52a0\u8f6f\u4ef6\u5305\uff1a\\ python-libnvinfer \\ \u5982\u679c\u4f7f\u7528Python 3.x\uff1a\\ sudo apt-get install python3-libnvinfer-dev \\ \u5c06\u5b89\u88c5\u4ee5\u4e0b\u9644\u52a0\u8f6f\u4ef6\u5305\uff1a\\ python3-libnvinfer \\ \u5982\u679c\u60a8\u6253\u7b97\u5c06TensorRT\u4e0eTensorFlow\u7ed3\u5408\u4f7f\u7528\uff1a\\ sudo apt-get install uff-converter-tf \\ \u5982\u679c\u60a8\u8981\u8fd0\u884c\u9700\u8981ONNX\u7684\u793a\u4f8b \u56fe\u5f62\u5916\u79d1\u533b\u751f \u6216\u5c06Python\u6a21\u5757\u7528\u4e8e\u60a8\u81ea\u5df1\u7684\u9879\u76ee\uff0c\u8fd0\u884c\uff1a\\ sudo apt-get install onnx-graphsurgeon \\","title":"\u5b89\u88c5\uff54\uff45\uff4e\uff53\uff4f\uff52RT"},{"location":"deeplearning/#anaconda-tensorrt","text":"\u4e0b\u8f7dtar\u6587\u4ef6 TensorRT-7.0.0.11.Ubuntu-18.04.x86_64-gnu.cuda-10.0.cudnn7.6.tar.gz (wind1) star@xmatrix:~$ (wind1) star@xmatrix:~$ (wind1) star@xmatrix:~$ cd TensorRT (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ ls TensorRT TensorRT_1 TensorRT-7.0.0.11 TensorRT-7.0.0.11.Ubuntu-16.04.x86_64-gnu.cuda-10.0.cudnn7.6.tar.gz TensorRT-7.0.0(1).tar.gz (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ cd TensorRT-7.0.0.11 (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ ls bin data doc graphsurgeon include lib python samples targets TensorRT-Release-Notes.pdf uff (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ cd python (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ ls tensorrt-7.0.0.11-cp27-none-linux_x86_64.whl tensorrt-7.0.0.11-cp35-none-linux_x86_64.whl tensorrt-7.0.0.11-cp37-none-linux_x86_64.whl tensorrt-7.0.0.11-cp34-none-linux_x86_64.whl tensorrt-7.0.0.11-cp36-none-linux_x86_64.whl (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ pip --version pip 19.3.1 from /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages/pip (python 3.6) (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ pip3 --version pip 19.3.1 from /usr/local/lib/python3.5/dist-packages/pip (python 3.5) (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ python -m pip install tensorrt-7.0.0.11-cp36-none-linux_x86_64.whl Processing ./tensorrt-7.0.0.11-cp36-none-linux_x86_64.whl Installing collected packages: tensorrt Successfully installed tensorrt-7.0.0.11 (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ python Python 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) [GCC 7.3.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> >>> >>> import tensorrt >>> >>> exit(); (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ cd ../ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ ls bin data doc graphsurgeon include lib python samples targets TensorRT-Release-Notes.pdf uff (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ cd uff (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ ls uff-0.6.5-py2.py3-none-any.whl (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ python -m pip install uff-0.6.5-py2.py3-none-any.whl Processing ./uff-0.6.5-py2.py3-none-any.whl Requirement already satisfied: numpy>=1.11.0 in /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages (from uff==0.6.5) (1.16.4) Requirement already satisfied: protobuf>=3.3.0 in /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages (from uff==0.6.5) (3.11.2) Requirement already satisfied: six>=1.9 in /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages (from protobuf>=3.3.0->uff==0.6.5) (1.13.0) Requirement already satisfied: setuptools in /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages (from protobuf>=3.3.0->uff==0.6.5) (42.0.2.post20191203) Installing collected packages: uff Successfully installed uff-0.6.5 (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ python Python 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) [GCC 7.3.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> >>> >>> import tensorrt >>> import uff WARNING:tensorflow:From /home/star/anaconda3/envs/wind1/lib/python3.6 7.0.0.11 (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$","title":"anaconda tensorRT"},{"location":"deeplearning/#using-uff-converter-to-convert-the-frozen-tensorflow-model-to-a-uff-file","text":"conda activate wjj \\ pip install nvidia-pyindex \\ pip install uff \\ you need to find your uff installed path. \\ import uff print(uff.__path__) And after locating it , in it\u2019s bin folder there should be a script named as convert_to_uff.py . And now you need to open the terminal and simply type python3 convert_to_uff.py < path to the saved model >\\ In my case-->\\ python3 convert_to_uff.py /home/models/catsAndDogs.pb \\ And it will simply save the converted .uff in your .pb model location. And then this is how the next script should be done.","title":"Using UFF converter to convert the frozen tensorflow model to a UFF file"},{"location":"deeplearning/#nvidia","text":"NVIDIA\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6","title":"NVIDIA\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6"},{"location":"deeplearning/#could-not-create-cudnn-handle-cudnn_status_internal_error","text":"Use allow_growth memory option in TensorFlow and Keras, before your code. For Keras import tensorflow as tf config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True config.log_device_placement = True sess = tf.compat.v1.Session(config=config) tf.compat.v1.keras.backend.set_session(sess) For TensorFlow import tensorflow as tf config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True config.log_device_placement = True sess = tf.compat.v1.Session(config=config) \u8fd8\u6709\u4e00\u79cd\u60c5\u51b5,\u51cf\u5c11batch size","title":"Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR"},{"location":"deeplearning/#anchor-boxes","text":"One of the hardest concepts to grasp\u628a\u63e1 when learning about Convolutional Neural Networks for object detection is the idea of anchor boxes.It is also one of the most important parameters you can tune for improved performance on your dataset.In fact,if anchor oxes are not tuned correctly,your neural network will never even know that certain\u67d0\u4e9b small,large or irregular\u4e0d\u89c4\u5219 objects exist and will never have a chance to detect them.Luckily, there are some simple steps you can take to make sure you do not fall into this trap\u9677\u9631. what are anchor boxes? \\ when you use a neural network like yolo or ssd to predict multiple objects in a picture, the network is actually making thousands of predictions and only showing the ones that it decided were an object.The multiple predictions are output the following format:\\ Prediction 1: (X,Y,Height,Width),Class\\ ...\\ Prediction ~8000: (X,Y,Height,Width),Class Where the (X,Y,Height,Width) is called the \"bounding box\", or box surrounding the objects.This box and the object class are labelled manually by human annotators. In an extremely simplified example, imagine that we have a model that has two predictions and receives the following image:\\ We need to tell our network if each of its predictions is correct or not in order for it to be able to learn.But what do we tell the neural network it prediction should be? Should the predicted class be:\\ Prediction 1:Pear\\ Prediction 2:Apple Or should it be:\\ Prediction 1:Apple\\ Prediction 2:Pear What if the network predicts:\\ Prediction 1:Apple\\ Prediction 2:Apple We need our network's two predictors to be able to tell whether it is their job to predict the pear or the apple.To do this there are a several tools.Predictors can specialize in certain size objects, objects with a certain aspect ratio(tall vs. wide),or objects in different parts of the image.Most networks use all three criteria\u6807\u51c6.In our example of the pear/apple image,we could have Prediction 1 be for objects on the left and Prediction 2 for objects on the right side of the image.Then we would have our answer for what the network should be predict:\\ Prediction 1:Pear\\ Prediction 2:Apple Anchor Boxes in Practice\\ State of the art\u6700\u5148\u8fdb\u7684 object detection systems currently do the following:\\ 1. Create thousands of \"anchor boxes\" or \"prior boxes\" for each predictor that represent the ideal location, shape and size of the object it specializes\u4e13 in predicting. 2. For each anchor box,calculate which object's bounding box has the highest overlap divided by non-overlap.This is called Intersection Over Union or IOU. 3. If the highest IOU is greater than 50%, tell the anchor box that it should detect the object that gave the highest IOU. 4. Otherwise if the IOU is greater than 40%, tell the neural network that the true detection is ambiguous and not to learn from that example. 5. If the highest IOU is less than 40%,then the anchor box should predict that there is no object. This works well in practice and the thousands of predictors do a very good job of deciding whether their type of object apperars in an image. Using the default anchor box configuration can create predictors that are too specialized and ojects that appear in the image may not achieve an IOU of 50% with any of the anchor boxes.In this case, the neural network will never know these objects existed and will never learn to predict them. We can tweak\u8c03\u6574 our anchor boxes to be much smaller In xx net configuration, the smallest anchor box size is 32x32.This means that many objects smaller than this will go undetected. We can fix this by changing our default anchor box configurations. Reducing the smallest anchor box size, all of the things line up with at least one of our anchor boxes and our neural network can learn to detect them! Improving Anchor Box Configuration \\ As a general rule,you should ask yourself the following questions about your dataset before diving into training your model: 1. What is the smallest size box I want to be able to detect? 2. What is the largest size box I want to be able to detect? 3. What are the shapes the box can take? For example, a car detector might have short and wide anchor boxes as long as there is no chance of the car or the camera being turned on its side. You can get a rough estimate of these by actually calculating the most extreme\u6781\u7aef sizes\u3000and aspect ratios in the dataset.Yolo V3 uses K-means to estimate the ideal bounding boxes.Another option is to learn the anchor box configuration. Once you have thought through these questions you can start designing your anchor boxes. Be sure to test them by encoding your ground truth bounding boxes\u5730\u9762\u771f\u503c\u8fb9\u754c\u6846 and then decoding them as though\u5c31\u50cf they were predictions from your model.You should be able to recover the ground truth bounding boxes.","title":"Anchor boxes"},{"location":"deeplearning/#autonomous-self-learning-systems","text":"","title":"Autonomous self-learning systems"},{"location":"deeplearning/#preface","text":"This report describes the various processes that are part of the work on the main project at Oslo and Akershus University College(HIOA), department of engineering education,spring 2015. The report deals with the development of a self-learning algorithm and a demonstrator in the form of a robot that will avoid static and dynamic objects in an environment that is constantly changing.The thesis\u8bba\u6587 is given by HIOA.The report addresses the theory behind the most well-known and used self-learning algorithms,and discusses the advantages,disadvantages and uses of theese.It also contains a description of the technical solution for the demonstrator, and the method used in this project. The report is written within a topic that is considered new technical and is therefore assumed to be able to be used for futher research and/or learning within autonomous self-learning system. The reader is expected to have basic knowledge in electronics and information technology. We would like to thank our employer.Oslo and Akershus University College,for the opportunity to carry out the project and for financial support.We would also like to thank supervisor EZ for a good collaboration, as well as important and constructive guidance throughout the project period.","title":"Preface"},{"location":"deeplearning/#summary","text":"In today's society, self-learning systems are an increasingly relevant topic.Systems that are not explicity programmed to perform a specific task, but are even able to adapt,can be very useful. The system described in this thesis is realized with Q-learning by both the table method and the neural network.Q-learning is a learning algorithm based on experience.The algorithm involves an agent exploring his environment, where the environment is represented by a number of states. The agent experiments withs the environment by performing an action, and then observes the consequence of that action.The consequence is given in the form of a positive or negative reward. The goal of the method is to maximize the accumulated reward over time. Autonomous self-learning systems are becoming increasingly relevant because the system is able to adapt to partially or completely unknown situations.It learns from experience and needs less information at start-up as it acquires information along the way.In autonomous self-learning systems and self-propelled robots, avoiding obstacles is a key task.This report addresses a demonstrator of such a system, realized with Q-learning presented later in the reportk, and provides a description of the algorithm and results.","title":"Summary"},{"location":"deeplearning/#theory","text":"Before looking at the structure of a self-learning system, one can advantageouly look at what the concept of learning is.Learning is often defined as a lasting change in behavior as a result of experience(St. Olavs Hospital,undated).The property of organisms\u751f\u7269\u4f53 that is defined as learning is one of the cornerstones of what is called intelligence which, among other things, is defined as an ability to acquire and apply knowledge and skills.Humans and animals are considered intelligent, among other things,based on their ability to learn from experience.","title":"Theory"},{"location":"deeplearning/#machine-learning","text":"This subchapter is based on the theory of S.M,2009.Machine learning is a form of artifical intelligence that focuses on the development of self-learning algorithms. In most cases,self-learning systems deal with parts of natural intellignece, including memory,adaptation and generalization\u6982\u62ec.Unlike traditional non-learning systems, the method makes it possible to construct a system that is able to expand , adapt to new information and learn a given task without being specifically programmed for this.For machine learning,this system is called an agent.By using menmory,an agent can recognize the last time it was in a similiar situation and what action it took.Based on the outcome from the previous time, it can, if it was correct,choose to repeat the action, or try something new.By generallizing, the agent can recognize similarities in different situations, and thus use experienct from one situation and apply this experience in another. In order to realize\u5b9e\u73b0 this concept, machine learning uses principles\u539f\u7406 from statistics, mathematics, physics, neurology and biology. \uff37hen talking about machine learning and self-learning systems,algorithms are mainly the main product.The actual process in these algorithms can be compared to data mining\u6316\u6398.Data mining is a process that analyzes data from different perspectives and summarizes if into useful information.Both methods go through data to find patterns\u6a21\u5f0f,but instead of extractiong data for human interpretation,the information is used to improve the agent's understanding.For the agent to be able to learn, it must know how to improve , and whether it will get better or not.There are 15 more methods to solve this, which in turn provide servel main categories within machine learning:Supervised learning,unsupervised learning and reinforcement learning.","title":"machine learning"},{"location":"deeplearning/#supervised-learning","text":"An agent is given a training set with ,for example,pictures of a face and pictures without a face.The agent then prepared through a training process where it gives a forecast of what the picture is of. Whenever the forecast is incorrect, the agent is corrected.This process continues untill the model achieves a desired level of accuracy. Since the algorithm does not have a specific definition of what is a face and what is not, it must therefore learn this using examples.A good algorithm will eventually be able to estimate whether an image is of a face or not.The learning methods are best explained by examples:","title":"Supervised Learning"},{"location":"deeplearning/#unsupervised-learning","text":"When learning without supervision, information is not marked.This is ,the system is not told what is the image of a face and what is not.As a result, there is no correction or reward to indicate a potential solution, but the algorithm tries to identify the similarities between the images, swfor then categorize them and divide them into groups.","title":"Unsupervised Learning"},{"location":"deeplearning/#reinforcement-learning","text":"In reinforcement learning,the algorithm is told when the answer it gives is incorrect, but receives no suggestions on how to correct this.It must explore and try out different solutions untill it finds out how it gets the right answers.This is a kind of middle ground of supervised learning and unsupervised learning.Examples could be learning to play a board game or a robot that is going to learn to walk.Each time an agent performs an action,he or she recieves a reward or a penalty, based on how desirable\u53ef\u53d6\u7684 the outcome of the action is.For example.when an agent is trained to play a board game, he gets a positive reward for winning, and a negative reward for losing.All other cases give no reward.\\ \\ The activity mentioned above can be represent as a sequence of state-action reward:\\ \\ This means that the agent was in state s0, performed action a0,which resulted in it receiving reward r1 and ending up in state s1.Furthermore, it performed action a1, received reward r2, and ended up in state s2, and so on.\\ \uff34his sequence is made up of experiences where experience is given as:\\ \\ Experience shows that the agent was in state s,performed action a,received reward r, and ended up in state s', and represented by (s,a,s',r) In order for the agent to be able to learn from the sequences mentioned and thus call it an experience.It has a table called Q-table which acts as its memory.All data points stored in this table are called Q-values and represent how desirable\u53ef\u53d6\u7684 it is to perform a specific action in a specific state. One experience adds one data point Q(s,a) in the table that represents the agents current estimate\u4f30\u8ba1 of the optimal\u6700\u4f73 Q value. It is this information that the agent uses to learn an optimal pattern of action.The size of the table depends on how many conditions and actions are included in the problem you are trying to solve, where the number of conditions gives the number of rows ,while the number of actions gives the number of colums.For example if you have 20 states and 3 actions, you will get a table of 20x3. Before the agent begins to experiment, it knows nothing else what actions it is capable of performing, and the Q table is consequently empty.This is, all Q values are equal to 0. The key to the method described above is to update the Q value that is applied to the agent when it performs an action in a given state in the current data point when it gains an experience. This value is given by the Q function\\ \\ or more clearly:\\ \u03b1\uff0d\uff0dlearning rate\\ \u0393\uff0d\uff0ddiscount factor\\ r\uff0d\uff0dreward\\ The learning rate dicates\u6307\u793a how much of previouly\u5148\u524d acquired learning should be replaced with new information. By \u03b1=1,previous values are replaced with new information.while \u03b1=0 ,corresponds to no update.In other words,the agent will konw by \u03b1=1,assume that the last reward and resulting state are representative of future values. The discount factor indicates the weight of all futher step rewards.If \u0393=0,the agent will only consider current rewards, while it will think more long-term\u957f\u671f and strive\u52aa\u529b for higher future rewards as \u0393 approaches 1. The reward is defined when you create the program in the form of reward functions\u5956\u52b1\u662f\u5728\u521b\u5efa\u7a0b\u5e8f\u65f6\u4ee5\u5956\u52b1\u51fd\u6570\u7684\u5f62\u5f0f\u5b9a\u4e49\u7684,and can be positive or negative. What this reward value is defined as is not critical.On the other hand, It is important that there is a clear distinction\u533a\u522b between the reward for good deeds\u884c\u4e3a and the reward for bad deeds. When a new experience is added to the algorithm, a new Q value is estimated, and the old value of the Q table for the last experience is updated with the new one. Assume that the agent is a mouse that is placed in a room divded into six states, see below Figure.In state s6 there is a piece of cheese,while in state s5 there is a cat.If the agent finds the piece of cheese, it receives a reward of 10, while it receives a reward of -10 if it moves to the state where the cat is.These two states are called terminal states.The agent explores the environment untill one of the terminal states is reached,before starting a new iteration(attempt)\\ \uff34he agent can perform four actions: up,down,left,right.If it moves in a direction where there is a wall, it gets a reward of -1, and it remains in the same state. All other conditions have a value equal to 0.This is implemented in the algorithm using reward functions.\\ \\ The agent has no information about his surroundings other than that there are six states and four acitons, what state he is in at any given time and any rewards it receives as actions are performed.Nor does it know what a reward or punishment is.The Q table is illustrated in Table 1. By assuming the following sequences of experiences,(s,a,r,s') one can illustrate how a sequence updates the Q-table. For the sake of illustration, the learning rate \u03b1 and the discount factor \u0393 set equal to 1 and all Q-values have an initial value equal to 0.\\ After this sequence of experiences, the Q table with updated values will look like this:\\ \\ opp=up ned=down venstre=left h\u00d8yre=right Each experience will update the value of the state table for the performed state and action combination, and the Q values will converge\u6c47\u805a to optimal values over time.The more experiments the agent conducts\u884c\u4e3a, the better the estimates in the Qtable.In theory, the agent will eventually achieve an optimized Q-table, and will be able to choose the shortest path to the cheese each time, regardless of the starting state.","title":"Reinforcement Learning"},{"location":"deeplearning/#exploration-or-utilize","text":"One of the challenges with the Q-learning algorithm is that it does not directly tell what the agent should do, but rather functions as a tool that shows the agent what the optimal action is in a given condition.This is a challenge because the agent must explore the environment enough times to be able to build a solid foundation\u575a\u5b9e\u7684\u57fa\u7840 that provides a good estimate of the Q values.To address this, an exploration feature is implemented that determines the strategy\u6218\u7565 the agent will use to select actions, either explore or exploit. Exploration:The agent is bold\u80c6\u5927 and does not necessary choose the best course of action, with the intension of establishing a better estimate of the Q values. Utilize: The agent utilizes the experience it has already built up, and selects the optimal action for the condition it is in. That is, the action that gives the highest Q value. It is said that the agent is greedy\u8d2a\u5a6a The purpose of this feature is to establish a relationship between the two strategies. It is important to explore enough so that the agent builds a solid foundation of the Q-values, but it is also important to utilize the already acquired knowledge to ensure the highest possible reward. There are a number of ways to accomplish this, but two of the most popular methods are the greedy feature and the Boltzmann distribution, popularly called \"softmax\":","title":"Exploration or Utilize \u52d8\u63a2\u6216\u5f00\u53d1"},{"location":"deeplearning/#-greedy","text":"\u03b5-greedy is a strategy based on always choosing the optimal action except for \u03b5 of the aisles and choosing a random action of the aisles.\\ \\ there\\ a* is optimal action with probability 1-\u03b5\\ ar is random action with probability \u03b5\\ where\\ 0< \u03b5 < 1 It is possible to change over time. This allows the agent to choose a more random strategy at the beginning of the learning process, and then become more greedy as it acquires more knowledge. One of the challenges with greed is that it considers all actions except the optimal one as equal.\u5b83\u5c06\u6700\u4f73\u884c\u52a8\u4e4b\u5916\u7684\u6240\u6709\u884c\u52a8\u90fd\u89c6\u4e3a\u5e73\u7b49 If, in addition to an optimal action, there are two good actions as well as additional actions that look less promising,\u8fd8\u6709\u4e24\u4e2a\u597d\u7684\u52a8\u4f5c\u4ee5\u53ca\u770b\u8d77\u6765\u4e0d\u592a\u6709\u524d\u9014\u7684\u5176\u4ed6\u52a8\u4f5c\uff0c it will make sense\u6709\u610f\u4e49 for the agent to choose one of the two better ones instead of spending time exploring the bad actions. Using greed is as likely to explore a bad action as to explore one of the two better ones. One way to solve this is to use the Boltzmann distribution 2.3.2 Boltzmann distribution This strategy is known as \"softmax\" action selection and involves selecting an action with a probability that depends on the Q value. The probability of selecting action a in state s is proportional to\u3000 ,which means that the agent in state s chooses action a with probability \\ The parameter T specifies how random values are to be selected.\u53c2\u6570T\u6307\u5b9a\u5982\u4f55\u9009\u62e9\u968f\u673a\u503c\u3002 When T has a high value, actions are chosen to approximately the same extent.\u9009\u62e9\u7684\u52a8\u4f5c\u7684\u7a0b\u5ea6\u5927\u81f4\u76f8\u540c As T decreases, the actions with the highest Q value are more likely to be selected, while the best action is always selected when T-> 0 One of the advantages of the Q-learning algorithm is that it is exploration insensitive\u5bf9\u63a2\u7d22\u4e0d\u654f\u611f. That is, the Q values will converge to optimal values regardless of the agent's behavior as long as information is collected. This is one of the reasons why Q-learning is one of the most widely used and effective methods in reinforcement learning. On the other hand, it is a prerequisite that all combinations of condition / action are tried out many enough times, which in most cases means that the exploration and exploitation aspect must be addressed as it is not always possible for an agent to experiment infinitely. There are several methods for setting up an artificial neural network.One of the most common,which is also used in this project, is called a feedforward neural network, which means that the information is always fed from the input page and supplied further through the network.In other words,output from one layer + bias is used as input in the next.","title":"\u03b5-greedy"},{"location":"deeplearning/#fuzzy-logic","text":"The theory of fuzzy logic is based on S.K.J(2007)\\ Fuzzy logic is a decision-making or management model based on diffuse\u3000\u6269\u6563\uff0c\u6563\u53d1,and input data classified as degrees of membership in a diffuse quantity. A diffuse set has no sharp boundaries, and is a set of values that belong to the same definition, where the definition is a diffuse term often described by an adjective\u5f62\u5bb9\u8bcd. For example: big potatoes or tall people.A diffuse set is also called a semantically\u8bed\u4e49\u4e0a variable or set. Fuzzy logic classifies input data according to how large a membership the input data has to a set.The degree of membership is graded\u5206\u7ea7 between 0 and 1,and determined on the basis of a membership function\u6839\u636e\u96b6\u5c5e\u51fd\u6570\u786e\u5b9a.\uff34he member ship function is determined on the basis of knowledge of the field, and can be non-linear.Input data can have membership in servel different sets, but can only have full membership in one set.","title":"Fuzzy Logic\u6a21\u7cca\u7406\u8bba"},{"location":"deeplearning/#fuzzy-logic-behavior","text":"Diffusion\u6269\u6563: input data is graded according to the diffused quantities,and the diffused quantities are sent to subprocess 2 Logical rules:the diffused quantites are tested against rules.The degree of fulfillment \u5c65\u884c from each rule is sent to sub-process 3.The rules can be implented with {and, if, or},or other methods such as an impact\u5f71\u54cd matrix.The rules describe the relationship between the diffused input and output. Implication: the received data from sub-process 2 is tested againist rules that grade the impact on the overall consequence of each of the received data.The ratings are then sent to sub-process 4. Aggregation\u805a\u5408:all the consequence\u540e\u679c quantities are compiled into an aggregate quantity\u603b\u91cf,which is the union of the quantities from subprocess 3. A vdiffusion: The most representative element is extracted from the aggregate amount.This is usually the center of gravity\u91cd\u529b.That value will then be the control signal. The Fuzzy logic method has a number of advantages in that it is able to process imprecise\u4e0d\u7cbe\u786e data and model non-linear relationships between input and output.The principle enables an accurate and realistic\u73b0\u5b9e\u7684 classification of data, as data is rarely\u5f88\u5c11 only true or false.","title":"Fuzzy logic behavior"},{"location":"deeplearning/#technical-solution","text":"","title":"Technical solution"},{"location":"deeplearning/#agents","text":"Two versions of the agent were developed.The first version,which was later replaced, was built on the basis of drivetrain\u52a8\u529b\u603b\u6210 and mechanics\u673a\u68b0\u5e08 taken from 'WLtoys' model 'A969'. The radio-controlled car in scale 1:18 was delivered\u5df2\u4ea4\u4ed8 to drive with ratio and battery.The car measured 285x140x105mm(LxWxH) and could run for about 10 minutes on a fully charged battery. Since the car was to be equiped with an Xbee module and communicate via bluetooth.the car's original reciever was removed.The steering servo\u8f6c\u5411\u4f3a\u670d was adapted to the car's receiver and was therefore replaced in favour of a PWM-controllable servo.To improve the agent's running time, the original battery(7.4V/1100mAh) was replaced with a 7.4V/5200mAh \"Fire Bull\" type battery, which would thoretically provide approximately five times as long. To make room for all the electronics, a plate 1.5mm aluminum\u94dd was cut out and mounted on top of the car.The motor driver, microcontroller board and Xbee module were placed together with the motor and servo on the classis itself, while the battery, sensors and voltage regulator were mounted\u5b89\u88c5 on the new plate for easier access. After the mentioned changes, the agent had improved battery life, adjustable speed and the correct physical size, but it turned out not to have a good enough turning radius. It had particular problems in situations where it had to react in one direction, and then change direction. The car's turning radius was tested before the conversion was started, and it was measured that the car need 70cm to turn 90 degrees to the left or right(figure 3-4).It thus needed an area of 140cm in diameter to turn 180 degrees.It was therefore assumed that a round arena\u7ade\u6280\u573a with a diameter of about four meters was needed.The area was built,and it was found that four meters was enough to be able to drive around without obstacles,but that it became too samll when car had to avoid objects. The ultrasonic sensors are limited, through discretion\u901a\u8fc7\u659f\u914c,to integer values between 1cm and 120cm, If one is to create a look-up table for the entire spectrum\u8303\u56f4, the table will consist of 120^5 different combinations, and would with three possible actions end up with a look-up table of 120^5x3 number of places. This is a less good solution as the table would be very large(74 billion seats).A large lookup table means that the agent spends much more time learning all the combinations of conditions, and actions for these.In addition, such a solution is far too large to be implemented on a normal computer.This chanllenge is solved by discretizing\u79bb\u6563\u5316 or dividing, the continuous sensor area into smaller and more managerable state spaces.","title":"Agents"},{"location":"deeplearning/#421-condition-space-model","text":"All sensors can measure distances up to 400 cm. To avoid a large state table and to streamline the learning process, the sensor values are discretized down to 144 states. For the project described in this report, this also simplifies the reward function. Each snsor is divided into four zones based on distance in centimeters. The division is shown in Figure 4.4\\ \\ A condition consists of four parameters. Two for the left side and two for the right side of the agent's field of view.A state is a combination of distance to the nearest obstacle and in which sector the obstacle is located .a sector describes the angle at which an object is located, relative to the agent. The four parameter are defined by:\\ k1:zone\u533a left side \u533a\u57df\u5de6\u4fa7\\ k2:zone right side \u533a\u57df\u53f3\u4fa7\\ k3:sector\u90e8\u95e8 left side \u6247\u533a\u5de6\u4fa7\\ k4:sector right side \u6247\u533a\u53f3\u4fa7\\ k5:observation of dynamic or static obstacle left side \u89c2\u5bdf\u52a8\u6001\u6216\u9759\u6001\u969c\u788d\u7269\u5de6\u4fa7\\ k6:observation of dynamic or static obstacle right side \u89c2\u5bdf\u52a8\u6001\u6216\u9759\u6001\u969c\u788d\u7269\u53f3\u4fa7\\ S(state space model) = k1 x k2 x k3 x k4 x (k5 x k6)\\ The agent can perceive\u611f\u77e5 two obstacles simultaneously, but not on the same page.With two obstacles on the same side,the algorithm will prioritize\u4f18\u5148 the obstacle that is in the zone with the lowest numbering. The division of the sectors is illustrated in Figure below\\ \\ sector\u6247\u533a\\ The sectors have four different values, 0,1,2,3.The sensors perceive\u611f\u77e5 an object when the zone value is less than 2.The sectors are symmetrically\u5bf9\u79f0\u5730 distributed\u5206\u5e03.so that the agent's right field of view is discretized in the same way as the left.Sensors 1,2 and 3 on the left side correspond to sensors 3,4 and 5 for the right side.","title":"4.2.1 Condition space model"},{"location":"deeplearning/#4211-discretion-for-system-with-dynamic-obstacles","text":"In the simulation for a system with dynamic obstacles, the state space model is expanded with two additional parameters,k5 and k6. k5 indicates with the value 1 or 0 whether the obstacle detected on the left side is dynamic, while k6 does the same for the right side.The number 1 indicates a dynamic obstacle,while 0 indicates static.The state table has thus benn expanded to 576 states.,An object os classified as a dynamic obstacle in that the agent first, while standing still,performs two measurements in a row. Then the two measurements are compared, if the obstacle moves away from the agent, the first measurement will be positive number, and classified as a static obstacle.If on the other hand, the same calculation operation gives a negative number, the obstacle is classified as a dynamic obstacle and the agent must react accordingly.","title":"4.2.11 Discretion for system with dynamic obstacles"},{"location":"deeplearning/#4212-the-state-table","text":"The state table is structured as a table where all combinations of states and actions are represented as a table point.The state table for the physical model has 144 rows, and three columns that present each of the possible actions(left,right, straightfoward). A section of the condition table is shown in Table 7.","title":"4.2.1.2 the state table"},{"location":"deeplearning/#422-reward-function","text":"The reward feature determines the agent's immmediate reward after each experience.When implementing the reward function, a distinction is made between good actions that give a positive reward value,and bad actions that give a negative reward value.The reward function used for Q-learning with the table method is inspired by and shown under equation.\\ The reward function, r1 is defined to give a positive reward value when the agent drives straight ahead, while for turns a small negative reward value is given. The negative value is given to prioritize \u4f18\u5148 driving straight ahead. The parameter r2 gives a positve reward when the total change in the sensor values is positive. That is , the agent moves away from an obstacle. If the agent moves towards an obstacle , it becomes total the change in sensor values is negatie, and the agent recieve a negative reward. The function r3 gives the agent a negative reward value if it first turns to right and then to the left, or first to the left and then to right one after the other. This feature allows the agent to avoid an indecisive\u4f18\u67d4\u5be1\u65ad\u7684 sequence, where it only swings\u6447\u6446 back and forth. \uff34he function r gives the agent a reward value equal to the sum of r1,r2 and r3 if it does not collide, while in the event of a collision it gets a negative reward value of -100. 4.2.3 Exploration function The exploration function determines the agent's strategy for choosing actions. It can either explore the state space or take advantage of the experiences it already has. The function ensures an appropriate balance between these strategies (see chapter 2.3). The simulations for the exploration methods show that softmax gives the best results in the simulated model, and is thus used in the simulation. The parameter T is set equal to 24, and is reduced by 5% of its total value for each individual experiment.In the physical model, epsilon is used because it provides a better overview of how much coincidence there is for the agent to take an arbitrary action, and epsilon is set at a 5% chance of doing a random action.","title":"4.2.2 Reward function"},{"location":"deeplearning/#424-implementation","text":"The implementation of Q-learning with the table method is based on the same algorithm and reward function in the simulation and for the physical model.The different between the simulation and the physical agent is the exploration function and an additional extension of the state space in the simulation(see the discretization\u79bb\u6563\u5316 chapter).In the simulation, you have the choice between static and dynamic obstacles, and to adapt the agent to dynamic obstacle, the Q table is expanded.","title":"4.2.4 Implementation"},{"location":"deeplearning/#4241-the-q-learning-algorithm-with-the-table-method","text":"The Q-Learning algorithm with the table method At startup: - initialize state space S - initialize the action list A - initialize the Q table Q(s,a)(empty table) based on the state space S and the action list A For each attempt: 1) Observe initial sensor values 2) Discretize the sensor values and set it equal to s(initial mode) 3) Select an action a based on the exploration function and perform the action 4) Observe the agent's sensor values after the operation 5) Discretize the sensor values and set it equal to s'(Next mode) 6) Give the agent the reward r based on the reward function 7) Update the Q table 8) Set s=s'(Now state) 9) Keep last action, prev_a =a (Used in the reward function) 10) Repeat the operation from point iii)(a new step /experience ) If the agent has not reached its terminal state(collision or maximum number of steps). Start a new attempt if the agent has reached its terminal state(Restart from i).","title":"4.2.4.1 The Q-Learning algorithm with the table method"},{"location":"deeplearning/#4242-simulated-model","text":"In the simulation, a user interface is created that shows the user the simulation of the agent, two different graphs, buttons for different functions and information panel.The information panel show the user relevant\u76f8\u5173\u7684 values from the simulation.One graph shows an average of the number of steps for every tenth attempt, while the other shows the agent's accumulated reward.One step in the program corresponds to an experienced situation,whether it has been experienced before or is new.A new attempt is initialiated each time the agent has moved the maximum number of steps for the current attempt, or has collided with an obstacle .The maximum number of steps can be changed, but is normally 400-600.The dimensions of the simulation are scaled 1\uff1a10[cm] The ultrasonic sensors on the physical agent are limited to integer values from 1 to 120cm in the code of the agents microcontrollers.Distance less than 8 cm id defined as a collision. The initial state of the physical agent is an arbitrary position in the environment.The only requirement is that the initial state is not a terminal state(collision). The commands - command 1 : request sensor values - command 2: ask the agent to drive straight ahead - command 3: ask the agent to turn right - command 4: ask the agent to turn left - command 5: ask the agent to stop, sensor values <8 cm(collision)","title":"4.2.4.2 Simulated model"},{"location":"deeplearning/#43-q-learning-with-neural-network-linear-function-approximation","text":"Q-Learning with neural network is based on approximation of one or more linear Q-functions. Unlike Q-table learning.this method stores the Q functions in a neural network. By using this method ,discretization is not necessary, and the entire continuous sensor spectrum can be used. The behavior behind an artifical neuron is described in chapter 2.3.3\\ neural network \\ \\ input layer---------------------------------hidden layer-------------------------------------output layer The figure above shows the neural network as it is implemented in this system. In the network, all the neurous, including the bias from one layer, are connected to each individual neuron in the next layer. The neural network has three layers.Layer One, the input layer, has five neurons (one for each sensor),layer two has fifteen neurons, and layer three has three neurons according to the number of actions in the action list.Layer 1 is the start of the network, and the sensor values are used as input.and these must be in the interval\u95f4\u9694[-1,1] The weight matrix w(1) is used to weight the link between layer 1 and layer 2.means w(2) for layer 2 and layer 3.The matrices have a dimension of (number of neurons in layer L)x(number of neurons in Layer (l-1) + bias) w(1) has a dimension of 15x6 and w(2) dimension of 3x16. In the output layer, there are three neurons, and each individual neuron corresponds to a Q function.The number of neurons in the output layer is equal to the number of actions of the agent, and each individual neuron estimates a Q function with respect to an action in the agent's action list Equation(21)\\ These Q function estimate a Q value that is futher used to determine the optimal actions for the agent, where the optimal action for each condition is therefore defined as a = max(Q(s,a)). It is impossible to say which Q function are suitable for the system, since the neural network estimates the Q function based on the agent's experience. In order for the agent to achieve the highest total reward, it must therefore explore the environment long enough for the network to converge to an optimal function. \\ The figure shows the learning process of agent. The method needs initial values, and these are given by sensor values and the network's weight matrices.The weight matrices are initiallized with random values between 0 and 1, but not zero.If the weight matrices are initialized with a value equal to 0,all the neurons will have the same value,which means that the neural network will not be usable. This algorithm uses the agent's sensor values, as opposed to the table method which uses a discretization of values. The exploration function used is the same as for the table method, but the reward function is different. The purpose of this method is to find optimal Q functions by updating the weight machines after each experiment.","title":"4.3 Q-Learning with neural network (Linear function approximation)"},{"location":"deeplearning/#431-reward-function","text":"Since the agent's state space on the neural network is continuous, an equally elaborate reward function is not required as for the table method.The reward function is defined by the agent receiving a positive reward if it drives straight ahead, and a negative reward in the event of a turn and a collision. The reward value r must be in the interval [-1,1], and the function as defined for the system is shown in equation.\\","title":"4.3.1 Reward function"},{"location":"deeplearning/#432-feedforward","text":"As described in Chapter 2 Theory, neural network in this project is a forward-connected neural network. The neurons are activated using the function of the hyperbolic forceps\u53cc\u66f2\u7ebf\u94b3 given in equation. \\ The activated values become output values in this layer, and are used as input for the next layer.The three neurons in the output layer correspond to the Q funcitons when all data is fed into the network. \\ \\ The weight matrix w has m number of rows equal to the number of neurons,and columns n equal to the number of inputs.The matrix w(2) and the vector y are multiplied by each other and the product is used in the activation function described ealier in the chapter.The elements in both the weight matrix and the input vector are real numbers,and the value of the inputs is in the interval[-1,1].Equation(26-28) shows the Q functions of the three actions. The calculations in the figures above can be abbreviated\u7f29\u5199 and shown in equation 27 in vector form.\\ \\ Activation of the hidden layer takes place in the same way as for the output layer, and the input to this layer is the sensor values.\\","title":"4.3.2 Feedforward"},{"location":"deeplearning/#install-pytroch-gpu","text":"versison:1.3.0 conda install -c anaconda pytorch-gpu version:1.8.0 conda install -c conda-forge pytorch-gpu","title":"Install Pytroch-Gpu"},{"location":"deeplearning/#_2","text":"The projected depth values in the original depth maps are float32 and the unit is meter (m). However, we don't want to save float32 because it took too much storage. A common technique is that we can convert it to uint16 by int(depth * 256). This keeps certain degree of accuracy but takes less storage. That's why we need to divide the value by 256.","title":"\u51cf\u5c11\u5185\u5b58\u5360\u7528\u7684\u4e00\u79cd\u65b9\u6cd5"},{"location":"deeplearning_work_station/","text":"\u4e24\u4e2a\u9f13\u98ce\u5f0fGPU\\ SUS GeForce RTX 2080 Ti 11G Turbo Edition GDA\\ \u6dd8\u5b9d\u94fe\u63a5 \\ \u4e00\u4e2a20\u7ebf\u7a0bcpu\\ \u82f1\u7279\u5c14\u9177\u777fi9-9820X Skylake X 10\u68383.3Ghz\\ \u6dd8\u5b9d\u94fe\u63a5 x299\u4e3b\u677f\\ \u534e\u7855WS X299 SAGE LGA 2066 Intel X299\\ \u6dd8\u5b9d\u94fe\u63a5 \u4e3b\u673a\u58f3\\ \u6dd8\u5b9d\u94fe\u63a5 \u56fa\u6001\u786c\u76d8\\ HP EX920 M.2 1TB PCIe NVMe NAND SSD\\ \u4eac\u4e1c\u94fe\u63a5 \u673a\u68b0\u786c\u76d8\uff13\uff34\uff22\\ 128GB\u5185\u5b58\\ 128GB RAM\uff08\u968f\u673a\u5b58\u53d6\u5b58\u50a8\u5668\uff09\\ \u6dd8\u5b9d\u94fe\u63a5 \u7535\u6e90\\ 1600w PSU\\ \u6dd8\u5b9d\u94fe\u63a5 \u51b7\u5374\u5668\\ CPU\u98ce\u51b7\\ \u6dd8\u5b9d\u94fe\u63a5","title":"Deeplearning work station"},{"location":"github%E7%BD%91%E9%A1%B5%E4%B8%8D%E6%98%BE%E7%A4%BA%E5%9B%BE%E7%89%87/","text":"github\u4e0d\u663e\u793a\u56fe\u7247 \u4fee\u6539/etc/hosts\uff0c\u653e\u5165\u4e0b\u9762\u5185\u5bb9\uff1a # GitHub Start 140.82.113.3 github.com 140.82.113.3 gist.github.com 185.199.108.153 assets-cdn.github.com 199.232.68.133 raw.githubusercontent.com 199.232.68.133 gist.githubusercontent.com 199.232.68.133 cloud.githubusercontent.com 199.232.68.133 camo.githubusercontent.com 199.232.68.133 avatars0.githubusercontent.com 199.232.68.133 avatars1.githubusercontent.com 199.232.68.133 avatars2.githubusercontent.com 199.232.68.133 avatars3.githubusercontent.com 151.101.184.133 avatars4.githubusercontent.com 151.101.184.133 avatars5.githubusercontent.com 151.101.184.133 avatars6.githubusercontent.com 151.101.184.133 avatars7.githubusercontent.com 151.101.184.133 avatars8.githubusercontent.com # GitHub End \u5176\u4e2d\u5404\u4e2a\uff49\uff50\u9700\u8981\u4ece \u7f51\u7ad9\u67e5\u627e","title":"github\u4e0d\u663e\u793a\u56fe\u7247"},{"location":"github%E7%BD%91%E9%A1%B5%E4%B8%8D%E6%98%BE%E7%A4%BA%E5%9B%BE%E7%89%87/#github","text":"\u4fee\u6539/etc/hosts\uff0c\u653e\u5165\u4e0b\u9762\u5185\u5bb9\uff1a # GitHub Start 140.82.113.3 github.com 140.82.113.3 gist.github.com 185.199.108.153 assets-cdn.github.com 199.232.68.133 raw.githubusercontent.com 199.232.68.133 gist.githubusercontent.com 199.232.68.133 cloud.githubusercontent.com 199.232.68.133 camo.githubusercontent.com 199.232.68.133 avatars0.githubusercontent.com 199.232.68.133 avatars1.githubusercontent.com 199.232.68.133 avatars2.githubusercontent.com 199.232.68.133 avatars3.githubusercontent.com 151.101.184.133 avatars4.githubusercontent.com 151.101.184.133 avatars5.githubusercontent.com 151.101.184.133 avatars6.githubusercontent.com 151.101.184.133 avatars7.githubusercontent.com 151.101.184.133 avatars8.githubusercontent.com # GitHub End \u5176\u4e2d\u5404\u4e2a\uff49\uff50\u9700\u8981\u4ece \u7f51\u7ad9\u67e5\u627e","title":"github\u4e0d\u663e\u793a\u56fe\u7247"},{"location":"jekyll/","text":"sudo apt -y install ruby ruby-dev export GEM_HOME=$HOME/gems export PATH=$HOME/gems/bin:$PATH source ~/.bashrc gem install jekyll bundler jekyll new my-awesome-site cd my-awesome-site bundle install bundle exec jekyll serve","title":"Jekyll"},{"location":"lidar_demo%E6%95%99%E7%A8%8B/","text":"lidar_demo\u6559\u7a0b \u7cfb\u7edf\u73af\u5883 ros melodic User Guide \u7f16\u8bd1 \u521b\u5efa\uff52\uff4f\uff53\u73af\u5883\u3000\uff5e/catkin_ws/src\\ \u628alidar_demo.zip\u3000\u89e3\u538b\u5230\uff5e/catkin_ws/src\u76ee\u5f55\u4e0b\\ \u9700\u8981\u628a\uff5e/catkin_ws/src/lidar_demo/src/lidar_demo.cpp\u91cc ros::Subscriber sub = nh.subscribe<sensor_msgs::PointCloud2>(\"/points_raw\", 1, callback); \u4e2d\u7684\"/points_raw\" \u66f4\u6539\u4e3a\u96f7\u8fbe\u6570\u636e\u6e90\uff0c\u5373\u60f3\u8981\u8fdb\u884c\u805a\u7c7b\u8bc6\u522b\u7684\u70b9\u4e91\u7684\uff54\uff4f\uff50\uff49\uff43 \u5f00\u59cb\u7f16\u8bd1\\ cd ~/catkin_ws && catkin_make \u8fd0\u884c source devel/setup.bash \\ \u542f\u52a8\\ roslaunch lidar_demo lidar_demo.launch \\ \u5728\uff4c\uff41\uff55\uff4e\uff43\uff48\u6587\u4ef6\u91cc\u6709\uff15\u4e2a\u53c2\u6570\u53ef\u4ee5\u8c03\u8282 <!-- \u96f7\u8fbe\u6570\u636e\u6e90--> <arg name=\"topic_name\" default=\"/points_raw\"/> <!-- rviz \u663e\u793a\u65f6 Fixed Frame \u5904\u7684\u540d\u79f0--> <arg name=\"frame_id\" default=\"velodyne\"/> <!-- \uff10.01 \u662f1\u5206\u7c73--> <arg name=\"Cluster_D\" default=\"0.75\"/> <!-- \u8bc6\u522b\u7269\u4f53\u5305\u542b\u7684\u6700\u5c11\u70b9\u6570\uff0c\u5c11\u4e8e\u6b64\u70b9\u6570\u4e0d\u5212\u5206--> <arg name=\"Cluster_Min\" default=\"20\"/> <!-- \u8bc6\u522b\u7269\u4f53\u5305\u542b\u7684\u6700\u591a\u70b9\u6570\uff0c\u591a\u4e8e\u6b64\u70b9\u6570\u4e0d\u5212\u5206 (100000)--> <arg name=\"Cluster_Max\" default=\"1000\"/> frame_id \u5fc5\u987b\u548c\uff52\uff56\uff49\uff5a\u91cc\u7684\uff46ixed frame \u4e00\u81f4\\ Cluster_D \u662f\u805a\u884c\u534a\u5f84\uff0c0.01\u662f\uff11\u5206\u7c73\\ Cluster_Min\u3000\u8bc6\u522b\u7269\u4f53\u5305\u542b\u7684\u6700\u5c11\u70b9\u6570\uff0c\u5c11\u4e8e\u6b64\u70b9\u6570\u4e0d\u5212\u5206\\ Cluster_Max\u3000\u8bc6\u522b\u7269\u4f53\u5305\u542b\u7684\u6700\u591a\u70b9\u6570\uff0c\u591a\u4e8e\u6b64\u70b9\u6570\u4e0d\u5212\u5206\\ \uff23luster\u53c2\u6570\u6839\u636e\u6211\u4eec\u8bc6\u522b\u9700\u6c42\u53ef\u8c03","title":"lidar_demo\u6559\u7a0b"},{"location":"lidar_demo%E6%95%99%E7%A8%8B/#lidar_demo","text":"","title":"lidar_demo\u6559\u7a0b"},{"location":"lidar_demo%E6%95%99%E7%A8%8B/#_1","text":"ros melodic","title":"\u7cfb\u7edf\u73af\u5883"},{"location":"lidar_demo%E6%95%99%E7%A8%8B/#user-guide","text":"","title":"User Guide"},{"location":"lidar_demo%E6%95%99%E7%A8%8B/#_2","text":"\u521b\u5efa\uff52\uff4f\uff53\u73af\u5883\u3000\uff5e/catkin_ws/src\\ \u628alidar_demo.zip\u3000\u89e3\u538b\u5230\uff5e/catkin_ws/src\u76ee\u5f55\u4e0b\\ \u9700\u8981\u628a\uff5e/catkin_ws/src/lidar_demo/src/lidar_demo.cpp\u91cc ros::Subscriber sub = nh.subscribe<sensor_msgs::PointCloud2>(\"/points_raw\", 1, callback); \u4e2d\u7684\"/points_raw\" \u66f4\u6539\u4e3a\u96f7\u8fbe\u6570\u636e\u6e90\uff0c\u5373\u60f3\u8981\u8fdb\u884c\u805a\u7c7b\u8bc6\u522b\u7684\u70b9\u4e91\u7684\uff54\uff4f\uff50\uff49\uff43 \u5f00\u59cb\u7f16\u8bd1\\ cd ~/catkin_ws && catkin_make","title":"\u7f16\u8bd1"},{"location":"lidar_demo%E6%95%99%E7%A8%8B/#_3","text":"source devel/setup.bash \\ \u542f\u52a8\\ roslaunch lidar_demo lidar_demo.launch \\ \u5728\uff4c\uff41\uff55\uff4e\uff43\uff48\u6587\u4ef6\u91cc\u6709\uff15\u4e2a\u53c2\u6570\u53ef\u4ee5\u8c03\u8282 <!-- \u96f7\u8fbe\u6570\u636e\u6e90--> <arg name=\"topic_name\" default=\"/points_raw\"/> <!-- rviz \u663e\u793a\u65f6 Fixed Frame \u5904\u7684\u540d\u79f0--> <arg name=\"frame_id\" default=\"velodyne\"/> <!-- \uff10.01 \u662f1\u5206\u7c73--> <arg name=\"Cluster_D\" default=\"0.75\"/> <!-- \u8bc6\u522b\u7269\u4f53\u5305\u542b\u7684\u6700\u5c11\u70b9\u6570\uff0c\u5c11\u4e8e\u6b64\u70b9\u6570\u4e0d\u5212\u5206--> <arg name=\"Cluster_Min\" default=\"20\"/> <!-- \u8bc6\u522b\u7269\u4f53\u5305\u542b\u7684\u6700\u591a\u70b9\u6570\uff0c\u591a\u4e8e\u6b64\u70b9\u6570\u4e0d\u5212\u5206 (100000)--> <arg name=\"Cluster_Max\" default=\"1000\"/> frame_id \u5fc5\u987b\u548c\uff52\uff56\uff49\uff5a\u91cc\u7684\uff46ixed frame \u4e00\u81f4\\ Cluster_D \u662f\u805a\u884c\u534a\u5f84\uff0c0.01\u662f\uff11\u5206\u7c73\\ Cluster_Min\u3000\u8bc6\u522b\u7269\u4f53\u5305\u542b\u7684\u6700\u5c11\u70b9\u6570\uff0c\u5c11\u4e8e\u6b64\u70b9\u6570\u4e0d\u5212\u5206\\ Cluster_Max\u3000\u8bc6\u522b\u7269\u4f53\u5305\u542b\u7684\u6700\u591a\u70b9\u6570\uff0c\u591a\u4e8e\u6b64\u70b9\u6570\u4e0d\u5212\u5206\\ \uff23luster\u53c2\u6570\u6839\u636e\u6211\u4eec\u8bc6\u522b\u9700\u6c42\u53ef\u8c03","title":"\u8fd0\u884c"},{"location":"python/","text":"conda The following packages are not available from current channels: conda config --append channels conda-forge It tells conda to also look on the conda-forge channel when you search for packages. conda \u521b\u5efa\u73af\u5883 conda create -n xception_net python==3.6.5 numpy==1.17.4 scipy==1.3.3 h5py==2.10.0 Keras==2.3.1 tensorflow-gpu==1.15.0 As the comment at the top indicates, the output of conda list -e > requirements.txt can be used to create a conda virtual environment with conda create --name <env> --file requirements.txt cudnn_status_internal_error tensorflow You can try Allowing GPU memory growth with: import tensorflow as tf gpu = tf.config.experimental.list_physical_devices('GPU') tf.config.experimental.set_memory_growth(gpu[0], True) vscode activate conda env { \"ros.distro\": \"melodic\", \"python.autoComplete.extraPaths\": [ \"/home/pmjd/Disk/anaconda3/envs/wjj/lib/python3.6/site-packages\" ], \"python.terminal.activateEnvInCurrentTerminal\": true, \"python.condaPath\": \"/home/pmjd/Disk/anaconda3/bin/conda\", \"python.defaultInterpreterPath\": \"/home/pmjd/Disk/anaconda3/envs/wjj/bin/python\" } NameError: name 'xrange' is not defined try: # Python 2 xrange except NameError: # Python 3, xrange is now named range xrange = range TensorFlow ValueError: Cannot feed value of shape (64, 64, 3) for Tensor u'Placeholder:0', which has shape '(?, 64, 64, 3)' image has a shape of (64,64,3). Your input placeholder _x have a shape of (?, 64,64,3). The problem is that you're feeding the placeholder with a value of a different shape. You have to feed it with a value of (1, 64, 64, 3) = a batch of 1 image. Just reshape your image value to a batch with size one. np.expand_dims(img, axis=0) opencv conda opencv is not compatible with python 3. I had to install opencv3 for python 3. The marked answer in how could we install opencv on anaconda? explains how to install opencv(3) for anaconda: Run the following command: conda install -c https://conda.binstar.org/menpo opencv I realized that opencv3 is also available now, run the following command: conda install -c https://conda.binstar.org/menpo opencv3 Edit on Aug 18, 2016: You may like to add the \"menpo\" channel permanently by: conda config --add channels menpo And then opencv can be installed by: conda install opencv (or opencv3) Edit on Aug 14, 2017: \"clinicalgraphics\" channel provides relatively newer vtk version for very recent python3 conda install -c clinicalgraphics vtk Edit on April 16, 2020 (based on @AMC's comment): OpenCV can be installed through conda-forge (details see here) conda install -c conda-forge opencv pointcloud2 to array def pointcloud2_to_array(cloud_msg, squeeze=True): dtype_list = fields_to_dtype(cloud_msg.fields, cloud_msg.point_step) cloud_arr = np.fromstring(cloud_msg.data, dtype_list) cloud_arr = cloud_arr[ [fname for fname, _type in dtype_list if not (fname[:len(DUMMY_FIELD_PREFIX)] == DUMMY_FIELD_PREFIX)]] if squeeze and cloud_msg.height == 1: return np.reshape(cloud_arr, (cloud_msg.width,)) else: return np.reshape(cloud_arr, (cloud_msg.height, cloud_msg.width)) read pcd to array import numpy as np import open3d as o3d pcd = o3d.io.read_point_cloud(\"pointcloud_path.pcd\") out_arr = np.asarray(pcd.points) print (\"output array from input list : \", out_arr) kitti \u6570\u636e\u96c6\u4e0b\u8f7d \u56fe\u7247\u4e0b\u8f7d https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_image_2.zip \u70b9\u4e91\u4e0b\u8f7d https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_velodyne.zip \u6807\u7b7e\u4e0b\u8f7d https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_label_2.zip \u77eb\u6b63\u6587\u4ef6\u4e0b\u8f7d https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_calib.zip python \"//\" operator In python 2.x >>> 10/3 3 >>> # To get a floating point number from integer division: >>> 10.0/3 3.3333333333333335 >>> float(10)/3 3.3333333333333335 In python 3.x >>> 10/3 3.3333333333333335 >>> 10//3 3 python super() \u5185\u7f6e\u7684super()\u8fd4\u56de\u4e00\u4e2a\u4ee3\u7406\u5bf9\u8c61\uff08\u8d85\u7c7b\u7684\u4e34\u65f6\u5bf9\u8c61),\u8be5\u4ee3\u7406\u5bf9\u8c61\u5141\u8bb8\u6211\u4eec\u8bbf\u95ee\u57fa\u7c7b\u7684\u65b9\u6cd5\u3002\u5728python\u4e2d\uff0csuper()\u6709\u4e24\u4e2a\u4e3b\u8981\u7528\u4f8b\uff1a \u8ba9\u6211\u4eec\u907f\u514d\u663e\u793a\u4f7f\u7528\u57fa\u7c7b\u540d\u79f0 \u5904\u7406\u591a\u91cd\u7ee7\u627f\\ \u793a\u4f8b\uff11\uff1a\u5177\u6709\u5355\u7ee7\u627f\u7684super() \u5728\u5355\u7ee7\u627f\u7684\u60c5\u51b5\u4e0b\uff0c\u5b83\u5141\u8bb8\u6211\u4eec\u901a\u8fc7\u5f15\u7528\u57fa\u7c7bsuper() class Mammal(object): def __init__(self, mammalName): print(mammalName, 'is a warm-blooded animal.') class Dog(Mammal): def __init__(self): print('Dog has four legs.') super().__init__('Dog') d1 = Dog() \u8f93\u51fa Dog has four legs. Dog is a warm-blooded animal. \u8be5super()\u5185\u5efa\u8fd4\u56de\u4e00\u4e2a\u4ee3\u7406\u5bf9\u8c61\uff0c\u66ff\u4ee3\u5bf9\u8c61\uff0c\u53ef\u4ee5\u901a\u8fc7\u59d4\u6258\u8c03\u7528\u57fa\u7c7b\u7684\u65b9\u6cd5\uff0c\u8fd9\u79f0\u4e3a\u201c\u95f4\u63a5\"(\u4f7f\u7528\u5f15\u7528\u57fa\u7840\u5bf9\u8c61\u7684\u80fd\u529bsuper()) \u7531\u4e8e\u95f4\u63a5\u662f\u5728\u8fd0\u884c\u65f6\u8ba1\u7b97\u7684\uff0c\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u5728\u4e0d\u540c\u7684\u65f6\u95f4\u4f7f\u7528\u4e0d\u540c\u7684\u57fa\u7c7b\uff08\u5982\u679c\u9700\u8981\uff09 \u793a\u4f8b\uff12\uff1a\u5177\u6709\u591a\u91cd\u7ee7\u627f\u7684super() class Animal: def __init__(self, Animal): print(Animal, 'is an animal.'); class Mammal(Animal): def __init__(self, mammalName): print(mammalName, 'is a warm-blooded animal.') super().__init__(mammalName) class NonWingedMammal(Mammal): def __init__(self, NonWingedMammal): print(NonWingedMammal, \"can't fly.\") super().__init__(NonWingedMammal) class NonMarineMammal(Mammal): def __init__(self, NonMarineMammal): print(NonMarineMammal, \"can't swim.\") super().__init__(NonMarineMammal) class Dog(NonMarineMammal, NonWingedMammal): def __init__(self): print('Dog has 4 legs.'); super().__init__('Dog') d = Dog() print('') bat = NonMarineMammal('Bat') \u8f93\u51fa Dog has 4 legs. Dog can't swim. Dog can't flay. Dog is a warm-blooded animal. Dog is an animal. Bat can't swim. Bat is a warm-blooded animal. Bat is an animal. Method Resolution Order\u65b9\u6cd5\u89e3\u6790\u987a\u5e8f (MRO) >>> Dog.__mro__ (<class 'Dog'>, <class 'NonMarineMammal'>, <class 'NonWingedMammal'>, <class 'Mammal'>, <class 'Animal'>, <class 'object'>) Split method in python is outputing an index error one of your lines must be empty deque in python \uff50\uff59\uff54\uff48\uff4f\uff4e\u4e2d\u7684\u53cc\u7aef\u961f\u5217 # Python code to demonstrate deque from collections import deque # Declaring deque queue = deque(['name','age','DOB']) print(queue) ========================================================= Output: deque(['name', 'age', 'DOB']) append() :- This function is used to insert the value in its argument to the right end of deque. appendleft() :- This function is used to insert the value in its argument to the left end of deque. pop() :- This function is used to delete an argument from the right end of deque. popleft() :- This function is used to delete an argument from the left end of deque. index(ele, beg, end) :- This function returns the first index of the value mentioned in arguments, starting searching from beg till end index. insert(i, a) :- This function inserts the value mentioned in arguments(a) at index(i) specified in arguments. remove() :- This function removes the first occurrence of value mentioned in arguments. extend(iterable) :- This function is used to add multiple values at the right end of deque. The argument passed is an iterable. extendleft(iterable) :- This function is used to add multiple values at the left end of deque. The argument passed is an iterable. Order is reversed as a result of left appends. reverse() :- This function is used to reverse order of deque elements. rotate() :- This function rotates the deque by the number specified in arguments. If the number specified is negative, rotation occurs to left. Else rotation is to right. python random.sample() python pip \u4e0d\u80fd\u7528 PIP_NO_CACHE_DIR=off pip install gym num.linspace() in Python import numpy as np print(\"B\\n\", np.linspace(2.0, 3.0, num=5, retstep=True),\"\\n\") x = np.linspace(0, 2, 10) print(\"A\\n\", np.sin(x)) Output B (array([ 2. , 2.25, 2.5 , 2.75, 3. ]), 0.25) A [ 0. 0.22039774 0.42995636 0.6183698 0.77637192 0.8961922 0.9719379 0.99988386 0.9786557 0.90929743] pip \u4e0b\u8f7d\u63d0\u901f \u56fd\u5185\u6e90\uff1a \u65b0\u7248ubuntu\u8981\u6c42\u4f7f\u7528https\u6e90\uff0c\u8981\u6ce8\u610f\u3002 \u6e05\u534e\uff1ahttps://pypi.tuna.tsinghua.edu.cn/simple \u963f\u91cc\u4e91\uff1ahttp://mirrors.aliyun.com/pypi/simple \u4e2d\u56fd\u79d1\u6280\u5927\u5b66 https://pypi.mirrors.ustc.edu.cn/simple \u534e\u4e2d\u7406\u5de5\u5927\u5b66\uff1ahttp://pypi.hustunique.com \u5c71\u4e1c\u7406\u5de5\u5927\u5b66\uff1ahttp://pypi.sdutlinux.org \u8c46\u74e3\uff1ahttp://pypi.douban.com/simple \u4e34\u65f6\u4f7f\u7528\uff1a \u53ef\u4ee5\u5728\u4f7f\u7528pip\u7684\u65f6\u5019\u52a0\u53c2\u6570 pip install -i https://pypi.tuna.tsinghua.edu.cn/simple PIL.Image\u8f6c\u6362\u6210OpenCV\u683c\u5f0f import cv2 from PIL import Image import numpy image = Image.open(\"plane.jpg\") image.show() img = cv2.cvtColor(numpy.asarray(image),cv2.COLOR_RGB2BGR) cv2.imshow(\"OpenCV\",img) cv2.waitKey() OpenCV\u8f6c\u6362\u6210PIL.Image\u683c\u5f0f import cv2 from PIL import Image import numpy img = cv2.imread(\"plane.jpg\") cv2.imshow(\"OpenCV\",img) image = Image.fromarray(cv2.cvtColor(img,cv2.COLOR_BGR2RGB)) image.show() cv2.waitKey() \u6dfb\u52a0python\u8def\u5f84 xport PYTHONPATH=$PYTHONPATH:/home/dell/Deep/DeepSpeech/training Numpy numpy.amax() numpy.argmax numpy.argmax(a, axis=None, out=None)[source]Returns the indices of the maximum values along an axis. parameters \\ a: array_like ,Input array. axis:int, optional , By default, the index is into the flattened array, otherwise along the specified axis. outarray, optional , If provided, the result will be inserted into this array. It should be of the appropriate shape and dtype. Returns index_array:ndarray of ints Array of indices into the array. It has the same shape as a.shape with the dimension along axis removed. np.squeeze Function Remove dimensions of size 1 from ndarray You can use numpy.squeeze() to remove all dimensions of size 1 from the NumPy array ndarray. squeeze() is also provided as a method of ndarray . import numpy as np a = np.arange(6).reshape(1, 2, 1, 3, 1) print(a) # [[[[[0] # [1] # [2]]] # # # [[[3] # [4] # [5]]]]] print(a.shape) # (1, 2, 1, 3, 1) a_s = np.squeeze(a) print(a_s) # [[0 1 2] # [3 4 5]] print(a_s.shape) # (2, 3) By default, all dimensions with size 1 are removed, as in the example above. You can specify the index of the dimension to be removed in the second argument axis of numpy.squeeze() . Dimensions that are not the specified index are not removed. print(a.shape) # (1, 2, 1, 3, 1) print(np.squeeze(a, 0)) # [[[[0] # [1] # [2]]] # # # [[[3] # [4] # [5]]]] print(np.squeeze(a, 0).shape) # (2, 1, 3, 1) An error will occur if you specify a dimension whose size is not 1 or a dimension that does not exist. axis can also be specified as a negative value. -1 corresponds to the last dimension and can be specified by the position from the back. print(np.squeeze(a, -1)) # [[[[0 1 2]] # # [[3 4 5]]]] print(np.squeeze(a, -1).shape) # (1, 2, 1, 3) print(np.squeeze(a, -3)) # [[[[0] # [1] # [2]] # # [[3] # [4] # [5]]]] print(np.squeeze(a, -3).shape) # (1, 2, 3, 1) \u63d0\u53d6\u6587\u4ef6\u548c\u4e0d\u540c\u683c\u5f0f\u6587\u4ef6 # Get all filenames in the dataroot filenames = os.listdir(dataset_root) filenames = [_ for _ in filenames if _.endswith(\".h5\")]#\u63d0\u53d6h5\u6587\u4ef6 \u6839\u636e\u6587\u4ef6\u540d\u7ee7\u7eed\u63d0\u53d6\u6587\u4ef6 ver1_ori = [\"front\", \"back\"] filenames = [_ for _ in filenames if os.path.splitext(_)[0].split(\"_\")[-1] in ver1_ori] \u786e\u4fdd\u6587\u4ef6\u540d\u4e0d\u4e3a \u7a7a assert len(filenames) > 0 \u8fd4\u56de\u5b8c\u6574\u8def\u5f84 # Add to full data path filenames_original = [os.path.join(dataset_root, _) for _ in filenames] \u67e5\u770b\u662f\u5426\u5c5e\u4e8e\u5df2\u77e5\u7c7b # Check modalities avail_modality = [\"rgb\", \"rgbd\"] if not modality in avail_modality: raise ValueError(\"[Error] Unsupported modality. Consider \", avail_modality) .pkl file Your pkl file is, in fact, a serialized pickle file, which means it has been dumped using Python's pickle module. import pickle with open('serialized.pkl', 'rb') as f: data = pickle.load(f) Note gzip is only needed if the file is compressed: import gzip import pickle with gzip.open('mnist.pkl.gz', 'rb') as f: train_set, valid_set, test_set = pickle.load(f) Where each set can be further divided (i.e. for the training set) : train_x, train_y = train_set If you want to display the dataset import matplotlib.cm as cm import matplotlib.pyplot as plt plt.imshow(train_x[0].reshape((28, 28)), cmap=cm.Greys_r) plt.show() \u5224\u65ad\u6587\u4ef6\u662f\u5426\u5b58\u5728 if not os.path.exists(datapoint): raise ValueError(\"[Error] File does not exist.\") h5 file Open In order to open a HDF5 file with the h5py module you can use h5py.File(filename) . import h5py filename = \"vstoxx_data_31032014.h5\" h5 = h5py.File(filename,'r') futures_data = h5['futures_data'] # VSTOXX futures data options_data = h5['options_data'] # VSTOXX call option data h5.close() numpy.random.uniform we can get the random samples from uniform distribution and returns the random samples as numpy array by using this method. # import numpy import numpy as np import matplotlib.pyplot as plt # Using uniform() method gfg = np.random.uniform(2.1, 5.5, 1000) plt.hist(gfg, bins = 100, density = True) plt.show() isinstance() The isinstance() function returns True if the specified object is of the specified type, otherwise False . Parameter Description object Required, An Object type A type or class, or a tuple of types and/or classes \u5224\u65ad\u56fe\u50cf\u683c\u5f0f def _is_pil_image(img): if accimage is not None: return isinstance(img, (Image.Image, accimage.Image)) else: return isinstance(img, Image.Image) PIL\u56fe\u50cf\u4eae\u5ea6\u8c03\u6574 def adjust_brightness(img, brightness_factor): \"\"\"Adjust brightness of an Image. Args: img (PIL Image): PIL Image to be adjusted. brightness_factor (float): How much to adjust the brightness. Can be any non negative number. 0 gives a black image, 1 gives the original image while 2 increases the brightness by a factor of 2. Returns: PIL Image: Brightness adjusted image. \"\"\" if not _is_pil_image(img): raise TypeError('img should be PIL Image. Got {}'.format(type(img))) enhancer = ImageEnhance.Brightness(img) img = enhancer.enhance(brightness_factor) return img Contrast adjusted image def adjust_contrast(img, contrast_factor): \"\"\"Adjust contrast of an Image. Args: img (PIL Image): PIL Image to be adjusted. contrast_factor (float): How much to adjust the contrast. Can be any non negative number. 0 gives a solid gray image, 1 gives the original image while 2 increases the contrast by a factor of 2. Returns: PIL Image: Contrast adjusted image. \"\"\" if not _is_pil_image(img): raise TypeError('img should be PIL Image. Got {}'.format(type(img))) enhancer = ImageEnhance.Contrast(img) img = enhancer.enhance(contrast_factor) return img Saturation adjusted image def adjust_saturation(img, saturation_factor): \"\"\"Adjust color saturation of an image. Args: img (PIL Image): PIL Image to be adjusted. saturation_factor (float): How much to adjust the saturation. 0 will give a black and white image, 1 will give the original image while 2 will enhance the saturation by a factor of 2. Returns: PIL Image: Saturation adjusted image. \"\"\" if not _is_pil_image(img): raise TypeError('img should be PIL Image. Got {}'.format(type(img))) enhancer = ImageEnhance.Color(img) img = enhancer.enhance(saturation_factor) return img Hue adjusted image def adjust_hue(img, hue_factor): \"\"\"Adjust hue of an image. The image hue is adjusted by converting the image to HSV and cyclically shifting the intensities in the hue channel (H). The image is then converted back to original image mode. `hue_factor` is the amount of shift in H channel and must be in the interval `[-0.5, 0.5]`. See https://en.wikipedia.org/wiki/Hue for more details on Hue. Args: img (PIL Image): PIL Image to be adjusted. hue_factor (float): How much to shift the hue channel. Should be in [-0.5, 0.5]. 0.5 and -0.5 give complete reversal of hue channel in HSV space in positive and negative direction respectively. 0 means no shift. Therefore, both -0.5 and 0.5 will give an image with complementary colors while 0 gives the original image. Returns: PIL Image: Hue adjusted image. \"\"\" if not(-0.5 <= hue_factor <= 0.5): raise ValueError('hue_factor is not in [-0.5, 0.5].'.format(hue_factor)) if not _is_pil_image(img): raise TypeError('img should be PIL Image. Got {}'.format(type(img))) input_mode = img.mode if input_mode in {'L', '1', 'I', 'F'}: return img h, s, v = img.convert('HSV').split() np_h = np.array(h, dtype=np.uint8) # uint8 addition take cares of rotation across boundaries with np.errstate(over='ignore'): np_h += np.uint8(hue_factor * 255) h = Image.fromarray(np_h, 'L') img = Image.merge('HSV', (h, s, v)).convert(input_mode) return img Adjust gamma on IMG def adjust_gamma(img, gamma, gain=1): \"\"\"Perform gamma correction on an image. Also known as Power Law Transform. Intensities in RGB mode are adjusted based on the following equation: I_out = 255 * gain * ((I_in / 255) ** gamma) See https://en.wikipedia.org/wiki/Gamma_correction for more details. Args: img (PIL Image): PIL Image to be adjusted. gamma (float): Non negative real number. gamma larger than 1 make the shadows darker, while gamma smaller than 1 make dark regions lighter. gain (float): The constant multiplier. \"\"\" if not _is_pil_image(img): raise TypeError('img should be PIL Image. Got {}'.format(type(img))) if gamma < 0: raise ValueError('Gamma should be a non-negative real number') input_mode = img.mode img = img.convert('RGB') np_img = np.array(img, dtype=np.float32) np_img = 255 * gain * ((np_img / 255) ** gamma) np_img = np.uint8(np.clip(np_img, 0, 255)) img = Image.fromarray(np_img, 'RGB').convert(input_mode) return img np.clip(img, 0, 255) Given an interval, values outside the interval are clipped to the interval edges. For example, if an interval of [0, 1] is specified, values smaller than 0 become 0, and values larger than 1 become 1. Convert np.ndarray img to tensor class ToTensor(object): \"\"\"Convert a ``numpy.ndarray`` to tensor. Converts a numpy.ndarray (H x W x C) to a torch.FloatTensor of shape (C x H x W). \"\"\" def __call__(self, img): \"\"\"Convert a ``numpy.ndarray`` to tensor. Args: img (numpy.ndarray): Image to be converted to tensor. Returns: Tensor: Converted image. \"\"\" if not(_is_numpy_image(img)): raise TypeError('img should be ndarray. Got {}'.format(type(img))) if isinstance(img, np.ndarray): # handle numpy array if img.ndim == 3: img = torch.from_numpy(img.transpose((2, 0, 1)).copy()) elif img.ndim == 2: img = torch.from_numpy(img.copy()) else: raise RuntimeError('img should be ndarray with 2 or 3 dimensions. Got {}'.format(img.ndim)) # backward compatibility # return img.float().div(255) return img.float() math.floor Round numbers down to the nearest integer #Import math library import math # Round numbers down to the nearest integer print(math.floor(0.6)) print(math.floor(1.4)) print(math.floor(5.3)) print(math.floor(-5.3)) print(math.floor(22.6)) print(math.floor(10.0)) \u200b # 0 # 1 # 5 # -6 # 22 # 10 unsqueeze If you look at the shape of the array before and after, you see that before it was (4,) and after it is (1, 4) (when second parameter is 0) and (4, 1) (when second parameter is 1) . So a 1 was inserted in the shape of the array at axis 0 or 1, depending on the value of the second parameter. That is opposite of np.squeeze() (nomenclature borrowed from MATLAB) which removes axes of size 1 (singletons). >>> x = torch.tensor([1, 2, 3, 4]) >>> torch.unsqueeze(x, 0) tensor([[ 1, 2, 3, 4]]) >>> torch.unsqueeze(x, 1) tensor([[ 1], [ 2], [ 3], [ 4]]) np.transpose The transpose() function is used to permute\u7f6e\u6362 the dimensions of an array. import numpy as np a = np.ones((2,3,4)) print(a.shape) print(\"--------------------------------\") print(np.transpose(a,(2,0,1)).shape) print(\"--------------------------------\") print(np.transpose(a,(2,1,0)).shape) # (2, 3, 4) # -------------------------------- # (4, 2, 3) # -------------------------------- # (4, 3, 2) # transpose\u91cc\u7684 0\u4ee3\u8868\u7b2c\u4e00\u7ef4\u6848\u4f8b\u4e2d\u662f2\uff0c 1\u4ee3\u8868\u7b2c\u4e8c\u7ef4\u6848\u4f8b\u4e2d\u662f3\uff0c 2\u4ee3\u8868\u7b2c\u4e09\u7ef4\u6848\u4f8b\u4e2d\u662f4. What does axis=0 do in Numpy's sum function a = np.array([[1, 2, 3], [4, 5, 6]]) np.sum(a, axis = 0) # result array([5, 7, 9]) a = np.array([1, 2, 3]) np.sum(a, axis = 0) #result 6 Pad the points to 512 num_points = points.shape[0] target_size = 512 output_points = np.repeat(points[-1, :][None, ...], target_size, axis=0) output_points[:num_points, :] = points output_labels = np.repeat(labels[-1, :][None, ...], target_size, axis=0) output_labels[:num_points, :] = labels numpy.random.shuffle() With the help of numpy.random.shuffle() method, we can get the random positioning of different integer values in the numpy array or we can say that all the values in an array will be shuffled randomly. OrderedDict An OrderedDict is a dictionary subclass that remembers the order that keys were first inserted . Image to Array & Array to Image from PIL import Image import numpy as np im = Image.open('1.jpg') im2arr = np.array(im) # im2arr.shape: height x width x channel arr2im = Image.fromarray(im2arr)","title":"Python"},{"location":"python/#conda-the-following-packages-are-not-available-from-current-channels","text":"conda config --append channels conda-forge It tells conda to also look on the conda-forge channel when you search for packages.","title":"conda The following packages are not available from current channels:"},{"location":"python/#conda","text":"conda create -n xception_net python==3.6.5 numpy==1.17.4 scipy==1.3.3 h5py==2.10.0 Keras==2.3.1 tensorflow-gpu==1.15.0 As the comment at the top indicates, the output of conda list -e > requirements.txt can be used to create a conda virtual environment with conda create --name <env> --file requirements.txt","title":"conda \u521b\u5efa\u73af\u5883"},{"location":"python/#cudnn_status_internal_error-tensorflow","text":"You can try Allowing GPU memory growth with: import tensorflow as tf gpu = tf.config.experimental.list_physical_devices('GPU') tf.config.experimental.set_memory_growth(gpu[0], True)","title":"cudnn_status_internal_error tensorflow"},{"location":"python/#vscode-activate-conda-env","text":"{ \"ros.distro\": \"melodic\", \"python.autoComplete.extraPaths\": [ \"/home/pmjd/Disk/anaconda3/envs/wjj/lib/python3.6/site-packages\" ], \"python.terminal.activateEnvInCurrentTerminal\": true, \"python.condaPath\": \"/home/pmjd/Disk/anaconda3/bin/conda\", \"python.defaultInterpreterPath\": \"/home/pmjd/Disk/anaconda3/envs/wjj/bin/python\" }","title":"vscode activate conda env"},{"location":"python/#nameerror-name-xrange-is-not-defined","text":"try: # Python 2 xrange except NameError: # Python 3, xrange is now named range xrange = range","title":"NameError: name 'xrange' is not defined"},{"location":"python/#tensorflow-valueerror-cannot-feed-value-of-shape-64-64-3-for-tensor-uplaceholder0-which-has-shape-64-64-3","text":"image has a shape of (64,64,3). Your input placeholder _x have a shape of (?, 64,64,3). The problem is that you're feeding the placeholder with a value of a different shape. You have to feed it with a value of (1, 64, 64, 3) = a batch of 1 image. Just reshape your image value to a batch with size one. np.expand_dims(img, axis=0)","title":"TensorFlow ValueError: Cannot feed value of shape (64, 64, 3) for Tensor u'Placeholder:0', which has shape '(?, 64, 64, 3)'"},{"location":"python/#opencv-conda","text":"opencv is not compatible with python 3. I had to install opencv3 for python 3. The marked answer in how could we install opencv on anaconda? explains how to install opencv(3) for anaconda: Run the following command: conda install -c https://conda.binstar.org/menpo opencv I realized that opencv3 is also available now, run the following command: conda install -c https://conda.binstar.org/menpo opencv3 Edit on Aug 18, 2016: You may like to add the \"menpo\" channel permanently by: conda config --add channels menpo And then opencv can be installed by: conda install opencv (or opencv3) Edit on Aug 14, 2017: \"clinicalgraphics\" channel provides relatively newer vtk version for very recent python3 conda install -c clinicalgraphics vtk Edit on April 16, 2020 (based on @AMC's comment): OpenCV can be installed through conda-forge (details see here) conda install -c conda-forge opencv","title":"opencv conda"},{"location":"python/#pointcloud2-to-array","text":"def pointcloud2_to_array(cloud_msg, squeeze=True): dtype_list = fields_to_dtype(cloud_msg.fields, cloud_msg.point_step) cloud_arr = np.fromstring(cloud_msg.data, dtype_list) cloud_arr = cloud_arr[ [fname for fname, _type in dtype_list if not (fname[:len(DUMMY_FIELD_PREFIX)] == DUMMY_FIELD_PREFIX)]] if squeeze and cloud_msg.height == 1: return np.reshape(cloud_arr, (cloud_msg.width,)) else: return np.reshape(cloud_arr, (cloud_msg.height, cloud_msg.width))","title":"pointcloud2 to array"},{"location":"python/#read-pcd-to-array","text":"import numpy as np import open3d as o3d pcd = o3d.io.read_point_cloud(\"pointcloud_path.pcd\") out_arr = np.asarray(pcd.points) print (\"output array from input list : \", out_arr)","title":"read pcd to array"},{"location":"python/#kitti","text":"\u56fe\u7247\u4e0b\u8f7d https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_image_2.zip \u70b9\u4e91\u4e0b\u8f7d https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_velodyne.zip \u6807\u7b7e\u4e0b\u8f7d https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_label_2.zip \u77eb\u6b63\u6587\u4ef6\u4e0b\u8f7d https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_calib.zip","title":"kitti \u6570\u636e\u96c6\u4e0b\u8f7d"},{"location":"python/#python-operator","text":"In python 2.x >>> 10/3 3 >>> # To get a floating point number from integer division: >>> 10.0/3 3.3333333333333335 >>> float(10)/3 3.3333333333333335 In python 3.x >>> 10/3 3.3333333333333335 >>> 10//3 3","title":"python \"//\" operator"},{"location":"python/#python-super","text":"\u5185\u7f6e\u7684super()\u8fd4\u56de\u4e00\u4e2a\u4ee3\u7406\u5bf9\u8c61\uff08\u8d85\u7c7b\u7684\u4e34\u65f6\u5bf9\u8c61),\u8be5\u4ee3\u7406\u5bf9\u8c61\u5141\u8bb8\u6211\u4eec\u8bbf\u95ee\u57fa\u7c7b\u7684\u65b9\u6cd5\u3002\u5728python\u4e2d\uff0csuper()\u6709\u4e24\u4e2a\u4e3b\u8981\u7528\u4f8b\uff1a \u8ba9\u6211\u4eec\u907f\u514d\u663e\u793a\u4f7f\u7528\u57fa\u7c7b\u540d\u79f0 \u5904\u7406\u591a\u91cd\u7ee7\u627f\\","title":"python super()"},{"location":"python/#1super","text":"\u5728\u5355\u7ee7\u627f\u7684\u60c5\u51b5\u4e0b\uff0c\u5b83\u5141\u8bb8\u6211\u4eec\u901a\u8fc7\u5f15\u7528\u57fa\u7c7bsuper() class Mammal(object): def __init__(self, mammalName): print(mammalName, 'is a warm-blooded animal.') class Dog(Mammal): def __init__(self): print('Dog has four legs.') super().__init__('Dog') d1 = Dog() \u8f93\u51fa Dog has four legs. Dog is a warm-blooded animal. \u8be5super()\u5185\u5efa\u8fd4\u56de\u4e00\u4e2a\u4ee3\u7406\u5bf9\u8c61\uff0c\u66ff\u4ee3\u5bf9\u8c61\uff0c\u53ef\u4ee5\u901a\u8fc7\u59d4\u6258\u8c03\u7528\u57fa\u7c7b\u7684\u65b9\u6cd5\uff0c\u8fd9\u79f0\u4e3a\u201c\u95f4\u63a5\"(\u4f7f\u7528\u5f15\u7528\u57fa\u7840\u5bf9\u8c61\u7684\u80fd\u529bsuper()) \u7531\u4e8e\u95f4\u63a5\u662f\u5728\u8fd0\u884c\u65f6\u8ba1\u7b97\u7684\uff0c\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u5728\u4e0d\u540c\u7684\u65f6\u95f4\u4f7f\u7528\u4e0d\u540c\u7684\u57fa\u7c7b\uff08\u5982\u679c\u9700\u8981\uff09","title":"\u793a\u4f8b\uff11\uff1a\u5177\u6709\u5355\u7ee7\u627f\u7684super()"},{"location":"python/#2super","text":"class Animal: def __init__(self, Animal): print(Animal, 'is an animal.'); class Mammal(Animal): def __init__(self, mammalName): print(mammalName, 'is a warm-blooded animal.') super().__init__(mammalName) class NonWingedMammal(Mammal): def __init__(self, NonWingedMammal): print(NonWingedMammal, \"can't fly.\") super().__init__(NonWingedMammal) class NonMarineMammal(Mammal): def __init__(self, NonMarineMammal): print(NonMarineMammal, \"can't swim.\") super().__init__(NonMarineMammal) class Dog(NonMarineMammal, NonWingedMammal): def __init__(self): print('Dog has 4 legs.'); super().__init__('Dog') d = Dog() print('') bat = NonMarineMammal('Bat') \u8f93\u51fa Dog has 4 legs. Dog can't swim. Dog can't flay. Dog is a warm-blooded animal. Dog is an animal. Bat can't swim. Bat is a warm-blooded animal. Bat is an animal.","title":"\u793a\u4f8b\uff12\uff1a\u5177\u6709\u591a\u91cd\u7ee7\u627f\u7684super()"},{"location":"python/#method-resolution-order-mro","text":">>> Dog.__mro__ (<class 'Dog'>, <class 'NonMarineMammal'>, <class 'NonWingedMammal'>, <class 'Mammal'>, <class 'Animal'>, <class 'object'>)","title":"Method Resolution Order\u65b9\u6cd5\u89e3\u6790\u987a\u5e8f (MRO)"},{"location":"python/#split-method-in-python-is-outputing-an-index-error","text":"one of your lines must be empty","title":"Split method in python is outputing an index error"},{"location":"python/#deque-in-python-python","text":"# Python code to demonstrate deque from collections import deque # Declaring deque queue = deque(['name','age','DOB']) print(queue) ========================================================= Output: deque(['name', 'age', 'DOB']) append() :- This function is used to insert the value in its argument to the right end of deque. appendleft() :- This function is used to insert the value in its argument to the left end of deque. pop() :- This function is used to delete an argument from the right end of deque. popleft() :- This function is used to delete an argument from the left end of deque. index(ele, beg, end) :- This function returns the first index of the value mentioned in arguments, starting searching from beg till end index. insert(i, a) :- This function inserts the value mentioned in arguments(a) at index(i) specified in arguments. remove() :- This function removes the first occurrence of value mentioned in arguments. extend(iterable) :- This function is used to add multiple values at the right end of deque. The argument passed is an iterable. extendleft(iterable) :- This function is used to add multiple values at the left end of deque. The argument passed is an iterable. Order is reversed as a result of left appends. reverse() :- This function is used to reverse order of deque elements. rotate() :- This function rotates the deque by the number specified in arguments. If the number specified is negative, rotation occurs to left. Else rotation is to right.","title":"deque in python \uff50\uff59\uff54\uff48\uff4f\uff4e\u4e2d\u7684\u53cc\u7aef\u961f\u5217"},{"location":"python/#python-randomsample","text":"","title":"python random.sample()"},{"location":"python/#python-pip","text":"PIP_NO_CACHE_DIR=off pip install gym","title":"python pip \u4e0d\u80fd\u7528"},{"location":"python/#numlinspace-in-python","text":"import numpy as np print(\"B\\n\", np.linspace(2.0, 3.0, num=5, retstep=True),\"\\n\") x = np.linspace(0, 2, 10) print(\"A\\n\", np.sin(x)) Output B (array([ 2. , 2.25, 2.5 , 2.75, 3. ]), 0.25) A [ 0. 0.22039774 0.42995636 0.6183698 0.77637192 0.8961922 0.9719379 0.99988386 0.9786557 0.90929743]","title":"num.linspace() in Python"},{"location":"python/#pip","text":"\u56fd\u5185\u6e90\uff1a \u65b0\u7248ubuntu\u8981\u6c42\u4f7f\u7528https\u6e90\uff0c\u8981\u6ce8\u610f\u3002 \u6e05\u534e\uff1ahttps://pypi.tuna.tsinghua.edu.cn/simple \u963f\u91cc\u4e91\uff1ahttp://mirrors.aliyun.com/pypi/simple \u4e2d\u56fd\u79d1\u6280\u5927\u5b66 https://pypi.mirrors.ustc.edu.cn/simple \u534e\u4e2d\u7406\u5de5\u5927\u5b66\uff1ahttp://pypi.hustunique.com \u5c71\u4e1c\u7406\u5de5\u5927\u5b66\uff1ahttp://pypi.sdutlinux.org \u8c46\u74e3\uff1ahttp://pypi.douban.com/simple \u4e34\u65f6\u4f7f\u7528\uff1a \u53ef\u4ee5\u5728\u4f7f\u7528pip\u7684\u65f6\u5019\u52a0\u53c2\u6570 pip install -i https://pypi.tuna.tsinghua.edu.cn/simple","title":"pip \u4e0b\u8f7d\u63d0\u901f"},{"location":"python/#pilimageopencv","text":"import cv2 from PIL import Image import numpy image = Image.open(\"plane.jpg\") image.show() img = cv2.cvtColor(numpy.asarray(image),cv2.COLOR_RGB2BGR) cv2.imshow(\"OpenCV\",img) cv2.waitKey()","title":"PIL.Image\u8f6c\u6362\u6210OpenCV\u683c\u5f0f"},{"location":"python/#opencvpilimage","text":"import cv2 from PIL import Image import numpy img = cv2.imread(\"plane.jpg\") cv2.imshow(\"OpenCV\",img) image = Image.fromarray(cv2.cvtColor(img,cv2.COLOR_BGR2RGB)) image.show() cv2.waitKey()","title":"OpenCV\u8f6c\u6362\u6210PIL.Image\u683c\u5f0f"},{"location":"python/#python","text":"xport PYTHONPATH=$PYTHONPATH:/home/dell/Deep/DeepSpeech/training","title":"\u6dfb\u52a0python\u8def\u5f84"},{"location":"python/#numpy","text":"","title":"Numpy"},{"location":"python/#numpyamax","text":"","title":"numpy.amax()"},{"location":"python/#numpyargmax","text":"numpy.argmax(a, axis=None, out=None)[source]Returns the indices of the maximum values along an axis. parameters \\ a: array_like ,Input array. axis:int, optional , By default, the index is into the flattened array, otherwise along the specified axis. outarray, optional , If provided, the result will be inserted into this array. It should be of the appropriate shape and dtype. Returns index_array:ndarray of ints Array of indices into the array. It has the same shape as a.shape with the dimension along axis removed.","title":"numpy.argmax"},{"location":"python/#npsqueeze","text":"Function Remove dimensions of size 1 from ndarray You can use numpy.squeeze() to remove all dimensions of size 1 from the NumPy array ndarray. squeeze() is also provided as a method of ndarray . import numpy as np a = np.arange(6).reshape(1, 2, 1, 3, 1) print(a) # [[[[[0] # [1] # [2]]] # # # [[[3] # [4] # [5]]]]] print(a.shape) # (1, 2, 1, 3, 1) a_s = np.squeeze(a) print(a_s) # [[0 1 2] # [3 4 5]] print(a_s.shape) # (2, 3) By default, all dimensions with size 1 are removed, as in the example above. You can specify the index of the dimension to be removed in the second argument axis of numpy.squeeze() . Dimensions that are not the specified index are not removed. print(a.shape) # (1, 2, 1, 3, 1) print(np.squeeze(a, 0)) # [[[[0] # [1] # [2]]] # # # [[[3] # [4] # [5]]]] print(np.squeeze(a, 0).shape) # (2, 1, 3, 1) An error will occur if you specify a dimension whose size is not 1 or a dimension that does not exist. axis can also be specified as a negative value. -1 corresponds to the last dimension and can be specified by the position from the back. print(np.squeeze(a, -1)) # [[[[0 1 2]] # # [[3 4 5]]]] print(np.squeeze(a, -1).shape) # (1, 2, 1, 3) print(np.squeeze(a, -3)) # [[[[0] # [1] # [2]] # # [[3] # [4] # [5]]]] print(np.squeeze(a, -3).shape) # (1, 2, 3, 1)","title":"np.squeeze"},{"location":"python/#_1","text":"# Get all filenames in the dataroot filenames = os.listdir(dataset_root) filenames = [_ for _ in filenames if _.endswith(\".h5\")]#\u63d0\u53d6h5\u6587\u4ef6","title":"\u63d0\u53d6\u6587\u4ef6\u548c\u4e0d\u540c\u683c\u5f0f\u6587\u4ef6"},{"location":"python/#_2","text":"ver1_ori = [\"front\", \"back\"] filenames = [_ for _ in filenames if os.path.splitext(_)[0].split(\"_\")[-1] in ver1_ori]","title":"\u6839\u636e\u6587\u4ef6\u540d\u7ee7\u7eed\u63d0\u53d6\u6587\u4ef6"},{"location":"python/#_3","text":"assert len(filenames) > 0","title":"\u786e\u4fdd\u6587\u4ef6\u540d\u4e0d\u4e3a\u7a7a"},{"location":"python/#_4","text":"# Add to full data path filenames_original = [os.path.join(dataset_root, _) for _ in filenames]","title":"\u8fd4\u56de\u5b8c\u6574\u8def\u5f84"},{"location":"python/#_5","text":"# Check modalities avail_modality = [\"rgb\", \"rgbd\"] if not modality in avail_modality: raise ValueError(\"[Error] Unsupported modality. Consider \", avail_modality)","title":"\u67e5\u770b\u662f\u5426\u5c5e\u4e8e\u5df2\u77e5\u7c7b"},{"location":"python/#pkl-file","text":"Your pkl file is, in fact, a serialized pickle file, which means it has been dumped using Python's pickle module. import pickle with open('serialized.pkl', 'rb') as f: data = pickle.load(f) Note gzip is only needed if the file is compressed: import gzip import pickle with gzip.open('mnist.pkl.gz', 'rb') as f: train_set, valid_set, test_set = pickle.load(f) Where each set can be further divided (i.e. for the training set) : train_x, train_y = train_set If you want to display the dataset import matplotlib.cm as cm import matplotlib.pyplot as plt plt.imshow(train_x[0].reshape((28, 28)), cmap=cm.Greys_r) plt.show()","title":".pkl file"},{"location":"python/#_6","text":"if not os.path.exists(datapoint): raise ValueError(\"[Error] File does not exist.\")","title":"\u5224\u65ad\u6587\u4ef6\u662f\u5426\u5b58\u5728"},{"location":"python/#h5-file-open","text":"In order to open a HDF5 file with the h5py module you can use h5py.File(filename) . import h5py filename = \"vstoxx_data_31032014.h5\" h5 = h5py.File(filename,'r') futures_data = h5['futures_data'] # VSTOXX futures data options_data = h5['options_data'] # VSTOXX call option data h5.close()","title":"h5 file Open"},{"location":"python/#numpyrandomuniform","text":"we can get the random samples from uniform distribution and returns the random samples as numpy array by using this method. # import numpy import numpy as np import matplotlib.pyplot as plt # Using uniform() method gfg = np.random.uniform(2.1, 5.5, 1000) plt.hist(gfg, bins = 100, density = True) plt.show()","title":"numpy.random.uniform"},{"location":"python/#isinstance","text":"The isinstance() function returns True if the specified object is of the specified type, otherwise False . Parameter Description object Required, An Object type A type or class, or a tuple of types and/or classes","title":"isinstance()"},{"location":"python/#_7","text":"def _is_pil_image(img): if accimage is not None: return isinstance(img, (Image.Image, accimage.Image)) else: return isinstance(img, Image.Image)","title":"\u5224\u65ad\u56fe\u50cf\u683c\u5f0f"},{"location":"python/#pil","text":"def adjust_brightness(img, brightness_factor): \"\"\"Adjust brightness of an Image. Args: img (PIL Image): PIL Image to be adjusted. brightness_factor (float): How much to adjust the brightness. Can be any non negative number. 0 gives a black image, 1 gives the original image while 2 increases the brightness by a factor of 2. Returns: PIL Image: Brightness adjusted image. \"\"\" if not _is_pil_image(img): raise TypeError('img should be PIL Image. Got {}'.format(type(img))) enhancer = ImageEnhance.Brightness(img) img = enhancer.enhance(brightness_factor) return img","title":"PIL\u56fe\u50cf\u4eae\u5ea6\u8c03\u6574"},{"location":"python/#contrast-adjusted-image","text":"def adjust_contrast(img, contrast_factor): \"\"\"Adjust contrast of an Image. Args: img (PIL Image): PIL Image to be adjusted. contrast_factor (float): How much to adjust the contrast. Can be any non negative number. 0 gives a solid gray image, 1 gives the original image while 2 increases the contrast by a factor of 2. Returns: PIL Image: Contrast adjusted image. \"\"\" if not _is_pil_image(img): raise TypeError('img should be PIL Image. Got {}'.format(type(img))) enhancer = ImageEnhance.Contrast(img) img = enhancer.enhance(contrast_factor) return img","title":"Contrast adjusted image"},{"location":"python/#saturation-adjusted-image","text":"def adjust_saturation(img, saturation_factor): \"\"\"Adjust color saturation of an image. Args: img (PIL Image): PIL Image to be adjusted. saturation_factor (float): How much to adjust the saturation. 0 will give a black and white image, 1 will give the original image while 2 will enhance the saturation by a factor of 2. Returns: PIL Image: Saturation adjusted image. \"\"\" if not _is_pil_image(img): raise TypeError('img should be PIL Image. Got {}'.format(type(img))) enhancer = ImageEnhance.Color(img) img = enhancer.enhance(saturation_factor) return img","title":"Saturation adjusted image"},{"location":"python/#hue-adjusted-image","text":"def adjust_hue(img, hue_factor): \"\"\"Adjust hue of an image. The image hue is adjusted by converting the image to HSV and cyclically shifting the intensities in the hue channel (H). The image is then converted back to original image mode. `hue_factor` is the amount of shift in H channel and must be in the interval `[-0.5, 0.5]`. See https://en.wikipedia.org/wiki/Hue for more details on Hue. Args: img (PIL Image): PIL Image to be adjusted. hue_factor (float): How much to shift the hue channel. Should be in [-0.5, 0.5]. 0.5 and -0.5 give complete reversal of hue channel in HSV space in positive and negative direction respectively. 0 means no shift. Therefore, both -0.5 and 0.5 will give an image with complementary colors while 0 gives the original image. Returns: PIL Image: Hue adjusted image. \"\"\" if not(-0.5 <= hue_factor <= 0.5): raise ValueError('hue_factor is not in [-0.5, 0.5].'.format(hue_factor)) if not _is_pil_image(img): raise TypeError('img should be PIL Image. Got {}'.format(type(img))) input_mode = img.mode if input_mode in {'L', '1', 'I', 'F'}: return img h, s, v = img.convert('HSV').split() np_h = np.array(h, dtype=np.uint8) # uint8 addition take cares of rotation across boundaries with np.errstate(over='ignore'): np_h += np.uint8(hue_factor * 255) h = Image.fromarray(np_h, 'L') img = Image.merge('HSV', (h, s, v)).convert(input_mode) return img","title":"Hue adjusted image"},{"location":"python/#adjust-gamma-on-img","text":"def adjust_gamma(img, gamma, gain=1): \"\"\"Perform gamma correction on an image. Also known as Power Law Transform. Intensities in RGB mode are adjusted based on the following equation: I_out = 255 * gain * ((I_in / 255) ** gamma) See https://en.wikipedia.org/wiki/Gamma_correction for more details. Args: img (PIL Image): PIL Image to be adjusted. gamma (float): Non negative real number. gamma larger than 1 make the shadows darker, while gamma smaller than 1 make dark regions lighter. gain (float): The constant multiplier. \"\"\" if not _is_pil_image(img): raise TypeError('img should be PIL Image. Got {}'.format(type(img))) if gamma < 0: raise ValueError('Gamma should be a non-negative real number') input_mode = img.mode img = img.convert('RGB') np_img = np.array(img, dtype=np.float32) np_img = 255 * gain * ((np_img / 255) ** gamma) np_img = np.uint8(np.clip(np_img, 0, 255)) img = Image.fromarray(np_img, 'RGB').convert(input_mode) return img","title":"Adjust gamma on IMG"},{"location":"python/#npclipimg-0-255","text":"Given an interval, values outside the interval are clipped to the interval edges. For example, if an interval of [0, 1] is specified, values smaller than 0 become 0, and values larger than 1 become 1.","title":"np.clip(img, 0, 255)"},{"location":"python/#convert-npndarray-img-to-tensor","text":"class ToTensor(object): \"\"\"Convert a ``numpy.ndarray`` to tensor. Converts a numpy.ndarray (H x W x C) to a torch.FloatTensor of shape (C x H x W). \"\"\" def __call__(self, img): \"\"\"Convert a ``numpy.ndarray`` to tensor. Args: img (numpy.ndarray): Image to be converted to tensor. Returns: Tensor: Converted image. \"\"\" if not(_is_numpy_image(img)): raise TypeError('img should be ndarray. Got {}'.format(type(img))) if isinstance(img, np.ndarray): # handle numpy array if img.ndim == 3: img = torch.from_numpy(img.transpose((2, 0, 1)).copy()) elif img.ndim == 2: img = torch.from_numpy(img.copy()) else: raise RuntimeError('img should be ndarray with 2 or 3 dimensions. Got {}'.format(img.ndim)) # backward compatibility # return img.float().div(255) return img.float()","title":"Convert np.ndarray img to tensor"},{"location":"python/#mathfloor","text":"Round numbers down to the nearest integer #Import math library import math # Round numbers down to the nearest integer print(math.floor(0.6)) print(math.floor(1.4)) print(math.floor(5.3)) print(math.floor(-5.3)) print(math.floor(22.6)) print(math.floor(10.0)) \u200b # 0 # 1 # 5 # -6 # 22 # 10","title":"math.floor"},{"location":"python/#unsqueeze","text":"If you look at the shape of the array before and after, you see that before it was (4,) and after it is (1, 4) (when second parameter is 0) and (4, 1) (when second parameter is 1) . So a 1 was inserted in the shape of the array at axis 0 or 1, depending on the value of the second parameter. That is opposite of np.squeeze() (nomenclature borrowed from MATLAB) which removes axes of size 1 (singletons). >>> x = torch.tensor([1, 2, 3, 4]) >>> torch.unsqueeze(x, 0) tensor([[ 1, 2, 3, 4]]) >>> torch.unsqueeze(x, 1) tensor([[ 1], [ 2], [ 3], [ 4]])","title":"unsqueeze"},{"location":"python/#nptranspose","text":"The transpose() function is used to permute\u7f6e\u6362 the dimensions of an array. import numpy as np a = np.ones((2,3,4)) print(a.shape) print(\"--------------------------------\") print(np.transpose(a,(2,0,1)).shape) print(\"--------------------------------\") print(np.transpose(a,(2,1,0)).shape) # (2, 3, 4) # -------------------------------- # (4, 2, 3) # -------------------------------- # (4, 3, 2) # transpose\u91cc\u7684 0\u4ee3\u8868\u7b2c\u4e00\u7ef4\u6848\u4f8b\u4e2d\u662f2\uff0c 1\u4ee3\u8868\u7b2c\u4e8c\u7ef4\u6848\u4f8b\u4e2d\u662f3\uff0c 2\u4ee3\u8868\u7b2c\u4e09\u7ef4\u6848\u4f8b\u4e2d\u662f4.","title":"np.transpose"},{"location":"python/#what-does-axis0-do-in-numpys-sum-function","text":"a = np.array([[1, 2, 3], [4, 5, 6]]) np.sum(a, axis = 0) # result array([5, 7, 9]) a = np.array([1, 2, 3]) np.sum(a, axis = 0) #result 6","title":"What does axis=0 do in Numpy's sum function"},{"location":"python/#pad-the-points-to-512","text":"num_points = points.shape[0] target_size = 512 output_points = np.repeat(points[-1, :][None, ...], target_size, axis=0) output_points[:num_points, :] = points output_labels = np.repeat(labels[-1, :][None, ...], target_size, axis=0) output_labels[:num_points, :] = labels","title":"Pad the points to 512"},{"location":"python/#numpyrandomshuffle","text":"With the help of numpy.random.shuffle() method, we can get the random positioning of different integer values in the numpy array or we can say that all the values in an array will be shuffled randomly.","title":"numpy.random.shuffle()"},{"location":"python/#ordereddict","text":"An OrderedDict is a dictionary subclass that remembers the order that keys were first inserted .","title":"OrderedDict"},{"location":"python/#image-to-array-array-to-image","text":"from PIL import Image import numpy as np im = Image.open('1.jpg') im2arr = np.array(im) # im2arr.shape: height x width x channel arr2im = Image.fromarray(im2arr)","title":"Image to Array &amp; Array to Image"},{"location":"python%E6%9C%89%E7%94%A8%E7%9A%84%E5%87%BD%E6%95%B0/","text":"python \u6709\u7528\u7684\u51fd\u6570\uff08\u4e00\uff09 \"\"\" Adapt from fangchangma/sparse_to_dense.pytorch on github \"\"\" from __future__ import print_function from __future__ import division from __future__ import absolute_import import torch import torchvision import math import random from PIL import Image, ImageOps, ImageEnhance try: import accimage except ImportError: accimage = None import numpy as np import numbers import types import collections import scipy.ndimage.interpolation as itpl import scipy.misc as misc # Check whether the input is a numpy array def _is_numpy_image(img): return isinstance(img, np.ndarray) and (img.ndim in {2, 3}) # Check whether the input is a PIL image def _is_pil_image(img): if accimage is not None: return isinstance(img, (Image.Image, accimage.Image)) else: return isinstance(img, Image.Image) # Check wheter the input is a tensor def _is_tensor_image(img): return torch.is_tensor(img) and img.ndimension() == 3 def adjust_brightness(img, brightness_factor): \"\"\"Adjust brightness of an Image. Args: img (PIL Image): PIL Image to be adjusted. brightness_factor (float): How much to adjust the brightness. Can be any non negative number. 0 gives a black image, 1 gives the original image while 2 increases the brightness by a factor of 2. Returns: PIL Image: Brightness adjusted image. \"\"\" if not _is_pil_image(img): raise TypeError('img should be PIL Image. Got {}'.format(type(img))) enhancer = ImageEnhance.Brightness(img) img = enhancer.enhance(brightness_factor) return img def adjust_contrast(img, contrast_factor): \"\"\"Adjust contrast of an Image. Args: img (PIL Image): PIL Image to be adjusted. contrast_factor (float): How much to adjust the contrast. Can be any non negative number. 0 gives a solid gray image, 1 gives the original image while 2 increases the contrast by a factor of 2. Returns: PIL Image: Contrast adjusted image. \"\"\" if not _is_pil_image(img): raise TypeError('img should be PIL Image. Got {}'.format(type(img))) enhancer = ImageEnhance.Contrast(img) img = enhancer.enhance(contrast_factor) return img def adjust_saturation(img, saturation_factor): \"\"\"Adjust color saturation of an image. Args: img (PIL Image): PIL Image to be adjusted. saturation_factor (float): How much to adjust the saturation. 0 will give a black and white image, 1 will give the original image while 2 will enhance the saturation by a factor of 2. Returns: PIL Image: Saturation adjusted image. \"\"\" if not _is_pil_image(img): raise TypeError('img should be PIL Image. Got {}'.format(type(img))) enhancer = ImageEnhance.Color(img) img = enhancer.enhance(saturation_factor) return img def adjust_hue(img, hue_factor): \"\"\"Adjust hue of an image. The image hue is adjusted by converting the image to HSV and cyclically shifting the intensities in the hue channel (H). The image is then converted back to original image mode. `hue_factor` is the amount of shift in H channel and must be in the interval `[-0.5, 0.5]`. See https://en.wikipedia.org/wiki/Hue for more details on Hue. Args: img (PIL Image): PIL Image to be adjusted. hue_factor (float): How much to shift the hue channel. Should be in [-0.5, 0.5]. 0.5 and -0.5 give complete reversal of hue channel in HSV space in positive and negative direction respectively. 0 means no shift. Therefore, both -0.5 and 0.5 will give an image with complementary colors while 0 gives the original image. Returns: PIL Image: Hue adjusted image. \"\"\" if not(-0.5 <= hue_factor <= 0.5): raise ValueError('hue_factor is not in [-0.5, 0.5].'.format(hue_factor)) if not _is_pil_image(img): raise TypeError('img should be PIL Image. Got {}'.format(type(img))) input_mode = img.mode if input_mode in {'L', '1', 'I', 'F'}: return img h, s, v = img.convert('HSV').split() np_h = np.array(h, dtype=np.uint8) # uint8 addition take cares of rotation across boundaries with np.errstate(over='ignore'): np_h += np.uint8(hue_factor * 255) h = Image.fromarray(np_h, 'L') img = Image.merge('HSV', (h, s, v)).convert(input_mode) return img def adjust_gamma(img, gamma, gain=1): \"\"\"Perform gamma correction on an image. Also known as Power Law Transform. Intensities in RGB mode are adjusted based on the following equation: I_out = 255 * gain * ((I_in / 255) ** gamma) See https://en.wikipedia.org/wiki/Gamma_correction for more details. Args: img (PIL Image): PIL Image to be adjusted. gamma (float): Non negative real number. gamma larger than 1 make the shadows darker, while gamma smaller than 1 make dark regions lighter. gain (float): The constant multiplier. \"\"\" if not _is_pil_image(img): raise TypeError('img should be PIL Image. Got {}'.format(type(img))) if gamma < 0: raise ValueError('Gamma should be a non-negative real number') input_mode = img.mode img = img.convert('RGB') np_img = np.array(img, dtype=np.float32) np_img = 255 * gain * ((np_img / 255) ** gamma) np_img = np.uint8(np.clip(np_img, 0, 255)) img = Image.fromarray(np_img, 'RGB').convert(input_mode) return img class Compose(object): \"\"\"Composes several transforms together. Args: transforms (list of ``Transform`` objects): list of transforms to compose. Example: >>> transforms.Compose([ >>> transforms.CenterCrop(10), >>> transforms.ToTensor(), >>> ]) \"\"\" def __init__(self, transforms): self.transforms = transforms def __call__(self, img): for t in self.transforms: img = t(img) return img class ToTensor(object): \"\"\"Convert a ``numpy.ndarray`` to tensor. Converts a numpy.ndarray (H x W x C) to a torch.FloatTensor of shape (C x H x W). \"\"\" def __call__(self, img): \"\"\"Convert a ``numpy.ndarray`` to tensor. Args: img (numpy.ndarray): Image to be converted to tensor. Returns: Tensor: Converted image. \"\"\" if not(_is_numpy_image(img)): raise TypeError('img should be ndarray. Got {}'.format(type(img))) if isinstance(img, np.ndarray): # handle numpy array if img.ndim == 3: img = torch.from_numpy(img.transpose((2, 0, 1)).copy()) elif img.ndim == 2: img = torch.from_numpy(img.copy()) else: raise RuntimeError('img should be ndarray with 2 or 3 dimensions. Got {}'.format(img.ndim)) # backward compatibility # return img.float().div(255) return img.float() class NormalizeNumpyArray(object): \"\"\"Normalize a ``numpy.ndarray`` with mean and standard deviation. Given mean: ``(M1,...,Mn)`` and std: ``(M1,..,Mn)`` for ``n`` channels, this transform will normalize each channel of the input ``numpy.ndarray`` i.e. ``input[channel] = (input[channel] - mean[channel]) / std[channel]`` Args: mean (sequence): Sequence of means for each channel. std (sequence): Sequence of standard deviations for each channel. \"\"\" def __init__(self, mean, std): self.mean = mean self.std = std def __call__(self, img): \"\"\" Args: img (numpy.ndarray): Image of size (H, W, C) to be normalized. Returns: Tensor: Normalized image. \"\"\" if not(_is_numpy_image(img)): raise TypeError('img should be ndarray. Got {}'.format(type(img))) # TODO: make efficient print(img.shape) for i in range(3): img[:,:,i] = (img[:,:,i] - self.mean[i]) / self.std[i] return img class NormalizeTensor(object): \"\"\"Normalize an tensor image with mean and standard deviation. Given mean: ``(M1,...,Mn)`` and std: ``(M1,..,Mn)`` for ``n`` channels, this transform will normalize each channel of the input ``torch.*Tensor`` i.e. ``input[channel] = (input[channel] - mean[channel]) / std[channel]`` Args: mean (sequence): Sequence of means for each channel. std (sequence): Sequence of standard deviations for each channel. \"\"\" def __init__(self, mean, std): self.mean = mean self.std = std def __call__(self, tensor): \"\"\" Args: tensor (Tensor): Tensor image of size (C, H, W) to be normalized. Returns: Tensor: Normalized Tensor image. \"\"\" if not _is_tensor_image(tensor): raise TypeError('tensor is not a torch image.') # TODO: make efficient for t, m, s in zip(tensor, self.mean, self.std): t.sub_(m).div_(s) return tensor class Rotate(object): \"\"\"Rotates the given ``numpy.ndarray``. Args: angle (float): The rotation angle in degrees. \"\"\" def __init__(self, angle): self.angle = angle def __call__(self, img): \"\"\" Args: img (numpy.ndarray (C x H x W)): Image to be rotated. Returns: img (numpy.ndarray (C x H x W)): Rotated image. \"\"\" # order=0 means nearest-neighbor type interpolation return itpl.rotate(img, self.angle, reshape=False, prefilter=False, order=0) class Resize(object): \"\"\"Resize the the given ``numpy.ndarray`` to the given size. Args: size (sequence or int): Desired output size. If size is a sequence like (h, w), output size will be matched to this. If size is an int, smaller edge of the image will be matched to this number. i.e, if height > width, then image will be rescaled to (size * height / width, size) interpolation (int, optional): Desired interpolation. Default is ``PIL.Image.BILINEAR`` \"\"\" def __init__(self, size, interpolation='nearest'): assert isinstance(size, int) or isinstance(size, float) or \\ (isinstance(size, collections.Iterable) and len(size) == 2) self.size = size self.interpolation = interpolation def __call__(self, img): \"\"\" Args: img (PIL Image): Image to be scaled. Returns: PIL Image: Rescaled image. \"\"\" if img.ndim == 3: return misc.imresize(img, self.size, self.interpolation) elif img.ndim == 2: return misc.imresize(img, self.size, self.interpolation, 'F') else: RuntimeError('img should be ndarray with 2 or 3 dimensions. Got {}'.format(img.ndim)) class CenterCrop(object): \"\"\"Crops the given ``numpy.ndarray`` at the center. Args: size (sequence or int): Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. \"\"\" def __init__(self, size): if isinstance(size, numbers.Number): self.size = (int(size), int(size)) else: self.size = size @staticmethod def get_params(img, output_size): \"\"\"Get parameters for ``crop`` for center crop. Args: img (numpy.ndarray (C x H x W)): Image to be cropped. output_size (tuple): Expected output size of the crop. Returns: tuple: params (i, j, h, w) to be passed to ``crop`` for center crop. \"\"\" h = img.shape[0] w = img.shape[1] th, tw = output_size i = int(round((h - th) / 2.)) j = int(round((w - tw) / 2.)) # # randomized cropping # i = np.random.randint(i-3, i+4) # j = np.random.randint(j-3, j+4) return i, j, th, tw def __call__(self, img): \"\"\" Args: img (numpy.ndarray (C x H x W)): Image to be cropped. Returns: img (numpy.ndarray (C x H x W)): Cropped image. \"\"\" i, j, h, w = self.get_params(img, self.size) \"\"\" i: Upper pixel coordinate. j: Left pixel coordinate. h: Height of the cropped image. w: Width of the cropped image. \"\"\" if not(_is_numpy_image(img)): raise TypeError('img should be ndarray. Got {}'.format(type(img))) if img.ndim == 3: return img[i:i+h, j:j+w, :] elif img.ndim == 2: return img[i:i + h, j:j + w] else: raise RuntimeError('img should be ndarray with 2 or 3 dimensions. Got {}'.format(img.ndim)) class Lambda(object): \"\"\"Apply a user-defined lambda as a transform. Args: lambd (function): Lambda/function to be used for transform. \"\"\" def __init__(self, lambd): assert isinstance(lambd, types.LambdaType) self.lambd = lambd def __call__(self, img): return self.lambd(img) class HorizontalFlip(object): \"\"\"Horizontally flip the given ``numpy.ndarray``. Args: do_flip (boolean): whether or not do horizontal flip. \"\"\" def __init__(self, do_flip): self.do_flip = do_flip def __call__(self, img): \"\"\" Args: img (numpy.ndarray (C x H x W)): Image to be flipped. Returns: img (numpy.ndarray (C x H x W)): flipped image. \"\"\" if not(_is_numpy_image(img)): raise TypeError('img should be ndarray. Got {}'.format(type(img))) if self.do_flip: return np.fliplr(img) else: return img class ColorJitter(object): \"\"\"Randomly change the brightness, contrast and saturation of an image. Args: brightness (float): How much to jitter brightness. brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness]. contrast (float): How much to jitter contrast. contrast_factor is chosen uniformly from [max(0, 1 - contrast), 1 + contrast]. saturation (float): How much to jitter saturation. saturation_factor is chosen uniformly from [max(0, 1 - saturation), 1 + saturation]. hue(float): How much to jitter hue. hue_factor is chosen uniformly from [-hue, hue]. Should be >=0 and <= 0.5. \"\"\" def __init__(self, brightness=0, contrast=0, saturation=0, hue=0): self.brightness = brightness self.contrast = contrast self.saturation = saturation self.hue = hue @staticmethod def get_params(brightness, contrast, saturation, hue): \"\"\"Get a randomized transform to be applied on image. Arguments are same as that of __init__. Returns: Transform which randomly adjusts brightness, contrast and saturation in a random order. \"\"\" transforms = [] if brightness > 0: brightness_factor = np.random.uniform(max(0, 1 - brightness), 1 + brightness) transforms.append(Lambda(lambda img: adjust_brightness(img, brightness_factor))) if contrast > 0: contrast_factor = np.random.uniform(max(0, 1 - contrast), 1 + contrast) transforms.append(Lambda(lambda img: adjust_contrast(img, contrast_factor))) if saturation > 0: saturation_factor = np.random.uniform(max(0, 1 - saturation), 1 + saturation) transforms.append(Lambda(lambda img: adjust_saturation(img, saturation_factor))) if hue > 0: hue_factor = np.random.uniform(-hue, hue) transforms.append(Lambda(lambda img: adjust_hue(img, hue_factor))) np.random.shuffle(transforms) transform = Compose(transforms) return transform def __call__(self, img): \"\"\" Args: img (numpy.ndarray (C x H x W)): Input image. Returns: img (numpy.ndarray (C x H x W)): Color jittered image. \"\"\" if not(_is_numpy_image(img)): raise TypeError('img should be ndarray. Got {}'.format(type(img))) pil = Image.fromarray(img) transform = self.get_params(self.brightness, self.contrast, self.saturation, self.hue) return np.array(transform(pil)) # Easier version of color jitter def Colorjitter2(brightness=0, contrast=0, saturation=0): return torchvision.transforms.ColorJitter( brightness=brightness, contrast=contrast, saturation=saturation ) # Normalization using imagenet mean and variance def normalization_imagenet(inputs): # Construct the normalization normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) return normalize(inputs) # Denormalization using imagenet mean and variance def denormalization_imagenet(inputs): # Construct the denormalization mean_r = 0.485 mean_g = 0.456 mean_b = 0.406 std_r = 0.229 std_g = 0.224 std_b = 0.225 denormalize = torchvision.transforms.Normalize(mean=[-mean_r/std_r, -mean_g/std_g, -mean_b/std_b], std=[1/std_r, 1/std_g, 1/std_b]) return denormalize(inputs) # Denormalize batch of tensors def denormalization_batch(inputs): # Get the batch size batch_size = inputs.shape[0] tensor_list = [] for i in range(batch_size): tensor_list.append(torch.unsqueeze(denormalization_imagenet(inputs[i, :, :, :]), dim=0)) return torch.cat(tuple(tensor_list), dim=0) class Crop(object): \"\"\"Crops the given PIL Image to a rectangular region based on a given 4-tuple defining the left, upper pixel coordinated, hight and width size. Args: a tuple: (upper pixel coordinate, left pixel coordinate, hight, width)-tuple \"\"\" def __init__(self, i, j, h, w): \"\"\" i: Upper pixel coordinate. j: Left pixel coordinate. h: Height of the cropped image. w: Width of the cropped image. \"\"\" self.i = i self.j = j self.h = h self.w = w def __call__(self, img): \"\"\" Args: img (numpy.ndarray (C x H x W)): Image to be cropped. Returns: img (numpy.ndarray (C x H x W)): Cropped image. \"\"\" i, j, h, w = self.i, self.j, self.h, self.w if not(_is_numpy_image(img)): raise TypeError('img should be ndarray. Got {}'.format(type(img))) if img.ndim == 3: return img[i:i + h, j:j + w, :] elif img.ndim == 2: return img[i:i + h, j:j + w] else: raise RuntimeError( 'img should be ndarray with 2 or 3 dimensions. Got {}'.format(img.ndim)) def __repr__(self): return self.__class__.__name__ + '(i={0},j={1},h={2},w={3})'.format( self.i, self.j, self.h, self.w)","title":"python\u6709\u7528\u7684\u51fd\u6570(\u4e00)"},{"location":"python%E6%9C%89%E7%94%A8%E7%9A%84%E5%87%BD%E6%95%B0/#python","text":"\"\"\" Adapt from fangchangma/sparse_to_dense.pytorch on github \"\"\" from __future__ import print_function from __future__ import division from __future__ import absolute_import import torch import torchvision import math import random from PIL import Image, ImageOps, ImageEnhance try: import accimage except ImportError: accimage = None import numpy as np import numbers import types import collections import scipy.ndimage.interpolation as itpl import scipy.misc as misc # Check whether the input is a numpy array def _is_numpy_image(img): return isinstance(img, np.ndarray) and (img.ndim in {2, 3}) # Check whether the input is a PIL image def _is_pil_image(img): if accimage is not None: return isinstance(img, (Image.Image, accimage.Image)) else: return isinstance(img, Image.Image) # Check wheter the input is a tensor def _is_tensor_image(img): return torch.is_tensor(img) and img.ndimension() == 3 def adjust_brightness(img, brightness_factor): \"\"\"Adjust brightness of an Image. Args: img (PIL Image): PIL Image to be adjusted. brightness_factor (float): How much to adjust the brightness. Can be any non negative number. 0 gives a black image, 1 gives the original image while 2 increases the brightness by a factor of 2. Returns: PIL Image: Brightness adjusted image. \"\"\" if not _is_pil_image(img): raise TypeError('img should be PIL Image. Got {}'.format(type(img))) enhancer = ImageEnhance.Brightness(img) img = enhancer.enhance(brightness_factor) return img def adjust_contrast(img, contrast_factor): \"\"\"Adjust contrast of an Image. Args: img (PIL Image): PIL Image to be adjusted. contrast_factor (float): How much to adjust the contrast. Can be any non negative number. 0 gives a solid gray image, 1 gives the original image while 2 increases the contrast by a factor of 2. Returns: PIL Image: Contrast adjusted image. \"\"\" if not _is_pil_image(img): raise TypeError('img should be PIL Image. Got {}'.format(type(img))) enhancer = ImageEnhance.Contrast(img) img = enhancer.enhance(contrast_factor) return img def adjust_saturation(img, saturation_factor): \"\"\"Adjust color saturation of an image. Args: img (PIL Image): PIL Image to be adjusted. saturation_factor (float): How much to adjust the saturation. 0 will give a black and white image, 1 will give the original image while 2 will enhance the saturation by a factor of 2. Returns: PIL Image: Saturation adjusted image. \"\"\" if not _is_pil_image(img): raise TypeError('img should be PIL Image. Got {}'.format(type(img))) enhancer = ImageEnhance.Color(img) img = enhancer.enhance(saturation_factor) return img def adjust_hue(img, hue_factor): \"\"\"Adjust hue of an image. The image hue is adjusted by converting the image to HSV and cyclically shifting the intensities in the hue channel (H). The image is then converted back to original image mode. `hue_factor` is the amount of shift in H channel and must be in the interval `[-0.5, 0.5]`. See https://en.wikipedia.org/wiki/Hue for more details on Hue. Args: img (PIL Image): PIL Image to be adjusted. hue_factor (float): How much to shift the hue channel. Should be in [-0.5, 0.5]. 0.5 and -0.5 give complete reversal of hue channel in HSV space in positive and negative direction respectively. 0 means no shift. Therefore, both -0.5 and 0.5 will give an image with complementary colors while 0 gives the original image. Returns: PIL Image: Hue adjusted image. \"\"\" if not(-0.5 <= hue_factor <= 0.5): raise ValueError('hue_factor is not in [-0.5, 0.5].'.format(hue_factor)) if not _is_pil_image(img): raise TypeError('img should be PIL Image. Got {}'.format(type(img))) input_mode = img.mode if input_mode in {'L', '1', 'I', 'F'}: return img h, s, v = img.convert('HSV').split() np_h = np.array(h, dtype=np.uint8) # uint8 addition take cares of rotation across boundaries with np.errstate(over='ignore'): np_h += np.uint8(hue_factor * 255) h = Image.fromarray(np_h, 'L') img = Image.merge('HSV', (h, s, v)).convert(input_mode) return img def adjust_gamma(img, gamma, gain=1): \"\"\"Perform gamma correction on an image. Also known as Power Law Transform. Intensities in RGB mode are adjusted based on the following equation: I_out = 255 * gain * ((I_in / 255) ** gamma) See https://en.wikipedia.org/wiki/Gamma_correction for more details. Args: img (PIL Image): PIL Image to be adjusted. gamma (float): Non negative real number. gamma larger than 1 make the shadows darker, while gamma smaller than 1 make dark regions lighter. gain (float): The constant multiplier. \"\"\" if not _is_pil_image(img): raise TypeError('img should be PIL Image. Got {}'.format(type(img))) if gamma < 0: raise ValueError('Gamma should be a non-negative real number') input_mode = img.mode img = img.convert('RGB') np_img = np.array(img, dtype=np.float32) np_img = 255 * gain * ((np_img / 255) ** gamma) np_img = np.uint8(np.clip(np_img, 0, 255)) img = Image.fromarray(np_img, 'RGB').convert(input_mode) return img class Compose(object): \"\"\"Composes several transforms together. Args: transforms (list of ``Transform`` objects): list of transforms to compose. Example: >>> transforms.Compose([ >>> transforms.CenterCrop(10), >>> transforms.ToTensor(), >>> ]) \"\"\" def __init__(self, transforms): self.transforms = transforms def __call__(self, img): for t in self.transforms: img = t(img) return img class ToTensor(object): \"\"\"Convert a ``numpy.ndarray`` to tensor. Converts a numpy.ndarray (H x W x C) to a torch.FloatTensor of shape (C x H x W). \"\"\" def __call__(self, img): \"\"\"Convert a ``numpy.ndarray`` to tensor. Args: img (numpy.ndarray): Image to be converted to tensor. Returns: Tensor: Converted image. \"\"\" if not(_is_numpy_image(img)): raise TypeError('img should be ndarray. Got {}'.format(type(img))) if isinstance(img, np.ndarray): # handle numpy array if img.ndim == 3: img = torch.from_numpy(img.transpose((2, 0, 1)).copy()) elif img.ndim == 2: img = torch.from_numpy(img.copy()) else: raise RuntimeError('img should be ndarray with 2 or 3 dimensions. Got {}'.format(img.ndim)) # backward compatibility # return img.float().div(255) return img.float() class NormalizeNumpyArray(object): \"\"\"Normalize a ``numpy.ndarray`` with mean and standard deviation. Given mean: ``(M1,...,Mn)`` and std: ``(M1,..,Mn)`` for ``n`` channels, this transform will normalize each channel of the input ``numpy.ndarray`` i.e. ``input[channel] = (input[channel] - mean[channel]) / std[channel]`` Args: mean (sequence): Sequence of means for each channel. std (sequence): Sequence of standard deviations for each channel. \"\"\" def __init__(self, mean, std): self.mean = mean self.std = std def __call__(self, img): \"\"\" Args: img (numpy.ndarray): Image of size (H, W, C) to be normalized. Returns: Tensor: Normalized image. \"\"\" if not(_is_numpy_image(img)): raise TypeError('img should be ndarray. Got {}'.format(type(img))) # TODO: make efficient print(img.shape) for i in range(3): img[:,:,i] = (img[:,:,i] - self.mean[i]) / self.std[i] return img class NormalizeTensor(object): \"\"\"Normalize an tensor image with mean and standard deviation. Given mean: ``(M1,...,Mn)`` and std: ``(M1,..,Mn)`` for ``n`` channels, this transform will normalize each channel of the input ``torch.*Tensor`` i.e. ``input[channel] = (input[channel] - mean[channel]) / std[channel]`` Args: mean (sequence): Sequence of means for each channel. std (sequence): Sequence of standard deviations for each channel. \"\"\" def __init__(self, mean, std): self.mean = mean self.std = std def __call__(self, tensor): \"\"\" Args: tensor (Tensor): Tensor image of size (C, H, W) to be normalized. Returns: Tensor: Normalized Tensor image. \"\"\" if not _is_tensor_image(tensor): raise TypeError('tensor is not a torch image.') # TODO: make efficient for t, m, s in zip(tensor, self.mean, self.std): t.sub_(m).div_(s) return tensor class Rotate(object): \"\"\"Rotates the given ``numpy.ndarray``. Args: angle (float): The rotation angle in degrees. \"\"\" def __init__(self, angle): self.angle = angle def __call__(self, img): \"\"\" Args: img (numpy.ndarray (C x H x W)): Image to be rotated. Returns: img (numpy.ndarray (C x H x W)): Rotated image. \"\"\" # order=0 means nearest-neighbor type interpolation return itpl.rotate(img, self.angle, reshape=False, prefilter=False, order=0) class Resize(object): \"\"\"Resize the the given ``numpy.ndarray`` to the given size. Args: size (sequence or int): Desired output size. If size is a sequence like (h, w), output size will be matched to this. If size is an int, smaller edge of the image will be matched to this number. i.e, if height > width, then image will be rescaled to (size * height / width, size) interpolation (int, optional): Desired interpolation. Default is ``PIL.Image.BILINEAR`` \"\"\" def __init__(self, size, interpolation='nearest'): assert isinstance(size, int) or isinstance(size, float) or \\ (isinstance(size, collections.Iterable) and len(size) == 2) self.size = size self.interpolation = interpolation def __call__(self, img): \"\"\" Args: img (PIL Image): Image to be scaled. Returns: PIL Image: Rescaled image. \"\"\" if img.ndim == 3: return misc.imresize(img, self.size, self.interpolation) elif img.ndim == 2: return misc.imresize(img, self.size, self.interpolation, 'F') else: RuntimeError('img should be ndarray with 2 or 3 dimensions. Got {}'.format(img.ndim)) class CenterCrop(object): \"\"\"Crops the given ``numpy.ndarray`` at the center. Args: size (sequence or int): Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. \"\"\" def __init__(self, size): if isinstance(size, numbers.Number): self.size = (int(size), int(size)) else: self.size = size @staticmethod def get_params(img, output_size): \"\"\"Get parameters for ``crop`` for center crop. Args: img (numpy.ndarray (C x H x W)): Image to be cropped. output_size (tuple): Expected output size of the crop. Returns: tuple: params (i, j, h, w) to be passed to ``crop`` for center crop. \"\"\" h = img.shape[0] w = img.shape[1] th, tw = output_size i = int(round((h - th) / 2.)) j = int(round((w - tw) / 2.)) # # randomized cropping # i = np.random.randint(i-3, i+4) # j = np.random.randint(j-3, j+4) return i, j, th, tw def __call__(self, img): \"\"\" Args: img (numpy.ndarray (C x H x W)): Image to be cropped. Returns: img (numpy.ndarray (C x H x W)): Cropped image. \"\"\" i, j, h, w = self.get_params(img, self.size) \"\"\" i: Upper pixel coordinate. j: Left pixel coordinate. h: Height of the cropped image. w: Width of the cropped image. \"\"\" if not(_is_numpy_image(img)): raise TypeError('img should be ndarray. Got {}'.format(type(img))) if img.ndim == 3: return img[i:i+h, j:j+w, :] elif img.ndim == 2: return img[i:i + h, j:j + w] else: raise RuntimeError('img should be ndarray with 2 or 3 dimensions. Got {}'.format(img.ndim)) class Lambda(object): \"\"\"Apply a user-defined lambda as a transform. Args: lambd (function): Lambda/function to be used for transform. \"\"\" def __init__(self, lambd): assert isinstance(lambd, types.LambdaType) self.lambd = lambd def __call__(self, img): return self.lambd(img) class HorizontalFlip(object): \"\"\"Horizontally flip the given ``numpy.ndarray``. Args: do_flip (boolean): whether or not do horizontal flip. \"\"\" def __init__(self, do_flip): self.do_flip = do_flip def __call__(self, img): \"\"\" Args: img (numpy.ndarray (C x H x W)): Image to be flipped. Returns: img (numpy.ndarray (C x H x W)): flipped image. \"\"\" if not(_is_numpy_image(img)): raise TypeError('img should be ndarray. Got {}'.format(type(img))) if self.do_flip: return np.fliplr(img) else: return img class ColorJitter(object): \"\"\"Randomly change the brightness, contrast and saturation of an image. Args: brightness (float): How much to jitter brightness. brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness]. contrast (float): How much to jitter contrast. contrast_factor is chosen uniformly from [max(0, 1 - contrast), 1 + contrast]. saturation (float): How much to jitter saturation. saturation_factor is chosen uniformly from [max(0, 1 - saturation), 1 + saturation]. hue(float): How much to jitter hue. hue_factor is chosen uniformly from [-hue, hue]. Should be >=0 and <= 0.5. \"\"\" def __init__(self, brightness=0, contrast=0, saturation=0, hue=0): self.brightness = brightness self.contrast = contrast self.saturation = saturation self.hue = hue @staticmethod def get_params(brightness, contrast, saturation, hue): \"\"\"Get a randomized transform to be applied on image. Arguments are same as that of __init__. Returns: Transform which randomly adjusts brightness, contrast and saturation in a random order. \"\"\" transforms = [] if brightness > 0: brightness_factor = np.random.uniform(max(0, 1 - brightness), 1 + brightness) transforms.append(Lambda(lambda img: adjust_brightness(img, brightness_factor))) if contrast > 0: contrast_factor = np.random.uniform(max(0, 1 - contrast), 1 + contrast) transforms.append(Lambda(lambda img: adjust_contrast(img, contrast_factor))) if saturation > 0: saturation_factor = np.random.uniform(max(0, 1 - saturation), 1 + saturation) transforms.append(Lambda(lambda img: adjust_saturation(img, saturation_factor))) if hue > 0: hue_factor = np.random.uniform(-hue, hue) transforms.append(Lambda(lambda img: adjust_hue(img, hue_factor))) np.random.shuffle(transforms) transform = Compose(transforms) return transform def __call__(self, img): \"\"\" Args: img (numpy.ndarray (C x H x W)): Input image. Returns: img (numpy.ndarray (C x H x W)): Color jittered image. \"\"\" if not(_is_numpy_image(img)): raise TypeError('img should be ndarray. Got {}'.format(type(img))) pil = Image.fromarray(img) transform = self.get_params(self.brightness, self.contrast, self.saturation, self.hue) return np.array(transform(pil)) # Easier version of color jitter def Colorjitter2(brightness=0, contrast=0, saturation=0): return torchvision.transforms.ColorJitter( brightness=brightness, contrast=contrast, saturation=saturation ) # Normalization using imagenet mean and variance def normalization_imagenet(inputs): # Construct the normalization normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) return normalize(inputs) # Denormalization using imagenet mean and variance def denormalization_imagenet(inputs): # Construct the denormalization mean_r = 0.485 mean_g = 0.456 mean_b = 0.406 std_r = 0.229 std_g = 0.224 std_b = 0.225 denormalize = torchvision.transforms.Normalize(mean=[-mean_r/std_r, -mean_g/std_g, -mean_b/std_b], std=[1/std_r, 1/std_g, 1/std_b]) return denormalize(inputs) # Denormalize batch of tensors def denormalization_batch(inputs): # Get the batch size batch_size = inputs.shape[0] tensor_list = [] for i in range(batch_size): tensor_list.append(torch.unsqueeze(denormalization_imagenet(inputs[i, :, :, :]), dim=0)) return torch.cat(tuple(tensor_list), dim=0) class Crop(object): \"\"\"Crops the given PIL Image to a rectangular region based on a given 4-tuple defining the left, upper pixel coordinated, hight and width size. Args: a tuple: (upper pixel coordinate, left pixel coordinate, hight, width)-tuple \"\"\" def __init__(self, i, j, h, w): \"\"\" i: Upper pixel coordinate. j: Left pixel coordinate. h: Height of the cropped image. w: Width of the cropped image. \"\"\" self.i = i self.j = j self.h = h self.w = w def __call__(self, img): \"\"\" Args: img (numpy.ndarray (C x H x W)): Image to be cropped. Returns: img (numpy.ndarray (C x H x W)): Cropped image. \"\"\" i, j, h, w = self.i, self.j, self.h, self.w if not(_is_numpy_image(img)): raise TypeError('img should be ndarray. Got {}'.format(type(img))) if img.ndim == 3: return img[i:i + h, j:j + w, :] elif img.ndim == 2: return img[i:i + h, j:j + w] else: raise RuntimeError( 'img should be ndarray with 2 or 3 dimensions. Got {}'.format(img.ndim)) def __repr__(self): return self.__class__.__name__ + '(i={0},j={1},h={2},w={3})'.format( self.i, self.j, self.h, self.w)","title":"python \u6709\u7528\u7684\u51fd\u6570\uff08\u4e00\uff09"},{"location":"pytorch/","text":"Pytorch Exception in Thread: ValueError: signal number 32 out of range \u8bbe\u7f6e num_workers=0 Unable to create file (unable to lock file \u7531\u4e8e\u6709\u5e94\u7528\u7a0b\u5e8f\u6b63\u5728\u4f7f\u7528\u8be5\u6587\u4ef6 ps aux | grep main.py sudo kill HDF5 HDF5 simplifies the file structure to include only two major types of object: * DataSets, which are multimensional arrays of homogeneous type * Groups, which are container structures which can hold datasets and other groups. This results in a truly hierarchical\u7b49\u7ea7\u5236\uff0cfilesystem-like data format.In fact ,resources in an HDF5 file can be accessed using the POSIX-like syntax /path/to/resource . Metadata is stored in the form of user-defined, named attributes\u5c5e\u6027 attached to groups and datasets. More complex storage APIs representing images and tables can then be built up using datasets, groups and attributs. In addition to these advances in the file format, HDF5 includes an improved type system, and dataspace objects which represent selections over dataset regions. The API is also object-oriented\u9762\u5411\u5bf9\u8c61\u7684 with respect to datasets, groups, attributes, types, dataspaces and property lists. Because it uses B-trees to index table objects, HDF5 works well for time series data such as stock\u80a1\u7968 price series, network monitoring data, and 3D meteorological\u6c14\u8c61 data. The bulk of the data goes into straightforward arrays (the table objects) that can be accessed much more quickly than the rows of an SQL database, but B-tree access is available for non-array data. The HDF5 data storage mechanism can be simpler and faster than an SQL star schema. jupyter-lab show IMG from matplotlib.pyplot import imshow import numpy as np from PIL import Image %matplotlib inline pil_im = Image.open('data/empire.jpg', 'r') imshow(np.asarray(pil_im)) How to create and Use a Pytorch DataLoader In the early days of pytroch , you had to write completely custom code for data loading. Now however, the vast majority of PyTorch systems I've seen (and created myself) use the PyTorch DataSet and DataLoader interfaces to serve up training or test data.Briefly a Dataset object loads training or test data into memory , and a DataLoader object fetches\u83b7\u53d6 data from a Dataset and serves the data up in batches. You must write code to create a DataSet that matches your data and problem scenario\u8bbe\u60f3\uff1bNo two Dataset implementations are exactly the same.On the other hand,a DataLoader object is used mostly the same no matter which Dataset object it's associated with . For example: class MyDataSet(T.utils.data.Dataset): #implent custom code to load data here my_ds = MyDataSet(\"my_train_data.txt\") my_ldr = torch.utils.data.DataLoader(my_ds, 10, True) for (idx, batch) in enumerate(my_ldr): The code fragment shows you must implement a Dataset class yourself. Then you create a Dataset instance and pass it to a DataLoader constructor. The DataLoader object serves up batches of data, in this case with batch size = 10 training items in a random (True) order. This article explains how to create and use PyTorch Dataset and DataLoader objects. A good way to see where this article is headed is to take a look at the screenshot of a demo program in Figure 1 . The source data is a tiny 8-item file. Each line represents a person: sex(male = 1 0, female = 0 1), normalized age, region (east= 1 0 0, west = 0 1 0, central = 0 0 1), normalized income, and political\u653f\u6cbb\u7684 learning (conservative = 0, moderate = 1, liberal = 2).The goal of the demo is to serve up data in batches where the dependent variable to predict is political learning and the other variables are the predictors. The 8-item source data is stored in a tab-delimited file named people_train.txt and looks like: 1 0 0.171429 1 0 0 0.966805 0 0 1 0.085714 0 1 0 0.188797 1 1 0 0.000000 0 0 1 0.690871 2 1 0 0.057143 0 1 0 1.000000 1 0 1 1.000000 0 0 1 0.016598 2 1 0 0.171429 1 0 0 0.802905 0 0 1 0.171429 1 0 0 0.966805 1 1 0 0.257143 0 1 0 0.329876 0 Behind the scenes, the demo loads data into memory using a custom Dataset object ,and then serves the data up in randomly selected batches of size 3 rows/items. Because the source data has 8 lines, the first two batches have 3 data items , but the last batch has 2 items. The demo processes the source data twice, in other words,two epochs. This artical assumes you have intermediate or better skill with a C-family programming language. The demo program is coded using Python.which is used by PyTorch and which is essentially the primary language for deep neural networks.The complete source code for the demo program is presented in this article. The source code and source data are also available in the file download that accompanies this article. The demo program The demo program , with a few minor edits to save space, is presented in Listing 1 . I indent my Python programs using two spaces, rather than the more common four spaces or a tab character,as a matter of personal preference. Listing 1: DataLoader Demo Program import numpy as np import Torch as T device = T.device(\"cpu\") #predictors and label in same file #data has been normialized and encoded like: # sex age region income politic # [0] [2] [3] [6] [7] # 1 0 0.057143 0 1 0 0.690871 2 class PeopleDataset(T.utils.data.Dataset): def __init__(self, src_file, num_rows= None): x_tmp = np.loadtxt(src_file, max_rows=num_rows, usecols = rane(0, 7), delimiter=\"\\t\", skiprows=0, dtype=np.float32) y_tmp = np.loadtxt(src_file, max_rows=num_rows, usecols =7, delimiter=\"\\t\", skiprows=0, dtype= np.long) self.x_data = T.tensor(x_tmp, dtype=T.float32).to(device) self.y_data = T.tensor(y_tmp, dtype = T.long).to(device) def __len__ (self): return len(self.x_data) #required def __getitem__(self, idx): if T.is_tensor(idx): idx = idx.tolist() preds = self.x_data[idx, 0:7] pol = self.y_data[idx] sample =\\ {'predictors': preds, 'political':pol } return sample def main(): print(\"\\nBegin PyTorch DataLoader demo\" ) # 0. miscellaneous prep T.manual_seed(0) np.random.seed(0) print(\"\\nSource data looks like: \") print(\"1 0 0.171429 1 0 0 0.966805 0\") print(\"0 1 0.085714 0 1 0 0.188797 1\") print(\" . . . \") # 1. create Dataset and DataLoader object print(\"\\nCreating Dataset and DataLoader \") train_file = \"./people_train.txt\" train_ds = PeopleDataset(train_file, num_rows = 8) bat_size = 3 train_dir = T.utils.data.DataLoader(train_ds, batch_size=bat_size, shuffle=True) #2. iterate thru training data twice for epoch in range(2): print(\"\\n===========================\") print(\"Epoch = \" + str(epoch)) for (batch_idx, batch) in enumerate(train_ldr): print(\"\\nBatch = \" + str(batch_idx)) X = batch['predictors'] Y = batch['political'] print(X) print(Y) print(\"\\n============================\") print(\"\\nEnd demo\") if __name__ = \"__main__\": main() The excution of the demo program begins with: def main(): print(\"\\nBegin PyTorch DataLoader demo\" ) # 0. miscellaneous prep T.manual_seed(0) np.random.seed(0) . . . In almost all PyTorch programs, it's a good idea to set the system random number generator seed values so that your results will be reproducible. Unfortunately,because of execution across multiple processes, sometimes your results are not reproducible even if you set the random generator seeds. But if you don't set the seeds, your results will almost certainly not be reproducible. Next a Dataset and a DataLoader object are created: train_file = \".\\\\people_train.txt\" train_ds = PeopleDataset(train_file, num_rows=8) bat_size = 3 train_ldr = T.utils.data.DataLoader(train_ds, batch_size=bat_size, shuffle=True) The custom PeopleDataset object constructor accepts a path to the training data, and a num_rows parameter in case you want to load just part of a very large data file during system development. The built-in DataLoader class definition is housed in the torch.utils.data module. The class constructor has one required parameter, the Dataset that holds the data. There are 10 optional parameters . The demo specifies values for just the batch_size and shuffle parameters, and therefore uses the default values for the other 8 optional parameters. The demo concudes by using the DataLoader to iterate through the source data: for epoch in range(2): print(\"\\n==============================\\n\") print(\"Epoch = \" + str(epoch)) for (batch_idx, batch) in enumerate(train_ldr): print(\"\\nBatch = \" + str(batch_idx)) X = batch['predictors'] # [3,7] Y = batch['political'] # [3] print(X) print(Y) In neural network terminology\u672f\u8bed\uff0can epoch is one pass through all source data. The DataLoader class is designed so that it can be iterated using the enumerate() function, which returns a tuple with the current batch zero-based index value, and the actual batch of data.There is a tight coupling between a Dataset and its associated DataLoader, meaning you have to know the names of the keys used for the predictor values and the dependent variable values. In this case the two keys are \"predictors\" and \"political\". Implementing a Dataset Class You have a lot of flexibility when implementing a Dataset class. You are required to implement three methods and you can optionally add other methods depending on your source data The required methods are init (), len (),and getitem (). The demo PeopleDataset defines its init () method as def __init__(self, src_file, num_rows=None): x_tmp = np.loadtxt(src_file, max_rows=num_rows, usecols= range(0,7), delimiter=\"\\t\", skiprows=0, dtype=np.float32) y_tmp = np.loadtxt(src_file, max_rows= num_rows, usecols=7, delimiter=\"\\t\", skiprows = 0, dtype =np.long) self.x_data = T.tensor(x_tmp, dtype=T.float32).to(device) self.y_data = T.tensor(y_tmp, dtype=T.long).to(device) In situations where your source data is too large to fit into memory, you will have to read data into a buffer and refill the buffer when the buffer has been emptied . This is a fairly difficult task. The demo data stores both the predictor values and dependent values-to-predict in the same file. In situation where the predictor values and dependent variables values are in separate files, you'd have to pass in two source file names instead of just one. Another common alternative is to pass in just a single source directory and then use hard-coded file names for the training and test data. The demo init () method bulk-converts\u5927\u91cf\u8f6c\u6362 all Numpy array data to PyTorch tensors. An alternative is to leave the data in memory as Numpy arrays and then convert to batches of data to tensors in the getitem () method.Conversion from Numpy array data to PyTorch tensor data is an expensive operation so it's usually better to convert just once rather than repeatedly converting batches of data. The len () method is defined as: def __len__(self): return len(self.x_data) A Dataset object has to know how much data there is so that an associated DataLoader object knows how to iterate through all data in batches. The getitem () method is defined as : def __getitem__(self, idx): if T.is_tensor(idx): idx = idx.tolist() preds = self.x_data[idx, 0:7] pol = self.y_data[idx] sample = {'prediction':preds, 'political': pol} return sample It's common practice to name the parameter which specifies which data to fetch as \"idx\" but this is somewhat misleading because the idx parameter is usually a Python list of several idexes.The getitem () method checks to see if the idx parameter is a PyTorch tensor instead of a Python list, and if so, converts the tensor to a list. The method return value, sample, is a Python Dictionary object and so you must specify names for dictionary keys(\"predictors\" in the demo) and the dictionary values (\"political\" in the demo). Using a Dataset in DataLoader The demo program creates a relatively simple DataLoader object using just the Dataset object plus the batch_size and shuffle parameters. train_file = \"./people_train.txt\" train_ds = PeopleDataset(train_file, num_rows=8) bat_size = 3 train_ldr = T.utils.data.DataLoader(train_ds, batch_size=bat_size, shuffle = True) In some situations, instead of using a DataLoader to consume the data in a Dataset, it's useful to iterate through a Dataset directly. For example: def process_ds(model , ds): #ds is an iterable Dataset of tensors for i in range(len(ds)): inpts = ds[i]['predictors'] trgt = ds[i]['target'] oupt = model(inpts) #do something return somevalue Using other DataLoaders Once you understand how to create a custom Dataset and use it in a DataLoader, many of the built-in PyTorch library Dataset objects make more sense than they might otherwise. For example, the TorchVision module has data and functions that are useful for image processing . One of the Dataset classes in TorchVision holds the MNIST image data. There are 70,000 MNIST images. Each image is a handwritten digit from '0' to '9'. Each image has size 28x28 pixels and pixels are grayscale values from 0 to 255. A Dataset class for the MNIST images is defined in the torchvison.datasets package and is named MNIST. You can create a Dataset for MNIST training images like so: import torchvision as tv tform = tv.transforms.Compose([tv.transforms.ToTensor()]) mnist_train_ds = tv.datasets.MNIST(root=\"./MNIST_Data\", train=True, transform=tform, target_transform=None, download=True) After an MNIST Dataset object has been created, it can be used in a DataLoader as normal, for example mnist_train_dataldr = T.utils.data.DataLoader(mnist_train_ds,batch_size =2, shuffle = True) for (batch_idx, batch) in enumerate(mnist_train_dataldr): print(\"\") print(batch_idx) print(batch) input() To recap\u56de\u987e\uff0c there are many built-in Dataset classes defined in various PyTorch packages. They have different calling signatures, but they all read in data from some source (often a hard-coded URL), and have a way to convert their data to PyTorch tensors.After a built-in Dataset has been created, it can be processed by a DataLoader object using the enumerate() function.","title":"Pytorch"},{"location":"pytorch/#pytorch-exception-in-thread-valueerror-signal-number-32-out-of-range","text":"\u8bbe\u7f6e num_workers=0","title":"Pytorch Exception in Thread: ValueError: signal number 32 out of range"},{"location":"pytorch/#unable-to-create-file-unable-to-lock-file","text":"\u7531\u4e8e\u6709\u5e94\u7528\u7a0b\u5e8f\u6b63\u5728\u4f7f\u7528\u8be5\u6587\u4ef6 ps aux | grep main.py sudo kill","title":"Unable to create file (unable to lock file"},{"location":"pytorch/#hdf5","text":"HDF5 simplifies the file structure to include only two major types of object: * DataSets, which are multimensional arrays of homogeneous type * Groups, which are container structures which can hold datasets and other groups. This results in a truly hierarchical\u7b49\u7ea7\u5236\uff0cfilesystem-like data format.In fact ,resources in an HDF5 file can be accessed using the POSIX-like syntax /path/to/resource . Metadata is stored in the form of user-defined, named attributes\u5c5e\u6027 attached to groups and datasets. More complex storage APIs representing images and tables can then be built up using datasets, groups and attributs. In addition to these advances in the file format, HDF5 includes an improved type system, and dataspace objects which represent selections over dataset regions. The API is also object-oriented\u9762\u5411\u5bf9\u8c61\u7684 with respect to datasets, groups, attributes, types, dataspaces and property lists. Because it uses B-trees to index table objects, HDF5 works well for time series data such as stock\u80a1\u7968 price series, network monitoring data, and 3D meteorological\u6c14\u8c61 data. The bulk of the data goes into straightforward arrays (the table objects) that can be accessed much more quickly than the rows of an SQL database, but B-tree access is available for non-array data. The HDF5 data storage mechanism can be simpler and faster than an SQL star schema.","title":"HDF5"},{"location":"pytorch/#jupyter-lab-show-img","text":"from matplotlib.pyplot import imshow import numpy as np from PIL import Image %matplotlib inline pil_im = Image.open('data/empire.jpg', 'r') imshow(np.asarray(pil_im))","title":"jupyter-lab show IMG"},{"location":"pytorch/#how-to-create-and-use-a-pytorch-dataloader","text":"In the early days of pytroch , you had to write completely custom code for data loading. Now however, the vast majority of PyTorch systems I've seen (and created myself) use the PyTorch DataSet and DataLoader interfaces to serve up training or test data.Briefly a Dataset object loads training or test data into memory , and a DataLoader object fetches\u83b7\u53d6 data from a Dataset and serves the data up in batches. You must write code to create a DataSet that matches your data and problem scenario\u8bbe\u60f3\uff1bNo two Dataset implementations are exactly the same.On the other hand,a DataLoader object is used mostly the same no matter which Dataset object it's associated with . For example: class MyDataSet(T.utils.data.Dataset): #implent custom code to load data here my_ds = MyDataSet(\"my_train_data.txt\") my_ldr = torch.utils.data.DataLoader(my_ds, 10, True) for (idx, batch) in enumerate(my_ldr): The code fragment shows you must implement a Dataset class yourself. Then you create a Dataset instance and pass it to a DataLoader constructor. The DataLoader object serves up batches of data, in this case with batch size = 10 training items in a random (True) order. This article explains how to create and use PyTorch Dataset and DataLoader objects. A good way to see where this article is headed is to take a look at the screenshot of a demo program in Figure 1 . The source data is a tiny 8-item file. Each line represents a person: sex(male = 1 0, female = 0 1), normalized age, region (east= 1 0 0, west = 0 1 0, central = 0 0 1), normalized income, and political\u653f\u6cbb\u7684 learning (conservative = 0, moderate = 1, liberal = 2).The goal of the demo is to serve up data in batches where the dependent variable to predict is political learning and the other variables are the predictors. The 8-item source data is stored in a tab-delimited file named people_train.txt and looks like: 1 0 0.171429 1 0 0 0.966805 0 0 1 0.085714 0 1 0 0.188797 1 1 0 0.000000 0 0 1 0.690871 2 1 0 0.057143 0 1 0 1.000000 1 0 1 1.000000 0 0 1 0.016598 2 1 0 0.171429 1 0 0 0.802905 0 0 1 0.171429 1 0 0 0.966805 1 1 0 0.257143 0 1 0 0.329876 0 Behind the scenes, the demo loads data into memory using a custom Dataset object ,and then serves the data up in randomly selected batches of size 3 rows/items. Because the source data has 8 lines, the first two batches have 3 data items , but the last batch has 2 items. The demo processes the source data twice, in other words,two epochs. This artical assumes you have intermediate or better skill with a C-family programming language. The demo program is coded using Python.which is used by PyTorch and which is essentially the primary language for deep neural networks.The complete source code for the demo program is presented in this article. The source code and source data are also available in the file download that accompanies this article.","title":"How to create and Use a Pytorch DataLoader"},{"location":"pytorch/#the-demo-program","text":"The demo program , with a few minor edits to save space, is presented in Listing 1 . I indent my Python programs using two spaces, rather than the more common four spaces or a tab character,as a matter of personal preference. Listing 1: DataLoader Demo Program import numpy as np import Torch as T device = T.device(\"cpu\") #predictors and label in same file #data has been normialized and encoded like: # sex age region income politic # [0] [2] [3] [6] [7] # 1 0 0.057143 0 1 0 0.690871 2 class PeopleDataset(T.utils.data.Dataset): def __init__(self, src_file, num_rows= None): x_tmp = np.loadtxt(src_file, max_rows=num_rows, usecols = rane(0, 7), delimiter=\"\\t\", skiprows=0, dtype=np.float32) y_tmp = np.loadtxt(src_file, max_rows=num_rows, usecols =7, delimiter=\"\\t\", skiprows=0, dtype= np.long) self.x_data = T.tensor(x_tmp, dtype=T.float32).to(device) self.y_data = T.tensor(y_tmp, dtype = T.long).to(device) def __len__ (self): return len(self.x_data) #required def __getitem__(self, idx): if T.is_tensor(idx): idx = idx.tolist() preds = self.x_data[idx, 0:7] pol = self.y_data[idx] sample =\\ {'predictors': preds, 'political':pol } return sample def main(): print(\"\\nBegin PyTorch DataLoader demo\" ) # 0. miscellaneous prep T.manual_seed(0) np.random.seed(0) print(\"\\nSource data looks like: \") print(\"1 0 0.171429 1 0 0 0.966805 0\") print(\"0 1 0.085714 0 1 0 0.188797 1\") print(\" . . . \") # 1. create Dataset and DataLoader object print(\"\\nCreating Dataset and DataLoader \") train_file = \"./people_train.txt\" train_ds = PeopleDataset(train_file, num_rows = 8) bat_size = 3 train_dir = T.utils.data.DataLoader(train_ds, batch_size=bat_size, shuffle=True) #2. iterate thru training data twice for epoch in range(2): print(\"\\n===========================\") print(\"Epoch = \" + str(epoch)) for (batch_idx, batch) in enumerate(train_ldr): print(\"\\nBatch = \" + str(batch_idx)) X = batch['predictors'] Y = batch['political'] print(X) print(Y) print(\"\\n============================\") print(\"\\nEnd demo\") if __name__ = \"__main__\": main() The excution of the demo program begins with: def main(): print(\"\\nBegin PyTorch DataLoader demo\" ) # 0. miscellaneous prep T.manual_seed(0) np.random.seed(0) . . . In almost all PyTorch programs, it's a good idea to set the system random number generator seed values so that your results will be reproducible. Unfortunately,because of execution across multiple processes, sometimes your results are not reproducible even if you set the random generator seeds. But if you don't set the seeds, your results will almost certainly not be reproducible. Next a Dataset and a DataLoader object are created: train_file = \".\\\\people_train.txt\" train_ds = PeopleDataset(train_file, num_rows=8) bat_size = 3 train_ldr = T.utils.data.DataLoader(train_ds, batch_size=bat_size, shuffle=True) The custom PeopleDataset object constructor accepts a path to the training data, and a num_rows parameter in case you want to load just part of a very large data file during system development. The built-in DataLoader class definition is housed in the torch.utils.data module. The class constructor has one required parameter, the Dataset that holds the data. There are 10 optional parameters . The demo specifies values for just the batch_size and shuffle parameters, and therefore uses the default values for the other 8 optional parameters. The demo concudes by using the DataLoader to iterate through the source data: for epoch in range(2): print(\"\\n==============================\\n\") print(\"Epoch = \" + str(epoch)) for (batch_idx, batch) in enumerate(train_ldr): print(\"\\nBatch = \" + str(batch_idx)) X = batch['predictors'] # [3,7] Y = batch['political'] # [3] print(X) print(Y) In neural network terminology\u672f\u8bed\uff0can epoch is one pass through all source data. The DataLoader class is designed so that it can be iterated using the enumerate() function, which returns a tuple with the current batch zero-based index value, and the actual batch of data.There is a tight coupling between a Dataset and its associated DataLoader, meaning you have to know the names of the keys used for the predictor values and the dependent variable values. In this case the two keys are \"predictors\" and \"political\". Implementing a Dataset Class You have a lot of flexibility when implementing a Dataset class. You are required to implement three methods and you can optionally add other methods depending on your source data The required methods are init (), len (),and getitem (). The demo PeopleDataset defines its init () method as def __init__(self, src_file, num_rows=None): x_tmp = np.loadtxt(src_file, max_rows=num_rows, usecols= range(0,7), delimiter=\"\\t\", skiprows=0, dtype=np.float32) y_tmp = np.loadtxt(src_file, max_rows= num_rows, usecols=7, delimiter=\"\\t\", skiprows = 0, dtype =np.long) self.x_data = T.tensor(x_tmp, dtype=T.float32).to(device) self.y_data = T.tensor(y_tmp, dtype=T.long).to(device) In situations where your source data is too large to fit into memory, you will have to read data into a buffer and refill the buffer when the buffer has been emptied . This is a fairly difficult task. The demo data stores both the predictor values and dependent values-to-predict in the same file. In situation where the predictor values and dependent variables values are in separate files, you'd have to pass in two source file names instead of just one. Another common alternative is to pass in just a single source directory and then use hard-coded file names for the training and test data. The demo init () method bulk-converts\u5927\u91cf\u8f6c\u6362 all Numpy array data to PyTorch tensors. An alternative is to leave the data in memory as Numpy arrays and then convert to batches of data to tensors in the getitem () method.Conversion from Numpy array data to PyTorch tensor data is an expensive operation so it's usually better to convert just once rather than repeatedly converting batches of data. The len () method is defined as: def __len__(self): return len(self.x_data) A Dataset object has to know how much data there is so that an associated DataLoader object knows how to iterate through all data in batches. The getitem () method is defined as : def __getitem__(self, idx): if T.is_tensor(idx): idx = idx.tolist() preds = self.x_data[idx, 0:7] pol = self.y_data[idx] sample = {'prediction':preds, 'political': pol} return sample It's common practice to name the parameter which specifies which data to fetch as \"idx\" but this is somewhat misleading because the idx parameter is usually a Python list of several idexes.The getitem () method checks to see if the idx parameter is a PyTorch tensor instead of a Python list, and if so, converts the tensor to a list. The method return value, sample, is a Python Dictionary object and so you must specify names for dictionary keys(\"predictors\" in the demo) and the dictionary values (\"political\" in the demo). Using a Dataset in DataLoader The demo program creates a relatively simple DataLoader object using just the Dataset object plus the batch_size and shuffle parameters. train_file = \"./people_train.txt\" train_ds = PeopleDataset(train_file, num_rows=8) bat_size = 3 train_ldr = T.utils.data.DataLoader(train_ds, batch_size=bat_size, shuffle = True) In some situations, instead of using a DataLoader to consume the data in a Dataset, it's useful to iterate through a Dataset directly. For example: def process_ds(model , ds): #ds is an iterable Dataset of tensors for i in range(len(ds)): inpts = ds[i]['predictors'] trgt = ds[i]['target'] oupt = model(inpts) #do something return somevalue Using other DataLoaders Once you understand how to create a custom Dataset and use it in a DataLoader, many of the built-in PyTorch library Dataset objects make more sense than they might otherwise. For example, the TorchVision module has data and functions that are useful for image processing . One of the Dataset classes in TorchVision holds the MNIST image data. There are 70,000 MNIST images. Each image is a handwritten digit from '0' to '9'. Each image has size 28x28 pixels and pixels are grayscale values from 0 to 255. A Dataset class for the MNIST images is defined in the torchvison.datasets package and is named MNIST. You can create a Dataset for MNIST training images like so: import torchvision as tv tform = tv.transforms.Compose([tv.transforms.ToTensor()]) mnist_train_ds = tv.datasets.MNIST(root=\"./MNIST_Data\", train=True, transform=tform, target_transform=None, download=True) After an MNIST Dataset object has been created, it can be used in a DataLoader as normal, for example mnist_train_dataldr = T.utils.data.DataLoader(mnist_train_ds,batch_size =2, shuffle = True) for (batch_idx, batch) in enumerate(mnist_train_dataldr): print(\"\") print(batch_idx) print(batch) input() To recap\u56de\u987e\uff0c there are many built-in Dataset classes defined in various PyTorch packages. They have different calling signatures, but they all read in data from some source (often a hard-coded URL), and have a way to convert their data to PyTorch tensors.After a built-in Dataset has been created, it can be processed by a DataLoader object using the enumerate() function.","title":"The demo program"},{"location":"radar_camera_fusion/","text":"radar camera fusion \u6211\u4eec\u7684\u65b9\u6cd5\u79f0\u4e3aCenterFusion\uff0c\u5b83\u9996\u5148\u4f7f\u7528\u4e2d\u5fc3\u70b9\u68c0\u6d4b\u7f51\u7edc\u901a\u8fc7\u5728\u56fe\u50cf\u4e0a\u8bc6\u522b\u5bf9\u8c61\u7684\u4e2d\u5fc3\u70b9\u6765\u68c0\u6d4b\u5bf9\u8c61\u3002 \u7136\u540e\u5b83\u4f7f\u7528\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u89c6\u9525\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u5173\u952e\u6570\u636e\u5173\u8054\u95ee\u9898\uff0c\u4ece\u800c\u5c06\u96f7\u8fbe\u68c0\u6d4b\u7ed3\u679c\u4e0e\u76f8\u5bf9\u5e94\u5bf9\u8c61\u7684\u4e2d\u5fc3\u70b9\u76f8\u5173\u8054\u3002 \u76f8\u5173\u7684radar\u68c0\u6d4b\u7528\u4e8e\u751f\u6210\u57fa\u4e8eradar\u7684\u7279\u5f81\u56fe\u4ee5\u8865\u5145\u56fe\u50cf\u7279\u5f81\uff0c\u5e76\u56de\u5f52\u5230\u8bf8\u5982\u6df1\u5ea6\uff0c\u65cb\u8f6c\u548c\u901f\u5ea6\u4e4b\u7c7b\u7684\u5bf9\u8c61\u5c5e\u6027\u3002 radar\u4f7f\u7528\u591a\u666e\u52d2\u6548\u5e94\u5feb\u901f\u51c6\u786e\u7684\u786e\u5b9a\u7269\u4f53\u7684\u901f\u5ea6\u3002 \u89c6\u9525\u4f53\u5173\u8054\u673a\u5236 \u4f7f\u7528\u5bf9\u8c61\u76842d\u8fb9\u6846\u6781\u5176\u4f30\u8ba1\u7684\u6df1\u5ea6\u548c\u5927\u5c0f\uff0c\u4e3a\u8be5\u5bf9\u8c61\u521b\u5efa3d\u5174\u8da3\u533a\u57df\uff0c\u89c6\u9525\u4f53\u3002 \u5728roi\u5185\u79bb\u56fe\u50cf\u4e2d\u5fc3\u70b9\u6700\u8fd1\u7684\u70b9\uff0c\u5173\u8054\u3002 \u5176\u4e2d \u662f\u5c3a\u5bf8\u81ea\u9002\u5e94\u6807\u51c6\u504f\u5dee\u3002 For autonomous robots to navigate a complex environment , it is crucial \u81f3\u5173\u91cd\u8981\u7684 to understand the surrounding scene both geometrically and semantically.Modern autonomous robots employ multiple sets of sensors, including lidars, radars, and cameras. Managing the different reference frames and characteristics of the sensors, and merging their observations into a single representation complicates perception. Our approach consists of an ensemble of neural networks which take in sensor data from different modalities \u5f62\u5f0f and transform them into a single common top-down semantic grid representation.We find representation favourable as it is agnostic\u65e0\u5173 to sensor-specific reference frames and captures both the semantic and geometric information for the surrounding scene. Because the modalities share a single output representation,they can be easily aggregated \u6c47\u603bto produce a fused output.In this work we predict short-term semantic grids but the framework can be extended to other tasks.This approach offers a simple, extensible , end to end approach for multi-modal perception and prediction. The benifits of a shared top-down representation across modalities are threefold. First, it is an interpretable representation that better facilities\u4fc3\u8fdb debugging\u8c03\u8bd5 and reasoning\u63a8\u7406 about inherent\u56fa\u6709 failure modes\u6545\u969c\u6a21\u5f0f of each modality.Second it is independent of any particular sensors characteristics and so is easily extensible for adding new modalities.Finally , it simplifies the task of late fusion by sharing a spatial\u7a7a\u95f4\u7684 representation in a succinct manner. In this work we present a novel end-to-end framework that predicts the top-down view of the current scene($t_0$) as well as multiple timesteps into the future.The pipleline consists of a convolutional neural network for each of three sensor modalities : lidar, radar , camera.Each sensor modality predicts a sequence of top-down semantic grids, then these outputs are fused to produce a single output grid.We explore fusing using two different aggregation mechanisms. Estimating 3D orientation and translation of objects is essential for infrastructure-less .In case of monocular vision, successful methods have been mainly based on two ingredients\u56e0\u7d20: \uff081\uff09a network generating 2D region proposals(2D\u533a\u57df\u63d0\u6848)\u3002(2) a R-CNN structure predicting 3D object pose by utilizing the acquired regions of interest. We argue that the 2D detection network is redundant\u591a\u4f59\u7684 and introduces non-negligible \u4e0d\u53ef\u5ffd\u7565\u7684 noise for 3D detection.Hence,we propose a novel 3D object detection method, named SMOKE, in this paper that predicts a 3D bounding box for each detected object by combining a single keypoint estimate with regressed 3D variables.As a second contribution, we propose a multi-step disentangling approach for constructing the 3D bounding box ,which significantly improves both training convergence\u6536\u655b and detection accuracy. In constract to previous 3D detection techniques, our method does not require complicated pre/post-processing.extra data, and a refinement\u7ec6\u5316 stage. Despite of its structural\u7ed3\u6784 simplicity\uff0c our proposed SMOKE network outperforms all existing monocular 3D detection methods on the KITTI dataset , giving the best state-of-the-art result on both 3D objection dection and bird's eye view evaluation . The code will be made publicly available. Vison based object detection is an essential ingredient\u6210\u5206 of autonomous vehicle perception \u6d1e\u5bdf\u529b of autonomous vehicle perception and infrastructure\u57fa\u7840\u8bbe\u65bd less robot navigation in general. This type of detection methods are used to perceive \u611f\u77e5 the surrounding environment by detecting and classifying object instances\u5b9e\u4f8b into categories \u7c7b\u522b and identifying their locations and orientations. Recent developments in 2D object detection have achieved promising performance\u6709\u524d\u9014\u7684 on both detection accuracy and speed. In constract, 3D object detection have proven to be a more challenging task as it aims to estimate pose and location for each object simulataneously. Currently,the most successful 3D object detection methods heavily depend on Lidar point cloud. or LIDAR-Image fusion information.(features learned from the point cloud are key components of the detection network).However , LIDAR sensors are extremely expensive , have a short service life time and too heavy for autonomous robots. Hence LIdars are currently not considered to be econnomical\u6d41\u884c\u7684 to support autonomous vehicle operations . Alternatively , cameras are cost-effective, easily mountable and light-weight solutions for 3D object detection with long expected service time. Unlike lidar senors , a single camera in itself cannot obtain sufficient spatial information for the whole environment as single RGB images can not supply object location information or dimensional contour \u8f6e\u5ed3 in the real world. While binocular \u53cc\u76ee vision restores the missing spatial information. in many robot applications, especially UAVs , it is difficult to realize biocular vision. Hence , it is desirable to perform 3D detection on a monocular image even if it is a more difficult and chanllenging task. To enhance performance, geometry reasoning \u51e0\u4f55\u63a8\u7406 synthetic data \u7efc\u5408\u6570\u636e and post 3D-2D processing have also been used to improve 3D object detection on single image.By the knowledge of the authors , no reliable monocular 3D detection method has been introduced so far to learn 3D information directly from image plane avoiding the performance decrease that is inevitable\u4e0d\u53ef\u907f\u514d\u7684 with multi-stage method. In this paper we propose an innovative\u521b\u65b0 single-stage 3D object detection method that pairs each object with a single keypoint. We argue and later show that a 2D detection, which introduces nonnegligible noise in 3D parameter estimation, is redundant\u591a\u4f59\u7684 to perform 3D object detection.Furthermore 2D information can be naturally obtained if 3D variables and camera instrinsic matrix are already known.Consequently, our designed network eliminate \u6392\u9664 the 2D detection branch and estimates the projected 3D points on the image plane instead.A 3d parameter regression branch is added in parallel.This design results in a simple network structure with two estimation threads.Rather than regressing variables in a separate method by use multiple loss functions, we transform these variables together with projected keypoint to 8 corner representation of 3D boxes and regress them with a unified\u7edf\u4e00\u7684 loss function. As in most single state 2D object detection algorithms, our 3D detection approach only contains one classification and regression branch.Benefiting from the simple structure , the network exhibits \u5c55\u793a improved accuracy in learning 3D variables, has better convergence and less overall computional needs. Second constribution of our work is a multi-step disentanglement \u7ea0\u7f20 approach for 3D bounding box regression. Since all the geometry information is grouped into the parameter, it is difficult for the network to learn each variable accurately n a unified way.Our proposed method isolates\u5206\u79bb the contribution of each parameter in both the 3D bounding box ecoding phase and regression loss function , which significantly helps to train the whole network effectively. Our contribution is summarized as follows: We propose a one-stage monocular 3D object detection with a simple architecture that can precisely learn 3D geometry in an end-to-end fashion. We provide a multistep distanglement approach to improve the convergence\u6536\u655b of 3D parameters and detection accuracy. The result method outperforms all existing state-of-art monocular 3D object detection algorithms on the chanllenging KITTI dataset at the submission date November 2019. We formulate\u5b9a\u4e49 the monocular 3D object detection problem as follow: given a single RGB image ,with the width and the height of the image, find for each present object its category\u7c7b\u522b label C and its 3D bounding box B, where the latter is parameterized by 7 variables .Here, represent the height, width and length of each object in meters , and is the coordinates (in meters) of the object center in the camera coordinate frame. Variable is the yaw orientation of the corresponding cubic box. The roll and pitch angles are set to zero by following the Kitti annotation .Additionally , we take the mild\u6e29\u548c assumption that the camera instrinsic matrix K is known for both training and inference. Smoke approach In this section, we describe the smoke network that directly estimates 3D bounding boxes for detected object instances from monocular imagery. In constrast to previous techniques that leverage \u6760\u6746\u4f5c\u7528 2D proposals to prediect 3D bounding box, our method can detect 3D information with a simple single stage.The propose method can be divided into three parts:(1) backbone,(2) 3D detection (3) loss function.First , we briefly discuss the backbone for feature extraction , followed by the instruction of the the 3D detection network consisting of two separated branches. Finallly,we discuss the loss function design and the multi-step disentanglement to compute the regression loss.The overview of the network structure is depicted in Fig.2 backbone We use a hierarchical layer fusion network DLA-34 as the backbone to extract features since it can aggregate information across different layers.Following the same structure as in [], all the hierarchical aggregation connections are replaced by a Deformable\u53ef\u53d8\u5f62\u7684 convolution network (DCN) The output feature map is downsample 4 times with respect to the original image.Compared with the original implementation, we replace all BatchNorm(BN) operation with GroupNorm(GN) since it has been proven to be less sensitive to batch size and more robust to training noise.We also use this technique in the two prediction branches. which will be discussed later.This adjustment not only improve detection accuracy , but it also reduces considerably the training time. Later ,we provide performance comparison of BN and GN to demonstrate these properties. 3D detection network Keypoint branch: We define the keypoint estimation network similar to such that each object is represented by one specific keypoint. Instead of identifying the center of a 2D bounding box , the key point is defined as the projected 3D center of the object on the image plane. The comparison between 2D center points and 3D projected points is visualized in Fig3.The projected keypoints allow to fully recover 3D location for each object with camera parameters. Let represent the 3D center of each object in the camera frame.The projection of 3D points of points on the image plane can be obtained with the camera instrinsic matrix K in a homogeneous form: \\begin{bmatrix}z\\cdot x_c\\\\z\\cdot y_c\\\\z\\end{bmatrix}=K_{3\\times 3}\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}\\quad\\quad\\quad(1) For each ground truth keypoint, its corresponding downsampled location on the feature map is computed and distributed using a Gaussian Kernel following.The standard deviation is allocated based on the 3D bounding boxes of the ground truth projected to the image plane.Each 3D box on the image is represented by 8 2D points and the standard deviation ia computed by the smallest 2D box with that encircles the 3D box. Regression Branch \\logeach key point on the heatmap. Similar to other monocular 3D detection framework.The 3D information is encoded as an 8-tuple\u5143\u7ec4 \\tau=\\begin{bmatrix}\\delta_z&\\delta_{x_c}&\\delta_{y_c}&\\delta_h&\\delta_w&\\delta_l&sin(a)&cos(a)\\end{bmatrix}^T Here denotes the depth offset , is the discretization\u79bb\u6563\u5316 offset due to downsampling. denotes the resdual \u5269\u4f59 dimension. is the vectorial representation of the rotational angle . We encode all variables to be learnt in residual\u6b8b\u5dee representation to reduce the learning interval and ease the training task . The size of feature map for regression results in . Inspired by the lifting transformation\u63d0\u5347\u8f6c\u578b described in [],we introduce a similar operation F that converts projected 3D points to a 3D bounding box .For each object, its depth $z$ can be recovered by pre-defined scale and shift parameters and as Given the object depth $z$ , the location for each object in the camera frame can be recovered by using its discretized\u79bb\u6563\u7684 projected centroid $[x_c, y_c]^T$ on the image plane and the downsampling offset \\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}=K^{-1}_{3\\times 3}\\begin{bmatrix}z\\cdot(x_c+\\delta_{x_c})\\\\z\\cdot(y_c+\\delta_{y_c})\\\\z\\end{bmatrix} This equation is inverse of Eq.(1) In order to retrieve\u53d6\u56de object dimensions , we use a pre-calculated category-wise\u6309\u7c7b\u522b average dimension computed over the whole dataset. Each object dimension can be recovered by using the residual dimension offset : \\begin{bmatrix}h\\\\w\\\\l\\end{bmatrix}=\\begin{bmatrix}\\overline{h}\\cdot e^{\\delta_h}\\\\\\overline{w}\\cdot e^{\\delta_w}\\\\\\overline{l}\\cdot e^{\\delta_l}\\end{bmatrix}\\quad\\quad\\quad(4) Inspired by [],we choose to regress the observation angle \u03b1 instead of the yaw rotation \u03b8 for each object. We further change the observation angle with respect to the object head ,instead of the commonly used observation angle value , by simply adding . The difference between these two angles is shown in Figure4.Moreover,each is encoded as the vector $[sin(a)\\quad cos(a)]^T$ The yaw angle $\\theta$ can be obtained by utilizing $\\alpha_z$ and the object location: \\theta=\\alpha + arctan\\left(\\frac{x}{z}\\right)\\qquad(5) Finally,we can construct the 8 corners of the 3D bounding box in the camera frame by using the yaw rotation matrix $R_\\theta$, object dimensions $[h\\quad w\\quad l]^T$ and location $[x\\quad y\\quad z]^T$: B=R_\\theta\\begin{bmatrix} \\pm h/2\\\\\\pm w/2\\\\\\pm l/x \\end{bmatrix}+\\begin{bmatrix} x\\\\y\\\\z \\end{bmatrix}\\qquad\\qquad(6) Loss function We employ the penalty-reduce focal loss\u60e9\u7f5a\u51cf\u5c11\u7126\u70b9\u635f\u5931 [] in a point-wise manner on the downsampled heatmap.Let $s_{i,j}$ be the predicted score at the heatmap location $(i,j)$ and $y_{i,j}$ be the ground-truth value of each point assigned by Gaussian Kernel. Define $\\breve{y} {i,j}$ and $\\breve{s} {i,j}$ as : \\breve{y}_{i,j}=\\left\\{\\begin{matrix} 0,\\qquad if\\quad y_{i,j} =1\\\\y_{i,j} ,\\qquad otherwise\\end{matrix}\\right. \\breve{s}_{i,j}=\\left\\{\\begin{matrix} s_{i,j},\\qquad if\\quad y_{i,j}=1\\\\1-s_{i,j},\\qquad otherwise \\end{matrix}\\right. For simplicity,we only consider a single object class here.Then , the classification loss function is constructed as L_{cls}=-\\frac{1}{N}\\sum^{h,w}_{i,j=1}(1-\\breve{y}_{i,j})^\\beta(1-\\breve{s}_{i,j})^\\alpha log(\\breve{s}_{i,j})\\qquad(7) where $\\alpha$ and $\\beta$ are tunable hyper-parameters and N is the number of keypoints per image.The term $(1-y_{i,j})$ corresponds to penalty reduction for points around the groundtruth location. Regression Loss We regardless the 8D tuple $\\tau$ to construct the 3D bounding box for each object. We also add channel-wise action to the regressed parameters of dimension and orientation at each feature map location to preserve\u4fdd\u5b58 consistency\u4e00\u81f4\u6027 . The activation functions for the dimension and the orientation are chosen to be the sigmoid function $\\sigma$ and the $l_2$ norm ,respectively: \\begin{bmatrix} \\delta_h\\\\\\delta_w\\\\\\delta_l \\end{bmatrix}=\\sigma\\left(\\begin{bmatrix} O_h\\\\O_w\\\\O_l \\end{bmatrix}\\right)-\\frac{1}{2},\\quad\\begin{bmatrix} sin(a)\\\\cos(a) \\end{bmatrix}=\\begin{bmatrix} O_{sin}/\\sqrt{O^2_{sin}+O^2_{cos}}\\\\O_{cos}/\\sqrt{O^2_{sin}+O^2_{cos}} \\end{bmatrix} Here $o$ stands for the specific output of network. By adopting the keypoint lifting transformation introduced in Sec.4.2.we define the 3D bounding box regression loss as the $l_1$ distance between the predicted transform $\\hat{B}$ and the ground truth B: L_{reg}=\\frac{\\lambda}{N}||\\hat{B}-B||_1\\qquad(8) where $\\lambda$ is the scaling factor.This is used to ensure that neither the classification, nor the regression dominates\u4e3b\u5bfc the other.The disentangling transformation of loss has been proven to be an effective dynamic method to optimize 3D regression loss functions in [31] . Following this design , we extend the concept of loss disentanglement into a multi-step form. in Eq(3) , we use the projected 3D groundtruth points on the image plane $[x_c\\quad y_c]^T$ with the network predicted discretization offset $[\\hat{\\delta_{x_c}}\\quad \\hat{\\delta_{y_c}}]^T$ and depth $\\hat{z}$ to retrieve the location $[\\hat{x}\\quad \\hat{y}\\quad \\hat{z}]^T$ of each object. In Eq(5), we use the groundtruth location $[x\\quad y\\quad z]^T$ and the predicted observation angle $\\hat{a}_z$ to construct the estimated yaw orientation $\\hat{\\theta}$. The 8 corners representation of the 3D bounding box is also isolated \u9694\u79bb into three different groups following the concept of disentanglement , namely orientation, dimension and location.The final loss function can be represented by: L = L_{cls} + \\sum^3_{i=1}L_{reg}(\\hat{B_i}),\\qquad(9) where $i$ represents the number of groups we define in the 3D regression branch.The multi-step disentangling transformation divides the contribution of each parameter group to the final loss. In Sec.5.2, we show that this method significantly improves detection accuracy. Implementation In this section,we discuss the implementation of our proposed methodology in detail together with selection of the hyperparameters. Preprocessing : we avoid applying any complicated preprocessing method on the dataset.Instead, we only eliminate\u6392\u9664 objects whose 3D projected center point on the image plane is out of the image range. Note that the total number of projected center points outside the image boundary for the car instance is 1582.This accounts for only the $5.5$% of the entire set of 28742 labeled cars. Data Augmentation : Data augmentation techniques we used are random horizontal flip\u6c34\u5e73\u7ffb\u8f6c, random scale and shift. The scale ratio is set to 9 steps from 0.6 to 1.4, and the shift ratio is set to 5 steps from -0.2 to 0.2. Note that the scale and shift augmentation methods are only used for heatmap classification since the 3D information becomes inconsistent\u4e0d\u4e00\u81f4 with data augmentation. Hyperparameter Choice : In the backbone, the group number for GroupNorm is set to 32.For channels loss than 32, it is set to be 16. For Eq.7, we set $\\alpha=2$ and $\\beta=4$ in all experiments. Based on [31], the reference car size and depth statistic\u7edf\u8ba1\u6570\u636e we use are $[\\overline{h}\\quad \\overline{w}\\quad \\overline{l}]^T=[1.63\\quad 1.53\\quad 3.88]^T$ and $[\\mu_z\\quad \\sigma_z]^T=[28.01\\quad 16.32]^T$ (measured in meters). Trainning :Our optimization schedule\u4f18\u5316\u8868 is easy and straightforward. We use the original image resolution and pad it to $1280\\times 384$. We train the network with a batch size of 32 on 4 Geforce TITAN X GPUs for 60 epochs. The learning rate is set at $2.5\\times 10^{-4}$ and drops at 25 and 40 epochs by a factor of 10. During testing, we use the top 100 detected 3D projected points and filter it with a threshold of 0.25. No data augmentation method and NMS are used in the test precedure.Our implementation platform in Pytorch1.1, CUDA 10.0 and CUDNN 7.5. Depth Estimation from Monocular Images and Sparse Radar Data In this page, we explore the possibilty of achieving a more accurate depth estimation by fusing monocular images and Radar points using a deep neural network.We give a comprehensive study of the fusion between RGB images and Radar measurements from different accepts and proposed a working solution based on obeservations. We find that the noise existing in Radar measurements is one of the main key reasons that prevents one from applying the existing fusion methods developed for for LiDAR data and images to the new fusion problem between Radar data and images to the new fusion problem between Radar data and images.The experiments are conducted\u5b9e\u65bd on the nuScenes dataset, which is one of the first datasets which features Camera\uff0c Radar, and LiDAR recordings in diverse\u5404\u79cd\u5404\u6837\u7684 scenes and weather conditions. Extensive\u5e7f\u6cdb\u7684 experiments demonstrate that our method outperforms existing fusion methods. We also provide detailed ablation\u707c\u70e7 studies to show the effectiveness of each component in our method. Dense and robust depth estimation is an important component in self-driving system and unmanned\u65e0\u4eba aerial vehicles. While existing structure-light-based depth sensor or stereo camera can provide dense depth in indoor environments, the reliability of these sensors degrade a lot in outdoor applications. As a result, lots of research works focus on obtaining dense depth from monocular RGB images only. Recently, convolutional neural network (CNN) based methods have demonstrated impressive improvement on monocular depth estimation for both indoor and outdoor scenarios.However , there is still a gap between the accuracy and reliability of these methods and what the real-world applications need. Apart from estimating depth from monocular camera, to imporve the robustness of the system, some methods also take other sensor modalities\u5f62\u5f0f into consideration.While thses sensors,LiDARS is the most commonly used one. Many works have been conducted on dense depth estimation from RGB images and sparse LiDAR scans.In addition to depth estimation and completion tasks, different RGB + LiDAR fusion techniques are also extensively used in tasks such as 3D object detection. Although LiDAR provides more accurate depth measurements in outdoor scenario, high-end LiDAR sensors are still far from affordable for many applications. Compared with LiDAR, Radar is an automotive-grade sensor that has been used for decades on vehicles, but has not attracted lots of attention in self-driving research based on deep learning. One reason might be that Radar measurements are not included in most of the dominant self-driving datasets.Compared with LiDAR, Radar sensors offer longer sensing range(200m~300m), more attributes including velocities, dynamic states, and measurement uncertainties. Most importantly, the costs of these sensors are much lower than LiDAR.However, Radar measurements are much lower than LiDAR ,However, Radar measurement are typically sparser, noiser, and have a more limited vertical field of view. This work is to study the chanllenges of using Radar data for dense depth estimation and to propse a novel method for that aim. Given recently released nuScenes dataset consisting of RGB, LiDAR and Radar measurements.we are able to conduct experiments on cross-modality sensor fusion between RGB camera and Radar.Through our experiments we demonstrated that:1)Existing RGB + LiDAR fusion methods can be applied directly to RGB + Radar fusion task; and 2) with proper fusion strategies and a novel denosing operation, our proposed network is able to improve the performance of depth estimation by a good margin by using Radar measurements. According to our survey, our work is the first one that brings Radar sensors into dense depth estimation tasks. The contributions of this work include: 1) a detailed study on the challenges of using Radar data for dense depth estimation; and 2) a novel and carefully motivated network architecture for depth estimation with monocular images and sparse Radar data. RELATED WORKS RGB-based Depth Estimation. Depth estimation from monocular or stereo camera is a popular research topic in both computer vision and robotics.Early works used either geometry-based algorithms on stereo images, or handcrafted features on single images.Recent years, convolutional neural networks(CNN) have demonstrated their ability in image understanding, dense predictions ,etc. given large scale datasets. Therefore, lots of research works of monocular depth estimation are conducted.In general , most of them used the encoder-decoder architecturres. Xie . futher introduced skip connection strategy which is a frequently used technique to multi-level features in dense prediction tasks. On the other hand, Huang achieve state-of-the-art performance by introducing space increasing discretization (SID) and ordinal regression. In some semi-/self-supervised formu loss is used. and the smoothness constraint\u7ea6\u675f is further imposed to enhance local consistency\u4e00\u81f4\u6027.Patil proposed a recurrent\u53cd\u590d\u53d1\u4f5c network architecture to exploit the long-range spatiotemporal\u65f6\u7a7a structures across video frames to yeild more accurate depth maps.While good performance has been obtained with only RGB images, the methods still have difficulty in generalizing to new scenario and chanllenging weather and light conditions. This motivates the existing line of work that fuses camera data with Lidar data and our work that fuses camera data with radar data is cheaper to obtain. Depth Estimation with Camera and Lidar data while monocular depth estimation task attracts lots of attention, achieving more reliable and accurate predictions using multi-modality\u591a\u6a21\u6001 information is also a popular topic.Existing works either take the whole set of LiDAR points , (known as depth completion), or the downsampled set as model inputs.Ma first projected LiDAR points to 2D sparse depth map and then perform so called early fusion by direct concatenation with RGB images along channel, or concatenation feature maps after one shallow convolution block.J used a late fusion method to combine features from different modalities and improved the overall performance through multitask learning.Q proposed to predict dense depth map by combining predictions from RGB and surface normal pathways, where surface normal is treated as an intermediate representation. Moreover, confidence maps are predicted to down-weight mixed measurements from LiDAR caused by the displacement between camera and LiDAR. In this work our main focus is sensor fusion . Thus , we use the widely adopted encoder-decoder architecture and focus on the necessary extensions in order to effectively use Radar data instead. Post-processing and Refinement Methods \u540e\u5904\u7406\u548c\u4f18\u5316\u65b9\u6cd5. Apart from treating sparse point clouds as inputs to the model, some methods also tried to directly refine the dense predictions of the trained models.W proposed a simple add-on module that can improve the prediction of depth estiamtion model using similar methods used by white box adversarial attack. Since the refinement is done using iterative re-inference, no re-training is required.This method can be integrated into most deep learning based methods.Cheng learned an affinity \u4eb2\u548c\u529b matrix from data to refine the outputs of their CNN model.The recurrent refinement operation can also be extended to depth completion tasks. Fusion of images and radar Data. There are already works that fuse RGB images and Radar.given the fact that they are very much complementary\u8865\u5145.The line of work mainly focus on object detection and tracking.For instance,C fused\u878d\u5408 Radar data and images to detect small objects at a large distance. In [] and [] the authors enhance current 2D object detection networks by fusing camera data and projected sparse radar data in the network layers\uff0cwhile [] also performs free space semantic segmentation jointly.Both methods learn at which level the fusion of the sensor data is more beneficial for the task. There are also other datasets proposed for object detection with Radar data such as [].Exemplary\u793a\u8303\u6027\u7684 works on semantic segmentation with Radar point cloud have been conducted as well. For instance, in [] the authors have studied how the challenging task can be performed and provide results on a dataset with manually labeled radar reflections.Similar to these works for object detection and semantic segmentation, our work aims to study how the challenging task of dense depth estimation with radar data can be addressed with the popular deep neural network architectures. In the line of increasing the robustness of depth estimation, Vasudevan, et al. [] have proposed a novel method to estimate depth maps based on binaural\u53cc\u8033 sounds. Our Method Our whole method are divided into multiple main components. In the following section, we will go through each component in detail. Radar Data background Different from well established depth completion or depth estimation tasks.There is no prior research works on RGB + radar depth estimation task.Therefore, we provide a brief introduction to the task formulation and some key differences between Radar and LiDAR measurements,which will help readers to understand the motivations behind the components of our method. Data format. similar to LiDAR data, Radar measurements are recorded as sparse point clouds. The main difference is that , in addition to $x,y,z$,and $reflectance$, Radar data consist of additional measurements including the velocity along x and y direction, the standard deviation of the location and velocity measurements.and information such as the dynamic states of the measured object (encoded as discrete\u79bb\u6563\u7684 numbers). Limitation . While it seems that the radar data provide more information, it also introduces the following limitations compared with LiDAR data: * sparseness:In nuScenes dataset , there are more than 3000 LiDAR points after projection to the camera.However, there are less than 100 Radar points after the projection. * Limited vertical field of view.Because of the limitation of the sensor,Radar measurements mainly concentrate in the central horizontal bin (similar heights) as shwon in Fig.2. * Noisy measurements:Due to multiple reflections (Radar multipath problem) or other reasons , we have many noisy measurements as shown in Fig.2. * Inconsistency\u524d\u540e\u4e0d\u4e00\u81f4 with LiDAR data:Apart from noisy measurements, which are considered as outliers, the 3D points of Radar and LiDAR representing the same object can also be different.Since we typically use LiDAR measurements as ground truth, even noise-free Radar measurements are not perfect on the evaluation metrics. As we will show in Section IV, using Radar depth maps directly as the input of off-the-shelf\u73b0\u6210\u7684 RGB+LiDAR depth completion /prediction models resulted in marginal\u8fb9\u7f18 improvements. Problem formulation . In our RGB+ Radar formulation,each data sample from the dataset contains (1) an RGB image $x_{RGB}$ (2) a set of Radar measurements $R={r_n}^N_{n=1}$ from 3 nearest timestamps (3) a set of Lidar measurements $L={l_m}^M_{m=1}$ Radar measurements $R$ can be further projected to a single-channel 2D depth map $x_{Radar}$ using the perspective projection.Similarly,LiDAR measurements can be objected to 2D map y, which is treated as ground truth depth map in our experiments(section IV).Our model takes both $x_{RGB}$ and $x_{Radar}$ as inputs and predicts dense 2D depth map $\\tilde{y}$ which minimizes the metrics errors. Same as all the depth estimation /completion tasks, loss and metric error are computed over the pixels with ground truth measurements. CNN architecture Backbone . Since our goal is to perform a comprehensive\u7efc\u5408\u7684 study on different fusion methods, we need to choose an efficient and widely-used backbone. Thus, we fixed our backbone to Resnet18 , and explored different fusion methods based on it as a pilot\u98de\u884c\u5458 experiment. As illustrated in Fig.3, we apply different encoder fusion methods to the simple encoder-decoder architecture proposed by Ma, and compare their performance. According to the experiment (Section IV-C), late fusion and multi-layer fusion model have comparable performance.Therefore, we adopt\u91c7\u7eb3 late fusion as our main encoder backbone design in the following experiments for simplicity. Two-Stage architecture As shown by the pilot experiments(section IV-C) , a simple encoder-decoder architecture have some improvements on RGB + Radar depth prediction task if we can remove most of the noisy measurements. However, we don't have LiDAR ground truth to help us performing the filtering in real applications.and it's hard to perform outlier rejections\u5f02\u5e38\u5254\u9664 without information on the 3D structure or objects of the scene.Therefore, we come up with a 2-stage design to address the noisy measurement issue. As shown in Fig.1, our whole method contains two stages. The stage 1 model $f_{stage1}$ takes both RGB image $x_{RGB}$ and the radar depth map $x_{Radar}$ as inputs and predicts a coarse\u7c97 depth map $\\tilde{y}_{stage1}$ ,which gives us a dense 3D structure of the scene: \\tilde{y}_{stage1}=f_{stage1}(x_{RGB},x_{Radar})\\qquad\\quad(1) Then we compare the Radar depth map with the coarse prediction $\\tilde{y} {stage1}$ to reject some outliers (most details in next subsection) and obtain the filtered Radar depth map $\\tilde{x} {Radar}$. The assumption\u5047\u8bbe here is that although the predictions from stage 1 is not perfect, they are smooth and locally consistent. Therefore, they are suitable to reject outliner noises produced by Radar Multipath, which typically have certain margins with the correct depth values. The stage2 model $f_{stage2}$ takes $x_{RGB}$ , $\\tilde{x} {Radar}$ , and the prediction from stage1 $\\tilde{y} {stage1}$ to predict the final result $\\tilde{y}$: \\tilde{y} = f_{stage_2}(x_{RGB},\\tilde{x}_{Radar},\\tilde{y}_{stage1})\\qquad(2) Noise filtering module Since Radar measurements are not exactly consistent with the LiDAR measurements as we mentioned in Section III-A .we need to have some tolerance\u516c\u5dee in the filtering process.Otherwise , we will end up discarding\u4e22\u5f03 all the measurements in the set $R$. Instead of setting a fixed distance tolerance threshold $\\tau$,we empirically\u51ed\u7ecf\u9a8c found that an adaptive threshold gives us better results. We design the threshold to be a function of depth value $\\tau(d)$ : We have larger tolerance for larger depth values, which is similar to the space-increasing disrectization (SID) from Huan et al.[2]: \\tau(d)=exp\\left(\\frac{d*log(\\frac{\\beta}{\\alpha})}{K}+log(\\alpha)\\right)\\qquad(3) here we heuristically\u542f\u53d1\u5f0f\u7684 set $\\alpha=5$ and $\\beta=18$. Let $P$ denote the set pixel coordinates $(u,v)$ of the Radar measurements projected by the perspective projection function $proj(.)$ : $P={p_n}^N_{n=1}={proj(r_n)^N_{n=1}}$. The noise filtering module will keep the point $p_n$ if it satisfies the following constraint\u7ea6\u675f: |x_{Radar}(p_n)-\\tilde{y}_{stage1}(p_n)|\\leq\\tau(p_n),for\\quad p_n\\quad in\\quad P\\qquad(4) Loss Function By design, each component of our model is differentiable. Thus, our whole model is end-to-end trainable.Following the setting from [7] ,We apply $L1$ loss to both the predictions of stage1 ($\\tilde{y} {stage1}$) and stage2 ($\\tilde{y}$). Considering that the main purpose of $\\tilde{y} {stage1}$ is to filter outlier noises in $x_{Radar}$, we futher add edge-aware smoothness constraint to it . To effectively balance multiple loss terms, we follow the method proposed by Kendall L_{total} = e^{-w1}*(L1(\\tilde{y}_{stage1},y)+10^{-3}*L_{smooth})+e^{-w2}*L1(\\tilde{y},y) + \\sum_{i}w_i\\qquad(6) where $w1$ and $w2$ are optimized variables ,and $L_{smooth}$ is defined as : L_{smooth}=|\\triangledown_u(\\tilde{y}_{stage1})|e^{-|\\triangledown_u(x_{RGB})|} + |\\triangledown_v(\\tilde{y}_{stage1})|e^{-|\\triangledown_v(x_{RGB})|}\\qquad(8) $\\triangledown_u$ and $\\triangledown_v$ denote the gradient along 2D height and width directions sperately. Implementation details Unless stated otherwise, all the models are trained using a batch size of 16 and SGD optimizer with a learning rate of $0.001$ and a momentum\u52bf\u5934 of $0.9$ for 20 epochs. The learning rate is multiplied by 0.1 alter every 5 epochs.All the models we used in experiment action are implemented in PyTorch.The experiments are conducted on the desktop computers/clusters with Nvidia GTX1080Ti and TeslaV100 GPUs. All of our encoder network architectures (section III-B) are modified from the standard Reset18. For early fusion, we simply modified the input channels to 4 and randomly initialized the weights (weights of other layers were initialized from pre-trained models on Imagenet dataset). For mid, late, and multi-layer fusion , the depth branch has a similar architecture as the RGB branch.The only difference is that we change the number of channels to 1/4 of the original one. Fusion operations happened only on feature maps with same spatial resolution (width and height) . Regarding the decoder part, we kept the section from [] by using UpProj module as our upsampling operation. Experiment In an early fusion approach, the raw or pre-processed sensory data from different sensor modalities are fused together. With this approach, the network learns a joint representation from the sensing modalities. Early fusion methods are usually sensitive to spatial or temporal misalignment\u9519\u4f4d of the data. On the other hand, a late fusion approach combines the data from different modalities at the decision level, and provides more flexibility \u7075\u6d3b\u6027 for introducing new sensing modalities to the network.However a late fusion approach does not exploit\u5f00\u53d1\u5229\u7528 the full potential of the available sensing modalities , as it does not acquire the intermediate\u4e2d\u95f4\u7684 features obtained by learning a joint representation.A compromise between the early and late fusion approaches is referred to as middle fusion. It extracts features from different modalities individually and combines them at an intermediate stage, enabling the network to learn joint representations and creating a balance between sensitivity and flexibility. We propose CenterFusion, a middle-fusion approach to exploit radar and camera data for 3D object detection. CenterFusion focuses on associating radar detections to preliminary\u521d\u6b65\u7684 detection results obtained from the image, then generates radar feature maps and uses it in addition to image features to accurately estimate 3D bounding boxes for objects.Particularly, we generate perliminary 3D detections using a key point detection network,and propose a novel frustum-based \u57fa\u4e8e\u89c6\u9525 radar association method to accurately associate radar detections to their corresponding objects in 3D space. These radar detections are then mapped to the image plane and used to create feature maps to complement the image-based features.Finally, the fused features are used to accurately estimate object's 3D properties such as depth, rotation and velocity. The network architecture for CenterFusion is shown in Fig.1 We evaluate CenterFusion on the challenging nuscenes dataset ,where it outperforms all previous camera-based object detection methods in the 3D object detection benchmark. We also show that exploiting radar information significantly improves velocity estimation for object without using any temporal information. Radar Point Cloud Radars are active sensors that transmit radio waves to sense the environment and measure the reflected waves to determine the location and velocity of objects. Automotive radars usually report the detected objects as 2D points in BEV, providing the azimuth\u65b9\u4f4d\u89d2 and radial distance to the object. For every detection, the radar also reports the instantaneous velocity of the object in the radial direction.This radial velocity does not necessarily match the object's actual velocity vector in it's direction of movement. Fig.2 illustrates the difference between the radial as reported by the radar, and actual velocity of the object in the vehicle's coordinate system. We represent each radar detection as a 3D point in the egocentric coordinate system, and parameterize it as $P=(x,y,z,v_x,v_y)$ where $(x,y,z)$ is the position and $(v_x,v_y)$ is the reported radial velocity of the object in the $x$ and $y$ directions. The radial velocity is compensated by the ego vehicle's motion.For every scene,we aggregate 3 sweeps of the radar point cloud (detections within the past 0.25 seconds). The nuscenes dataset provides the calibration parameters needed for mapping the radar point clouds from the radar coordinates system to the egocentric and camera coordinate systems. CenterNet CenterNet represents the state-of-the-art in 3D object detection using single camera. It takes an image $I \\in \\mathbb{R}^{W\\times H\\times 3}$ as input and generates a keypoint heatmap $\\hat{Y}\\in[0,1]^{\\frac{W}{R}\\times\\frac{H}{R}\\times C}$ as output where $W$ and $H$ are the image width and height.$R$ is the downsampling ratio and $C$ is the number of object categories\u7c7b\u522b. A prediction of $\\hat{Y} {x,y,c} = 1$ as the output indicates a detected object of class $c$ centered at position $(x,y)$ on the image. The ground-truth heatmap $Y\\in [0, 1]^{\\frac{W}{R}\\times\\frac{H}{R}\\times C}$ is generated from the ground-truth 2D bounding boxes using a Gaussian kernel.For each bounding box center point $p_i\\in R^2$ of class $c$ in the image, a Gaussian heatmap is generated on $Y {:,:,c}$ , the final value of Y for class $c$ at position $q\\in R^2$ is defined as Y_{qc} = \\underset{i}{max}exp\\left(-\\frac{(p_i-q)^2}{2\\sigma^2_i}\\right) Figure 3,Frustum\u89c6\u9525 association. An object detected using the image features(left),generating the ROI frustum based on object's 3D bounding box(middle), and the BEV of the ROI frustum showing radar detections inside the frustum (right).$\\delta$ is used to increase the frustum size in the testing phase.$\\hat{d}$ is the ground_truth depth in the training phase and the estimated object depth in the testing phase Expanding radar points to 3D pillars( top image). Directly mapping the pillars to the image and replacing with radar depth information results in poor association with object's center and many overlapping depth values(middle image),Frustum association accurately maps the radar detections to the center of objects and minimizes overlapping(bottom image). Radar detections are only associated to objects with a valid ground truth or detection box, and only if all or part of the radar detection pillar\u652f\u67f1 is inside the box.Frustum association also prevents associating radar detections caused by background objects such as buildings to foreground objects, as seen in the case of pedestrains on the right hand side of the image where $\\sigma_i$ is a size-adaptive\u5c3a\u5bf8\u9002\u5e94 standard deviation\u504f\u5dee,controlling the size of the heatmap for every object based on its size. A fully convolutional encode-decoder network is used to predict $\\hat{Y}$. To generate 3D bounding boxes,sperate network heads are used to regress object's depth, dimensions and orientation directly from the detected center points.Depth is calculated as an additional output channel $\\hat{D}\\in [0,1]^{\\frac{W}{R}\\times\\frac{H}{R}}$ after applying the inverse sigmoidal transformation used in Eigen et al. to the original depth domain. The object dimensions are directly regressed to their absoluate values in meter as three output channels $\\hat{\\Gamma}\\in[0,1]^{\\frac{W}{R}\\times\\frac{H}{R}\\times3}$ .Orientation is encoded as two bins with 4 scalars in each bin, following the orientation representation in M. For each center point, a local offset is also predicted to compensate\u8865\u507f for the discretization\u79bb\u6563\u5316 error caused by the output strides in the backbone network. Given the annotated objects $p_0, p_1, ...$ in an image, the training objective is defined as below based on the focal loss: L_k=\\frac{1}{N}\\sum_{xyc}\\left\\{\\begin{matrix}(1-\\hat{Y}_{xyc})^{\\alpha}log(\\hat{Y}_{xyc})\\qquad Y_{xyc}=1\\\\\\\\ (1-Y_{xyc})^{\\beta}(\\hat{Y}_{xyc})^\\alpha log(1-\\hat{Y}_{xyc})\\qquad otherwise\\end{matrix}\\right. where $N$ is the number of objects, $Y\\in [0,1]^{\\frac{W}{R}\\times\\frac{H}{R}\\times C}$ is the annotated object's ground-truth heatmap and $\\alpha$ and $\\beta$ are focal loss hyperparameters. Detection identifies\u8bc6\u522b objects as axis-aligned boxes in an image.Most successful object detectors enumerate\u5217\u4e3e a nearly exhaustive\u8be6\u5c3d\u7684 list of potential object locations and classify each.This is wasteful, inefficient,and requires additional post-processing. In this paper, we take a different approach.We model an object as a single point - the center point of its bounding box.Our detector uses keypoint estimation to find center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose. Our center point based approach, CenterNet, is end-to-end differnetiable, simpler,faster,and more accurate than corresponding bounding box based detectors.CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1% AP at 142 FPS. We use the same approach to estimate 3D bounding box in the KITTI benchmark and human pose on the COCO keypoint dataset. Our method performs competitively with sophisticated multi-stage methods and runs in real-time. Object detection powers many vision tasks like instance segmentation, pose estimation, tracking, and action recognition.It has down-stream applications in surveillance\u76d1\u89c6\uff0c autonomous driving, and visual question answering. Current object detectors represent each object through an axis-aligned bounding box, that tightly encompasses the object.They then reduce object detection to image classification of an extensive number of potential object bounding boxes.For each bounding box, the classifer determines if the image content is a specific object or background. One stage detectors, slide a complex arrangement of possible bounding boxes called anchors, over the image and classify them directly without specifying the box content.Two-stage detectors recompute image features for each potential box, then classify those features.Post-processing, namely non-maxima suppression, then removes duplicated detections for the same instance by computing bounding box IoU. This post-processing is hard to differentiate and train, hence most current detectors are not end-to-end trainable. Nonetheless, over the past five years, this idea has achieved good empirical success.Sliding window based object detectors are however a bit wasteful , as they need to enumerate all possible object locations and diemnsions. In this paper, we provide a much simpler and more efficient alternative. We represent objects by a single point at their bounding box center (see Fig 2), Other properties such as object size, dimension, 3D extent, orientation, and pose are then regressed directly from image features at the center location. Object detection is then a standard keypoint estimation problem.We simply feed the input image to a fully convolutional network that generates a heatmap.Peaks in this heatmap correspond to object centers.Image features at each peak predict the objects bounding box height and weight. The model trains using standard dense supervised\u76d1\u7763 learning.Inference is a single network forward-pass, without non-maximal suppression for post-processing. We model an object as the center point of its bounding box. The bounding box size and other object properties are inffered from the keypoint feature at the center.Best viewed in color Our method is general and can be extended to other tasks with minor effort. We provide experiments on 3D obect detection and multi-person human pose estimation, by predicting additional outputs at each center point (see Figure 4). For 3D bounding box estimation, we regress to the object absolute depth, 3D bounding box dimensions, and object orientation. For human pose estimation, we consider the 2D joint locations as offsets from the center and directly regress to them at the center point location. The simplicity of our method, CenterNet, allows it to run at a very high speed.With a simple Resnet-18 and up-convolutional layers, our network runs at 142 FPS with 28.1% COCO bounding box AP. With a carefully designed keypoint detection network. DLA-34 , our network achieves 37.4% COCO AP at 52 FPS. Related work Object detection by region classfication. One of the first successful deep object detectors, RCNN, enumerates object location from a larget set of region candidates,crops them , and classifies each using a deep network. Fast-RCNN crops image features instead, to save computation. However, both methods rely on slow low-level region proposal methods. Object detection with implicit(\u9690\u542b\u7684) anchors.Faster RCNN generates region proposal within the detection network. It samples fixed-shape bounding boxes (anchors) around a low-resolution image grid and classifies each into \u201cforeground or not\u201d. An anchor is labeled foreground with a $>0.7$ overlap with any ground truth object, background with a $<0.3$ overlap, or ignored otherwise. Each generated region proposal is again classified. Changing the proposal classifier to a multi-class classification forms the basis of one-stage detectors. Several improvements to one-stage detectors include anchor shape points, different feature resolution, and loss re-weighting among different samples. Our approach is closely related to anchor-based one-stage approaches.A center point can be seen as a single shape-agnostic \u4e0d\u53ef\u77e5\u5f62\u72b6 anchor.(see Figure3) .However,there are a few important differences. First, our CenterNet assigns the \"anchor\" based solely\u809a\u5b50 on location, not box overlap. We have no manual thresholds for foreground and background classification. Second, we only have one positive \"anchor\" per object. and hence do not need NonMaximum Suppression(NMS). We simply extract local peaks in the keypoint heatmap. Third, CenterNet uses a larger output resolution (output stride of 4) compared to traditional object detectors(output stride of 16). This eliminates the need for multiple anchors. Object detection by keypoint estimation We are not the first to use keypoint estimation for object detection. CornerNet detects two bounding box corners as keypoints, while ExtremeNet detects the top-,left-,bottom-,right-most, and center points of all objects. Both these methods build on the same robust keypoint estimation network as our CenterNet.However, they require a combinatorial\u7ec4\u5408 grouping stage after keypoint detection, which significantly slows down each algorithm. Our CenterNet , on the other hand,simply extracts a single center point per object without the need for grouping or post-processing. Monocular 3D object detection 3D bounding box estimation powers autonomous driving . Deep3Dbox use a slow-RCNN style framework, by first detecting 2D objects and then feeding each object into a 3D estimation network. 3D RCNN adds an additional head to Faster-RCNN followed by a 3D projection. Deep Manta uses a coarse-to-fine\u4ece\u7c97\u5230\u7ec6 Faster-RCNN trained on many tasks. Our method is similar to one-stage version of Deep3Box or 3DRCNN. As such, CenterNet is much simpler and faster than competing methods. standard anchor based detection. Anchors count as positive with an overlap IoU>0.7 to any object, negative with overlap IoU<0.3, or are ignored otherwise Center point based detection.The center pixel is assigned to the object. Nearby points have a reduced negative loss. Object size is regressed Preliminary Let $I\\in R^{W\\times H\\times3}$ be an input image of width $W$ and height $H$. Our aim is to produce a keypoint heatmap $\\hat{Y}\\in[0,1]^{\\frac{W}{R}\\times\\frac{H}{R}\\times C}$, where $R$ is the output stride and $C$ is the number of keypoint types. Keypoint types include $C=17$ human joints in human pose estimation, or $C=80$ object categorical in object detection. We use the default output stride of $R = 4$ in literature. The output stride downsamples the output prediction by a factor R. A prediction $\\hat{Y} {x,y,c}=1$ corresponds to a detected keypoint, while $\\hat{Y} {x,y,c} =0$ is background. We use serveral different fully-convolutional encoder-decoder networks to predict $\\hat{Y}$ from an image $I$: A stacked hourglass network\u5806\u53e0\u7684\u6c99\u6f0f\u7f51\u7edc upconvolutional residual network (ResNet) and deep layer aggregation\u805a\u5408 (DLA). We train the keypoint prediction network following Law and Deng. For each ground truth keypoint $p \\in R^2$ of class $c$ , we compute a low-resolution equivalent\u76f8\u7b49\u7684 $\\widetilde{p}= \\left\\lfloor\\frac{p}{R}\\right\\rfloor$. We then splat\u6454\u5f97\u75db all ground truth keypoints onto a heatmap $Y\\in [0,1]^{\\frac{W}{R}\\times\\frac{H}{R}\\times C}$ using a Gaussian kernel $Y_{xyc}=exp\\left(-\\frac{(x-\\widetilde{p}_x)^2+(y-\\widetilde{p}_y)^2}{2\\sigma^2_p}\\right)$, where $\\sigma_p$ is an object size-adaptive standard deviation\u5c3a\u5bf8\u9002\u5e94\u6807\u51c6\u504f\u5dee. If two Gaussians of the same class overlap, we take the element-wise maximum. The training objective is a penalty-reduced\u51cf\u5211 pixel-wise logistic regression with focal loss: L_k=\\frac{-1}{N}\\sum_{xyz}\\left\\{\\begin{matrix} (1-\\hat{Y}_{xyc})^\\alpha log(\\hat{Y}_{xyc})\\qquad if\\quad Y_{xyc} = 1\\\\\\\\ (1-Y_{xyc})^\\beta(\\hat{Y}_{xyc})^\\alpha log(1-\\hat{Y}_{xyc}) \\qquad otherwise \\end{matrix}\\right. where $\\alpha$ and $\\beta$ are hyper-parameters of the focal loss, and $N$ is the number of keypoints in image $I$ . The normalization by $N$ is chosen as to normalize all positive focal loss instances to 1. We use $\\alpha=2$ and $\\beta=4$ in all our experiments,following Law and Debug. To recover the discretization\u79bb\u6563\u5316 error caused by the output stride, we additionally predict a local offset $\\hat{O}\\in R^{\\frac{W}{R}\\times\\frac{H}{R}\\times2}$ for each center point. All classes $c$ share the same offset prediction.The offset is trained with an $L1$ loss L_{off} = \\frac{1}{N}\\sum_p\\left|\\hat{O}_{\\widetilde{p}}-\\left(\\frac{p}{R}-\\widetilde{p}\\right)\\right|\\qquad(2) The supervision acts only at keypoints locations $\\widetilde{p}$, all other locations are ignored. In the next section,we will show how to extend this keypoint estimator to a general purpose object detector. Objects as Points Let $(x_1^{(k)},y_1^{(k)},x_2^{(k)},y_2^{(k)})$ be the bounding box of object $k$ with category $c_k$. Its center point is lies at $p_k=(\\frac{x_1^{(k)}+x_2^{(k)}}{2},\\frac{y_1^{(k)}+y_2^{(k)}}{2})$. We use our keypoint estimator $\\hat{Y}$ to predict all center points. In addition, we regress to the object size $s_k=(x_2^{(k)}-x_1^{(k)},y_2^{(k)}-y_1^{(k)})$ for each object $k$.To limit the computational burden\u8ba1\u7b97\u8d1f\u62c5 , we use a single size prediction $\\hat{S} \\in R^{\\frac{W}{R}\\times\\frac{H}{R}\\times C}$ for all object categories. We use an $L1$ loss at the center point similar to Objective 2: L_{size}=\\frac{1}{N}\\sum^N_{k=1}\\left|\\hat{S}_{p_k}-s_k\\right|\\qquad(3) We do not normalize\u5f52\u4e00\u5316 the scale and directly use the raw pixel coordinates. We instead scale the loss by a constant $\\lambda_{size}$.The overall training objective is L_{det} = L_k + \\lambda_{size}L_{size} + \\lambda_{off}L_{off}\\qquad(4) We set $\\lambda_{size}=0.1$ and $\\lambda_{off}=1$ in all our experiments unless specified otherwise. We use a single network to predict the keypoints $\\hat{Y}$, offset $\\hat{O}$ , and size $\\hat{S}$ The network predicts a toatal of $C + 4$ outputs at each location. All outputs share a common fully-convolutional backbone network. For each modality, the features of the backbone are then passed through a separate 3x3 convolution, ReLU and another 1x1 convolution. Figure4 shows an overview of the network output. Section 5 and supplementary material contain additional architectural details. From points to bounding boxes At inference time, we first extract the peaks in the heatmap for each category independently. We detect all responses whose value is greater or equal to its 8-connected neighbors and keep the top 100 peaks. Let $\\hat{P} c$ be the set of $n$ detected center points $\\hat{P} ={(\\hat{x}_i,\\hat{y}_i)}^n {i=1}$ of class $c$. Each keypoint location is given by an integer coordinates $(x_i, y_i)$. We use the keypoint values $\\hat{Y}_{x_iy_ic}$ as a measure of its detection confidence, and produce a bounding box at location. (\\hat{x}_i+ \\delta\\hat{x}_i-\\hat{w}_i/2, \\hat{y}_i+\\delta\\hat{y}_i-\\hat{h}_i/2,\\hat{x}_i+\\delta\\hat{x}_i+\\hat{w}_i/2,\\hat{y}_i+\\delta\\hat{y}_i+\\hat{h}_i/2) Outputs of our network for different tasks: top for object detection, middle for 3D object detection,bottom for pose estimation. All modalities are produced from a common backbone, with a different 3x3 and 1x1 output convolutions separated by a ReLU. The number in brackets indicates the output channels. See section 4 for details where $(\\delta\\hat{x} i,\\delta\\hat{y}_i)=\\hat{O} {\\hat{x} i,\\hat{y}_i}$ is the offset prediction and $(\\hat{w}_i,\\hat{h}_i) = \\hat{S} {\\hat{x}_i,\\hat{y}_i}$ is the size prediction.","title":"radar_camera"},{"location":"radar_camera_fusion/#radar-camera-fusion","text":"\u6211\u4eec\u7684\u65b9\u6cd5\u79f0\u4e3aCenterFusion\uff0c\u5b83\u9996\u5148\u4f7f\u7528\u4e2d\u5fc3\u70b9\u68c0\u6d4b\u7f51\u7edc\u901a\u8fc7\u5728\u56fe\u50cf\u4e0a\u8bc6\u522b\u5bf9\u8c61\u7684\u4e2d\u5fc3\u70b9\u6765\u68c0\u6d4b\u5bf9\u8c61\u3002 \u7136\u540e\u5b83\u4f7f\u7528\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u89c6\u9525\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u5173\u952e\u6570\u636e\u5173\u8054\u95ee\u9898\uff0c\u4ece\u800c\u5c06\u96f7\u8fbe\u68c0\u6d4b\u7ed3\u679c\u4e0e\u76f8\u5bf9\u5e94\u5bf9\u8c61\u7684\u4e2d\u5fc3\u70b9\u76f8\u5173\u8054\u3002 \u76f8\u5173\u7684radar\u68c0\u6d4b\u7528\u4e8e\u751f\u6210\u57fa\u4e8eradar\u7684\u7279\u5f81\u56fe\u4ee5\u8865\u5145\u56fe\u50cf\u7279\u5f81\uff0c\u5e76\u56de\u5f52\u5230\u8bf8\u5982\u6df1\u5ea6\uff0c\u65cb\u8f6c\u548c\u901f\u5ea6\u4e4b\u7c7b\u7684\u5bf9\u8c61\u5c5e\u6027\u3002 radar\u4f7f\u7528\u591a\u666e\u52d2\u6548\u5e94\u5feb\u901f\u51c6\u786e\u7684\u786e\u5b9a\u7269\u4f53\u7684\u901f\u5ea6\u3002","title":"radar camera fusion"},{"location":"radar_camera_fusion/#_1","text":"\u4f7f\u7528\u5bf9\u8c61\u76842d\u8fb9\u6846\u6781\u5176\u4f30\u8ba1\u7684\u6df1\u5ea6\u548c\u5927\u5c0f\uff0c\u4e3a\u8be5\u5bf9\u8c61\u521b\u5efa3d\u5174\u8da3\u533a\u57df\uff0c\u89c6\u9525\u4f53\u3002 \u5728roi\u5185\u79bb\u56fe\u50cf\u4e2d\u5fc3\u70b9\u6700\u8fd1\u7684\u70b9\uff0c\u5173\u8054\u3002 \u5176\u4e2d \u662f\u5c3a\u5bf8\u81ea\u9002\u5e94\u6807\u51c6\u504f\u5dee\u3002 For autonomous robots to navigate a complex environment , it is crucial \u81f3\u5173\u91cd\u8981\u7684 to understand the surrounding scene both geometrically and semantically.Modern autonomous robots employ multiple sets of sensors, including lidars, radars, and cameras. Managing the different reference frames and characteristics of the sensors, and merging their observations into a single representation complicates perception. Our approach consists of an ensemble of neural networks which take in sensor data from different modalities \u5f62\u5f0f and transform them into a single common top-down semantic grid representation.We find representation favourable as it is agnostic\u65e0\u5173 to sensor-specific reference frames and captures both the semantic and geometric information for the surrounding scene. Because the modalities share a single output representation,they can be easily aggregated \u6c47\u603bto produce a fused output.In this work we predict short-term semantic grids but the framework can be extended to other tasks.This approach offers a simple, extensible , end to end approach for multi-modal perception and prediction. The benifits of a shared top-down representation across modalities are threefold. First, it is an interpretable representation that better facilities\u4fc3\u8fdb debugging\u8c03\u8bd5 and reasoning\u63a8\u7406 about inherent\u56fa\u6709 failure modes\u6545\u969c\u6a21\u5f0f of each modality.Second it is independent of any particular sensors characteristics and so is easily extensible for adding new modalities.Finally , it simplifies the task of late fusion by sharing a spatial\u7a7a\u95f4\u7684 representation in a succinct manner. In this work we present a novel end-to-end framework that predicts the top-down view of the current scene($t_0$) as well as multiple timesteps into the future.The pipleline consists of a convolutional neural network for each of three sensor modalities : lidar, radar , camera.Each sensor modality predicts a sequence of top-down semantic grids, then these outputs are fused to produce a single output grid.We explore fusing using two different aggregation mechanisms. Estimating 3D orientation and translation of objects is essential for infrastructure-less .In case of monocular vision, successful methods have been mainly based on two ingredients\u56e0\u7d20: \uff081\uff09a network generating 2D region proposals(2D\u533a\u57df\u63d0\u6848)\u3002(2) a R-CNN structure predicting 3D object pose by utilizing the acquired regions of interest. We argue that the 2D detection network is redundant\u591a\u4f59\u7684 and introduces non-negligible \u4e0d\u53ef\u5ffd\u7565\u7684 noise for 3D detection.Hence,we propose a novel 3D object detection method, named SMOKE, in this paper that predicts a 3D bounding box for each detected object by combining a single keypoint estimate with regressed 3D variables.As a second contribution, we propose a multi-step disentangling approach for constructing the 3D bounding box ,which significantly improves both training convergence\u6536\u655b and detection accuracy. In constract to previous 3D detection techniques, our method does not require complicated pre/post-processing.extra data, and a refinement\u7ec6\u5316 stage. Despite of its structural\u7ed3\u6784 simplicity\uff0c our proposed SMOKE network outperforms all existing monocular 3D detection methods on the KITTI dataset , giving the best state-of-the-art result on both 3D objection dection and bird's eye view evaluation . The code will be made publicly available. Vison based object detection is an essential ingredient\u6210\u5206 of autonomous vehicle perception \u6d1e\u5bdf\u529b of autonomous vehicle perception and infrastructure\u57fa\u7840\u8bbe\u65bd less robot navigation in general. This type of detection methods are used to perceive \u611f\u77e5 the surrounding environment by detecting and classifying object instances\u5b9e\u4f8b into categories \u7c7b\u522b and identifying their locations and orientations. Recent developments in 2D object detection have achieved promising performance\u6709\u524d\u9014\u7684 on both detection accuracy and speed. In constract, 3D object detection have proven to be a more challenging task as it aims to estimate pose and location for each object simulataneously. Currently,the most successful 3D object detection methods heavily depend on Lidar point cloud. or LIDAR-Image fusion information.(features learned from the point cloud are key components of the detection network).However , LIDAR sensors are extremely expensive , have a short service life time and too heavy for autonomous robots. Hence LIdars are currently not considered to be econnomical\u6d41\u884c\u7684 to support autonomous vehicle operations . Alternatively , cameras are cost-effective, easily mountable and light-weight solutions for 3D object detection with long expected service time. Unlike lidar senors , a single camera in itself cannot obtain sufficient spatial information for the whole environment as single RGB images can not supply object location information or dimensional contour \u8f6e\u5ed3 in the real world. While binocular \u53cc\u76ee vision restores the missing spatial information. in many robot applications, especially UAVs , it is difficult to realize biocular vision. Hence , it is desirable to perform 3D detection on a monocular image even if it is a more difficult and chanllenging task. To enhance performance, geometry reasoning \u51e0\u4f55\u63a8\u7406 synthetic data \u7efc\u5408\u6570\u636e and post 3D-2D processing have also been used to improve 3D object detection on single image.By the knowledge of the authors , no reliable monocular 3D detection method has been introduced so far to learn 3D information directly from image plane avoiding the performance decrease that is inevitable\u4e0d\u53ef\u907f\u514d\u7684 with multi-stage method. In this paper we propose an innovative\u521b\u65b0 single-stage 3D object detection method that pairs each object with a single keypoint. We argue and later show that a 2D detection, which introduces nonnegligible noise in 3D parameter estimation, is redundant\u591a\u4f59\u7684 to perform 3D object detection.Furthermore 2D information can be naturally obtained if 3D variables and camera instrinsic matrix are already known.Consequently, our designed network eliminate \u6392\u9664 the 2D detection branch and estimates the projected 3D points on the image plane instead.A 3d parameter regression branch is added in parallel.This design results in a simple network structure with two estimation threads.Rather than regressing variables in a separate method by use multiple loss functions, we transform these variables together with projected keypoint to 8 corner representation of 3D boxes and regress them with a unified\u7edf\u4e00\u7684 loss function. As in most single state 2D object detection algorithms, our 3D detection approach only contains one classification and regression branch.Benefiting from the simple structure , the network exhibits \u5c55\u793a improved accuracy in learning 3D variables, has better convergence and less overall computional needs. Second constribution of our work is a multi-step disentanglement \u7ea0\u7f20 approach for 3D bounding box regression. Since all the geometry information is grouped into the parameter, it is difficult for the network to learn each variable accurately n a unified way.Our proposed method isolates\u5206\u79bb the contribution of each parameter in both the 3D bounding box ecoding phase and regression loss function , which significantly helps to train the whole network effectively. Our contribution is summarized as follows: We propose a one-stage monocular 3D object detection with a simple architecture that can precisely learn 3D geometry in an end-to-end fashion. We provide a multistep distanglement approach to improve the convergence\u6536\u655b of 3D parameters and detection accuracy. The result method outperforms all existing state-of-art monocular 3D object detection algorithms on the chanllenging KITTI dataset at the submission date November 2019. We formulate\u5b9a\u4e49 the monocular 3D object detection problem as follow: given a single RGB image ,with the width and the height of the image, find for each present object its category\u7c7b\u522b label C and its 3D bounding box B, where the latter is parameterized by 7 variables .Here, represent the height, width and length of each object in meters , and is the coordinates (in meters) of the object center in the camera coordinate frame. Variable is the yaw orientation of the corresponding cubic box. The roll and pitch angles are set to zero by following the Kitti annotation .Additionally , we take the mild\u6e29\u548c assumption that the camera instrinsic matrix K is known for both training and inference.","title":"\u89c6\u9525\u4f53\u5173\u8054\u673a\u5236"},{"location":"radar_camera_fusion/#smoke-approach","text":"In this section, we describe the smoke network that directly estimates 3D bounding boxes for detected object instances from monocular imagery. In constrast to previous techniques that leverage \u6760\u6746\u4f5c\u7528 2D proposals to prediect 3D bounding box, our method can detect 3D information with a simple single stage.The propose method can be divided into three parts:(1) backbone,(2) 3D detection (3) loss function.First , we briefly discuss the backbone for feature extraction , followed by the instruction of the the 3D detection network consisting of two separated branches. Finallly,we discuss the loss function design and the multi-step disentanglement to compute the regression loss.The overview of the network structure is depicted in Fig.2","title":"Smoke approach"},{"location":"radar_camera_fusion/#backbone","text":"We use a hierarchical layer fusion network DLA-34 as the backbone to extract features since it can aggregate information across different layers.Following the same structure as in [], all the hierarchical aggregation connections are replaced by a Deformable\u53ef\u53d8\u5f62\u7684 convolution network (DCN) The output feature map is downsample 4 times with respect to the original image.Compared with the original implementation, we replace all BatchNorm(BN) operation with GroupNorm(GN) since it has been proven to be less sensitive to batch size and more robust to training noise.We also use this technique in the two prediction branches. which will be discussed later.This adjustment not only improve detection accuracy , but it also reduces considerably the training time. Later ,we provide performance comparison of BN and GN to demonstrate these properties.","title":"backbone"},{"location":"radar_camera_fusion/#3d-detection-network","text":"Keypoint branch: We define the keypoint estimation network similar to such that each object is represented by one specific keypoint. Instead of identifying the center of a 2D bounding box , the key point is defined as the projected 3D center of the object on the image plane. The comparison between 2D center points and 3D projected points is visualized in Fig3.The projected keypoints allow to fully recover 3D location for each object with camera parameters. Let represent the 3D center of each object in the camera frame.The projection of 3D points of points on the image plane can be obtained with the camera instrinsic matrix K in a homogeneous form: \\begin{bmatrix}z\\cdot x_c\\\\z\\cdot y_c\\\\z\\end{bmatrix}=K_{3\\times 3}\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}\\quad\\quad\\quad(1) For each ground truth keypoint, its corresponding downsampled location on the feature map is computed and distributed using a Gaussian Kernel following.The standard deviation is allocated based on the 3D bounding boxes of the ground truth projected to the image plane.Each 3D box on the image is represented by 8 2D points and the standard deviation ia computed by the smallest 2D box with that encircles the 3D box.","title":"3D detection network"},{"location":"radar_camera_fusion/#regression-branch","text":"\\logeach key point on the heatmap. Similar to other monocular 3D detection framework.The 3D information is encoded as an 8-tuple\u5143\u7ec4 \\tau=\\begin{bmatrix}\\delta_z&\\delta_{x_c}&\\delta_{y_c}&\\delta_h&\\delta_w&\\delta_l&sin(a)&cos(a)\\end{bmatrix}^T Here denotes the depth offset , is the discretization\u79bb\u6563\u5316 offset due to downsampling. denotes the resdual \u5269\u4f59 dimension. is the vectorial representation of the rotational angle . We encode all variables to be learnt in residual\u6b8b\u5dee representation to reduce the learning interval and ease the training task . The size of feature map for regression results in . Inspired by the lifting transformation\u63d0\u5347\u8f6c\u578b described in [],we introduce a similar operation F that converts projected 3D points to a 3D bounding box .For each object, its depth $z$ can be recovered by pre-defined scale and shift parameters and as Given the object depth $z$ , the location for each object in the camera frame can be recovered by using its discretized\u79bb\u6563\u7684 projected centroid $[x_c, y_c]^T$ on the image plane and the downsampling offset \\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}=K^{-1}_{3\\times 3}\\begin{bmatrix}z\\cdot(x_c+\\delta_{x_c})\\\\z\\cdot(y_c+\\delta_{y_c})\\\\z\\end{bmatrix} This equation is inverse of Eq.(1) In order to retrieve\u53d6\u56de object dimensions , we use a pre-calculated category-wise\u6309\u7c7b\u522b average dimension computed over the whole dataset. Each object dimension can be recovered by using the residual dimension offset : \\begin{bmatrix}h\\\\w\\\\l\\end{bmatrix}=\\begin{bmatrix}\\overline{h}\\cdot e^{\\delta_h}\\\\\\overline{w}\\cdot e^{\\delta_w}\\\\\\overline{l}\\cdot e^{\\delta_l}\\end{bmatrix}\\quad\\quad\\quad(4) Inspired by [],we choose to regress the observation angle \u03b1 instead of the yaw rotation \u03b8 for each object. We further change the observation angle with respect to the object head ,instead of the commonly used observation angle value , by simply adding . The difference between these two angles is shown in Figure4.Moreover,each is encoded as the vector $[sin(a)\\quad cos(a)]^T$ The yaw angle $\\theta$ can be obtained by utilizing $\\alpha_z$ and the object location: \\theta=\\alpha + arctan\\left(\\frac{x}{z}\\right)\\qquad(5) Finally,we can construct the 8 corners of the 3D bounding box in the camera frame by using the yaw rotation matrix $R_\\theta$, object dimensions $[h\\quad w\\quad l]^T$ and location $[x\\quad y\\quad z]^T$: B=R_\\theta\\begin{bmatrix} \\pm h/2\\\\\\pm w/2\\\\\\pm l/x \\end{bmatrix}+\\begin{bmatrix} x\\\\y\\\\z \\end{bmatrix}\\qquad\\qquad(6)","title":"Regression Branch"},{"location":"radar_camera_fusion/#loss-function","text":"We employ the penalty-reduce focal loss\u60e9\u7f5a\u51cf\u5c11\u7126\u70b9\u635f\u5931 [] in a point-wise manner on the downsampled heatmap.Let $s_{i,j}$ be the predicted score at the heatmap location $(i,j)$ and $y_{i,j}$ be the ground-truth value of each point assigned by Gaussian Kernel. Define $\\breve{y} {i,j}$ and $\\breve{s} {i,j}$ as : \\breve{y}_{i,j}=\\left\\{\\begin{matrix} 0,\\qquad if\\quad y_{i,j} =1\\\\y_{i,j} ,\\qquad otherwise\\end{matrix}\\right. \\breve{s}_{i,j}=\\left\\{\\begin{matrix} s_{i,j},\\qquad if\\quad y_{i,j}=1\\\\1-s_{i,j},\\qquad otherwise \\end{matrix}\\right. For simplicity,we only consider a single object class here.Then , the classification loss function is constructed as L_{cls}=-\\frac{1}{N}\\sum^{h,w}_{i,j=1}(1-\\breve{y}_{i,j})^\\beta(1-\\breve{s}_{i,j})^\\alpha log(\\breve{s}_{i,j})\\qquad(7) where $\\alpha$ and $\\beta$ are tunable hyper-parameters and N is the number of keypoints per image.The term $(1-y_{i,j})$ corresponds to penalty reduction for points around the groundtruth location.","title":"Loss function"},{"location":"radar_camera_fusion/#regression-loss","text":"We regardless the 8D tuple $\\tau$ to construct the 3D bounding box for each object. We also add channel-wise action to the regressed parameters of dimension and orientation at each feature map location to preserve\u4fdd\u5b58 consistency\u4e00\u81f4\u6027 . The activation functions for the dimension and the orientation are chosen to be the sigmoid function $\\sigma$ and the $l_2$ norm ,respectively: \\begin{bmatrix} \\delta_h\\\\\\delta_w\\\\\\delta_l \\end{bmatrix}=\\sigma\\left(\\begin{bmatrix} O_h\\\\O_w\\\\O_l \\end{bmatrix}\\right)-\\frac{1}{2},\\quad\\begin{bmatrix} sin(a)\\\\cos(a) \\end{bmatrix}=\\begin{bmatrix} O_{sin}/\\sqrt{O^2_{sin}+O^2_{cos}}\\\\O_{cos}/\\sqrt{O^2_{sin}+O^2_{cos}} \\end{bmatrix} Here $o$ stands for the specific output of network. By adopting the keypoint lifting transformation introduced in Sec.4.2.we define the 3D bounding box regression loss as the $l_1$ distance between the predicted transform $\\hat{B}$ and the ground truth B: L_{reg}=\\frac{\\lambda}{N}||\\hat{B}-B||_1\\qquad(8) where $\\lambda$ is the scaling factor.This is used to ensure that neither the classification, nor the regression dominates\u4e3b\u5bfc the other.The disentangling transformation of loss has been proven to be an effective dynamic method to optimize 3D regression loss functions in [31] . Following this design , we extend the concept of loss disentanglement into a multi-step form. in Eq(3) , we use the projected 3D groundtruth points on the image plane $[x_c\\quad y_c]^T$ with the network predicted discretization offset $[\\hat{\\delta_{x_c}}\\quad \\hat{\\delta_{y_c}}]^T$ and depth $\\hat{z}$ to retrieve the location $[\\hat{x}\\quad \\hat{y}\\quad \\hat{z}]^T$ of each object. In Eq(5), we use the groundtruth location $[x\\quad y\\quad z]^T$ and the predicted observation angle $\\hat{a}_z$ to construct the estimated yaw orientation $\\hat{\\theta}$. The 8 corners representation of the 3D bounding box is also isolated \u9694\u79bb into three different groups following the concept of disentanglement , namely orientation, dimension and location.The final loss function can be represented by: L = L_{cls} + \\sum^3_{i=1}L_{reg}(\\hat{B_i}),\\qquad(9) where $i$ represents the number of groups we define in the 3D regression branch.The multi-step disentangling transformation divides the contribution of each parameter group to the final loss. In Sec.5.2, we show that this method significantly improves detection accuracy.","title":"Regression Loss"},{"location":"radar_camera_fusion/#implementation","text":"In this section,we discuss the implementation of our proposed methodology in detail together with selection of the hyperparameters. Preprocessing : we avoid applying any complicated preprocessing method on the dataset.Instead, we only eliminate\u6392\u9664 objects whose 3D projected center point on the image plane is out of the image range. Note that the total number of projected center points outside the image boundary for the car instance is 1582.This accounts for only the $5.5$% of the entire set of 28742 labeled cars. Data Augmentation : Data augmentation techniques we used are random horizontal flip\u6c34\u5e73\u7ffb\u8f6c, random scale and shift. The scale ratio is set to 9 steps from 0.6 to 1.4, and the shift ratio is set to 5 steps from -0.2 to 0.2. Note that the scale and shift augmentation methods are only used for heatmap classification since the 3D information becomes inconsistent\u4e0d\u4e00\u81f4 with data augmentation. Hyperparameter Choice : In the backbone, the group number for GroupNorm is set to 32.For channels loss than 32, it is set to be 16. For Eq.7, we set $\\alpha=2$ and $\\beta=4$ in all experiments. Based on [31], the reference car size and depth statistic\u7edf\u8ba1\u6570\u636e we use are $[\\overline{h}\\quad \\overline{w}\\quad \\overline{l}]^T=[1.63\\quad 1.53\\quad 3.88]^T$ and $[\\mu_z\\quad \\sigma_z]^T=[28.01\\quad 16.32]^T$ (measured in meters). Trainning :Our optimization schedule\u4f18\u5316\u8868 is easy and straightforward. We use the original image resolution and pad it to $1280\\times 384$. We train the network with a batch size of 32 on 4 Geforce TITAN X GPUs for 60 epochs. The learning rate is set at $2.5\\times 10^{-4}$ and drops at 25 and 40 epochs by a factor of 10. During testing, we use the top 100 detected 3D projected points and filter it with a threshold of 0.25. No data augmentation method and NMS are used in the test precedure.Our implementation platform in Pytorch1.1, CUDA 10.0 and CUDNN 7.5.","title":"Implementation"},{"location":"radar_camera_fusion/#depth-estimation-from-monocular-images-and-sparse-radar-data","text":"In this page, we explore the possibilty of achieving a more accurate depth estimation by fusing monocular images and Radar points using a deep neural network.We give a comprehensive study of the fusion between RGB images and Radar measurements from different accepts and proposed a working solution based on obeservations. We find that the noise existing in Radar measurements is one of the main key reasons that prevents one from applying the existing fusion methods developed for for LiDAR data and images to the new fusion problem between Radar data and images to the new fusion problem between Radar data and images.The experiments are conducted\u5b9e\u65bd on the nuScenes dataset, which is one of the first datasets which features Camera\uff0c Radar, and LiDAR recordings in diverse\u5404\u79cd\u5404\u6837\u7684 scenes and weather conditions. Extensive\u5e7f\u6cdb\u7684 experiments demonstrate that our method outperforms existing fusion methods. We also provide detailed ablation\u707c\u70e7 studies to show the effectiveness of each component in our method. Dense and robust depth estimation is an important component in self-driving system and unmanned\u65e0\u4eba aerial vehicles. While existing structure-light-based depth sensor or stereo camera can provide dense depth in indoor environments, the reliability of these sensors degrade a lot in outdoor applications. As a result, lots of research works focus on obtaining dense depth from monocular RGB images only. Recently, convolutional neural network (CNN) based methods have demonstrated impressive improvement on monocular depth estimation for both indoor and outdoor scenarios.However , there is still a gap between the accuracy and reliability of these methods and what the real-world applications need. Apart from estimating depth from monocular camera, to imporve the robustness of the system, some methods also take other sensor modalities\u5f62\u5f0f into consideration.While thses sensors,LiDARS is the most commonly used one. Many works have been conducted on dense depth estimation from RGB images and sparse LiDAR scans.In addition to depth estimation and completion tasks, different RGB + LiDAR fusion techniques are also extensively used in tasks such as 3D object detection. Although LiDAR provides more accurate depth measurements in outdoor scenario, high-end LiDAR sensors are still far from affordable for many applications. Compared with LiDAR, Radar is an automotive-grade sensor that has been used for decades on vehicles, but has not attracted lots of attention in self-driving research based on deep learning. One reason might be that Radar measurements are not included in most of the dominant self-driving datasets.Compared with LiDAR, Radar sensors offer longer sensing range(200m~300m), more attributes including velocities, dynamic states, and measurement uncertainties. Most importantly, the costs of these sensors are much lower than LiDAR.However, Radar measurements are much lower than LiDAR ,However, Radar measurement are typically sparser, noiser, and have a more limited vertical field of view. This work is to study the chanllenges of using Radar data for dense depth estimation and to propse a novel method for that aim. Given recently released nuScenes dataset consisting of RGB, LiDAR and Radar measurements.we are able to conduct experiments on cross-modality sensor fusion between RGB camera and Radar.Through our experiments we demonstrated that:1)Existing RGB + LiDAR fusion methods can be applied directly to RGB + Radar fusion task; and 2) with proper fusion strategies and a novel denosing operation, our proposed network is able to improve the performance of depth estimation by a good margin by using Radar measurements. According to our survey, our work is the first one that brings Radar sensors into dense depth estimation tasks. The contributions of this work include: 1) a detailed study on the challenges of using Radar data for dense depth estimation; and 2) a novel and carefully motivated network architecture for depth estimation with monocular images and sparse Radar data.","title":"Depth Estimation from Monocular Images and Sparse Radar Data"},{"location":"radar_camera_fusion/#related-works","text":"RGB-based Depth Estimation. Depth estimation from monocular or stereo camera is a popular research topic in both computer vision and robotics.Early works used either geometry-based algorithms on stereo images, or handcrafted features on single images.Recent years, convolutional neural networks(CNN) have demonstrated their ability in image understanding, dense predictions ,etc. given large scale datasets. Therefore, lots of research works of monocular depth estimation are conducted.In general , most of them used the encoder-decoder architecturres. Xie . futher introduced skip connection strategy which is a frequently used technique to multi-level features in dense prediction tasks. On the other hand, Huang achieve state-of-the-art performance by introducing space increasing discretization (SID) and ordinal regression. In some semi-/self-supervised formu loss is used. and the smoothness constraint\u7ea6\u675f is further imposed to enhance local consistency\u4e00\u81f4\u6027.Patil proposed a recurrent\u53cd\u590d\u53d1\u4f5c network architecture to exploit the long-range spatiotemporal\u65f6\u7a7a structures across video frames to yeild more accurate depth maps.While good performance has been obtained with only RGB images, the methods still have difficulty in generalizing to new scenario and chanllenging weather and light conditions. This motivates the existing line of work that fuses camera data with Lidar data and our work that fuses camera data with radar data is cheaper to obtain. Depth Estimation with Camera and Lidar data while monocular depth estimation task attracts lots of attention, achieving more reliable and accurate predictions using multi-modality\u591a\u6a21\u6001 information is also a popular topic.Existing works either take the whole set of LiDAR points , (known as depth completion), or the downsampled set as model inputs.Ma first projected LiDAR points to 2D sparse depth map and then perform so called early fusion by direct concatenation with RGB images along channel, or concatenation feature maps after one shallow convolution block.J used a late fusion method to combine features from different modalities and improved the overall performance through multitask learning.Q proposed to predict dense depth map by combining predictions from RGB and surface normal pathways, where surface normal is treated as an intermediate representation. Moreover, confidence maps are predicted to down-weight mixed measurements from LiDAR caused by the displacement between camera and LiDAR. In this work our main focus is sensor fusion . Thus , we use the widely adopted encoder-decoder architecture and focus on the necessary extensions in order to effectively use Radar data instead. Post-processing and Refinement Methods \u540e\u5904\u7406\u548c\u4f18\u5316\u65b9\u6cd5. Apart from treating sparse point clouds as inputs to the model, some methods also tried to directly refine the dense predictions of the trained models.W proposed a simple add-on module that can improve the prediction of depth estiamtion model using similar methods used by white box adversarial attack. Since the refinement is done using iterative re-inference, no re-training is required.This method can be integrated into most deep learning based methods.Cheng learned an affinity \u4eb2\u548c\u529b matrix from data to refine the outputs of their CNN model.The recurrent refinement operation can also be extended to depth completion tasks. Fusion of images and radar Data. There are already works that fuse RGB images and Radar.given the fact that they are very much complementary\u8865\u5145.The line of work mainly focus on object detection and tracking.For instance,C fused\u878d\u5408 Radar data and images to detect small objects at a large distance. In [] and [] the authors enhance current 2D object detection networks by fusing camera data and projected sparse radar data in the network layers\uff0cwhile [] also performs free space semantic segmentation jointly.Both methods learn at which level the fusion of the sensor data is more beneficial for the task. There are also other datasets proposed for object detection with Radar data such as [].Exemplary\u793a\u8303\u6027\u7684 works on semantic segmentation with Radar point cloud have been conducted as well. For instance, in [] the authors have studied how the challenging task can be performed and provide results on a dataset with manually labeled radar reflections.Similar to these works for object detection and semantic segmentation, our work aims to study how the challenging task of dense depth estimation with radar data can be addressed with the popular deep neural network architectures. In the line of increasing the robustness of depth estimation, Vasudevan, et al. [] have proposed a novel method to estimate depth maps based on binaural\u53cc\u8033 sounds.","title":"RELATED WORKS"},{"location":"radar_camera_fusion/#our-method","text":"Our whole method are divided into multiple main components. In the following section, we will go through each component in detail.","title":"Our Method"},{"location":"radar_camera_fusion/#radar-data-background","text":"Different from well established depth completion or depth estimation tasks.There is no prior research works on RGB + radar depth estimation task.Therefore, we provide a brief introduction to the task formulation and some key differences between Radar and LiDAR measurements,which will help readers to understand the motivations behind the components of our method. Data format. similar to LiDAR data, Radar measurements are recorded as sparse point clouds. The main difference is that , in addition to $x,y,z$,and $reflectance$, Radar data consist of additional measurements including the velocity along x and y direction, the standard deviation of the location and velocity measurements.and information such as the dynamic states of the measured object (encoded as discrete\u79bb\u6563\u7684 numbers). Limitation . While it seems that the radar data provide more information, it also introduces the following limitations compared with LiDAR data: * sparseness:In nuScenes dataset , there are more than 3000 LiDAR points after projection to the camera.However, there are less than 100 Radar points after the projection. * Limited vertical field of view.Because of the limitation of the sensor,Radar measurements mainly concentrate in the central horizontal bin (similar heights) as shwon in Fig.2. * Noisy measurements:Due to multiple reflections (Radar multipath problem) or other reasons , we have many noisy measurements as shown in Fig.2. * Inconsistency\u524d\u540e\u4e0d\u4e00\u81f4 with LiDAR data:Apart from noisy measurements, which are considered as outliers, the 3D points of Radar and LiDAR representing the same object can also be different.Since we typically use LiDAR measurements as ground truth, even noise-free Radar measurements are not perfect on the evaluation metrics. As we will show in Section IV, using Radar depth maps directly as the input of off-the-shelf\u73b0\u6210\u7684 RGB+LiDAR depth completion /prediction models resulted in marginal\u8fb9\u7f18 improvements. Problem formulation . In our RGB+ Radar formulation,each data sample from the dataset contains (1) an RGB image $x_{RGB}$ (2) a set of Radar measurements $R={r_n}^N_{n=1}$ from 3 nearest timestamps (3) a set of Lidar measurements $L={l_m}^M_{m=1}$ Radar measurements $R$ can be further projected to a single-channel 2D depth map $x_{Radar}$ using the perspective projection.Similarly,LiDAR measurements can be objected to 2D map y, which is treated as ground truth depth map in our experiments(section IV).Our model takes both $x_{RGB}$ and $x_{Radar}$ as inputs and predicts dense 2D depth map $\\tilde{y}$ which minimizes the metrics errors. Same as all the depth estimation /completion tasks, loss and metric error are computed over the pixels with ground truth measurements.","title":"Radar Data background"},{"location":"radar_camera_fusion/#cnn-architecture","text":"Backbone . Since our goal is to perform a comprehensive\u7efc\u5408\u7684 study on different fusion methods, we need to choose an efficient and widely-used backbone. Thus, we fixed our backbone to Resnet18 , and explored different fusion methods based on it as a pilot\u98de\u884c\u5458 experiment. As illustrated in Fig.3, we apply different encoder fusion methods to the simple encoder-decoder architecture proposed by Ma, and compare their performance. According to the experiment (Section IV-C), late fusion and multi-layer fusion model have comparable performance.Therefore, we adopt\u91c7\u7eb3 late fusion as our main encoder backbone design in the following experiments for simplicity. Two-Stage architecture As shown by the pilot experiments(section IV-C) , a simple encoder-decoder architecture have some improvements on RGB + Radar depth prediction task if we can remove most of the noisy measurements. However, we don't have LiDAR ground truth to help us performing the filtering in real applications.and it's hard to perform outlier rejections\u5f02\u5e38\u5254\u9664 without information on the 3D structure or objects of the scene.Therefore, we come up with a 2-stage design to address the noisy measurement issue. As shown in Fig.1, our whole method contains two stages. The stage 1 model $f_{stage1}$ takes both RGB image $x_{RGB}$ and the radar depth map $x_{Radar}$ as inputs and predicts a coarse\u7c97 depth map $\\tilde{y}_{stage1}$ ,which gives us a dense 3D structure of the scene: \\tilde{y}_{stage1}=f_{stage1}(x_{RGB},x_{Radar})\\qquad\\quad(1) Then we compare the Radar depth map with the coarse prediction $\\tilde{y} {stage1}$ to reject some outliers (most details in next subsection) and obtain the filtered Radar depth map $\\tilde{x} {Radar}$. The assumption\u5047\u8bbe here is that although the predictions from stage 1 is not perfect, they are smooth and locally consistent. Therefore, they are suitable to reject outliner noises produced by Radar Multipath, which typically have certain margins with the correct depth values. The stage2 model $f_{stage2}$ takes $x_{RGB}$ , $\\tilde{x} {Radar}$ , and the prediction from stage1 $\\tilde{y} {stage1}$ to predict the final result $\\tilde{y}$: \\tilde{y} = f_{stage_2}(x_{RGB},\\tilde{x}_{Radar},\\tilde{y}_{stage1})\\qquad(2)","title":"CNN architecture"},{"location":"radar_camera_fusion/#noise-filtering-module","text":"Since Radar measurements are not exactly consistent with the LiDAR measurements as we mentioned in Section III-A .we need to have some tolerance\u516c\u5dee in the filtering process.Otherwise , we will end up discarding\u4e22\u5f03 all the measurements in the set $R$. Instead of setting a fixed distance tolerance threshold $\\tau$,we empirically\u51ed\u7ecf\u9a8c found that an adaptive threshold gives us better results. We design the threshold to be a function of depth value $\\tau(d)$ : We have larger tolerance for larger depth values, which is similar to the space-increasing disrectization (SID) from Huan et al.[2]: \\tau(d)=exp\\left(\\frac{d*log(\\frac{\\beta}{\\alpha})}{K}+log(\\alpha)\\right)\\qquad(3) here we heuristically\u542f\u53d1\u5f0f\u7684 set $\\alpha=5$ and $\\beta=18$. Let $P$ denote the set pixel coordinates $(u,v)$ of the Radar measurements projected by the perspective projection function $proj(.)$ : $P={p_n}^N_{n=1}={proj(r_n)^N_{n=1}}$. The noise filtering module will keep the point $p_n$ if it satisfies the following constraint\u7ea6\u675f: |x_{Radar}(p_n)-\\tilde{y}_{stage1}(p_n)|\\leq\\tau(p_n),for\\quad p_n\\quad in\\quad P\\qquad(4)","title":"Noise filtering module"},{"location":"radar_camera_fusion/#loss-function_1","text":"By design, each component of our model is differentiable. Thus, our whole model is end-to-end trainable.Following the setting from [7] ,We apply $L1$ loss to both the predictions of stage1 ($\\tilde{y} {stage1}$) and stage2 ($\\tilde{y}$). Considering that the main purpose of $\\tilde{y} {stage1}$ is to filter outlier noises in $x_{Radar}$, we futher add edge-aware smoothness constraint to it . To effectively balance multiple loss terms, we follow the method proposed by Kendall L_{total} = e^{-w1}*(L1(\\tilde{y}_{stage1},y)+10^{-3}*L_{smooth})+e^{-w2}*L1(\\tilde{y},y) + \\sum_{i}w_i\\qquad(6) where $w1$ and $w2$ are optimized variables ,and $L_{smooth}$ is defined as : L_{smooth}=|\\triangledown_u(\\tilde{y}_{stage1})|e^{-|\\triangledown_u(x_{RGB})|} + |\\triangledown_v(\\tilde{y}_{stage1})|e^{-|\\triangledown_v(x_{RGB})|}\\qquad(8) $\\triangledown_u$ and $\\triangledown_v$ denote the gradient along 2D height and width directions sperately.","title":"Loss Function"},{"location":"radar_camera_fusion/#implementation-details","text":"Unless stated otherwise, all the models are trained using a batch size of 16 and SGD optimizer with a learning rate of $0.001$ and a momentum\u52bf\u5934 of $0.9$ for 20 epochs. The learning rate is multiplied by 0.1 alter every 5 epochs.All the models we used in experiment action are implemented in PyTorch.The experiments are conducted on the desktop computers/clusters with Nvidia GTX1080Ti and TeslaV100 GPUs. All of our encoder network architectures (section III-B) are modified from the standard Reset18. For early fusion, we simply modified the input channels to 4 and randomly initialized the weights (weights of other layers were initialized from pre-trained models on Imagenet dataset). For mid, late, and multi-layer fusion , the depth branch has a similar architecture as the RGB branch.The only difference is that we change the number of channels to 1/4 of the original one. Fusion operations happened only on feature maps with same spatial resolution (width and height) . Regarding the decoder part, we kept the section from [] by using UpProj module as our upsampling operation.","title":"Implementation details"},{"location":"radar_camera_fusion/#experiment","text":"In an early fusion approach, the raw or pre-processed sensory data from different sensor modalities are fused together. With this approach, the network learns a joint representation from the sensing modalities. Early fusion methods are usually sensitive to spatial or temporal misalignment\u9519\u4f4d of the data. On the other hand, a late fusion approach combines the data from different modalities at the decision level, and provides more flexibility \u7075\u6d3b\u6027 for introducing new sensing modalities to the network.However a late fusion approach does not exploit\u5f00\u53d1\u5229\u7528 the full potential of the available sensing modalities , as it does not acquire the intermediate\u4e2d\u95f4\u7684 features obtained by learning a joint representation.A compromise between the early and late fusion approaches is referred to as middle fusion. It extracts features from different modalities individually and combines them at an intermediate stage, enabling the network to learn joint representations and creating a balance between sensitivity and flexibility. We propose CenterFusion, a middle-fusion approach to exploit radar and camera data for 3D object detection. CenterFusion focuses on associating radar detections to preliminary\u521d\u6b65\u7684 detection results obtained from the image, then generates radar feature maps and uses it in addition to image features to accurately estimate 3D bounding boxes for objects.Particularly, we generate perliminary 3D detections using a key point detection network,and propose a novel frustum-based \u57fa\u4e8e\u89c6\u9525 radar association method to accurately associate radar detections to their corresponding objects in 3D space. These radar detections are then mapped to the image plane and used to create feature maps to complement the image-based features.Finally, the fused features are used to accurately estimate object's 3D properties such as depth, rotation and velocity. The network architecture for CenterFusion is shown in Fig.1 We evaluate CenterFusion on the challenging nuscenes dataset ,where it outperforms all previous camera-based object detection methods in the 3D object detection benchmark. We also show that exploiting radar information significantly improves velocity estimation for object without using any temporal information.","title":"Experiment"},{"location":"radar_camera_fusion/#radar-point-cloud","text":"Radars are active sensors that transmit radio waves to sense the environment and measure the reflected waves to determine the location and velocity of objects. Automotive radars usually report the detected objects as 2D points in BEV, providing the azimuth\u65b9\u4f4d\u89d2 and radial distance to the object. For every detection, the radar also reports the instantaneous velocity of the object in the radial direction.This radial velocity does not necessarily match the object's actual velocity vector in it's direction of movement. Fig.2 illustrates the difference between the radial as reported by the radar, and actual velocity of the object in the vehicle's coordinate system. We represent each radar detection as a 3D point in the egocentric coordinate system, and parameterize it as $P=(x,y,z,v_x,v_y)$ where $(x,y,z)$ is the position and $(v_x,v_y)$ is the reported radial velocity of the object in the $x$ and $y$ directions. The radial velocity is compensated by the ego vehicle's motion.For every scene,we aggregate 3 sweeps of the radar point cloud (detections within the past 0.25 seconds). The nuscenes dataset provides the calibration parameters needed for mapping the radar point clouds from the radar coordinates system to the egocentric and camera coordinate systems.","title":"Radar Point Cloud"},{"location":"radar_camera_fusion/#centernet","text":"CenterNet represents the state-of-the-art in 3D object detection using single camera. It takes an image $I \\in \\mathbb{R}^{W\\times H\\times 3}$ as input and generates a keypoint heatmap $\\hat{Y}\\in[0,1]^{\\frac{W}{R}\\times\\frac{H}{R}\\times C}$ as output where $W$ and $H$ are the image width and height.$R$ is the downsampling ratio and $C$ is the number of object categories\u7c7b\u522b. A prediction of $\\hat{Y} {x,y,c} = 1$ as the output indicates a detected object of class $c$ centered at position $(x,y)$ on the image. The ground-truth heatmap $Y\\in [0, 1]^{\\frac{W}{R}\\times\\frac{H}{R}\\times C}$ is generated from the ground-truth 2D bounding boxes using a Gaussian kernel.For each bounding box center point $p_i\\in R^2$ of class $c$ in the image, a Gaussian heatmap is generated on $Y {:,:,c}$ , the final value of Y for class $c$ at position $q\\in R^2$ is defined as Y_{qc} = \\underset{i}{max}exp\\left(-\\frac{(p_i-q)^2}{2\\sigma^2_i}\\right) Figure 3,Frustum\u89c6\u9525 association. An object detected using the image features(left),generating the ROI frustum based on object's 3D bounding box(middle), and the BEV of the ROI frustum showing radar detections inside the frustum (right).$\\delta$ is used to increase the frustum size in the testing phase.$\\hat{d}$ is the ground_truth depth in the training phase and the estimated object depth in the testing phase Expanding radar points to 3D pillars( top image). Directly mapping the pillars to the image and replacing with radar depth information results in poor association with object's center and many overlapping depth values(middle image),Frustum association accurately maps the radar detections to the center of objects and minimizes overlapping(bottom image). Radar detections are only associated to objects with a valid ground truth or detection box, and only if all or part of the radar detection pillar\u652f\u67f1 is inside the box.Frustum association also prevents associating radar detections caused by background objects such as buildings to foreground objects, as seen in the case of pedestrains on the right hand side of the image where $\\sigma_i$ is a size-adaptive\u5c3a\u5bf8\u9002\u5e94 standard deviation\u504f\u5dee,controlling the size of the heatmap for every object based on its size. A fully convolutional encode-decoder network is used to predict $\\hat{Y}$. To generate 3D bounding boxes,sperate network heads are used to regress object's depth, dimensions and orientation directly from the detected center points.Depth is calculated as an additional output channel $\\hat{D}\\in [0,1]^{\\frac{W}{R}\\times\\frac{H}{R}}$ after applying the inverse sigmoidal transformation used in Eigen et al. to the original depth domain. The object dimensions are directly regressed to their absoluate values in meter as three output channels $\\hat{\\Gamma}\\in[0,1]^{\\frac{W}{R}\\times\\frac{H}{R}\\times3}$ .Orientation is encoded as two bins with 4 scalars in each bin, following the orientation representation in M. For each center point, a local offset is also predicted to compensate\u8865\u507f for the discretization\u79bb\u6563\u5316 error caused by the output strides in the backbone network. Given the annotated objects $p_0, p_1, ...$ in an image, the training objective is defined as below based on the focal loss: L_k=\\frac{1}{N}\\sum_{xyc}\\left\\{\\begin{matrix}(1-\\hat{Y}_{xyc})^{\\alpha}log(\\hat{Y}_{xyc})\\qquad Y_{xyc}=1\\\\\\\\ (1-Y_{xyc})^{\\beta}(\\hat{Y}_{xyc})^\\alpha log(1-\\hat{Y}_{xyc})\\qquad otherwise\\end{matrix}\\right. where $N$ is the number of objects, $Y\\in [0,1]^{\\frac{W}{R}\\times\\frac{H}{R}\\times C}$ is the annotated object's ground-truth heatmap and $\\alpha$ and $\\beta$ are focal loss hyperparameters. Detection identifies\u8bc6\u522b objects as axis-aligned boxes in an image.Most successful object detectors enumerate\u5217\u4e3e a nearly exhaustive\u8be6\u5c3d\u7684 list of potential object locations and classify each.This is wasteful, inefficient,and requires additional post-processing. In this paper, we take a different approach.We model an object as a single point - the center point of its bounding box.Our detector uses keypoint estimation to find center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose. Our center point based approach, CenterNet, is end-to-end differnetiable, simpler,faster,and more accurate than corresponding bounding box based detectors.CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1% AP at 142 FPS. We use the same approach to estimate 3D bounding box in the KITTI benchmark and human pose on the COCO keypoint dataset. Our method performs competitively with sophisticated multi-stage methods and runs in real-time. Object detection powers many vision tasks like instance segmentation, pose estimation, tracking, and action recognition.It has down-stream applications in surveillance\u76d1\u89c6\uff0c autonomous driving, and visual question answering. Current object detectors represent each object through an axis-aligned bounding box, that tightly encompasses the object.They then reduce object detection to image classification of an extensive number of potential object bounding boxes.For each bounding box, the classifer determines if the image content is a specific object or background. One stage detectors, slide a complex arrangement of possible bounding boxes called anchors, over the image and classify them directly without specifying the box content.Two-stage detectors recompute image features for each potential box, then classify those features.Post-processing, namely non-maxima suppression, then removes duplicated detections for the same instance by computing bounding box IoU. This post-processing is hard to differentiate and train, hence most current detectors are not end-to-end trainable. Nonetheless, over the past five years, this idea has achieved good empirical success.Sliding window based object detectors are however a bit wasteful , as they need to enumerate all possible object locations and diemnsions. In this paper, we provide a much simpler and more efficient alternative. We represent objects by a single point at their bounding box center (see Fig 2), Other properties such as object size, dimension, 3D extent, orientation, and pose are then regressed directly from image features at the center location. Object detection is then a standard keypoint estimation problem.We simply feed the input image to a fully convolutional network that generates a heatmap.Peaks in this heatmap correspond to object centers.Image features at each peak predict the objects bounding box height and weight. The model trains using standard dense supervised\u76d1\u7763 learning.Inference is a single network forward-pass, without non-maximal suppression for post-processing. We model an object as the center point of its bounding box. The bounding box size and other object properties are inffered from the keypoint feature at the center.Best viewed in color Our method is general and can be extended to other tasks with minor effort. We provide experiments on 3D obect detection and multi-person human pose estimation, by predicting additional outputs at each center point (see Figure 4). For 3D bounding box estimation, we regress to the object absolute depth, 3D bounding box dimensions, and object orientation. For human pose estimation, we consider the 2D joint locations as offsets from the center and directly regress to them at the center point location. The simplicity of our method, CenterNet, allows it to run at a very high speed.With a simple Resnet-18 and up-convolutional layers, our network runs at 142 FPS with 28.1% COCO bounding box AP. With a carefully designed keypoint detection network. DLA-34 , our network achieves 37.4% COCO AP at 52 FPS.","title":"CenterNet"},{"location":"radar_camera_fusion/#related-work","text":"Object detection by region classfication. One of the first successful deep object detectors, RCNN, enumerates object location from a larget set of region candidates,crops them , and classifies each using a deep network. Fast-RCNN crops image features instead, to save computation. However, both methods rely on slow low-level region proposal methods. Object detection with implicit(\u9690\u542b\u7684) anchors.Faster RCNN generates region proposal within the detection network. It samples fixed-shape bounding boxes (anchors) around a low-resolution image grid and classifies each into \u201cforeground or not\u201d. An anchor is labeled foreground with a $>0.7$ overlap with any ground truth object, background with a $<0.3$ overlap, or ignored otherwise. Each generated region proposal is again classified. Changing the proposal classifier to a multi-class classification forms the basis of one-stage detectors. Several improvements to one-stage detectors include anchor shape points, different feature resolution, and loss re-weighting among different samples. Our approach is closely related to anchor-based one-stage approaches.A center point can be seen as a single shape-agnostic \u4e0d\u53ef\u77e5\u5f62\u72b6 anchor.(see Figure3) .However,there are a few important differences. First, our CenterNet assigns the \"anchor\" based solely\u809a\u5b50 on location, not box overlap. We have no manual thresholds for foreground and background classification. Second, we only have one positive \"anchor\" per object. and hence do not need NonMaximum Suppression(NMS). We simply extract local peaks in the keypoint heatmap. Third, CenterNet uses a larger output resolution (output stride of 4) compared to traditional object detectors(output stride of 16). This eliminates the need for multiple anchors. Object detection by keypoint estimation We are not the first to use keypoint estimation for object detection. CornerNet detects two bounding box corners as keypoints, while ExtremeNet detects the top-,left-,bottom-,right-most, and center points of all objects. Both these methods build on the same robust keypoint estimation network as our CenterNet.However, they require a combinatorial\u7ec4\u5408 grouping stage after keypoint detection, which significantly slows down each algorithm. Our CenterNet , on the other hand,simply extracts a single center point per object without the need for grouping or post-processing. Monocular 3D object detection 3D bounding box estimation powers autonomous driving . Deep3Dbox use a slow-RCNN style framework, by first detecting 2D objects and then feeding each object into a 3D estimation network. 3D RCNN adds an additional head to Faster-RCNN followed by a 3D projection. Deep Manta uses a coarse-to-fine\u4ece\u7c97\u5230\u7ec6 Faster-RCNN trained on many tasks. Our method is similar to one-stage version of Deep3Box or 3DRCNN. As such, CenterNet is much simpler and faster than competing methods. standard anchor based detection. Anchors count as positive with an overlap IoU>0.7 to any object, negative with overlap IoU<0.3, or are ignored otherwise Center point based detection.The center pixel is assigned to the object. Nearby points have a reduced negative loss. Object size is regressed","title":"Related work"},{"location":"radar_camera_fusion/#preliminary","text":"Let $I\\in R^{W\\times H\\times3}$ be an input image of width $W$ and height $H$. Our aim is to produce a keypoint heatmap $\\hat{Y}\\in[0,1]^{\\frac{W}{R}\\times\\frac{H}{R}\\times C}$, where $R$ is the output stride and $C$ is the number of keypoint types. Keypoint types include $C=17$ human joints in human pose estimation, or $C=80$ object categorical in object detection. We use the default output stride of $R = 4$ in literature. The output stride downsamples the output prediction by a factor R. A prediction $\\hat{Y} {x,y,c}=1$ corresponds to a detected keypoint, while $\\hat{Y} {x,y,c} =0$ is background. We use serveral different fully-convolutional encoder-decoder networks to predict $\\hat{Y}$ from an image $I$: A stacked hourglass network\u5806\u53e0\u7684\u6c99\u6f0f\u7f51\u7edc upconvolutional residual network (ResNet) and deep layer aggregation\u805a\u5408 (DLA). We train the keypoint prediction network following Law and Deng. For each ground truth keypoint $p \\in R^2$ of class $c$ , we compute a low-resolution equivalent\u76f8\u7b49\u7684 $\\widetilde{p}= \\left\\lfloor\\frac{p}{R}\\right\\rfloor$. We then splat\u6454\u5f97\u75db all ground truth keypoints onto a heatmap $Y\\in [0,1]^{\\frac{W}{R}\\times\\frac{H}{R}\\times C}$ using a Gaussian kernel $Y_{xyc}=exp\\left(-\\frac{(x-\\widetilde{p}_x)^2+(y-\\widetilde{p}_y)^2}{2\\sigma^2_p}\\right)$, where $\\sigma_p$ is an object size-adaptive standard deviation\u5c3a\u5bf8\u9002\u5e94\u6807\u51c6\u504f\u5dee. If two Gaussians of the same class overlap, we take the element-wise maximum. The training objective is a penalty-reduced\u51cf\u5211 pixel-wise logistic regression with focal loss: L_k=\\frac{-1}{N}\\sum_{xyz}\\left\\{\\begin{matrix} (1-\\hat{Y}_{xyc})^\\alpha log(\\hat{Y}_{xyc})\\qquad if\\quad Y_{xyc} = 1\\\\\\\\ (1-Y_{xyc})^\\beta(\\hat{Y}_{xyc})^\\alpha log(1-\\hat{Y}_{xyc}) \\qquad otherwise \\end{matrix}\\right. where $\\alpha$ and $\\beta$ are hyper-parameters of the focal loss, and $N$ is the number of keypoints in image $I$ . The normalization by $N$ is chosen as to normalize all positive focal loss instances to 1. We use $\\alpha=2$ and $\\beta=4$ in all our experiments,following Law and Debug. To recover the discretization\u79bb\u6563\u5316 error caused by the output stride, we additionally predict a local offset $\\hat{O}\\in R^{\\frac{W}{R}\\times\\frac{H}{R}\\times2}$ for each center point. All classes $c$ share the same offset prediction.The offset is trained with an $L1$ loss L_{off} = \\frac{1}{N}\\sum_p\\left|\\hat{O}_{\\widetilde{p}}-\\left(\\frac{p}{R}-\\widetilde{p}\\right)\\right|\\qquad(2) The supervision acts only at keypoints locations $\\widetilde{p}$, all other locations are ignored. In the next section,we will show how to extend this keypoint estimator to a general purpose object detector.","title":"Preliminary"},{"location":"radar_camera_fusion/#objects-as-points","text":"Let $(x_1^{(k)},y_1^{(k)},x_2^{(k)},y_2^{(k)})$ be the bounding box of object $k$ with category $c_k$. Its center point is lies at $p_k=(\\frac{x_1^{(k)}+x_2^{(k)}}{2},\\frac{y_1^{(k)}+y_2^{(k)}}{2})$. We use our keypoint estimator $\\hat{Y}$ to predict all center points. In addition, we regress to the object size $s_k=(x_2^{(k)}-x_1^{(k)},y_2^{(k)}-y_1^{(k)})$ for each object $k$.To limit the computational burden\u8ba1\u7b97\u8d1f\u62c5 , we use a single size prediction $\\hat{S} \\in R^{\\frac{W}{R}\\times\\frac{H}{R}\\times C}$ for all object categories. We use an $L1$ loss at the center point similar to Objective 2: L_{size}=\\frac{1}{N}\\sum^N_{k=1}\\left|\\hat{S}_{p_k}-s_k\\right|\\qquad(3) We do not normalize\u5f52\u4e00\u5316 the scale and directly use the raw pixel coordinates. We instead scale the loss by a constant $\\lambda_{size}$.The overall training objective is L_{det} = L_k + \\lambda_{size}L_{size} + \\lambda_{off}L_{off}\\qquad(4) We set $\\lambda_{size}=0.1$ and $\\lambda_{off}=1$ in all our experiments unless specified otherwise. We use a single network to predict the keypoints $\\hat{Y}$, offset $\\hat{O}$ , and size $\\hat{S}$ The network predicts a toatal of $C + 4$ outputs at each location. All outputs share a common fully-convolutional backbone network. For each modality, the features of the backbone are then passed through a separate 3x3 convolution, ReLU and another 1x1 convolution. Figure4 shows an overview of the network output. Section 5 and supplementary material contain additional architectural details. From points to bounding boxes At inference time, we first extract the peaks in the heatmap for each category independently. We detect all responses whose value is greater or equal to its 8-connected neighbors and keep the top 100 peaks. Let $\\hat{P} c$ be the set of $n$ detected center points $\\hat{P} ={(\\hat{x}_i,\\hat{y}_i)}^n {i=1}$ of class $c$. Each keypoint location is given by an integer coordinates $(x_i, y_i)$. We use the keypoint values $\\hat{Y}_{x_iy_ic}$ as a measure of its detection confidence, and produce a bounding box at location. (\\hat{x}_i+ \\delta\\hat{x}_i-\\hat{w}_i/2, \\hat{y}_i+\\delta\\hat{y}_i-\\hat{h}_i/2,\\hat{x}_i+\\delta\\hat{x}_i+\\hat{w}_i/2,\\hat{y}_i+\\delta\\hat{y}_i+\\hat{h}_i/2) Outputs of our network for different tasks: top for object detection, middle for 3D object detection,bottom for pose estimation. All modalities are produced from a common backbone, with a different 3x3 and 1x1 output convolutions separated by a ReLU. The number in brackets indicates the output channels. See section 4 for details where $(\\delta\\hat{x} i,\\delta\\hat{y}_i)=\\hat{O} {\\hat{x} i,\\hat{y}_i}$ is the offset prediction and $(\\hat{w}_i,\\hat{h}_i) = \\hat{S} {\\hat{x}_i,\\hat{y}_i}$ is the size prediction.","title":"Objects as Points"},{"location":"ros%20openai/","text":"ubuntu18 melodic sudo apt-get install python-pip python3-vcstool python3-pyqt4 pyqt5-dev-tools libbluetooth-dev libspnav-dev pyqt4-dev-tools libcwiid-dev cmake gcc g++ qt4-qmake libqt4-dev libusb-dev libftdi-dev python3-defusedxml python3-vcstool ros-melodic-octomap-msgs ros-melodic-joy ros-melodic-geodesy ros-melodic-octomap-ros ros-melodic-control-toolbox ros-melodic-pluginlib ros-melodic-trajectory-msgs ros-melodic-control-msgs ros-melodic-std-srvs ros-melodic-nodelet ros-melodic-urdf ros-melodic-rviz ros-melodic-kdl-conversions ros-melodic-eigen-conversions ros-melodic-tf2-sensor-msgs ros-melodic-pcl-ros ros-melodic-navigation ros-melodic-sophus sudo pip install gym sudo apt-get install python-skimage sudo pip install h5py pip install tensorflow-gpu (if you have a gpu if not then just pip install tensorflow) sudo pip install keras cd ~ git clone https://github.com/erlerobot/gym-gazebo cd gym-gazebo sudo pip install -e . cd gym-gazebo/gym_gazebo/envs/installation bash setup_melodic.bash Tensorflow \u7b97\u6cd5\u529f\u80fd \u8bb0\u5f55\u4eea\u8bbe\u7f6e \u968f\u673a\u79cd\u5b50\u8bbe\u7f6e \u73af\u5883\u5b9e\u4f8b\u5316 \u4e3a\u8ba1\u7b97\u56fe\u5236\u4f5c\u5360\u4f4d\u7b26 actor_critic \u901a\u8fc7\u4f5c\u4e3a\u53c2\u6570\u4f20\u9012\u7ed9\u7b97\u6cd5\u51fd\u6570\u6765\u6784\u5efa\u53c2\u4e0e\u8005\u8bc4\u8bba\u8ba1\u7b97\u56fe \u5b9e\u4f8b\u5316\u4f53\u9a8c\u7f13\u51b2\u533a \u5efa\u7acb\u7279\u5b9a\u4e8e\u7b97\u6cd5\u7684\u635f\u5931\u51fd\u6570\u548c\u8bca\u65ad\u7684\u8ba1\u7b97\u56fe \u8fdb\u884c\u57f9\u8bad \u8fdb\u884ctf\u4f1a\u8bdd\u5e76\u521d\u59cb\u5316\u53c2\u6570 \u901a\u8fc7\u8bb0\u5f55\u5668\u8bbe\u7f6e\u6a21\u578b\u4fdd\u5b58 \u5b9a\u4e49\u8fd0\u884c\u7b97\u6cd5\u4e3b\u5faa\u73af\u6240\u9700\u7684\u529f\u80fd\uff08\u4f8b\u5982\uff0c\u53d6\u51b3\u4e8e\u7b97\u6cd5\uff0c\u6838\u5fc3\u66f4\u65b0\u529f\u80fd\uff0c\u83b7\u53d6\u64cd\u4f5c\u529f\u80fd\u548c\u6d4b\u8bd5\u4ee3\u7406\u529f\u80fd\uff09 \u8fd0\u884c\u7b97\u6cd5\u7684\u4e3b\u5faa\u73af\uff1a \u5728\u73af\u5883\u4e2d\u8fd0\u884c\u4ee3\u7406 \u6839\u636e\u7b97\u6cd5\u7684\u4e3b\u8981\u65b9\u7a0b\u5f0f\u5b9a\u65f6\u66f4\u65b0\u4ee3\u7406\u53c2\u6570 \u8bb0\u5f55\u5173\u952e\u6027\u80fd\u6307\u6807\u5e76\u4fdd\u5b58\u4ee3\u7406 \u6838\u5fc3\u6587\u4ef6 \u6838\u5fc3\u6587\u4ef6\u4e0d\u50cf\u7b97\u6cd5\u6587\u4ef6\u90a3\u6837\u7d27\u5bc6\u5730\u9644\u7740\u5728\u6a21\u7248\u4e0a\uff0c\u4f46\u5177\u6709\u4e00\u4e9b\u8fd1\u4f3c\u7684\u7ed3\u6784\uff1a \u4ec5tensorflow:\u4e0e\u5236\u4f5c\u548c\u7ba1\u7406\u5360\u4f4d\u7b26\u6709\u5173\u7684\u529f\u80fd \u7528\u4e8e\u5efa\u7acbactor_critic \u7279\u5b9a\u7b97\u6cd5\u7684\u65b9\u6cd5\u76f8\u5173\u7684\u8ba1\u7b97\u56fe\u90e8\u5206\u7684\u529f\u80fd \u4efb\u4f55\u5176\u4ed6\u6709\u7528\u7684\u529f\u80fd \u4e8e\u7b97\u6cd5\u517c\u5bb9\u7684MLP actor_critic \u7684\u5b9e\u73b0\uff0c\u5176\u4e2d\u7b56\u7565\u548c\u503c\u51fd\u6570\u90fd\u7531\u7b80\u5355\u7684MLP\u8868\u793a\u3002 \u4f53\u9a8c\u6df1\u5ea6RL\u7684\u6700\u4f73\u65b9\u6cd5\u4e4b\u4e00\u662f\u8fd0\u884c\u7b97\u6cd5\uff0c\u5e76\u67e5\u770b\u5b83\u4eec\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u7684\u6267\u884c\u60c5\u51b5\u3002Spinning Up\u968f\u9644spinup/run.py,\u8fd9\u662f\u4e00\u4e2a\u65b9\u4fbf\u7684\u5de5\u5177\uff0c\u53ef\u4ee5\u8ba9\u4f60\u4ece\u547d\u4ee4\u884c\u8f7b\u677e\u7684\u542f\u52a8\u5404\u79cd\u597d\u7b97\u6cd5\uff08\u53ef\u4ee5\u9009\u62e9\u8d85\u53c2\u6570\uff09 \u8fd1\u7aef\u7b56\u7565\u4f18\u5316 PPO\u53d7\u5230\u4e0eTRPO\u76f8\u540c\u95ee\u9898\u7684\u6fc0\u52b1\uff1a\u6211\u4eec\u5982\u4f55\u624d\u80fd\u4f7f\u7528\u5f53\u524d\u62e5\u6709\u7684\u6570\u636e\u5728\u7b56\u7565\u4e0a\u91c7\u53d6\u6700\u5927\u53ef\u80fd\u7684\u6539\u8fdb\u6b65\u9aa4\uff0c\u800c\u53c8\u4e0d\u4f1a\u8d70\u7684\u592a\u8fdc\u800c\u610f\u5916\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002TRPO\u5c1d\u8bd5\u4f7f\u7528\u590d\u6742\u7684\u4e8c\u9636\u65b9\u6cd5\u89e3\u51b3\u6b64\u95ee\u9898\u3002PPO\u662f\u4e00\u9636\u65b9\u6cd5\u7cfb\u5217\uff0c\u5b83\u4f7f\u7528\u5176\u4ed6\u4e00\u4e9b\u6280\u5de7\u6765\u4f7f\u65b0\u7b56\u7565\u63a5\u8fd1\u4e8e\u65e7\u7b56\u7565\u3002PPO\u65b9\u6cd5\u660e\u663e\u66f4\u6613\u4e8e\u5b9e\u73b0\uff0c\u5e76\u4e14\u4ece\u7ecf\u9a8c\u4e0a\u770b\uff0c\u5176\u6027\u80fd\u81f3\u5c11\u4e0eTRPO\u76f8\u540c\u3002PPO\u6709\u4e24\u79cd\u4e3b\u8981\u53d8\u4f53\uff1aPPO-Penalty\uff0c PPO-Clip\u3002 PPO-Penalty \u5927\u7ea6\u89e3\u51b3\u4e86\u50cfTRPO\u8fd9\u6837\u53d7KL\u7ea6\u675f\u7684\u66f4\u65b0\u3002\u4f46\u662f\u4f1a\u60e9\u7f5a\u76ee\u6807\u51fd\u6570\u4e2d\u7684KL\u6563\u5ea6\uff0c\u800c\u4e0d\u4f7f\u5176\u6210\u4e3a\u786c\u7ea6\u675f\uff0c\u5e76\u4f1a\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u81ea\u52a8\u8c03\u6574\u60e9\u7f5a\u7cfb\u6570\uff0c\u4ee5\u4f7f\u5176\u9002\u5f53\u7f29\u653e\u3002 PPO-Clip \u5728\u76ee\u6807\u4e2d\u6ca1\u6709KL\u6563\u5ea6\u9879\uff0c \u4e5f\u6ca1\u6709\u4efb\u4f55\u7ea6\u675f\u3002\u53d6\u800c\u4ee3\u4e4b\u7684\u662f\u4f9d\u9760\u5bf9\u76ee\u6807\u51fd\u6570\u7684\u4e13\u95e8\u88c1\u526a\u6765\u6d88\u9664\u65b0\u653f\u7b56\u8fdc\u79bb\u65e7\u653f\u7b56\u7684\u52a8\u673a\u3002 \u8981\u95fb\u901f\u89c8 PPO\u662f\u4e00\u79cd\u57fa\u4e8e\u7b56\u7565\u7684\u7b97\u6cd5 PPO\u53ef\u4ee5\u5e94\u7528\u4e8e\u5177\u6709\u79bb\u6563\u7684\u6216\u8fde\u7eed\u7684\u52a8\u4f5c\u7a7a\u95f4\u7684\u73af\u5883 PPO\u7684Spinningup\u5b9e\u73b0\u652f\u6301\u4e0eMPI\u5e76\u884c\u5316 PPO\u66f4\u65b0\u7b56\u7565\u516c\u5f0f $$ \\theta_{k+1}=arg\\underset{\\theta}{max}\\underset{s,\\alpha\\sim\\pi\\theta_k}{E}[L(s,\\alpha,\\theta_k,\\theta)] $$ \u901a\u5e38\u91c7\u53d6\u591a\u4e2a\u6b65\u9aa4\uff08\u901a\u5e38\u662f\u5c0f\u6279\u91cf\uff09SGD\u6765\u6700\u5927\u5316\u76ee\u6807 $$ L(s,\\alpha,\\theta_k,\\theta)=min(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)}A^{\\pi\\theta_k}(s,\\alpha),clip(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)},1-\\epsilon,1+\\epsilon)A^{\\pi\\theta_k}(s,\\alpha)) $$ $\\epsilon$\u662f\u4e00\u4e2a\u8d85\u53c2\u6570\u7c97\u7565\u5730\u8bf4\u51fa\u65b0\u653f\u7b56\u88ab\u5141\u8bb8\u4e0e\u65e7\u653f\u7b56\u76f8\u8ddd\u591a\u8fdc \u8fd9\u662f\u4e00\u4e2a\u5f88\u590d\u6742\u7684\u65b9\u7a0b\u5f0f\uff0c\u7b2c\u4e00\u773c\u5f88\u96be\u8bf4\u660e\u767d\u5b83\u5728\u505a\u4ec0\u4e48\uff0c\u6216\u8005\u5b83\u5982\u4f55\u5e2e\u52a9\u65b0\u7b56\u7565\u9760\u8fd1\u65e7\u7b56\u7565\u3002\u4e8b\u5b9e\u8bc1\u660e\u6709\u4e00\u4e2a\u76f8\u5f53\u7b80\u5316\u7684\u7248\u672c\uff0c\u8f83\u5bb9\u6613\u7406\u89e3\uff0c\u4e5f\u662f\u6211\u4eec\u5728\u4ee3\u7801\u4e2d\u5b9e\u73b0\u7684\u7248\u672c $$ L(s,\\alpha,\\theta_k,\\theta) = min\\left(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)}A^{\\pi\\theta_k}(s,\\alpha), g\\left(\\epsilon,A^{\\pi\\theta_k}(s,\\alpha)\\right)\\right) $$ $$ g(\\epsilon,A)=\\left{ \\begin{matrix}(1+\\epsilon)A\\quad A\\geq0&\\\\(1-\\epsilon)A\\quad A<0\\end{matrix} \\right. $$ \u4e3a\u4e86\u5f04\u6e05\u695a\u4ece\u4e2d\u5f97\u5230\u7684\u76f4\u89c9\uff0c\u8ba9\u6211\u770b\u4e00\u4e0b\u5355\u4e2a\u7684 \u72b6\u6001\u2014\u52a8\u4f5c \u5bf9\uff0c\u5e76\u8003\u8651\u6848\u4f8b \u4f18\u52bf\u662f\u79ef\u6781 \u7684\uff0c\u5047\u8bbe\u8be5\u72b6\u6001\u2014\u52a8\u4f5c \u5bf9\u7684\u4f18\u52bf\u4e3a\u6b63\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b83\u5bf9\u76ee\u6807\u7684\u8d21\u732e\u51cf\u5c0f\u4e3a\uff1a $$ L(s,\\alpha,\\theta,\\theta_k)=min\\left(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)},(1+\\epsilon)\\right)A^{\\pi\\theta_k}(s,\\alpha) $$ \u56e0\u4e3a\u4f18\u52bf\u662f\u79ef\u6781\u7684\uff0c\u6240\u4ee5\u5982\u679c\u91c7\u53d6\u884c\u52a8\u7684\u53ef\u80fd\u6027\u66f4\u5927\uff08\u4e5f\u5c31\u662f$\\pi_{\\theta}(\\alpha|s)$\u589e\u52a0\uff09\uff0c\u76ee\u6807\u5c31\u4f1a\u589e\u52a0\u3002\u4f46\u662fterm\u4e2d\u7684min\u9650\u5236\u4e86\u53ef\u4ee5\u589e\u52a0\u591a\u5c11\u76ee\u6807\u3002\u4e00\u65e6$\\pi_\\theta(\\alpha|s)>(1+\\epsilon)\\pi_{\\theta_k}(\\alpha|s)$,\u8fd9\u65f6min\u8e22\u8fdb\uff0cterm\u5230\u8fbe$(1+\\epsilon)A^{\\pi\\theta_k}(s,\\alpha)$,\u56e0\u6b64\u7528\u8fc7\u8fdc\u79bb\u65e7\u653f\u7b56\uff0c\u65b0\u653f\u7b56\u4e0d\u4f1a\u83b7\u76ca\u3002 \u4f18\u52bf\u662f\u8d1f\u9762 \u7684\uff0c\u5047\u8bbe\u8be5\u72b6\u6001\u2014\u52a8\u4f5c \u5bf9\u7684\u4f18\u52bf\u662f\u8d1f\u7684\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b83\u5bf9\u76ee\u6807\u7684\u8d21\u732e\u51cf\u5c0f\u4e3a\uff1a $$ L(s,\\alpha,\\theta,\\theta_k)=max\\left(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)},(1-\\epsilon)\\right)A^{\\pi\\theta_k}(s,\\alpha) $$ \u56e0\u4e3a\u4f18\u52bf\u662f\u8d1f\u7684\uff0c\u6240\u4ee5\u5982\u679c \u91c7\u53d6\u884c\u52a8\u7684\u53ef\u80fd\u6027\u66f4\u5c0f\uff08\u4e5f\u5c31\u662f$\\pi_\\theta(\\alpha|s)$\u51cf\u5c0f\uff09\uff0c\u76ee\u6807\u5c31\u4f1a\u589e\u52a0\u3002\u4f46\u662fterm\u4e2d\u7684max\u9650\u5236\u4e86\u53ef\u4ee5\u589e\u52a0\u591a\u5c11\u76ee\u6807\u3002\u4e00\u65e6$\\pi_\\theta(\\alpha|s)<(1-\\epsilon)\\pi_{\\theta_k}(\\alpha|s)$,\u8fd9\u65f6max\u8e22\u8fdb\uff0cterm\u5230\u8fbe$(1-\\epsilon)A^{\\pi\\theta_k}(s,\\alpha)$,\u56e0\u6b64\uff0c\u518d\u6b21\uff0c\u8fdc\u79bb\u65e7\u653f\u7b56\uff0c\u4e0d\u4f1a\u4f7f\u65b0\u653f\u7b56\u53d7\u76ca\u3002 \u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u901a\u8fc7\u53bb\u9664\u4f7f\u653f\u7b56\u53d1\u751f\u91cd\u5927\u53d8\u5316\u7684\u6fc0\u52b1\uff0c\u88c1\u526a\u5145\u5f53\u7740\u6b63\u5219\u5316\u5668\u3002\u800c\u8d85\u53c2\u6570$\\epsilon$\u5219\u5bf9\u5e94\u4e8e\u65b0\u653f\u7b56\u4e0e\u65e7\u653f\u7b56\u53ef\u4ee5\u6709\u591a\u8fdc\uff0c\u540c\u65f6\u4ecd\u4f7f\u76ee\u6807\u53d7\u76ca\u3002 \u63a2\u7d22\u4e0e\u53d1\u73b0 PPO\u6309\u57fa\u4e8e\u7b56\u7565\u65b9\u5f0f\u8bad\u7ec3\u968f\u673a\u7b56\u7565\uff0c\u8fd9\u610f\u5473\u7740\u5b83\u4f1a\u6839\u636e\u5176\u968f\u673a\u7b56\u7565\u7684\u6700\u65b0\u7248\u672c\u901a\u8fc7\u91c7\u6837\u64cd\u4f5c\u6765\u8fdb\u884c\u63a2\u7d22\u3002\u52a8\u4f5c\u9009\u62e9\u7684\u968f\u673a\u6027\u53d6\u51b3\u4e8e\u521d\u59cb\u6761\u4ef6\u548c\u8bad\u7ec3\u6b65\u9aa4\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u4ee3\u7406\u9010\u6b65\u51cf\u5c11\u968f\u673a\u6027\uff0c\u56e0\u4e3a\u66f4\u65b0\u89c4\u5219\u9f13\u52b1\u5b83\u5229\u7528\u5df2\u7ecf\u53d1\u73b0\u7684\u5956\u52b1\u3002\u8fd9\u53ef\u80fd\u5bfc\u81f4\u7b56\u7565\u9677\u5165\u5c40\u90e8\u6700\u4f18\u72b6\u6001\u3002 \u4f2a\u4ee3\u7801 PPO-CLIP \u8f93\u5165\uff1a\u521d\u59cb\u7b56\u7565\u53c2\u6570$\\theta_0$,\u521d\u59cb\u503c\u529f\u80fd\u53c2\u6570$\\phi_0$ for k = 0, 1, 2, ... do \u5728\u73af\u5883\u4e2d\u901a\u8fc7\u6267\u884c\u7b56\u7565$\\pi_k=\\pi(\\theta_k)$\uff0c\u6536\u96c6\u4e00\u7ec4\u8f68\u8ff9$D_k={\\tau_i}$ \u7b97\u5956\u52b1$\\hat{R}_i$ \u57fa\u4e8e\u5f53\u524d\u7684value function $V_{\\phi k}$,\u8ba1\u7b97advantage estimates $\\hat{A}_t$ \u901a\u8fc7\u6700\u5927\u5316PPO-Clip\u76ee\u6807\uff0c\u66f4\u65b0\u7b56\u7565 $$ \\theta_{k+1}=arg\\ \\underset{\\theta}{max}\\frac{1}{|D_k|T}\\sum_{\\tau\\in D_k}\\sum^T_{t=0}min\\left(\\frac{\\pi_\\theta(\\alpha_t|s_t)}{\\pi_{\\theta_k}(\\alpha_t|s_t)}A^{\\pi\\theta_k}(s_t,\\alpha_t),g(\\epsilon,A^{\\pi\\theta_k}(s_t,\\alpha_t))\\right) $$ \u901a\u5e38\u901a\u8fc7\u4e0eAdam\u4e00\u8d77\u7684\u968f\u673a\u68af\u5ea6\u4e0a\u5347 \u901a\u8fc7\u5747\u65b9\u8bef\u5dee\u56de\u5f52\u62df\u5408value function $$ \\phi_{k+1}=arg\\ \\underset{\\phi}{min}\\frac{1}{|D_k|T}\\sum_{\\tau\\in D_k}\\sum^T_{t=0}(V_\\phi(s_t)-\\hat{R}_t)^2 $$ \u7279\u522b\u662f\u901a\u8fc7\u4e00\u4e9b\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5 end for","title":"Ros openai"},{"location":"ros%20openai/#ubuntu18-melodic","text":"sudo apt-get install python-pip python3-vcstool python3-pyqt4 pyqt5-dev-tools libbluetooth-dev libspnav-dev pyqt4-dev-tools libcwiid-dev cmake gcc g++ qt4-qmake libqt4-dev libusb-dev libftdi-dev python3-defusedxml python3-vcstool ros-melodic-octomap-msgs ros-melodic-joy ros-melodic-geodesy ros-melodic-octomap-ros ros-melodic-control-toolbox ros-melodic-pluginlib ros-melodic-trajectory-msgs ros-melodic-control-msgs ros-melodic-std-srvs ros-melodic-nodelet ros-melodic-urdf ros-melodic-rviz ros-melodic-kdl-conversions ros-melodic-eigen-conversions ros-melodic-tf2-sensor-msgs ros-melodic-pcl-ros ros-melodic-navigation ros-melodic-sophus sudo pip install gym sudo apt-get install python-skimage sudo pip install h5py pip install tensorflow-gpu (if you have a gpu if not then just pip install tensorflow) sudo pip install keras cd ~ git clone https://github.com/erlerobot/gym-gazebo cd gym-gazebo sudo pip install -e . cd gym-gazebo/gym_gazebo/envs/installation bash setup_melodic.bash","title":"ubuntu18 melodic"},{"location":"ros%20openai/#tensorflow","text":"\u8bb0\u5f55\u4eea\u8bbe\u7f6e \u968f\u673a\u79cd\u5b50\u8bbe\u7f6e \u73af\u5883\u5b9e\u4f8b\u5316 \u4e3a\u8ba1\u7b97\u56fe\u5236\u4f5c\u5360\u4f4d\u7b26 actor_critic \u901a\u8fc7\u4f5c\u4e3a\u53c2\u6570\u4f20\u9012\u7ed9\u7b97\u6cd5\u51fd\u6570\u6765\u6784\u5efa\u53c2\u4e0e\u8005\u8bc4\u8bba\u8ba1\u7b97\u56fe \u5b9e\u4f8b\u5316\u4f53\u9a8c\u7f13\u51b2\u533a \u5efa\u7acb\u7279\u5b9a\u4e8e\u7b97\u6cd5\u7684\u635f\u5931\u51fd\u6570\u548c\u8bca\u65ad\u7684\u8ba1\u7b97\u56fe \u8fdb\u884c\u57f9\u8bad \u8fdb\u884ctf\u4f1a\u8bdd\u5e76\u521d\u59cb\u5316\u53c2\u6570 \u901a\u8fc7\u8bb0\u5f55\u5668\u8bbe\u7f6e\u6a21\u578b\u4fdd\u5b58 \u5b9a\u4e49\u8fd0\u884c\u7b97\u6cd5\u4e3b\u5faa\u73af\u6240\u9700\u7684\u529f\u80fd\uff08\u4f8b\u5982\uff0c\u53d6\u51b3\u4e8e\u7b97\u6cd5\uff0c\u6838\u5fc3\u66f4\u65b0\u529f\u80fd\uff0c\u83b7\u53d6\u64cd\u4f5c\u529f\u80fd\u548c\u6d4b\u8bd5\u4ee3\u7406\u529f\u80fd\uff09 \u8fd0\u884c\u7b97\u6cd5\u7684\u4e3b\u5faa\u73af\uff1a \u5728\u73af\u5883\u4e2d\u8fd0\u884c\u4ee3\u7406 \u6839\u636e\u7b97\u6cd5\u7684\u4e3b\u8981\u65b9\u7a0b\u5f0f\u5b9a\u65f6\u66f4\u65b0\u4ee3\u7406\u53c2\u6570 \u8bb0\u5f55\u5173\u952e\u6027\u80fd\u6307\u6807\u5e76\u4fdd\u5b58\u4ee3\u7406 \u6838\u5fc3\u6587\u4ef6 \u6838\u5fc3\u6587\u4ef6\u4e0d\u50cf\u7b97\u6cd5\u6587\u4ef6\u90a3\u6837\u7d27\u5bc6\u5730\u9644\u7740\u5728\u6a21\u7248\u4e0a\uff0c\u4f46\u5177\u6709\u4e00\u4e9b\u8fd1\u4f3c\u7684\u7ed3\u6784\uff1a \u4ec5tensorflow:\u4e0e\u5236\u4f5c\u548c\u7ba1\u7406\u5360\u4f4d\u7b26\u6709\u5173\u7684\u529f\u80fd \u7528\u4e8e\u5efa\u7acbactor_critic \u7279\u5b9a\u7b97\u6cd5\u7684\u65b9\u6cd5\u76f8\u5173\u7684\u8ba1\u7b97\u56fe\u90e8\u5206\u7684\u529f\u80fd \u4efb\u4f55\u5176\u4ed6\u6709\u7528\u7684\u529f\u80fd \u4e8e\u7b97\u6cd5\u517c\u5bb9\u7684MLP actor_critic \u7684\u5b9e\u73b0\uff0c\u5176\u4e2d\u7b56\u7565\u548c\u503c\u51fd\u6570\u90fd\u7531\u7b80\u5355\u7684MLP\u8868\u793a\u3002 \u4f53\u9a8c\u6df1\u5ea6RL\u7684\u6700\u4f73\u65b9\u6cd5\u4e4b\u4e00\u662f\u8fd0\u884c\u7b97\u6cd5\uff0c\u5e76\u67e5\u770b\u5b83\u4eec\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u7684\u6267\u884c\u60c5\u51b5\u3002Spinning Up\u968f\u9644spinup/run.py,\u8fd9\u662f\u4e00\u4e2a\u65b9\u4fbf\u7684\u5de5\u5177\uff0c\u53ef\u4ee5\u8ba9\u4f60\u4ece\u547d\u4ee4\u884c\u8f7b\u677e\u7684\u542f\u52a8\u5404\u79cd\u597d\u7b97\u6cd5\uff08\u53ef\u4ee5\u9009\u62e9\u8d85\u53c2\u6570\uff09","title":"Tensorflow \u7b97\u6cd5\u529f\u80fd"},{"location":"ros%20openai/#_1","text":"PPO\u53d7\u5230\u4e0eTRPO\u76f8\u540c\u95ee\u9898\u7684\u6fc0\u52b1\uff1a\u6211\u4eec\u5982\u4f55\u624d\u80fd\u4f7f\u7528\u5f53\u524d\u62e5\u6709\u7684\u6570\u636e\u5728\u7b56\u7565\u4e0a\u91c7\u53d6\u6700\u5927\u53ef\u80fd\u7684\u6539\u8fdb\u6b65\u9aa4\uff0c\u800c\u53c8\u4e0d\u4f1a\u8d70\u7684\u592a\u8fdc\u800c\u610f\u5916\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002TRPO\u5c1d\u8bd5\u4f7f\u7528\u590d\u6742\u7684\u4e8c\u9636\u65b9\u6cd5\u89e3\u51b3\u6b64\u95ee\u9898\u3002PPO\u662f\u4e00\u9636\u65b9\u6cd5\u7cfb\u5217\uff0c\u5b83\u4f7f\u7528\u5176\u4ed6\u4e00\u4e9b\u6280\u5de7\u6765\u4f7f\u65b0\u7b56\u7565\u63a5\u8fd1\u4e8e\u65e7\u7b56\u7565\u3002PPO\u65b9\u6cd5\u660e\u663e\u66f4\u6613\u4e8e\u5b9e\u73b0\uff0c\u5e76\u4e14\u4ece\u7ecf\u9a8c\u4e0a\u770b\uff0c\u5176\u6027\u80fd\u81f3\u5c11\u4e0eTRPO\u76f8\u540c\u3002PPO\u6709\u4e24\u79cd\u4e3b\u8981\u53d8\u4f53\uff1aPPO-Penalty\uff0c PPO-Clip\u3002 PPO-Penalty \u5927\u7ea6\u89e3\u51b3\u4e86\u50cfTRPO\u8fd9\u6837\u53d7KL\u7ea6\u675f\u7684\u66f4\u65b0\u3002\u4f46\u662f\u4f1a\u60e9\u7f5a\u76ee\u6807\u51fd\u6570\u4e2d\u7684KL\u6563\u5ea6\uff0c\u800c\u4e0d\u4f7f\u5176\u6210\u4e3a\u786c\u7ea6\u675f\uff0c\u5e76\u4f1a\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u81ea\u52a8\u8c03\u6574\u60e9\u7f5a\u7cfb\u6570\uff0c\u4ee5\u4f7f\u5176\u9002\u5f53\u7f29\u653e\u3002 PPO-Clip \u5728\u76ee\u6807\u4e2d\u6ca1\u6709KL\u6563\u5ea6\u9879\uff0c \u4e5f\u6ca1\u6709\u4efb\u4f55\u7ea6\u675f\u3002\u53d6\u800c\u4ee3\u4e4b\u7684\u662f\u4f9d\u9760\u5bf9\u76ee\u6807\u51fd\u6570\u7684\u4e13\u95e8\u88c1\u526a\u6765\u6d88\u9664\u65b0\u653f\u7b56\u8fdc\u79bb\u65e7\u653f\u7b56\u7684\u52a8\u673a\u3002","title":"\u8fd1\u7aef\u7b56\u7565\u4f18\u5316"},{"location":"ros%20openai/#_2","text":"PPO\u662f\u4e00\u79cd\u57fa\u4e8e\u7b56\u7565\u7684\u7b97\u6cd5 PPO\u53ef\u4ee5\u5e94\u7528\u4e8e\u5177\u6709\u79bb\u6563\u7684\u6216\u8fde\u7eed\u7684\u52a8\u4f5c\u7a7a\u95f4\u7684\u73af\u5883 PPO\u7684Spinningup\u5b9e\u73b0\u652f\u6301\u4e0eMPI\u5e76\u884c\u5316 PPO\u66f4\u65b0\u7b56\u7565\u516c\u5f0f $$ \\theta_{k+1}=arg\\underset{\\theta}{max}\\underset{s,\\alpha\\sim\\pi\\theta_k}{E}[L(s,\\alpha,\\theta_k,\\theta)] $$ \u901a\u5e38\u91c7\u53d6\u591a\u4e2a\u6b65\u9aa4\uff08\u901a\u5e38\u662f\u5c0f\u6279\u91cf\uff09SGD\u6765\u6700\u5927\u5316\u76ee\u6807 $$ L(s,\\alpha,\\theta_k,\\theta)=min(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)}A^{\\pi\\theta_k}(s,\\alpha),clip(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)},1-\\epsilon,1+\\epsilon)A^{\\pi\\theta_k}(s,\\alpha)) $$ $\\epsilon$\u662f\u4e00\u4e2a\u8d85\u53c2\u6570\u7c97\u7565\u5730\u8bf4\u51fa\u65b0\u653f\u7b56\u88ab\u5141\u8bb8\u4e0e\u65e7\u653f\u7b56\u76f8\u8ddd\u591a\u8fdc \u8fd9\u662f\u4e00\u4e2a\u5f88\u590d\u6742\u7684\u65b9\u7a0b\u5f0f\uff0c\u7b2c\u4e00\u773c\u5f88\u96be\u8bf4\u660e\u767d\u5b83\u5728\u505a\u4ec0\u4e48\uff0c\u6216\u8005\u5b83\u5982\u4f55\u5e2e\u52a9\u65b0\u7b56\u7565\u9760\u8fd1\u65e7\u7b56\u7565\u3002\u4e8b\u5b9e\u8bc1\u660e\u6709\u4e00\u4e2a\u76f8\u5f53\u7b80\u5316\u7684\u7248\u672c\uff0c\u8f83\u5bb9\u6613\u7406\u89e3\uff0c\u4e5f\u662f\u6211\u4eec\u5728\u4ee3\u7801\u4e2d\u5b9e\u73b0\u7684\u7248\u672c $$ L(s,\\alpha,\\theta_k,\\theta) = min\\left(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)}A^{\\pi\\theta_k}(s,\\alpha), g\\left(\\epsilon,A^{\\pi\\theta_k}(s,\\alpha)\\right)\\right) $$ $$ g(\\epsilon,A)=\\left{ \\begin{matrix}(1+\\epsilon)A\\quad A\\geq0&\\\\(1-\\epsilon)A\\quad A<0\\end{matrix} \\right. $$ \u4e3a\u4e86\u5f04\u6e05\u695a\u4ece\u4e2d\u5f97\u5230\u7684\u76f4\u89c9\uff0c\u8ba9\u6211\u770b\u4e00\u4e0b\u5355\u4e2a\u7684 \u72b6\u6001\u2014\u52a8\u4f5c \u5bf9\uff0c\u5e76\u8003\u8651\u6848\u4f8b \u4f18\u52bf\u662f\u79ef\u6781 \u7684\uff0c\u5047\u8bbe\u8be5\u72b6\u6001\u2014\u52a8\u4f5c \u5bf9\u7684\u4f18\u52bf\u4e3a\u6b63\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b83\u5bf9\u76ee\u6807\u7684\u8d21\u732e\u51cf\u5c0f\u4e3a\uff1a $$ L(s,\\alpha,\\theta,\\theta_k)=min\\left(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)},(1+\\epsilon)\\right)A^{\\pi\\theta_k}(s,\\alpha) $$ \u56e0\u4e3a\u4f18\u52bf\u662f\u79ef\u6781\u7684\uff0c\u6240\u4ee5\u5982\u679c\u91c7\u53d6\u884c\u52a8\u7684\u53ef\u80fd\u6027\u66f4\u5927\uff08\u4e5f\u5c31\u662f$\\pi_{\\theta}(\\alpha|s)$\u589e\u52a0\uff09\uff0c\u76ee\u6807\u5c31\u4f1a\u589e\u52a0\u3002\u4f46\u662fterm\u4e2d\u7684min\u9650\u5236\u4e86\u53ef\u4ee5\u589e\u52a0\u591a\u5c11\u76ee\u6807\u3002\u4e00\u65e6$\\pi_\\theta(\\alpha|s)>(1+\\epsilon)\\pi_{\\theta_k}(\\alpha|s)$,\u8fd9\u65f6min\u8e22\u8fdb\uff0cterm\u5230\u8fbe$(1+\\epsilon)A^{\\pi\\theta_k}(s,\\alpha)$,\u56e0\u6b64\u7528\u8fc7\u8fdc\u79bb\u65e7\u653f\u7b56\uff0c\u65b0\u653f\u7b56\u4e0d\u4f1a\u83b7\u76ca\u3002 \u4f18\u52bf\u662f\u8d1f\u9762 \u7684\uff0c\u5047\u8bbe\u8be5\u72b6\u6001\u2014\u52a8\u4f5c \u5bf9\u7684\u4f18\u52bf\u662f\u8d1f\u7684\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b83\u5bf9\u76ee\u6807\u7684\u8d21\u732e\u51cf\u5c0f\u4e3a\uff1a $$ L(s,\\alpha,\\theta,\\theta_k)=max\\left(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)},(1-\\epsilon)\\right)A^{\\pi\\theta_k}(s,\\alpha) $$ \u56e0\u4e3a\u4f18\u52bf\u662f\u8d1f\u7684\uff0c\u6240\u4ee5\u5982\u679c \u91c7\u53d6\u884c\u52a8\u7684\u53ef\u80fd\u6027\u66f4\u5c0f\uff08\u4e5f\u5c31\u662f$\\pi_\\theta(\\alpha|s)$\u51cf\u5c0f\uff09\uff0c\u76ee\u6807\u5c31\u4f1a\u589e\u52a0\u3002\u4f46\u662fterm\u4e2d\u7684max\u9650\u5236\u4e86\u53ef\u4ee5\u589e\u52a0\u591a\u5c11\u76ee\u6807\u3002\u4e00\u65e6$\\pi_\\theta(\\alpha|s)<(1-\\epsilon)\\pi_{\\theta_k}(\\alpha|s)$,\u8fd9\u65f6max\u8e22\u8fdb\uff0cterm\u5230\u8fbe$(1-\\epsilon)A^{\\pi\\theta_k}(s,\\alpha)$,\u56e0\u6b64\uff0c\u518d\u6b21\uff0c\u8fdc\u79bb\u65e7\u653f\u7b56\uff0c\u4e0d\u4f1a\u4f7f\u65b0\u653f\u7b56\u53d7\u76ca\u3002 \u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u901a\u8fc7\u53bb\u9664\u4f7f\u653f\u7b56\u53d1\u751f\u91cd\u5927\u53d8\u5316\u7684\u6fc0\u52b1\uff0c\u88c1\u526a\u5145\u5f53\u7740\u6b63\u5219\u5316\u5668\u3002\u800c\u8d85\u53c2\u6570$\\epsilon$\u5219\u5bf9\u5e94\u4e8e\u65b0\u653f\u7b56\u4e0e\u65e7\u653f\u7b56\u53ef\u4ee5\u6709\u591a\u8fdc\uff0c\u540c\u65f6\u4ecd\u4f7f\u76ee\u6807\u53d7\u76ca\u3002","title":"\u8981\u95fb\u901f\u89c8"},{"location":"ros%20openai/#_3","text":"PPO\u6309\u57fa\u4e8e\u7b56\u7565\u65b9\u5f0f\u8bad\u7ec3\u968f\u673a\u7b56\u7565\uff0c\u8fd9\u610f\u5473\u7740\u5b83\u4f1a\u6839\u636e\u5176\u968f\u673a\u7b56\u7565\u7684\u6700\u65b0\u7248\u672c\u901a\u8fc7\u91c7\u6837\u64cd\u4f5c\u6765\u8fdb\u884c\u63a2\u7d22\u3002\u52a8\u4f5c\u9009\u62e9\u7684\u968f\u673a\u6027\u53d6\u51b3\u4e8e\u521d\u59cb\u6761\u4ef6\u548c\u8bad\u7ec3\u6b65\u9aa4\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u4ee3\u7406\u9010\u6b65\u51cf\u5c11\u968f\u673a\u6027\uff0c\u56e0\u4e3a\u66f4\u65b0\u89c4\u5219\u9f13\u52b1\u5b83\u5229\u7528\u5df2\u7ecf\u53d1\u73b0\u7684\u5956\u52b1\u3002\u8fd9\u53ef\u80fd\u5bfc\u81f4\u7b56\u7565\u9677\u5165\u5c40\u90e8\u6700\u4f18\u72b6\u6001\u3002","title":"\u63a2\u7d22\u4e0e\u53d1\u73b0"},{"location":"ros%20openai/#_4","text":"PPO-CLIP \u8f93\u5165\uff1a\u521d\u59cb\u7b56\u7565\u53c2\u6570$\\theta_0$,\u521d\u59cb\u503c\u529f\u80fd\u53c2\u6570$\\phi_0$ for k = 0, 1, 2, ... do \u5728\u73af\u5883\u4e2d\u901a\u8fc7\u6267\u884c\u7b56\u7565$\\pi_k=\\pi(\\theta_k)$\uff0c\u6536\u96c6\u4e00\u7ec4\u8f68\u8ff9$D_k={\\tau_i}$ \u7b97\u5956\u52b1$\\hat{R}_i$ \u57fa\u4e8e\u5f53\u524d\u7684value function $V_{\\phi k}$,\u8ba1\u7b97advantage estimates $\\hat{A}_t$ \u901a\u8fc7\u6700\u5927\u5316PPO-Clip\u76ee\u6807\uff0c\u66f4\u65b0\u7b56\u7565 $$ \\theta_{k+1}=arg\\ \\underset{\\theta}{max}\\frac{1}{|D_k|T}\\sum_{\\tau\\in D_k}\\sum^T_{t=0}min\\left(\\frac{\\pi_\\theta(\\alpha_t|s_t)}{\\pi_{\\theta_k}(\\alpha_t|s_t)}A^{\\pi\\theta_k}(s_t,\\alpha_t),g(\\epsilon,A^{\\pi\\theta_k}(s_t,\\alpha_t))\\right) $$ \u901a\u5e38\u901a\u8fc7\u4e0eAdam\u4e00\u8d77\u7684\u968f\u673a\u68af\u5ea6\u4e0a\u5347 \u901a\u8fc7\u5747\u65b9\u8bef\u5dee\u56de\u5f52\u62df\u5408value function $$ \\phi_{k+1}=arg\\ \\underset{\\phi}{min}\\frac{1}{|D_k|T}\\sum_{\\tau\\in D_k}\\sum^T_{t=0}(V_\\phi(s_t)-\\hat{R}_t)^2 $$ \u7279\u522b\u662f\u901a\u8fc7\u4e00\u4e9b\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5 end for","title":"\u4f2a\u4ee3\u7801"},{"location":"tf_document/","text":"tf.compat.v1.train.Saver tf.compat.v1.train.Saver \\ Saves and restores\u8fd8\u539f variables tf.compat.v1.train.Saver( var_list=None, reshape=False, sharded=False, max_to_keep=5, keep_checkpoint_every_n_hours=10000.0, name=None, restore_sequentially=False, saver_def=None, builder=None, defer_build=False, allow_empty=False, write_version=tf.train.SaverDef.V2, pad_step_number=False, save_relative_paths=False, filename=None ) The Saver class adds ops\u884c\u52a8 to save and restore variables to and form checkpoints.It also provides convenience method to run these ops. Checkpoints are binary files in a proprietary\u6240\u6709\u6743 format which map variable names to tensor values.Checkpoints \u662f\u4e13\u6709\u683c\u5f0f\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u8be5\u6587\u4ef6\u5c06\u53d8\u91cf\u540d\u6620\u5c04\u5230\u5f20\u91cf\u503c.The best way to examine\u5ba1\u67e5 the contents of a checkpoints is to load it using a Saver var_list A list of Variable/SaveableObject, or a dictionary mapping names to SaveableObjects.If None,defaults to the list of all saveable objects. reshape If True,allows restoring\u6062\u590d parameters from a checkpoint where the variables have a different shape. shareded If True,shard the checkpoints,one per device\u6bcf\u4e2a\u8bbe\u5907\u4e00\u4e2a max_to_keep Maximum number of recent checkpoints to keep,Default to 5 keep_checkpoint_every_n_hours How often to keep checkpoints,Defaults to 10,000 hours. name String, Optional name to use as a prefix when adding operations. restore_sequentially A Bool,which if true,causes restore of different variables to happen sequentially within each device.This can lower memory usage when restoring large models. saver_def Optional SaverDef proto\u539f\u578b to use instead of running the builder.This is only useful for specialty code that wants to recreate a Saver object for a previously built Graph that had a Saver.The saver_def proto should be the one returned by the as_saver_def() call of the Saver that was created for that Graph. builder Optional SaverBuilder to use if a saver_def was not provided. Default to BuikSaverBuilder(). defer_build If True,defer adding the save and restore ops to the build() call. In that case build() should be called before finalizing\u5b9a\u6848 the graph or using the saver. allow_empty If False(defalut) rasie an error if there are no variables in the graph.Otherwise, construct the saver anyway and make it a no-op. write_version controls what format to use when saving checkpoints. It also affects certain filepath matching logic. The V2 format is the recommended choice: it is much more optimized than V1 in terms of memory required and latency incurred during restore. Regardless of this flag, the Saver is able to restore from both V2 and V1 checkpoints. pad_step_number if True, pads the global step number in the checkpoint filepaths to some fixed width (8 by default). This is turned off by default. save_relative_paths If True, will write relative paths to the checkpoint state file. This is needed if the user wants to copy the checkpoint directory and reload from the copied directory. filename If known at graph construction time, filename used for variable loading/saving. tf.compat.v1.variable_scope tf.compat.v1.variable_scope \\ A context manager for define ops that create variables(layers)\u7528\u4e8e\u5b9a\u4e49\u521b\u5efa\u53d8\u91cf\uff08\u5c42\uff09\u7684\u64cd\u4f5c\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668 tf.compat.v1.variable_scope( name_or_scope, default_name=None, values=None, initializer=None, regularizer=None, caching_device=None, partitioner=None, custom_getter=None, reuse=None, dtype=None, use_resource=None, constraint=None, auxiliary_name_scope=True ) This context manager validates\u9a8c\u8bc1 that the values are from the same graph,ensures that graph is the default graph,and pushes a name scope\u8303\u56f4 and a variable scope. Variable scope allows you to create new variables and to share already created ones while providing checks to not create or share by accident.\u53d8\u91cf\u4f5c\u7528\u57df\u5141\u8bb8\u60a8\u521b\u5efa\u65b0\u53d8\u91cf\u5e76\u5171\u4eab\u5df2\u521b\u5efa\u7684\u53d8\u91cf\uff0c\u540c\u65f6\u63d0\u4f9b\u68c0\u67e5\u4ee5\u9632\u6b62\u610f\u5916\u521b\u5efa\u6216\u5171\u4eab. Keep in mind that the counters for default_name are discarded\u4e22\u5f03 once the parent scope is exited. Therefore when the code re-enters the scope (for instance by saving it), all nested\u5d4c\u5957\u7684 default_name counters will be restarted. Note that reuse flag is inherited: if we open a resuing scope,then all its sub-scope become reusing as well. A note about name scoping:Setting reuse does not impact the naming of other ops such as mult. reuse True,None,or tf.compat.v1.AUTO_REUSE;if True, we go into reuse mode for this scope as well as all sub-scopes; if tf.compat.v1.AUTO_REUSE, we create variables if they do not exist, and return them otherwise; if None, we inherit the parent scope's reuse flag. When eager execution is enabled, new variables are always created unless an EagerVariableStore or template is currently active. tf.compat.v1.layers.dense tf.compat.v1.layers.dense \\ Functional interface for the densely-connected layer. tf.compat.v1.layers.dense( inputs, units, activation=None, use_bias=True, kernel_initializer=None, bias_initializer=tf.zeros_initializer(), kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, trainable=True, name=None, reuse=None ) The layer implement the operation: output=activation(inputs * kernel + bias) where activation is the activation function passed as the activation argument(if not \uff2eone ),kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer(only if use_bias is True). inputs Tensor input. units Integer or Long, dimensionality of the output space. activation Activation function(callable), Set it to None to maintain a linear activation. use_bias Boolean, whether the layer uses a bias kernel_initializer Initializer function for the weight matrix. If None (default), weights are initialized using the default initializer used by tf.compat.v1.get_variable. bias_initializer Initializer function for the bias kernel_regularizer Regularizer\u6b63\u5219\u5316\u5668 function for the weight matrix bias_regularizer Regularizer function for the bias. activity_regularizer Regularizer function for the output. kernel_constraint An optional projection function to be applied to the kernel after being updated by an Optimizer (e.g. used to implement norm constraints or value constraints for layer weights). The function must take as input the unprojected variable and must return the projected variable (which must have the same shape). Constraints\u7ea6\u675f\u6761\u4ef6 are not safe to use when doing asynchronous distributed training. bias_constraint An optional projection function to be applied to the bias after being updated by an Optimizer. trainable Boolean, if True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable). name String, the name of the layer. reuse Boolean, whether to reuse the weights of a previous layer by the same name. tf.compat.v1.layers.BatchNormalization tf.compat.v1.layers.BatchNormalization \\ Batch Normalization layer tf.compat.v1.layers.BatchNormalization( axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer=tf.zeros_initializer(), gamma_initializer=tf.ones_initializer(), moving_mean_initializer=tf.zeros_initializer(), moving_variance_initializer=tf.ones_initializer(), beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None, renorm=False, renorm_clipping=None, renorm_momentum=0.99, fused=None, trainable=True, virtual_batch_size=None, adjustment=None, name=None, **kwargs ) fused if None or True, use a faster, fused implementation if possible. If False, use the system recommended implementation. tf.math.reduce_max tf.math.reduce_max \\ Computes the maximum of elements across dimensions of a tensor tf.math.reduce_max( input_tensor, axis=None, keepdims=False, name=None ) Reduces input_tensor along the dimensions given in axis.Unless keepdims is true, the rank of the tensor is reduced by 1 for each of the entires in axis, which must be unique. If keepdims is true, the reduced dimensions are retained with length 1. If axis is None,all dimensions are reduced, and a tensor with a single element is returned. input_tensor The tensor to reduce.Should have real numeric type axis The dimensions to reduce.If None (the default),reduces all dimensions.Must be in range [-rank(input_tensor),rank(input_tensor)] keepdims If true,retains reduced dimensions with length 1. name A name for the operation(optional). tf.tile tf.tile \\ Constructs\u6784\u9020 a tensor by tiling\u5e73\u94fa a given tensor tf.tile( input, multiples, name=None ) This operation creates a new tensor by replicating\u590d\u5236 input multiples times.The output tensor's i'th dimension has input.dims(i)*multiples[i] elements, and the values of input are replicated multiples[i] times along the i'th dimension. input A tensor,1-D or higher multiples A tensor. Must be one of the following types: int32, int64, 1-D Length must be the same as the number of dimensions in input. name A name for the operation(optional). tf.concat tf.concat \\ Concatenates tensors along one dimension. tf.concat( values, axis, name='concat' ) tf.cast tf.cast \\ Cast a tensor to a new type. tf.cast( x, dtype, name=None ) The operation supports data types (for x and dtype) of uint8, uint16, uint32, uint64, int8, int16, int32, int64, float16, float32, float64, complex64, complex128, bfloat16. In case of casting from complex types (complex64, complex128) to real types, only the real part of x is returned. In case of casting from real types to complex types (complex64, complex128), the imaginary part of the returned value is set to 0. The handling of complex types here matches the behavior of numpy. tf.where Return the elements where condition is True (multiplexing x and y). tf.where( condition, x=None, y=None, name=None ) \\","title":"tf_document"},{"location":"tf_document/#tfcompatv1trainsaver","text":"tf.compat.v1.train.Saver \\ Saves and restores\u8fd8\u539f variables tf.compat.v1.train.Saver( var_list=None, reshape=False, sharded=False, max_to_keep=5, keep_checkpoint_every_n_hours=10000.0, name=None, restore_sequentially=False, saver_def=None, builder=None, defer_build=False, allow_empty=False, write_version=tf.train.SaverDef.V2, pad_step_number=False, save_relative_paths=False, filename=None ) The Saver class adds ops\u884c\u52a8 to save and restore variables to and form checkpoints.It also provides convenience method to run these ops. Checkpoints are binary files in a proprietary\u6240\u6709\u6743 format which map variable names to tensor values.Checkpoints \u662f\u4e13\u6709\u683c\u5f0f\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u8be5\u6587\u4ef6\u5c06\u53d8\u91cf\u540d\u6620\u5c04\u5230\u5f20\u91cf\u503c.The best way to examine\u5ba1\u67e5 the contents of a checkpoints is to load it using a Saver var_list A list of Variable/SaveableObject, or a dictionary mapping names to SaveableObjects.If None,defaults to the list of all saveable objects. reshape If True,allows restoring\u6062\u590d parameters from a checkpoint where the variables have a different shape. shareded If True,shard the checkpoints,one per device\u6bcf\u4e2a\u8bbe\u5907\u4e00\u4e2a max_to_keep Maximum number of recent checkpoints to keep,Default to 5 keep_checkpoint_every_n_hours How often to keep checkpoints,Defaults to 10,000 hours. name String, Optional name to use as a prefix when adding operations. restore_sequentially A Bool,which if true,causes restore of different variables to happen sequentially within each device.This can lower memory usage when restoring large models. saver_def Optional SaverDef proto\u539f\u578b to use instead of running the builder.This is only useful for specialty code that wants to recreate a Saver object for a previously built Graph that had a Saver.The saver_def proto should be the one returned by the as_saver_def() call of the Saver that was created for that Graph. builder Optional SaverBuilder to use if a saver_def was not provided. Default to BuikSaverBuilder(). defer_build If True,defer adding the save and restore ops to the build() call. In that case build() should be called before finalizing\u5b9a\u6848 the graph or using the saver. allow_empty If False(defalut) rasie an error if there are no variables in the graph.Otherwise, construct the saver anyway and make it a no-op. write_version controls what format to use when saving checkpoints. It also affects certain filepath matching logic. The V2 format is the recommended choice: it is much more optimized than V1 in terms of memory required and latency incurred during restore. Regardless of this flag, the Saver is able to restore from both V2 and V1 checkpoints. pad_step_number if True, pads the global step number in the checkpoint filepaths to some fixed width (8 by default). This is turned off by default. save_relative_paths If True, will write relative paths to the checkpoint state file. This is needed if the user wants to copy the checkpoint directory and reload from the copied directory. filename If known at graph construction time, filename used for variable loading/saving.","title":"tf.compat.v1.train.Saver"},{"location":"tf_document/#tfcompatv1variable_scope","text":"tf.compat.v1.variable_scope \\ A context manager for define ops that create variables(layers)\u7528\u4e8e\u5b9a\u4e49\u521b\u5efa\u53d8\u91cf\uff08\u5c42\uff09\u7684\u64cd\u4f5c\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668 tf.compat.v1.variable_scope( name_or_scope, default_name=None, values=None, initializer=None, regularizer=None, caching_device=None, partitioner=None, custom_getter=None, reuse=None, dtype=None, use_resource=None, constraint=None, auxiliary_name_scope=True ) This context manager validates\u9a8c\u8bc1 that the values are from the same graph,ensures that graph is the default graph,and pushes a name scope\u8303\u56f4 and a variable scope. Variable scope allows you to create new variables and to share already created ones while providing checks to not create or share by accident.\u53d8\u91cf\u4f5c\u7528\u57df\u5141\u8bb8\u60a8\u521b\u5efa\u65b0\u53d8\u91cf\u5e76\u5171\u4eab\u5df2\u521b\u5efa\u7684\u53d8\u91cf\uff0c\u540c\u65f6\u63d0\u4f9b\u68c0\u67e5\u4ee5\u9632\u6b62\u610f\u5916\u521b\u5efa\u6216\u5171\u4eab. Keep in mind that the counters for default_name are discarded\u4e22\u5f03 once the parent scope is exited. Therefore when the code re-enters the scope (for instance by saving it), all nested\u5d4c\u5957\u7684 default_name counters will be restarted. Note that reuse flag is inherited: if we open a resuing scope,then all its sub-scope become reusing as well. A note about name scoping:Setting reuse does not impact the naming of other ops such as mult. reuse True,None,or tf.compat.v1.AUTO_REUSE;if True, we go into reuse mode for this scope as well as all sub-scopes; if tf.compat.v1.AUTO_REUSE, we create variables if they do not exist, and return them otherwise; if None, we inherit the parent scope's reuse flag. When eager execution is enabled, new variables are always created unless an EagerVariableStore or template is currently active.","title":"tf.compat.v1.variable_scope"},{"location":"tf_document/#tfcompatv1layersdense","text":"tf.compat.v1.layers.dense \\ Functional interface for the densely-connected layer. tf.compat.v1.layers.dense( inputs, units, activation=None, use_bias=True, kernel_initializer=None, bias_initializer=tf.zeros_initializer(), kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, trainable=True, name=None, reuse=None ) The layer implement the operation: output=activation(inputs * kernel + bias) where activation is the activation function passed as the activation argument(if not \uff2eone ),kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer(only if use_bias is True). inputs Tensor input. units Integer or Long, dimensionality of the output space. activation Activation function(callable), Set it to None to maintain a linear activation. use_bias Boolean, whether the layer uses a bias kernel_initializer Initializer function for the weight matrix. If None (default), weights are initialized using the default initializer used by tf.compat.v1.get_variable. bias_initializer Initializer function for the bias kernel_regularizer Regularizer\u6b63\u5219\u5316\u5668 function for the weight matrix bias_regularizer Regularizer function for the bias. activity_regularizer Regularizer function for the output. kernel_constraint An optional projection function to be applied to the kernel after being updated by an Optimizer (e.g. used to implement norm constraints or value constraints for layer weights). The function must take as input the unprojected variable and must return the projected variable (which must have the same shape). Constraints\u7ea6\u675f\u6761\u4ef6 are not safe to use when doing asynchronous distributed training. bias_constraint An optional projection function to be applied to the bias after being updated by an Optimizer. trainable Boolean, if True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable). name String, the name of the layer. reuse Boolean, whether to reuse the weights of a previous layer by the same name.","title":"tf.compat.v1.layers.dense"},{"location":"tf_document/#tfcompatv1layersbatchnormalization","text":"tf.compat.v1.layers.BatchNormalization \\ Batch Normalization layer tf.compat.v1.layers.BatchNormalization( axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer=tf.zeros_initializer(), gamma_initializer=tf.ones_initializer(), moving_mean_initializer=tf.zeros_initializer(), moving_variance_initializer=tf.ones_initializer(), beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None, renorm=False, renorm_clipping=None, renorm_momentum=0.99, fused=None, trainable=True, virtual_batch_size=None, adjustment=None, name=None, **kwargs ) fused if None or True, use a faster, fused implementation if possible. If False, use the system recommended implementation.","title":"tf.compat.v1.layers.BatchNormalization"},{"location":"tf_document/#tfmathreduce_max","text":"tf.math.reduce_max \\ Computes the maximum of elements across dimensions of a tensor tf.math.reduce_max( input_tensor, axis=None, keepdims=False, name=None ) Reduces input_tensor along the dimensions given in axis.Unless keepdims is true, the rank of the tensor is reduced by 1 for each of the entires in axis, which must be unique. If keepdims is true, the reduced dimensions are retained with length 1. If axis is None,all dimensions are reduced, and a tensor with a single element is returned. input_tensor The tensor to reduce.Should have real numeric type axis The dimensions to reduce.If None (the default),reduces all dimensions.Must be in range [-rank(input_tensor),rank(input_tensor)] keepdims If true,retains reduced dimensions with length 1. name A name for the operation(optional).","title":"tf.math.reduce_max"},{"location":"tf_document/#tftile","text":"tf.tile \\ Constructs\u6784\u9020 a tensor by tiling\u5e73\u94fa a given tensor tf.tile( input, multiples, name=None ) This operation creates a new tensor by replicating\u590d\u5236 input multiples times.The output tensor's i'th dimension has input.dims(i)*multiples[i] elements, and the values of input are replicated multiples[i] times along the i'th dimension. input A tensor,1-D or higher multiples A tensor. Must be one of the following types: int32, int64, 1-D Length must be the same as the number of dimensions in input. name A name for the operation(optional).","title":"tf.tile"},{"location":"tf_document/#tfconcat","text":"tf.concat \\ Concatenates tensors along one dimension. tf.concat( values, axis, name='concat' )","title":"tf.concat"},{"location":"tf_document/#tfcast","text":"tf.cast \\ Cast a tensor to a new type. tf.cast( x, dtype, name=None ) The operation supports data types (for x and dtype) of uint8, uint16, uint32, uint64, int8, int16, int32, int64, float16, float32, float64, complex64, complex128, bfloat16. In case of casting from complex types (complex64, complex128) to real types, only the real part of x is returned. In case of casting from real types to complex types (complex64, complex128), the imaginary part of the returned value is set to 0. The handling of complex types here matches the behavior of numpy.","title":"tf.cast"},{"location":"tf_document/#tfwhere","text":"Return the elements where condition is True (multiplexing x and y). tf.where( condition, x=None, y=None, name=None ) \\","title":"tf.where"},{"location":"ubuntu18/","text":"\u66f4\u6539mac\u5730\u5740 \u4f7f\u7528ifconfig\u3000\u67e5\u770b\u7f51\u53e3 \u5b89\u88c5macmanager sudo apt install macchanger \u968f\u673a\u751f\u6210\u4e00\u4e2amac\u5730\u5740 sudo macchanger -r enp60s0 enp60s0 \u662f\u7f51\u53e3\uff0c -r \u4ee3\u8868\u7684\u662f\u968f\u673a random \u7684\u610f\u601d\uff0c macchanger \u4f1a\u5e2e\u6211\u4eec\u4fee\u6539\u6210\u4e00\u4e2a\u968f\u673a\u4ea7\u751f\u7684 MAC \u53f7 \u4fee\u6539\u4e3a\u6307\u5b9a\u7684mac\u5730\u5740 sudo macchanger -m AA:BB:CC:DD:EE:FF enp60s0 \\ ubuntu18.04 \u7f51\u53e3\u521b\u5efa\u7f51\u7edc\u5171\u4eab \u7ec8\u7aef\u8f93\u5165nm-connection-editor\u6253\u5f00\u7f51\u7edc\u8fde\u63a5 \u521b\u5efa\u4ee5\u592a\u7f51\u94fe\u63a5 \u914d\u7f6e\u7f51\u7edc\u94fe\u63a5 \u5c06\u5176\u4ed6\u9700\u8981\u4e0a\u7f51\u7684\u8bbe\u5907\u901a\u8fc7\u7f51\u7ebf\u94fe\u63a5\u5230\u5171\u4eab\u7f51\u7edc\u5373\u53ef xvfb\u3000ssh xvfb-run -s \"-screen 0 1400x900x24\" python <your_script.py> x forward ssh -X username@ip vscode \u63d2\u4ef6\u3000Remote-ssh conda install jupyter github\u4e0b\u8f7d\u63d0\u901f git clone https://github.com/Amritpal-001/Reinforcement-learning-projects.git \u6539\u4e3a git clone http://hub.fastgit.org/Amritpal-001/Reinforcement-learning-projects.git github \u4e0b\u8f7d\u5de5\u5177 https://d.serctl.com/ https://www.python.org/ftp \u4e0b\u8f7d\u6162 wget https://www.python.org/ftp/python/3.7.5/Python-3.7.5.tgz \u6539\u4e3a wget https://npm.taobao.org/mirrors/python/3.7.5/Python-3.7.5.tgz pip \u4e0b\u8f7d\u4e1c\u897f\u6162 pip3.7.5 install -i http://mirrors.aliyun.com/pypi/simple/ psutil decorator numpy protobuf==3.11^C scipy sympy cffi grpcio grpcio-tools requests --user --trusted-host mirrors.aliyun.com mkdocs mkdocs build mkdocs serve push sites folders vscode python ros debug \u9996\u5148 catkin_make -DCMAKE_BUILD_TYPE=DEBUG \u5176\u6b21\u70b9\u51fbdebug\u6309\u94ae\uff0c\u9009\u62e9\u751f\u6210\u65b0\u7684launch\u6587\u4ef6\u3002 \u7136\u540e\u70b9\u51fb show , \uff44ebug\u65f6\u9047\u5230\u8def\u5f84\u95ee\u9898,\u4f8b\u5982rosmsg\u8def\u5f84\uff0c\u6700\u597d\u5728\u6587\u4ef6\u5c5e\u6027\u91cc\u590d\u5236\u8def\u5f84\uff0c\u4e0d\u7136\u5bb9\u6613\u51fa\u9519 import sys sys.path.append(\"/home/pmjd/Downloads/catkin_ws/devel/lib/python2.7/dist-packages\") Tilix doesnt open in the folder from where it is split Update ~.bashrc (or ~.zshrc if you are using zsh) to execute vte.sh directly, this involves adding the following line at the end of the file. if [[ $TILIX_ID ]]; then source /etc/profile.d/vte.sh fi On Ubuntu (18.04), a symlink is probably missing. You can create it with: ln -s /etc/profile.d/vte-2.91.sh /etc/profile.d/vte.sh sudo dist-upgrade \u662f\u6bc1\u706d\u6027\u7684\uff0c\u4f1a\u5347\u7ea7cuda github init git config --global user.email \"you@example.com\" git config --global user.name \"Your Name\" git add . git commit -m \"1\" git push \u547d\u4ee4\u884c\u6253\u5f00\u6587\u4ef6\u7ba1\u7406\u5668 nautilus --browser ~/\u6587\u6863 \u5b89\u88c5\u6c49\u8bed\u8f93\u5165\u6cd5 \u5148\u5728\u8bbe\u7f6e\u91cc\u8bbe\u7f6elanguage\u91cc\u6dfb\u52a0Chinese sudo apt-get install ibus-libpinyin ibus-clutter Docker\u521b\u5efa\u5bb9\u5668 \u6784\u5efaubuntu18.04\u6620\u50cf \u5b89\u88c5\u4f9d\u8d56 sudo apt-get install debootstrap sudo apt install docker.io sudo chmod 666 /var/run/docker.sock \u521b\u5efaubuntu 18.04 \u955c\u50cf sudo debootstrap bionic bionic > /dev/null sudo tar -C bionic -c . | docker import - bionic/smart_eye \u6d4b\u8bd5 docker run bionic cat /etc/lsb-release \u67e5\u770b\u955c\u50cf docker images \u5220\u9664\u955c\u50cf docker rmi [IMAGE ID] \u67e5\u770b\u5bb9\u5668\u8fd0\u884c\u60c5\u51b5 sudo docker ps -a \u9000\u51fa\u5bb9\u5668 sudo docker stop \u5bb9\u5668id \u5220\u9664\u5bb9\u5668 sudo docker rm \u5bb9\u5668id \u542f\u52a8\u955c\u50cf docker run -it bionic/smart_eye /bin/bash \u9000\u51fa\u955c\u50cf exit \u6587\u4ef6\u4f20\u9012 \u4ece\u672c\u5730\u81f3docker docker cp FILE_PATH \u5bb9\u5668ID:/root \u4ecedocker \u81f3\u672c\u5730 docker cp \u5bb9\u5668ID:/root/data.tar /home/user \u66f4\u6539docker image\u5b58\u653e\u8def\u5f84 sudo service docker stop sudo touch /etc/docker/daemon.json daemon.json { \"data-root\":\"/home/pmjd/docker\" } sudo service docker start Docker\u66f4\u65b0apt source.list sudo nano /etc/apt/sources.list \u6dfb\u52a0 deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse Docker \u542f\u52a8\u5bb9\u5668 docker start \u5bb9\u5668ID docker attach \u5bb9\u5668ID \u5bfc\u51fa\u5bb9\u5668\u5feb\u7167 docker export c91c33f28594 > smart_eye_docker.tar \u5bb9\u5668\u5feb\u7167\u5bfc\u5165\u4e3a\u955c\u50cf cat smart_eye_docker.tar | docker import - test/smart_eye:v1 \u518d\u4fdd\u5b58\u6b64\u955c\u50cf docker save -o smart_eye_image_docker.tar test/smart_eye \u518d\u52a0\u8f7d\u955c\u50cf docker load --input samrt_eye_image_docker.tar \u542f\u52a8 docker run -it test/smart_eye:v1 /bin/bash docker \u542f\u52a8bash docker start c91c33f28594 docker exec -it c91c33f28594 /home/run.sh docker exec mycontainer /bin/sh -c \"cmd1;cmd2;...;cmdn\" docker bash file example #!/bin/sh docker run -it --net host --add-host in_release_docker:127.0.0.1 --add-host localhost:127.0.0.1 --hostname in_release_docker --rm promote/smart_eye:v1 /bin/bash -c \"/home/run.sh\" \u542f\u52a8\u5feb\u6377\u65b9\u5f0f 1.desktop [Desktop Entry] Name=Smart_eye GenericName=3D modeler Keywords=3d;cg;modeling;animation;painting;sculpting;texturing;video editing;video tracking;rendering;render engine;cycles;game engine;python; Exec=/bin/bash -c '/home/promote/run.sh' #Icon=/home/pmjd/Disk/blender-2.90.1-linux64/blender.svg Terminal=true Type=Application Categories=Graphics;3DGraphics; MimeType=application/x-blender; \u754c\u9762\u663e\u793a xhost + -e DISPLAY=${DISPLAY} -v /tmp/.X11-unix:/tmp/.X11-unix \u5b89\u88c5NVIDIA Container Toolkit distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\ && curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \\ && curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list curl -s -L https://nvidia.github.io/nvidia-container-runtime/experimental/$distribution/nvidia-container-runtime.list | sudo tee /etc/apt/sources.list.d/nvidia-container-runtime.list sudo apt-get update sudo apt-get install -y nvidia-docker2 \u8fd9\u4e2a\u65f6\u5019 /etc/docker/daemon.json \u5185\u5bb9\u4fee\u6539\u4e3a { \"runtimes\": { \"nvidia\": { \"path\": \"nvidia-container-runtime\", \"runtimeArgs\": [] } }, \"data-root\":\"/home/pmjd/docker\" } \u91cd\u542fdocker sudo systemctl restart docker \u4e0b\u8f7d\u8fd0\u884c\uff1a sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi sudo docker run --rm --gpus all nvidia/cuda:10.1-base nvidia-smi \u4e0b\u8f7d\u8fd0\u884ctensorflow: docker run --gpus all --runtime=nvidia -it tensorflow/tensorflow:2.3.0-gpu bash docker\u914d\u7f6e\u7f51\u7edc --net=host docker \u652f\u6301\u9ea6\u514b,\u6c49\u8bed docker run -it --volume=/run/user/1000/pulse:/run/user/1000/pulse --user promote --gpus all --runtime=nvidia --name=xiaomeng -e LANG=C.UTF-8 --device /dev/snd promote/xiaomeng:v1.6 /bin/bash bash \u767b\u5165ssh #!/usr/bin/expect set timeout 3 spawn ssh -X promote@192.168.1.2 expect \"*password*\" {send \"123456\\r\"} expect \"$ \" { send \"bash /home/username/1.sh\\r\" } interact Nano \u5220\u9664\u884c ctrl+k Nano\u663e\u793a\u884c\u53f7 alt+shift+3 \u4fee\u6539\u9ed8\u8ba4python\u7248\u672c \u5220\u9664/usr/bin \u4e0b\u7684Python\u94fe\u63a5 sudo rm /usr/bin/python \u7528\u4e0b\u9762\u547d\u4ee4\u5efa\u7acb\u65b0\u7684\u94fe\u63a5 sudo ln -s /usr/bin/python3.6 /usr/bin/python \u7528\u4e0b\u9762\u7684\u547d\u4ee4\u8fd8\u539f2\u7248\u672c sudo ln -s /usr/bin/python2.7 /usr/bin/python \u66f4\u79d1\u5b66\u7684\u505a\u6cd5\u662f\uff1a sudo update-alternatives --install /usr/bin/python python /usr/bin/python2.7 1 sudo update-alternatives --install /usr/bin/python python /usr/bin/python3.6 2 sudo update-alternatives --config python \u9009\u62e9\u8981\u4f7f\u7528\u7684\u7248\u672c\uff0c\u56de\u8f66\uff0c\u641e\u5b9a \u8bbe\u7f6e\u9ed8\u8ba4pip\u7248\u672c sudo update-alternatives --install /usr/bin/pip pip /usr/bin/pip2 1 sudo update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 2 sudo update-alternatives --config pip pip\u5347\u7ea7 sudo pip install --upgrade pip \u684c\u9762\u5feb\u6377\u65b9\u5f0f [Desktop Entry] Name=smart_view GenericName=3D modeler Keywords=python; Exec=/bin/bash -c 'source /opt/ros/melodic/setup.bash;rosrun image_view image_view image:=/smart_eye_view' #Icon=/home/pmjd/Disk/blender-2.90.1-linux64/blender.svg Terminal=false Type=Application Categories=Graphics;3DGraphics; MimeType=application/x-blender; Name[en_US]=smart_view md\u516c\u5f0f\u91cc\u6dfb\u52a0\u7a7a\u683c \\quad \u6216\u8005 \\+\u7a7a\u683c \u6216\u8005 &nbsp md\u6dfb\u52a0\u591a\u884c\u516c\u5f0f ![](https://latex.codecogs.com/svg.latex?\\Large&space;\\theta_{k+1}=arg\\quad\\underset{\\theta}{max}) <img src=\"https://latex.codecogs.com/svg.image?\\&space;\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}=K^{-1}_{3\\times 3}\\begin{bmatrix}z\\cdot(x_c+\\delta_{x_c})\\\\z\\cdot(y_c+\\delta_{y_c})\\\\z\\end{bmatrix}\"/> \\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix} =K^{-1}_{3\\times 3} \\begin{bmatrix}z\\cdot(x_c+\\delta_{x_c})\\\\z\\cdot(y_c+\\delta_{y_c})\\\\z\\end{bmatrix} \"/> md\u6dfb\u52a0\u884c\u5185\u516c\u5f0f $\\theta$ $\\theta$ mkdocs \u663e\u793a\u516c\u5f0f pip install https://github.com/mitya57/python-markdown-math/archive/master.zip \u5728 config.yaml \u4e2d\u6dfb\u52a0 extra_javascript: - https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML - mathjaxhelper.js markdown_extensions: - mdx_math \u5728docs\u6587\u4ef6\u5939\u4e0b\u65b0\u5efa mathjaxhelper.js MathJax.Hub.Config({ \"tex2jax\": { inlineMath: [ [ '$', '$' ] ] } }); MathJax.Hub.Config({ config: [\"MMLorHTML.js\"], jax: [\"input/TeX\", \"output/HTML-CSS\", \"output/NativeMML\"], extensions: [\"MathMenu.js\", \"MathZoom.js\"] }); \u8fd9\u6837\u5c31\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 $\u516c\u5f0f$ \u548c $$\u516c\u5f0f$$ wget \u65ad\u7f51\u91cd\u8fde wget -t 0 -c < \u6587\u4ef6\u5730\u5740> -c \u8868\u793a\u65ad\u70b9\u7eed\u8fde -t \u8868\u793a \u65ad\u4e86\u4f1a\u6bcf\u7ecf\u8fc7\u51e0\u79d2\u91cd\u65b0\u8fde\u63a5\u8fde\u63a5\u4e00\u6b21\uff0c0\u8868\u793a\u4e00\u76f4\u4e0d\u65ad\u91cd\u8fde\uff0c\u6709\u6700\u5927\u8fde\u63a5\u6b21\u6570\u7684\u3002 \u6ce8\u610f\uff1a \u8bb0\u5f97\u5728\u539f\u6765\u7684\u76ee\u5f55\u4e0b\u6267\u884c\u8fd9\u4e2a\u547d\u4ee4\uff0c\u624d\u4f1a\u63a5\u7740\u4e0a\u6587\u4e0b\u8f7d\uff0c\u627e\u4e0d\u5230\u6587\u4ef6\u76f4\u63a5\u91cd\u65b0\u4e0b\u8f7d \u6307\u4ee4\u67e5\u770b\u786c\u76d8\u5b58\u50a8\u60c5\u51b5 df -h \u67e5\u770b\u6240\u6709\u786c\u76d8 lsblk \u6302\u8f7d\u786c\u76d8 sudo fdisk -l #\u67e5\u770b\u78c1\u76d8\u4fe1\u606f sudo blkid #\u67e5\u770b\u5206\u533a mkdir NeDisk # \u521b\u5efa\u6302\u8f7d\u70b9 df -kh #\u67e5\u770b\u5df2\u6709\u6302\u8f7d\uff0c\u786e\u5b9a\u662f\u5426\u5df2\u6302\u8f7d sudo umount /dev/sda5 #\u5378\u8f7d\u5df2\u6302\u8f7d sudo nano /etc/fstab sudo blkid /dev/sda5 # \u627e\u5230\u5176UUID #\u7136\u540e,\u6211\u4eec\u6309\u7167/etc/fstab\u6587\u4ef6\u4e2d\u7684\u683c\u5f0f\u6dfb\u52a0\u4e00\u884c\u5982\u4e0b\u5185\u5bb9: UUID=0001D3CE0001E53B /home/ubuntu/NewDisk ntfs defaults 0 2 #\u5176\u4e2d\u7b2c\u4e00\u5217\u4e3aUUID, \u7b2c\u4e8c\u5217\u4e3a\u6302\u8f7d\u76ee\u5f55\uff08\u8be5\u76ee\u5f55\u5fc5\u987b\u4e3a\u7a7a\u76ee\u5f55\uff09\uff0c\u7b2c\u4e09\u5217\u4e3a\u6587\u4ef6\u7cfb\u7edf\u7c7b\u578b\uff0c\u7b2c\u56db\u5217\u4e3a\u53c2\u6570\uff0c\u7b2c\u4e94\u52170\u8868\u793a\u4e0d\u5907\u4efd\uff0c\u6700\u540e\u4e00\u5217\u5fc5\u987b\u4e3a\uff12\u62160(\u9664\u975e\u5f15\u5bfc\u5206\u533a\u4e3a1) sudo mount -a \u67e5\u770b\u6587\u4ef6\u5939\u7684\u5927\u5c0f du * -sh ubuntu vscode \u7ec8\u7aef\u7a7a\u767d File -> Preferences -> Setting -> Features -> Terminal -> Inherit Env ubuntu \u67e5\u770b\u663e\u5361 lspci -vnn |grep VGA -A 12 vscode python debug \u5e26\u53c2 { \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"Python: Current File\", \"type\": \"python\", \"request\": \"launch\", \"program\": \"${file}\", \"console\": \"integratedTerminal\", \"args\": [ \"--evaluate\",\"/home/promote/NeDisk/radar_depth/pretrained/resnet18_multistage.pth.tar\", \"--data\",\"nuscenes\" ] } ] } \u6dfb\u52a0\"open in code\" \u53f3\u952e\u5feb\u6377\u65b9\u5f0f wget -qO- https://raw.githubusercontent.com/cra0zy/code-nautilus/master/install.sh | bash","title":"Ubuntu18"},{"location":"ubuntu18/#mac","text":"\u4f7f\u7528ifconfig\u3000\u67e5\u770b\u7f51\u53e3 \u5b89\u88c5macmanager sudo apt install macchanger \u968f\u673a\u751f\u6210\u4e00\u4e2amac\u5730\u5740 sudo macchanger -r enp60s0 enp60s0 \u662f\u7f51\u53e3\uff0c -r \u4ee3\u8868\u7684\u662f\u968f\u673a random \u7684\u610f\u601d\uff0c macchanger \u4f1a\u5e2e\u6211\u4eec\u4fee\u6539\u6210\u4e00\u4e2a\u968f\u673a\u4ea7\u751f\u7684 MAC \u53f7 \u4fee\u6539\u4e3a\u6307\u5b9a\u7684mac\u5730\u5740 sudo macchanger -m AA:BB:CC:DD:EE:FF enp60s0 \\","title":"\u66f4\u6539mac\u5730\u5740"},{"location":"ubuntu18/#ubuntu1804","text":"\u7ec8\u7aef\u8f93\u5165nm-connection-editor\u6253\u5f00\u7f51\u7edc\u8fde\u63a5 \u521b\u5efa\u4ee5\u592a\u7f51\u94fe\u63a5 \u914d\u7f6e\u7f51\u7edc\u94fe\u63a5 \u5c06\u5176\u4ed6\u9700\u8981\u4e0a\u7f51\u7684\u8bbe\u5907\u901a\u8fc7\u7f51\u7ebf\u94fe\u63a5\u5230\u5171\u4eab\u7f51\u7edc\u5373\u53ef","title":"ubuntu18.04 \u7f51\u53e3\u521b\u5efa\u7f51\u7edc\u5171\u4eab"},{"location":"ubuntu18/#xvfb-ssh","text":"xvfb-run -s \"-screen 0 1400x900x24\" python <your_script.py>","title":"xvfb\u3000ssh"},{"location":"ubuntu18/#x-forward","text":"ssh -X username@ip","title":"x forward"},{"location":"ubuntu18/#vscode-remote-ssh","text":"conda install jupyter","title":"vscode \u63d2\u4ef6\u3000Remote-ssh"},{"location":"ubuntu18/#github","text":"git clone https://github.com/Amritpal-001/Reinforcement-learning-projects.git \u6539\u4e3a git clone http://hub.fastgit.org/Amritpal-001/Reinforcement-learning-projects.git","title":"github\u4e0b\u8f7d\u63d0\u901f"},{"location":"ubuntu18/#github_1","text":"https://d.serctl.com/","title":"github \u4e0b\u8f7d\u5de5\u5177"},{"location":"ubuntu18/#httpswwwpythonorgftp","text":"wget https://www.python.org/ftp/python/3.7.5/Python-3.7.5.tgz \u6539\u4e3a wget https://npm.taobao.org/mirrors/python/3.7.5/Python-3.7.5.tgz","title":"https://www.python.org/ftp \u4e0b\u8f7d\u6162"},{"location":"ubuntu18/#pip","text":"pip3.7.5 install -i http://mirrors.aliyun.com/pypi/simple/ psutil decorator numpy protobuf==3.11^C scipy sympy cffi grpcio grpcio-tools requests --user --trusted-host mirrors.aliyun.com","title":"pip \u4e0b\u8f7d\u4e1c\u897f\u6162"},{"location":"ubuntu18/#mkdocs","text":"mkdocs build mkdocs serve push sites folders","title":"mkdocs"},{"location":"ubuntu18/#vscode-python-ros-debug","text":"\u9996\u5148 catkin_make -DCMAKE_BUILD_TYPE=DEBUG \u5176\u6b21\u70b9\u51fbdebug\u6309\u94ae\uff0c\u9009\u62e9\u751f\u6210\u65b0\u7684launch\u6587\u4ef6\u3002 \u7136\u540e\u70b9\u51fb show , \uff44ebug\u65f6\u9047\u5230\u8def\u5f84\u95ee\u9898,\u4f8b\u5982rosmsg\u8def\u5f84\uff0c\u6700\u597d\u5728\u6587\u4ef6\u5c5e\u6027\u91cc\u590d\u5236\u8def\u5f84\uff0c\u4e0d\u7136\u5bb9\u6613\u51fa\u9519 import sys sys.path.append(\"/home/pmjd/Downloads/catkin_ws/devel/lib/python2.7/dist-packages\")","title":"vscode python ros debug"},{"location":"ubuntu18/#tilix-doesnt-open-in-the-folder-from-where-it-is-split","text":"Update ~.bashrc (or ~.zshrc if you are using zsh) to execute vte.sh directly, this involves adding the following line at the end of the file. if [[ $TILIX_ID ]]; then source /etc/profile.d/vte.sh fi On Ubuntu (18.04), a symlink is probably missing. You can create it with: ln -s /etc/profile.d/vte-2.91.sh /etc/profile.d/vte.sh","title":"Tilix doesnt open in the folder from where it is split"},{"location":"ubuntu18/#sudo-dist-upgrade-cuda","text":"","title":"sudo dist-upgrade \u662f\u6bc1\u706d\u6027\u7684\uff0c\u4f1a\u5347\u7ea7cuda"},{"location":"ubuntu18/#github-init","text":"git config --global user.email \"you@example.com\" git config --global user.name \"Your Name\" git add . git commit -m \"1\" git push","title":"github init"},{"location":"ubuntu18/#_1","text":"nautilus --browser ~/\u6587\u6863","title":"\u547d\u4ee4\u884c\u6253\u5f00\u6587\u4ef6\u7ba1\u7406\u5668"},{"location":"ubuntu18/#_2","text":"\u5148\u5728\u8bbe\u7f6e\u91cc\u8bbe\u7f6elanguage\u91cc\u6dfb\u52a0Chinese sudo apt-get install ibus-libpinyin ibus-clutter","title":"\u5b89\u88c5\u6c49\u8bed\u8f93\u5165\u6cd5"},{"location":"ubuntu18/#docker","text":"","title":"Docker\u521b\u5efa\u5bb9\u5668"},{"location":"ubuntu18/#ubuntu1804_1","text":"","title":"\u6784\u5efaubuntu18.04\u6620\u50cf"},{"location":"ubuntu18/#_3","text":"sudo apt-get install debootstrap sudo apt install docker.io sudo chmod 666 /var/run/docker.sock","title":"\u5b89\u88c5\u4f9d\u8d56"},{"location":"ubuntu18/#ubuntu-1804","text":"sudo debootstrap bionic bionic > /dev/null sudo tar -C bionic -c . | docker import - bionic/smart_eye","title":"\u521b\u5efaubuntu 18.04 \u955c\u50cf"},{"location":"ubuntu18/#_4","text":"docker run bionic cat /etc/lsb-release \u67e5\u770b\u955c\u50cf docker images \u5220\u9664\u955c\u50cf docker rmi [IMAGE ID] \u67e5\u770b\u5bb9\u5668\u8fd0\u884c\u60c5\u51b5 sudo docker ps -a \u9000\u51fa\u5bb9\u5668 sudo docker stop \u5bb9\u5668id \u5220\u9664\u5bb9\u5668 sudo docker rm \u5bb9\u5668id \u542f\u52a8\u955c\u50cf docker run -it bionic/smart_eye /bin/bash \u9000\u51fa\u955c\u50cf exit \u6587\u4ef6\u4f20\u9012 \u4ece\u672c\u5730\u81f3docker docker cp FILE_PATH \u5bb9\u5668ID:/root \u4ecedocker \u81f3\u672c\u5730 docker cp \u5bb9\u5668ID:/root/data.tar /home/user","title":"\u6d4b\u8bd5"},{"location":"ubuntu18/#docker-image","text":"sudo service docker stop sudo touch /etc/docker/daemon.json daemon.json { \"data-root\":\"/home/pmjd/docker\" } sudo service docker start","title":"\u66f4\u6539docker image\u5b58\u653e\u8def\u5f84"},{"location":"ubuntu18/#dockerapt-sourcelist","text":"sudo nano /etc/apt/sources.list \u6dfb\u52a0 deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse","title":"Docker\u66f4\u65b0apt source.list"},{"location":"ubuntu18/#docker_1","text":"docker start \u5bb9\u5668ID docker attach \u5bb9\u5668ID \u5bfc\u51fa\u5bb9\u5668\u5feb\u7167 docker export c91c33f28594 > smart_eye_docker.tar \u5bb9\u5668\u5feb\u7167\u5bfc\u5165\u4e3a\u955c\u50cf cat smart_eye_docker.tar | docker import - test/smart_eye:v1 \u518d\u4fdd\u5b58\u6b64\u955c\u50cf docker save -o smart_eye_image_docker.tar test/smart_eye \u518d\u52a0\u8f7d\u955c\u50cf docker load --input samrt_eye_image_docker.tar \u542f\u52a8 docker run -it test/smart_eye:v1 /bin/bash docker \u542f\u52a8bash docker start c91c33f28594 docker exec -it c91c33f28594 /home/run.sh docker exec mycontainer /bin/sh -c \"cmd1;cmd2;...;cmdn\" docker bash file example #!/bin/sh docker run -it --net host --add-host in_release_docker:127.0.0.1 --add-host localhost:127.0.0.1 --hostname in_release_docker --rm promote/smart_eye:v1 /bin/bash -c \"/home/run.sh\" \u542f\u52a8\u5feb\u6377\u65b9\u5f0f 1.desktop [Desktop Entry] Name=Smart_eye GenericName=3D modeler Keywords=3d;cg;modeling;animation;painting;sculpting;texturing;video editing;video tracking;rendering;render engine;cycles;game engine;python; Exec=/bin/bash -c '/home/promote/run.sh' #Icon=/home/pmjd/Disk/blender-2.90.1-linux64/blender.svg Terminal=true Type=Application Categories=Graphics;3DGraphics; MimeType=application/x-blender; \u754c\u9762\u663e\u793a xhost + -e DISPLAY=${DISPLAY} -v /tmp/.X11-unix:/tmp/.X11-unix","title":"Docker \u542f\u52a8\u5bb9\u5668"},{"location":"ubuntu18/#nvidia-container-toolkit","text":"distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\ && curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \\ && curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list curl -s -L https://nvidia.github.io/nvidia-container-runtime/experimental/$distribution/nvidia-container-runtime.list | sudo tee /etc/apt/sources.list.d/nvidia-container-runtime.list sudo apt-get update sudo apt-get install -y nvidia-docker2 \u8fd9\u4e2a\u65f6\u5019 /etc/docker/daemon.json \u5185\u5bb9\u4fee\u6539\u4e3a { \"runtimes\": { \"nvidia\": { \"path\": \"nvidia-container-runtime\", \"runtimeArgs\": [] } }, \"data-root\":\"/home/pmjd/docker\" } \u91cd\u542fdocker sudo systemctl restart docker \u4e0b\u8f7d\u8fd0\u884c\uff1a sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi sudo docker run --rm --gpus all nvidia/cuda:10.1-base nvidia-smi \u4e0b\u8f7d\u8fd0\u884ctensorflow: docker run --gpus all --runtime=nvidia -it tensorflow/tensorflow:2.3.0-gpu bash docker\u914d\u7f6e\u7f51\u7edc --net=host docker \u652f\u6301\u9ea6\u514b,\u6c49\u8bed docker run -it --volume=/run/user/1000/pulse:/run/user/1000/pulse --user promote --gpus all --runtime=nvidia --name=xiaomeng -e LANG=C.UTF-8 --device /dev/snd promote/xiaomeng:v1.6 /bin/bash","title":"\u5b89\u88c5NVIDIA Container Toolkit"},{"location":"ubuntu18/#bash-ssh","text":"#!/usr/bin/expect set timeout 3 spawn ssh -X promote@192.168.1.2 expect \"*password*\" {send \"123456\\r\"} expect \"$ \" { send \"bash /home/username/1.sh\\r\" } interact","title":"bash \u767b\u5165ssh"},{"location":"ubuntu18/#nano","text":"ctrl+k Nano\u663e\u793a\u884c\u53f7 alt+shift+3","title":"Nano \u5220\u9664\u884c"},{"location":"ubuntu18/#python","text":"\u5220\u9664/usr/bin \u4e0b\u7684Python\u94fe\u63a5 sudo rm /usr/bin/python \u7528\u4e0b\u9762\u547d\u4ee4\u5efa\u7acb\u65b0\u7684\u94fe\u63a5 sudo ln -s /usr/bin/python3.6 /usr/bin/python \u7528\u4e0b\u9762\u7684\u547d\u4ee4\u8fd8\u539f2\u7248\u672c sudo ln -s /usr/bin/python2.7 /usr/bin/python \u66f4\u79d1\u5b66\u7684\u505a\u6cd5\u662f\uff1a sudo update-alternatives --install /usr/bin/python python /usr/bin/python2.7 1 sudo update-alternatives --install /usr/bin/python python /usr/bin/python3.6 2 sudo update-alternatives --config python \u9009\u62e9\u8981\u4f7f\u7528\u7684\u7248\u672c\uff0c\u56de\u8f66\uff0c\u641e\u5b9a","title":"\u4fee\u6539\u9ed8\u8ba4python\u7248\u672c"},{"location":"ubuntu18/#pip_1","text":"sudo update-alternatives --install /usr/bin/pip pip /usr/bin/pip2 1 sudo update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 2 sudo update-alternatives --config pip","title":"\u8bbe\u7f6e\u9ed8\u8ba4pip\u7248\u672c"},{"location":"ubuntu18/#pip_2","text":"sudo pip install --upgrade pip","title":"pip\u5347\u7ea7"},{"location":"ubuntu18/#_5","text":"[Desktop Entry] Name=smart_view GenericName=3D modeler Keywords=python; Exec=/bin/bash -c 'source /opt/ros/melodic/setup.bash;rosrun image_view image_view image:=/smart_eye_view' #Icon=/home/pmjd/Disk/blender-2.90.1-linux64/blender.svg Terminal=false Type=Application Categories=Graphics;3DGraphics; MimeType=application/x-blender; Name[en_US]=smart_view","title":"\u684c\u9762\u5feb\u6377\u65b9\u5f0f"},{"location":"ubuntu18/#md","text":"\\quad \u6216\u8005 \\+\u7a7a\u683c \u6216\u8005 &nbsp","title":"md\u516c\u5f0f\u91cc\u6dfb\u52a0\u7a7a\u683c"},{"location":"ubuntu18/#md_1","text":"![](https://latex.codecogs.com/svg.latex?\\Large&space;\\theta_{k+1}=arg\\quad\\underset{\\theta}{max}) <img src=\"https://latex.codecogs.com/svg.image?\\&space;\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}=K^{-1}_{3\\times 3}\\begin{bmatrix}z\\cdot(x_c+\\delta_{x_c})\\\\z\\cdot(y_c+\\delta_{y_c})\\\\z\\end{bmatrix}\"/> \\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix} =K^{-1}_{3\\times 3} \\begin{bmatrix}z\\cdot(x_c+\\delta_{x_c})\\\\z\\cdot(y_c+\\delta_{y_c})\\\\z\\end{bmatrix} \"/>","title":"md\u6dfb\u52a0\u591a\u884c\u516c\u5f0f"},{"location":"ubuntu18/#md_2","text":"$\\theta$ $\\theta$","title":"md\u6dfb\u52a0\u884c\u5185\u516c\u5f0f"},{"location":"ubuntu18/#mkdocs_1","text":"pip install https://github.com/mitya57/python-markdown-math/archive/master.zip \u5728 config.yaml \u4e2d\u6dfb\u52a0 extra_javascript: - https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML - mathjaxhelper.js markdown_extensions: - mdx_math \u5728docs\u6587\u4ef6\u5939\u4e0b\u65b0\u5efa mathjaxhelper.js MathJax.Hub.Config({ \"tex2jax\": { inlineMath: [ [ '$', '$' ] ] } }); MathJax.Hub.Config({ config: [\"MMLorHTML.js\"], jax: [\"input/TeX\", \"output/HTML-CSS\", \"output/NativeMML\"], extensions: [\"MathMenu.js\", \"MathZoom.js\"] }); \u8fd9\u6837\u5c31\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 $\u516c\u5f0f$ \u548c $$\u516c\u5f0f$$","title":"mkdocs \u663e\u793a\u516c\u5f0f"},{"location":"ubuntu18/#wget","text":"wget -t 0 -c < \u6587\u4ef6\u5730\u5740> -c \u8868\u793a\u65ad\u70b9\u7eed\u8fde -t \u8868\u793a \u65ad\u4e86\u4f1a\u6bcf\u7ecf\u8fc7\u51e0\u79d2\u91cd\u65b0\u8fde\u63a5\u8fde\u63a5\u4e00\u6b21\uff0c0\u8868\u793a\u4e00\u76f4\u4e0d\u65ad\u91cd\u8fde\uff0c\u6709\u6700\u5927\u8fde\u63a5\u6b21\u6570\u7684\u3002 \u6ce8\u610f\uff1a \u8bb0\u5f97\u5728\u539f\u6765\u7684\u76ee\u5f55\u4e0b\u6267\u884c\u8fd9\u4e2a\u547d\u4ee4\uff0c\u624d\u4f1a\u63a5\u7740\u4e0a\u6587\u4e0b\u8f7d\uff0c\u627e\u4e0d\u5230\u6587\u4ef6\u76f4\u63a5\u91cd\u65b0\u4e0b\u8f7d","title":"wget \u65ad\u7f51\u91cd\u8fde"},{"location":"ubuntu18/#_6","text":"df -h","title":"\u6307\u4ee4\u67e5\u770b\u786c\u76d8\u5b58\u50a8\u60c5\u51b5"},{"location":"ubuntu18/#_7","text":"lsblk","title":"\u67e5\u770b\u6240\u6709\u786c\u76d8"},{"location":"ubuntu18/#_8","text":"sudo fdisk -l #\u67e5\u770b\u78c1\u76d8\u4fe1\u606f sudo blkid #\u67e5\u770b\u5206\u533a mkdir NeDisk # \u521b\u5efa\u6302\u8f7d\u70b9 df -kh #\u67e5\u770b\u5df2\u6709\u6302\u8f7d\uff0c\u786e\u5b9a\u662f\u5426\u5df2\u6302\u8f7d sudo umount /dev/sda5 #\u5378\u8f7d\u5df2\u6302\u8f7d sudo nano /etc/fstab sudo blkid /dev/sda5 # \u627e\u5230\u5176UUID #\u7136\u540e,\u6211\u4eec\u6309\u7167/etc/fstab\u6587\u4ef6\u4e2d\u7684\u683c\u5f0f\u6dfb\u52a0\u4e00\u884c\u5982\u4e0b\u5185\u5bb9: UUID=0001D3CE0001E53B /home/ubuntu/NewDisk ntfs defaults 0 2 #\u5176\u4e2d\u7b2c\u4e00\u5217\u4e3aUUID, \u7b2c\u4e8c\u5217\u4e3a\u6302\u8f7d\u76ee\u5f55\uff08\u8be5\u76ee\u5f55\u5fc5\u987b\u4e3a\u7a7a\u76ee\u5f55\uff09\uff0c\u7b2c\u4e09\u5217\u4e3a\u6587\u4ef6\u7cfb\u7edf\u7c7b\u578b\uff0c\u7b2c\u56db\u5217\u4e3a\u53c2\u6570\uff0c\u7b2c\u4e94\u52170\u8868\u793a\u4e0d\u5907\u4efd\uff0c\u6700\u540e\u4e00\u5217\u5fc5\u987b\u4e3a\uff12\u62160(\u9664\u975e\u5f15\u5bfc\u5206\u533a\u4e3a1) sudo mount -a","title":"\u6302\u8f7d\u786c\u76d8"},{"location":"ubuntu18/#_9","text":"du * -sh","title":"\u67e5\u770b\u6587\u4ef6\u5939\u7684\u5927\u5c0f"},{"location":"ubuntu18/#ubuntu-vscode","text":"File -> Preferences -> Setting -> Features -> Terminal -> Inherit Env","title":"ubuntu vscode \u7ec8\u7aef\u7a7a\u767d"},{"location":"ubuntu18/#ubuntu","text":"lspci -vnn |grep VGA -A 12","title":"ubuntu \u67e5\u770b\u663e\u5361"},{"location":"ubuntu18/#vscode-python-debug","text":"{ \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"Python: Current File\", \"type\": \"python\", \"request\": \"launch\", \"program\": \"${file}\", \"console\": \"integratedTerminal\", \"args\": [ \"--evaluate\",\"/home/promote/NeDisk/radar_depth/pretrained/resnet18_multistage.pth.tar\", \"--data\",\"nuscenes\" ] } ] }","title":"vscode python debug \u5e26\u53c2"},{"location":"ubuntu18/#open-in-code","text":"wget -qO- https://raw.githubusercontent.com/cra0zy/code-nautilus/master/install.sh | bash","title":"\u6dfb\u52a0\"open in code\" \u53f3\u952e\u5feb\u6377\u65b9\u5f0f"},{"location":"ubuntu20/","text":"vscode Debug cpp Ros node catkin_make -DCMAKE_BUILD_TYPE=RelWithDebInfo \u5bf9\u4e8eintelliSenseEngine\u81ea\u52a8\u5b8c\u6210\u529f\u80fd\u6b63\u786e\u8bbe\u7f6e \"C_Cpp.intelliSenseEngine\": \"Tag Parser\" Debuge->add_Configuration->{} ROS:Launch launch.json { // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"ROS: Launch\", \"type\": \"ros\", \"request\": \"launch\", \"target\": \"/home/pmjd/Disk/radar/src/ars_40X/launch/ars_40X.launch\" } ] } \u8d77\u4e00\u4e2aroscore \u5c31\u53ef\u4ee5debug\u4e86 \u4e5f\u53ef\u4ee5 debug ros \u8282\u70b9\uff0c Debuge->add_Configuration->{} ROS:Attach \u542f\u52a8terminal \uff0c rosrun ars_40 deal_img \u5f00\u59cbdebug, \u9009\u62e9c++ \u548c\u8282\u70b9\u540d\u5b57 \u66f4\u6539apt\u955c\u50cf\u6e90 sudo nano /etc/apt/sources.list # \u9ed8\u8ba4\u6ce8\u91ca\u4e86\u6e90\u7801\u955c\u50cf\u4ee5\u63d0\u9ad8 apt update \u901f\u5ea6\uff0c\u5982\u6709\u9700\u8981\u53ef\u81ea\u884c\u53d6\u6d88\u6ce8\u91ca deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-updates main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-updates main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-backports main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-security main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-security main restricted universe multiverse # \u9884\u53d1\u5e03\u8f6f\u4ef6\u6e90\uff0c\u4e0d\u5efa\u8bae\u542f\u7528 # deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-proposed main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-proposed main restricted universe multiverse \u6e90\u7f51\u5740 ubuntu20 gcc \u7248\u672c ubuntu20 \u9ed8\u8ba4gcc\u7248\u672c\u4e3a9.3.0 cuda 10.0\u4ec5\u652f\u6301 gcc <8,\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u5b89\u88c5gcc7 \u9700\u8981\u5728\u955c\u50cf\u6e90\u91cc\u6dfb\u52a0 deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse \u7136\u540e\uff0c sudo apt-get update ,\u518d sudo apt-get install gcc-7 gcc \u7248\u672c \u63a7\u5236 sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 7 sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-7 7 ubuntu20\u5b89\u88c5 cuda10.0 \u5207\u6362gcc\u5230\u7248\u672c7 \u5728Cuda\u5b98\u7f51\uff0c\u4e0b\u8f7dcuda10.0 Linux -> x86_64 -> ubuntu -> 18.04 -> runfile(local) sudo sh cuda***.run \u6309 q \u952e\uff0c\u8f93\u5165 accept \uff0c \u5728\u9009\u62e9\u662f\u5426\u5b89\u88c5\u9a71\u52a8\u5904\uff0c\u9009\u62e9 no \u5b89\u88c5\u5b8c\u6210\uff0c\u4fee\u6539 nano ~/.bashrc \u5728\u672b\u5c3e\u6dfb\u52a0 export CUDA_HOME=/usr/local/cuda-10.0 export PATH=$PATH:$CUDA_HOME/bin export LD_LIBRARY_PATH=${CUDA_HOME}/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} \u5b89\u88c5cudnn \u4e0b\u8f7d cuDNN v7.6.5 for Cuda10 runtime library for ubuntu18.04 deb sudo dpkg -i libcudnnxxxxxxxxxxxxxxxxxx.deb \u5b89\u88c5pytorch 1.3.1 cuda 10.0 pip install torch==1.3.1+cu100 torchvision==0.4.2+cu100 -f https://download.pytorch.org/whl/torch_stable.html","title":"Ubuntu20"},{"location":"ubuntu20/#vscode-debug-cpp-ros-node","text":"catkin_make -DCMAKE_BUILD_TYPE=RelWithDebInfo \u5bf9\u4e8eintelliSenseEngine\u81ea\u52a8\u5b8c\u6210\u529f\u80fd\u6b63\u786e\u8bbe\u7f6e \"C_Cpp.intelliSenseEngine\": \"Tag Parser\" Debuge->add_Configuration->{} ROS:Launch launch.json { // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"ROS: Launch\", \"type\": \"ros\", \"request\": \"launch\", \"target\": \"/home/pmjd/Disk/radar/src/ars_40X/launch/ars_40X.launch\" } ] } \u8d77\u4e00\u4e2aroscore \u5c31\u53ef\u4ee5debug\u4e86 \u4e5f\u53ef\u4ee5 debug ros \u8282\u70b9\uff0c Debuge->add_Configuration->{} ROS:Attach \u542f\u52a8terminal \uff0c rosrun ars_40 deal_img \u5f00\u59cbdebug, \u9009\u62e9c++ \u548c\u8282\u70b9\u540d\u5b57","title":"vscode Debug cpp Ros node"},{"location":"ubuntu20/#apt","text":"sudo nano /etc/apt/sources.list # \u9ed8\u8ba4\u6ce8\u91ca\u4e86\u6e90\u7801\u955c\u50cf\u4ee5\u63d0\u9ad8 apt update \u901f\u5ea6\uff0c\u5982\u6709\u9700\u8981\u53ef\u81ea\u884c\u53d6\u6d88\u6ce8\u91ca deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-updates main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-updates main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-backports main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-security main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-security main restricted universe multiverse # \u9884\u53d1\u5e03\u8f6f\u4ef6\u6e90\uff0c\u4e0d\u5efa\u8bae\u542f\u7528 # deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-proposed main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-proposed main restricted universe multiverse \u6e90\u7f51\u5740","title":"\u66f4\u6539apt\u955c\u50cf\u6e90"},{"location":"ubuntu20/#ubuntu20-gcc","text":"ubuntu20 \u9ed8\u8ba4gcc\u7248\u672c\u4e3a9.3.0 cuda 10.0\u4ec5\u652f\u6301 gcc <8,\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u5b89\u88c5gcc7 \u9700\u8981\u5728\u955c\u50cf\u6e90\u91cc\u6dfb\u52a0 deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse \u7136\u540e\uff0c sudo apt-get update ,\u518d sudo apt-get install gcc-7 gcc \u7248\u672c \u63a7\u5236 sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 7 sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-7 7","title":"ubuntu20 gcc \u7248\u672c"},{"location":"ubuntu20/#ubuntu20-cuda100","text":"\u5207\u6362gcc\u5230\u7248\u672c7 \u5728Cuda\u5b98\u7f51\uff0c\u4e0b\u8f7dcuda10.0 Linux -> x86_64 -> ubuntu -> 18.04 -> runfile(local) sudo sh cuda***.run \u6309 q \u952e\uff0c\u8f93\u5165 accept \uff0c \u5728\u9009\u62e9\u662f\u5426\u5b89\u88c5\u9a71\u52a8\u5904\uff0c\u9009\u62e9 no \u5b89\u88c5\u5b8c\u6210\uff0c\u4fee\u6539 nano ~/.bashrc \u5728\u672b\u5c3e\u6dfb\u52a0 export CUDA_HOME=/usr/local/cuda-10.0 export PATH=$PATH:$CUDA_HOME/bin export LD_LIBRARY_PATH=${CUDA_HOME}/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} \u5b89\u88c5cudnn \u4e0b\u8f7d cuDNN v7.6.5 for Cuda10 runtime library for ubuntu18.04 deb sudo dpkg -i libcudnnxxxxxxxxxxxxxxxxxx.deb","title":"ubuntu20\u5b89\u88c5 cuda10.0"},{"location":"ubuntu20/#pytorch-131-cuda-100","text":"pip install torch==1.3.1+cu100 torchvision==0.4.2+cu100 -f https://download.pytorch.org/whl/torch_stable.html","title":"\u5b89\u88c5pytorch 1.3.1 cuda 10.0"},{"location":"util_wiki/","text":"rqt_image_view \u63d0\u793a ImageView.callback_image() could not convert image from '8UC3' to 'rgb8' ([8UC3] is not a color format. but [rgb8] is. The conversion does not make sense) \u89e3\u51b3\u529e\u6cd5:\\ - \u9996\u5148\u5728\uff4f\uff50\uff45\uff4e\uff43\uff56\u5904\u628a\uff18\uff35\uff23\uff13\u683c\u5f0f\u7684\uff4d\uff41\uff54\u8f6c\u5316\u4e3a\uff52\uff47\uff42\u683c\u5f0f cv::cvtColor(compoundFrame, compoundFrame, cv::COLOR_RGB2BGR); - \u7136\u540ecv_bridge\u7f16\u7801\u683c\u5f0f\u6539\u4e3asensor_msgs::image_encodings::RGB8 img_bridge = cv_bridge::CvImage(headers, sensor_msgs::image_encodings::RGB8, compoundFrame); Ros\u81ea\u5b9a\u4e49\uff4d\uff53\uff47\u7f16\u8bd1\u65f6\u62a5\u9519\uff1a\u7f3a\u5c11\u5934\u6587\u4ef6 \u53ea\u9700\u5728CMakeLists.txt\u91cc\u6dfb\u52a0\\ add_dependencies(GetLaneExtDemo smart_eye_gencpp) \\ smart_eye \u53ef\u6362\u6210\u4efb\u610fpkg \u540d\u79f0\\ GetLaneExtDemo\u3000\u4e3a\u8282\u70b9\u540d\u79f0 ModuleNotFoundError: No module named \u2018rospkg\u2019 pip install rospkg //\u66f4\u65b0\u65b9\u5f0f1 sudo apt-get install python-rospkg //\u66f4\u65b0\u65b9\u5f0f2 //\u7f51\u4e0a\u8bf4\u6709\u7684\u65b9\u5f0f1\u80fd\u89e3\u51b3\uff0c\u6709\u7684\u65b9\u5f0f2\u53ef\u4ee5\u89e3\u51b3\uff0c\u7528pip\u66f4\u65b0\u7684\u524d\u63d0\u662f\u5b89\u88c5\u4e86pip ImportError: No module named genmsg \u547d\u4ee4\uff1a\u628asudo make \u6539\u4e3a\u3000make,\u5c31\u53ef\u4ee5\u627e\u5230\u5e93\uff0c\u76ee\u524d\u539f\u56e0\u4e0d\u660e ui_mainwindow.h: No such file or directory set(CMAKE_AUTOUIC ON) set(CMAKE_INCLUDE_CURRENT_DIR ON) git\u7684\u547d\u4ee4\u884c\u5e94\u7528 git status git add . git commit -m \"q\" git push \u867d\u7136\u6709\u4e0d\u540c\u7684\uff52\uff4f\uff53\u5de5\u4f5c\u7a7a\u95f4 \u4f46\u662f\u6700\u597d\u4e0d\u8981\u6709\u76f8\u540c\u7684\u8282\u70b9\u540d\u5b57\u3002\u56e0\u4e3a\u5bb9\u6613\u5f15\u8d77\u6df7\u4e71\uff0c\u8fd0\u884c\u6df7\u4e71\u7b49\u3002\u3002\u3002","title":"rqt_image_view \u63d0\u793a ImageView.callback_image() could not convert image from '8UC3' to 'rgb8' ([8UC3] is not a color format. but [rgb8] is. The conversion does not make sense)"},{"location":"util_wiki/#rqt_image_view-imageviewcallback_image-could-not-convert-image-from-8uc3-to-rgb8-8uc3-is-not-a-color-format-but-rgb8-is-the-conversion-does-not-make-sense","text":"\u89e3\u51b3\u529e\u6cd5:\\ - \u9996\u5148\u5728\uff4f\uff50\uff45\uff4e\uff43\uff56\u5904\u628a\uff18\uff35\uff23\uff13\u683c\u5f0f\u7684\uff4d\uff41\uff54\u8f6c\u5316\u4e3a\uff52\uff47\uff42\u683c\u5f0f cv::cvtColor(compoundFrame, compoundFrame, cv::COLOR_RGB2BGR); - \u7136\u540ecv_bridge\u7f16\u7801\u683c\u5f0f\u6539\u4e3asensor_msgs::image_encodings::RGB8 img_bridge = cv_bridge::CvImage(headers, sensor_msgs::image_encodings::RGB8, compoundFrame);","title":"rqt_image_view \u63d0\u793a ImageView.callback_image() could not convert image from '8UC3' to 'rgb8' ([8UC3] is not a color format. but [rgb8] is. The conversion does not make sense)"},{"location":"util_wiki/#rosmsg","text":"\u53ea\u9700\u5728CMakeLists.txt\u91cc\u6dfb\u52a0\\ add_dependencies(GetLaneExtDemo smart_eye_gencpp) \\ smart_eye \u53ef\u6362\u6210\u4efb\u610fpkg \u540d\u79f0\\ GetLaneExtDemo\u3000\u4e3a\u8282\u70b9\u540d\u79f0","title":"Ros\u81ea\u5b9a\u4e49\uff4d\uff53\uff47\u7f16\u8bd1\u65f6\u62a5\u9519\uff1a\u7f3a\u5c11\u5934\u6587\u4ef6"},{"location":"util_wiki/#modulenotfounderror-no-module-named-rospkg","text":"pip install rospkg //\u66f4\u65b0\u65b9\u5f0f1 sudo apt-get install python-rospkg //\u66f4\u65b0\u65b9\u5f0f2 //\u7f51\u4e0a\u8bf4\u6709\u7684\u65b9\u5f0f1\u80fd\u89e3\u51b3\uff0c\u6709\u7684\u65b9\u5f0f2\u53ef\u4ee5\u89e3\u51b3\uff0c\u7528pip\u66f4\u65b0\u7684\u524d\u63d0\u662f\u5b89\u88c5\u4e86pip","title":"ModuleNotFoundError: No module named \u2018rospkg\u2019"},{"location":"util_wiki/#importerror-no-module-named-genmsg","text":"\u547d\u4ee4\uff1a\u628asudo make \u6539\u4e3a\u3000make,\u5c31\u53ef\u4ee5\u627e\u5230\u5e93\uff0c\u76ee\u524d\u539f\u56e0\u4e0d\u660e","title":"ImportError: No module named genmsg"},{"location":"util_wiki/#ui_mainwindowh-no-such-file-or-directory","text":"set(CMAKE_AUTOUIC ON) set(CMAKE_INCLUDE_CURRENT_DIR ON)","title":"ui_mainwindow.h: No such file or directory"},{"location":"util_wiki/#git","text":"git status git add . git commit -m \"q\" git push","title":"git\u7684\u547d\u4ee4\u884c\u5e94\u7528"},{"location":"util_wiki/#ros","text":"\u4f46\u662f\u6700\u597d\u4e0d\u8981\u6709\u76f8\u540c\u7684\u8282\u70b9\u540d\u5b57\u3002\u56e0\u4e3a\u5bb9\u6613\u5f15\u8d77\u6df7\u4e71\uff0c\u8fd0\u884c\u6df7\u4e71\u7b49\u3002\u3002\u3002","title":"\u867d\u7136\u6709\u4e0d\u540c\u7684\uff52\uff4f\uff53\u5de5\u4f5c\u7a7a\u95f4"},{"location":"voxelnet/","text":"VoxelNet To interface a highly sparse\u7a00\u758f LIDAR point cloud with a region proposal network(RPN\u533a\u57df\u5efa\u8bae\u7f51\u7edc),most existing efforts have focused on hand-crafted\u624b\u5de5\u5236\u4f5c feature representations\u7279\u5f81\u8868\u793a\uff0cfor example, a bird's eye view projectiong\u9e1f\u77b0\u56fe. In this work,we remove the need of manual feature engineering for 3D point clouds and purpose VoxelNet, a generic 3D detection network that unifies feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network. Specially,VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a unified feature representation through the newly introduced voxel feature encoding(VFE) layer.In this way,the point cloud is encoded as a descriptive volumetric representation, which is then connected to a RPN to generate detections. Our network learns an effective discriminative representation\u533a\u5206\u6027\u8868\u793a of objects with various geometrics, leading to encouraging results in 3D detection of pedestrains and cyclists, based on only Lidar. \\ VoxelNet architecture. The feature learning network takes a raw point cloud as input, partitions the space into voxels, and transforms points within each voxel to a vector representation characterizing the shape information.The space is represented as a sparse 4D tensor.The convolutional middle layers processes the 4D tensor to aggregate spatial context \u805a\u5408\u7a7a\u95f4\u8bed\u5883.Finally,a RPN generates the 3D detection. Scalling up 3D feature learning networks to orders of magnitude more points and to 3D detection tasks are the main challenges that we address in this paper. We present VoxelNet, a generic 3D detection framework that simultaneously\u540c\u65f6 learns a discriminative\u5224\u522b\u6027 feature representation from point clouds and predicts accurate 3D bounding boxes, in an end-to-end fashion\u65b9\u5f0f. We design a novel\u65b0\u9896\u7684 voxel feature encoding(VFE) layer,which enables inter-point interaction\u70b9\u95f4\u4ea4\u4e92\u3000within a voxel,by combining point-wise features\u9010\u70b9\u7279\u5f81 with a locally aggregated feature\u805a\u5408\u7279\u5f81. Stacking \u5806\u53e0 multiple VFE layers allows learning complex features for characterizing\u8868\u5f81 local 3D shape information. Specially, VoxelNet divides the point cloud into equally spaced 3D voxel\u7b49\u8ddd\u7684\uff13\uff24\u4f53\u7d20 ,encodes each voxel via stacked VFE layers, and then 3D convolution furture\u8fdb\u4e00\u6b65 aggregate\u805a\u5408 local voxel features, transforming the pointcloud into a high-dimensional volumetric\u4f53\u79ef representation. Finally,a RPN consumes the volumetric represetation and yields\u4ea7\u751f the detection result. This efficient algorithm benefits both from the sparse\u7a00\u758f\u7684 point structure and efficient parallel processing on the voxel grid\u4f53\u7d20\u7f51\u683c\u7684\u9ad8\u6548\u5e76\u884c\u5904\u7406. VoxelNet Architecture The proposed VoxelNet consists of three functional blocks:(1) Feature learning network,(2) Convolutional middle layers, and (3) Region proposal network Feature Learning Network Voxel Partition Given a point cloud, we subdivide the 3D space into equally spaced voxels as shown in Figure 2.Suppose the point cloud encompasses\u5305\u542b 3D space with range D,H,W\uff08\u70b9\u4e91\u5c3a\u5bf8\uff09 along the Z,Y,X axes respectively.We define each voxel of size vD,vH,and vW\uff08\u4f53\u7d20\u5c3a\u5bf8\uff09 accordingly.The resulting\u6240\u751f\u6210\u7684 3D voxel grid is of size D'=D/vD, H'=H/vH, W'=W/vW\uff08\u70b9\u4e91\u5305\u542b\u4f53\u7d20\u4e2a\u6570\uff09.Here, for simplicity,we assume D,H,W are a multiple of vD,vH,vW.\u6211\u4eec\u5047\u8bbeD,H,W\u662fvD, vH, vW\u7684\u500d\u6570\u3002 Grouping We group the points according to the voxel they reside in\u6211\u4eec\u6839\u636e\u5b83\u4eec\u6240\u5728\u7684\u4f53\u7d20\u5bf9\u70b9\u8fdb\u884c\u5206\u7ec4.Due to factors such as distance,occlusion\u906e\u6321,object's relative pose\u7269\u4f53\u7684\u76f8\u5bf9\u59ff\u52bf,and non-uniform\u4e0d\u5747\u5300 sampling,the LiDAR point cloud is sparse and highly variable\u9ad8\u5ea6\u53ef\u53d8\u7684 point density\u70b9\u5bc6\u5ea6 throughout the space.Therefore, after gouping, a voxel will contain a variable number of points. Random Sampling Typically a high-definition LiDAR point cloud is composed of ~100k points. Directly processing all the points not only imposes\u52a0\u5f3a increased memory/efficiency burdens\u8d1f\u62c5\u3000on the computing platform, but also highly variable point density throughout the space might bias the detection.To this end\u56e0\u6b64,we randomly sample a fixed number, T,of points from those voxels containing more than T points.This sampling strategy\u6218\u7565 has two purpose,(1)computational saving;and (2)decrease the imbalance of points between the voxels which reduces the sampling bias, and adds more variation\u53d8\u5316 to training. Stacked Voxel Feature Encoding \\ \\ The key innovation\u9769\u65b0 is the chain\u94fe of VFE layers.For simplicity, Figure 2 illustrates the hierachical\u9636\u7ea7\u5f0f feature encoding process for one voxel.Without loss of generality\u6982\u8981,(\u5728\u4e0d\u5931\u4e00\u822c\u6027\u7684\u524d\u63d0\u4e0b)we use VFE Layer-1 to describe the details in the following paragraph\u6bb5\u843d.Figure 3 shows the architecture for VFE Layer-1.\\ Denote\u8868\u793a as a non-empty voxel containing t\u2264\uff34 LiDAR points,where contains XYZ coordinates for the i-th point and is the received reflectance\u53cd\u5c04\u7387.We first compute the local mean\u5c40\u90e8\u5747\u503c as the centroid\u8d28\u5fc3 of all the points in V, denoted as .Then we augment\u589e\u52a0 each point with the relative offset w.r.t. the centriod and obtain the input feature set , Next, each is transformed through the fully connected network(FCN) into a feature space,where we can aggregate\u6c47\u603b information from the point features to ecode the shape of the surface contained within the voxel.The FCN is composed of a linear layer, a batch normalization(BN\u6279\u91cf\u6807\u51c6\u5316) layer and a rectified\u7ea0\u6b63\u7684 linear unit(ReLU) layer.After obtaining point-wise feature representations, we use element-wise MaxPooling across all associated to V to get the locally aggregated feature Finally, we augment each with to form the point-wise concatenated\u7ea7\u8054\u7684 feature as Thus we obtain the output feature set .All non-empty voxels are ecoded in the same way and they share the same set of parameters in FCN. We use to represent the i-th VFE layer that transforms input features of dimension into output features of dimension . The linear layer learns a matrix of size , and the point-wise concatenation yields the output of dimension . Because the output feature combines both point-wise features and locally aggregated feature, stacking VFE layers encodes point interactions\u4e92\u52a8\u3000within a voxel and enables the final feature representation to learn descriptive shape information. The voxel-wise feature is obtained by transforming the output of VFE-n into via FCN and applying element-wise Maxpool where C is the dimension of the voxel-wise feature, as shown in Figure2. Saparse Tensor Representation \u7a00\u758f\u5f20\u91cf\u8868\u793a\u3000By processing only the non-empty voxels, we obtain a list of voxel features,each uniquely\u72ec\u7279\u7684 associated to the spatial\u7a7a\u95f4 coordinates of a pictular non-empty voxel.The obtained list of voxel-wise features can be represented as a sparse 4D tensor, of size C x D' x H' x W' as shown in Figure2.Although the point cloud contains ~100k points, more than 90% of voxels typically are empty.Representing non-empty voxel features as a sparse tensor greatly reduce the memory usage and computation cost during backpropagation\u53cd\u5411\u4f20\u64ad, and it is a critical step in our efficient implementation. Convolutional Middle Layers We use ConvMD to represent an M-dimensional convolution operator where and are the number of input and output channels, k,s, and p are the M-dimensional vectors correspoinding to kernel size, stride size and padding\u586b\u5145 size respectively.When the size across the M-dimensions are the same, we use a scalar to represent the size e.g. k for k=(k,k,k). Each convolutional middle layer applies 3D convolution,BN layer, and ReLU layer sequentially\u4f9d\u6b21.The convolutional middle layers aggregate voxel-wise features within a progressively\u9010\u6b65 expanding\u6269\u5927\u7684 receptive\u63a5\u6536 field\uff0cadding more context to the shape description.The detialed sizes of the filters in the convolutional middle layers are explained in Section 3. Region Proposal Network \\ Recently, region proposal\u63d0\u6848 networks have become an important building block of top-performing object detection frameworks. In this work,we make servel key modifications to the RPN architecture proposed in [34], and combine it with the feature learning network and convolutional middle layers to form an end-to-end trainable pipeline\u7ba1\u9053. The input to our RPN is the feature map provided by the convolutional middle layers.The architecture of this network is illustrate in Figure 4.The network has three blocks of fully convolutional layers.The first layer of each block downsamples the feature map by half via a convolution with a stride size of 2,followed by a sequence of convolutions of stride 1(xq means q applications of the filter).After each convolution layer,BN and ReLU operations are applied.We then upsample the output of every block to a fixed size and concatanate to construct the high resolution feature map. Finally, this feature map is mapped to the desired learning targets:(1) a probability score map and (2) a regression map.\uff32\uff30\uff2e\u7684\u8f93\u5165\u662f\u5377\u79ef\u4e2d\u95f4\u5c42\u63d0\u4f9b\u7684\u7279\u5f81\u56fe\u3002\u8be5\u7f51\u7edc\u7684\u4f53\u7cfb\u7ed3\u6784\u5982\u56fe\uff14\u6240\u793a\u3002\u8be5\u7f51\u7edc\u5177\u6709\u4e09\u4e2a\u5b8c\u5168\u5377\u57fa\u5c42\u7684\u5757\u3002\u6bcf\u4e2a\u5757\u7684\u7b2c\u4e00\u5c42\u901a\u8fc7\u6b65\u5e45\u4e3a\uff12\u7684\u5377\u79ef\u5bf9\u7279\u5f81\u56fe\u8fdb\u884c\u4e00\u534a\u4e0b\u91c7\u6837\uff0c\u7136\u540e\u662f\u6b65\u5e45\uff11\u7684\u5377\u79ef\u5e8f\u5217\uff08xq \u8868\u793a\u6ee4\u6ce2\u5668\u7684\uff51\u4e2a\u5e94\u7528\uff09\u3002\u5728\u6bcf\u4e2a\u5377\u57fa\u5c42\u4e4b\u540e\uff0cBN \u548cReLU \u64cd\u4f5c\u88ab\u5e94\u7528\u3002\u7136\u540e\uff0c\u6211\u4eec\u5c06\u6bcf\u4e2a\u5757\u7684\u8f93\u51fa\u4e0a\u91c7\u6837\u5230\u56fa\u5b9a\u5927\u5c0f\uff0c\u5e76\u6c47\u603b\u4ee5\u6784\u5efa\u9ad8\u5206\u8fa8\u7387\u7279\u5f81\u56fe\u3002\u6700\u540e\uff0c\u5c06\u6b64\u7279\u5f81\u56fe\u6620\u5c04\u5230\u6240\u9700\u7684\u5b66\u4e60\u76ee\u6807\uff1a\uff08\uff11\uff09\u6982\u7387\u5206\u6570\u56fe\u548c\uff08\uff12\uff09\u56de\u5f52\u56fe\u3002 Loss Function Let be the set of positive anchors\u951a\u70b9 and be the set of negative anchors. We parameterize a 3D ground truth box as ,where represent the center location, are length ,width,height of the box, and is the yaw rotation around Z-axis. To retrieve\u627e\u56de the ground truth box from a matching positive anchorparameterized as ,we define the residual vector\u6b8b\u5dee\u5411\u91cf containing the 7 regression targets corresponding to center location \u25b3x,\u25b3y,\u25b3z,three dimensions \u25b3l, \u25b3w, \u25b3h, and the rotation \u25b3\u0398, which are computed as :\\ \\ where is the diagonal\u5bf9\u89d2\u7ebf of the base of the anchor box.Here, we aim to directly estimate the oriented\u5b9a\u5411\u7684 3D box and normallize \u0394x and \u0394y homogeneously\u5747\u5300\u7684 with the diagonal\u5bf9\u89d2\u7ebf ,We define the loss function as follows:\\ \\ where and represent the softmax output for positive anchor and negative anchor respectively,while and are the regression\u56de\u5f52 output and ground truth for positive anchor .The first two terms are the normalized classification loss for and , where the stands for binary cross entropy loss\u4e8c\u5143\u4ea4\u53c9\u71b5\u635f\u5931 and \u03b1\uff0c\u03b2 are postive constants balancing the relative importance. The last term is the regression loss, where we use the SmoothL1 function. Efficient Implementation Gpus are optimized\u4f18\u5316 for processing dense\u7a20\u5bc6 tensor structures\uff0eThe problem with working directly with the point cloud is that the points are sparsely distributed across sapce and each voxel has a variable number of points.We devised\u8bbe\u8ba1\u7684 a method that converts the point cloud into a dense tensor structure where stacked VFE operations can be processed in parallel across points and voxels.\\ \\ The method is summarized in Figure 5. We initialize a KxTx7 dimensional tensor structure to store the voxel input feature where K is the maximum number of non-empty voxels, T is the maximum number of points per voxel, and 7 is the input encoding dimension for each point.The points are randomized\u968f\u673a\u7684 before processing.For each point in pointcloud, we check if the corresponding voxel already exists. This lookup operation is done efficiently in O(1) using a hash table where the voxel coordinate is used as the hash key.If the voxel is already initialized we insert the point to voxel location if there are less than T points, otherwise the point is ignored.If the voxel is not initialized, we initialize a new voxel, store its coordinate in the voxel coordinate buffer, and insert the point to this voxel location.The voxel input feature and coordinate buffers can be constructed\u5efa via a signle pass over the point list, therefore its complexity is O(n).\u4f53\u7d20\u8f93\u5165\u7279\u5f81\u548c\u5750\u6807\u7f13\u51b2\u533a\u53ef\u4ee5\u901a\u8fc7\u5bf9\u70b9\u5217\u8868\u7684\u4e00\u6b21\u904d\u5386\u6765\u6784\u9020\uff0c\u56e0\u6b64\u5176\u590d\u6742\u5ea6\u4e3aO\uff08n\uff09\u3002To further improve the memory/compute efficiency it is possible to only store a limited number of voxels(K) and ignore points coming from voxels with few points. After the voxel input buffer is constructed,the stacked VFE only involves\u6d89\u53ca point level and voxel level dense operations which can be computed on a GPU in parallel.Note that, after concatenation\u7ea7\u8054 operations in VFE, we reset the features corresponding to empty points to zero such that they do not affect the computed voxel features.Finally, using the stored coordinate buffer we reorganize\u6539\u7ec4 the computed sparse voxel-wise structures to the dense voxel grid.\u6700\u540e\uff0c\u4f7f\u7528\u5b58\u50a8\u7684\u5750\u6807\u7f13\u51b2\u533a\uff0c\u6211\u4eec\u5c06\u8ba1\u7b97\u7684\u7a00\u758f\u4f53\u7d20\u7ed3\u6784\u91cd\u7ec4\u4e3a\u5bc6\u96c6\u4f53\u7d20\u7f51\u683c.The following convolutional middle layers and RPN operations work on a dense voxel grid which can be efficiently implemented on a GPU. Training Details Network Details Our experimental setup\u5b9e\u9a8c\u8bbe\u7f6e is based on the LiDAR specifications of the KITTI dataset.\\ Car Detection For this task,we consider point clouds within the range of [-3,1]x[-40,40]x[0,70.4]meters along Z,Y,X axis respectively.Points taht are Points that are projected outside of image boundaries are removed.We choose a voxel size of meters,which leads to D'=10,H'=400,W'=352. We set T = 35 as the maximum number of randomly sampled points in each non-empty voxel.We use two VFE layers VFE-1(7,32) and VFE-2(32,128).The final FCN maps VFE-2 output to .Thus our feature learning net generate a sparse tensor of shape 128x10x400x352.To aggregate voxel-wise features, we employ three convolution middle layers sequentially as Conv3D(128,64,3,(2,1,1),(1,1,1)),Conv3D(64,64,3,(1,1,1),(0,1,1)),and Conv3D(64,64,3,(2,1,1),(1,1,1)), which yields a 4D tensor of size 64x2x400x352.After reshaping, the input to RPN is a feature map of size 128x400x352,where the dimensions correspond to channel,height, and width of the 3D tensor.Figure 4 illustrates the detailed network architecture for this task.Unlike, we use only one anchor size, meters, centered at\u96c6\u4e2d\u4e8e meters with two rotations, 0 and 90 degrees.Our anchor matching criteria\u5339\u914d\u6807\u51c6 as follows: An anchor is considered as positive if it has the highest Intersection over Union(IoU) with a ground truth or its IoU with ground truth is above 0.6(in bird's eye view).An anchor is considered as negative if the IoU between it and all ground true boxes is less than 0.45. We treat anchors as don't care if they have 0.45\u2264IoU\u22640.6 with any ground truth.We set \u03b1\uff1d1.5 and \u03b2=1 in Eqn.2. Pedestrain and Cyclist Detection The input range is [-3,1]x[-20,20]x[0,48] meters along Z,Y,X axis respectively.We use the same voxel size as for car detection,which yields D=10, H=200, W=240.We set T=45 in order to obtain more LiDAR points for better capturing shape information.The feature learning network and convolutional middle layers ate identical\u76f8\u540c to the networks used in car detection task.For the RPN, we make one modification to block 1 in Figure 4 by changing the stride size in the first 2D convolution from 2 to 1. This allows finer resolution in anchor matching, which is necessary for detecting pedestrains and cyclists.We use anchor size meters centered at with 0 and 90 degrees rotation for pedestrain detection and use anchor size meters centered at with 0 and 90 degrees rotation for cyclist detecction.The specific anchor matching criteria is as follows:We assign an anchor as positive if it has the highest IoU with a ground truth, or its IoU with ground truth is above 0.5. An anchor is considered as negative if its IoU with every ground truth is less than 0.35.For anchors having 0.35\u2264IoU\u22640.5 with any ground truth,we treat them as don't care. During training, we use stochastic\u968f\u673a gradient descent(SGD) with learning rate 0.01 for the first 150 epochs and decrease the learning rate to 0.001 for the last 10 epochs.We use a batchsize of 16 point clouds. Data Augmentation \u6570\u636e\u6269\u5c55 \uff37ith less than 4000 training point clouds, training our network from scratch will inevitably \u4e0d\u53ef\u907f\u514d suffer from overfitting. To reduce this issue, we introduce three different forms of data augmentation. The augmented training data are generated on-the-fly\u5373\u65f6\u3000without the need to be stored on disk. Define set as the whole point cloud, consisting of N points.We parameterize a 3D bounding box ,where are center locations, l,w,h are length, width, height, and \u03b8 is the yaw rotation around Z-axis.We define as the set containing all LiDAR points within , where p=[x,y,z,r] denotes a particular LiDAR point in the whole set M. The first form of data augmentation applies perturbation\u6444\u52a8 independently to each ground truth 3D bounding box together with those LiDAR points within the box.Specifically,around Z-axis we rotate and the associated with respect to ( ) by a uniformally\u7edf\u4e00\u7684 distributed random variable . Then we add a translation (\u0394x,\u0394y,\u0394z) to the XYZ components of and to each point in ,where \u0394x,\u0394y,\u0394z are drawn independently from a Gaussian distribution with mean zero and standard deviation 1.0. To avoid physically impossible outcomes, we perform a collision\u78b0\u649e test between any two boxes after the perturbation and revert\u8fd8\u539f to orignal if a collision is detected.Since the perturbation is applied to each ground truth box and the associated LiDAR points independently, the network is able to learn from substantially\u5b9e\u8d28\u4e0a more variations than from the orignal training data. Secondly,we apply global scaling to all ground truth boxes and to whole point cloud M. Specifically, we multiply the XYZ coordinates and the three dimensions of each , and the XYZ coordinates of all points in M with a random variable drawn from uniform distribution [0.95,1.05]. Introducing global scale augementation improves robustness of the network for detecting objects with various sizes and distances as shown in image-based classification and detection tasks. Finally, we apply global rotation to all ground truth boxes and to the whole point cloud M. The rotation is applied along Z-axis and around(0,0,0). The global rotation offset is determined by sampling from uniform distribution [-\u03c0/4,\u03c0/4].By rotating the entire point cloud, we simulate the vehicle making a turn.","title":"voxelnet"},{"location":"voxelnet/#voxelnet","text":"To interface a highly sparse\u7a00\u758f LIDAR point cloud with a region proposal network(RPN\u533a\u57df\u5efa\u8bae\u7f51\u7edc),most existing efforts have focused on hand-crafted\u624b\u5de5\u5236\u4f5c feature representations\u7279\u5f81\u8868\u793a\uff0cfor example, a bird's eye view projectiong\u9e1f\u77b0\u56fe. In this work,we remove the need of manual feature engineering for 3D point clouds and purpose VoxelNet, a generic 3D detection network that unifies feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network. Specially,VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a unified feature representation through the newly introduced voxel feature encoding(VFE) layer.In this way,the point cloud is encoded as a descriptive volumetric representation, which is then connected to a RPN to generate detections. Our network learns an effective discriminative representation\u533a\u5206\u6027\u8868\u793a of objects with various geometrics, leading to encouraging results in 3D detection of pedestrains and cyclists, based on only Lidar. \\ VoxelNet architecture. The feature learning network takes a raw point cloud as input, partitions the space into voxels, and transforms points within each voxel to a vector representation characterizing the shape information.The space is represented as a sparse 4D tensor.The convolutional middle layers processes the 4D tensor to aggregate spatial context \u805a\u5408\u7a7a\u95f4\u8bed\u5883.Finally,a RPN generates the 3D detection. Scalling up 3D feature learning networks to orders of magnitude more points and to 3D detection tasks are the main challenges that we address in this paper. We present VoxelNet, a generic 3D detection framework that simultaneously\u540c\u65f6 learns a discriminative\u5224\u522b\u6027 feature representation from point clouds and predicts accurate 3D bounding boxes, in an end-to-end fashion\u65b9\u5f0f. We design a novel\u65b0\u9896\u7684 voxel feature encoding(VFE) layer,which enables inter-point interaction\u70b9\u95f4\u4ea4\u4e92\u3000within a voxel,by combining point-wise features\u9010\u70b9\u7279\u5f81 with a locally aggregated feature\u805a\u5408\u7279\u5f81. Stacking \u5806\u53e0 multiple VFE layers allows learning complex features for characterizing\u8868\u5f81 local 3D shape information. Specially, VoxelNet divides the point cloud into equally spaced 3D voxel\u7b49\u8ddd\u7684\uff13\uff24\u4f53\u7d20 ,encodes each voxel via stacked VFE layers, and then 3D convolution furture\u8fdb\u4e00\u6b65 aggregate\u805a\u5408 local voxel features, transforming the pointcloud into a high-dimensional volumetric\u4f53\u79ef representation. Finally,a RPN consumes the volumetric represetation and yields\u4ea7\u751f the detection result. This efficient algorithm benefits both from the sparse\u7a00\u758f\u7684 point structure and efficient parallel processing on the voxel grid\u4f53\u7d20\u7f51\u683c\u7684\u9ad8\u6548\u5e76\u884c\u5904\u7406.","title":"VoxelNet"},{"location":"voxelnet/#voxelnet-architecture","text":"The proposed VoxelNet consists of three functional blocks:(1) Feature learning network,(2) Convolutional middle layers, and (3) Region proposal network","title":"VoxelNet Architecture"},{"location":"voxelnet/#feature-learning-network","text":"Voxel Partition Given a point cloud, we subdivide the 3D space into equally spaced voxels as shown in Figure 2.Suppose the point cloud encompasses\u5305\u542b 3D space with range D,H,W\uff08\u70b9\u4e91\u5c3a\u5bf8\uff09 along the Z,Y,X axes respectively.We define each voxel of size vD,vH,and vW\uff08\u4f53\u7d20\u5c3a\u5bf8\uff09 accordingly.The resulting\u6240\u751f\u6210\u7684 3D voxel grid is of size D'=D/vD, H'=H/vH, W'=W/vW\uff08\u70b9\u4e91\u5305\u542b\u4f53\u7d20\u4e2a\u6570\uff09.Here, for simplicity,we assume D,H,W are a multiple of vD,vH,vW.\u6211\u4eec\u5047\u8bbeD,H,W\u662fvD, vH, vW\u7684\u500d\u6570\u3002 Grouping We group the points according to the voxel they reside in\u6211\u4eec\u6839\u636e\u5b83\u4eec\u6240\u5728\u7684\u4f53\u7d20\u5bf9\u70b9\u8fdb\u884c\u5206\u7ec4.Due to factors such as distance,occlusion\u906e\u6321,object's relative pose\u7269\u4f53\u7684\u76f8\u5bf9\u59ff\u52bf,and non-uniform\u4e0d\u5747\u5300 sampling,the LiDAR point cloud is sparse and highly variable\u9ad8\u5ea6\u53ef\u53d8\u7684 point density\u70b9\u5bc6\u5ea6 throughout the space.Therefore, after gouping, a voxel will contain a variable number of points. Random Sampling Typically a high-definition LiDAR point cloud is composed of ~100k points. Directly processing all the points not only imposes\u52a0\u5f3a increased memory/efficiency burdens\u8d1f\u62c5\u3000on the computing platform, but also highly variable point density throughout the space might bias the detection.To this end\u56e0\u6b64,we randomly sample a fixed number, T,of points from those voxels containing more than T points.This sampling strategy\u6218\u7565 has two purpose,(1)computational saving;and (2)decrease the imbalance of points between the voxels which reduces the sampling bias, and adds more variation\u53d8\u5316 to training. Stacked Voxel Feature Encoding \\ \\ The key innovation\u9769\u65b0 is the chain\u94fe of VFE layers.For simplicity, Figure 2 illustrates the hierachical\u9636\u7ea7\u5f0f feature encoding process for one voxel.Without loss of generality\u6982\u8981,(\u5728\u4e0d\u5931\u4e00\u822c\u6027\u7684\u524d\u63d0\u4e0b)we use VFE Layer-1 to describe the details in the following paragraph\u6bb5\u843d.Figure 3 shows the architecture for VFE Layer-1.\\ Denote\u8868\u793a as a non-empty voxel containing t\u2264\uff34 LiDAR points,where contains XYZ coordinates for the i-th point and is the received reflectance\u53cd\u5c04\u7387.We first compute the local mean\u5c40\u90e8\u5747\u503c as the centroid\u8d28\u5fc3 of all the points in V, denoted as .Then we augment\u589e\u52a0 each point with the relative offset w.r.t. the centriod and obtain the input feature set , Next, each is transformed through the fully connected network(FCN) into a feature space,where we can aggregate\u6c47\u603b information from the point features to ecode the shape of the surface contained within the voxel.The FCN is composed of a linear layer, a batch normalization(BN\u6279\u91cf\u6807\u51c6\u5316) layer and a rectified\u7ea0\u6b63\u7684 linear unit(ReLU) layer.After obtaining point-wise feature representations, we use element-wise MaxPooling across all associated to V to get the locally aggregated feature Finally, we augment each with to form the point-wise concatenated\u7ea7\u8054\u7684 feature as Thus we obtain the output feature set .All non-empty voxels are ecoded in the same way and they share the same set of parameters in FCN. We use to represent the i-th VFE layer that transforms input features of dimension into output features of dimension . The linear layer learns a matrix of size , and the point-wise concatenation yields the output of dimension . Because the output feature combines both point-wise features and locally aggregated feature, stacking VFE layers encodes point interactions\u4e92\u52a8\u3000within a voxel and enables the final feature representation to learn descriptive shape information. The voxel-wise feature is obtained by transforming the output of VFE-n into via FCN and applying element-wise Maxpool where C is the dimension of the voxel-wise feature, as shown in Figure2. Saparse Tensor Representation \u7a00\u758f\u5f20\u91cf\u8868\u793a\u3000By processing only the non-empty voxels, we obtain a list of voxel features,each uniquely\u72ec\u7279\u7684 associated to the spatial\u7a7a\u95f4 coordinates of a pictular non-empty voxel.The obtained list of voxel-wise features can be represented as a sparse 4D tensor, of size C x D' x H' x W' as shown in Figure2.Although the point cloud contains ~100k points, more than 90% of voxels typically are empty.Representing non-empty voxel features as a sparse tensor greatly reduce the memory usage and computation cost during backpropagation\u53cd\u5411\u4f20\u64ad, and it is a critical step in our efficient implementation.","title":"Feature Learning Network"},{"location":"voxelnet/#convolutional-middle-layers","text":"We use ConvMD to represent an M-dimensional convolution operator where and are the number of input and output channels, k,s, and p are the M-dimensional vectors correspoinding to kernel size, stride size and padding\u586b\u5145 size respectively.When the size across the M-dimensions are the same, we use a scalar to represent the size e.g. k for k=(k,k,k). Each convolutional middle layer applies 3D convolution,BN layer, and ReLU layer sequentially\u4f9d\u6b21.The convolutional middle layers aggregate voxel-wise features within a progressively\u9010\u6b65 expanding\u6269\u5927\u7684 receptive\u63a5\u6536 field\uff0cadding more context to the shape description.The detialed sizes of the filters in the convolutional middle layers are explained in Section 3.","title":"Convolutional Middle Layers"},{"location":"voxelnet/#region-proposal-network","text":"\\ Recently, region proposal\u63d0\u6848 networks have become an important building block of top-performing object detection frameworks. In this work,we make servel key modifications to the RPN architecture proposed in [34], and combine it with the feature learning network and convolutional middle layers to form an end-to-end trainable pipeline\u7ba1\u9053. The input to our RPN is the feature map provided by the convolutional middle layers.The architecture of this network is illustrate in Figure 4.The network has three blocks of fully convolutional layers.The first layer of each block downsamples the feature map by half via a convolution with a stride size of 2,followed by a sequence of convolutions of stride 1(xq means q applications of the filter).After each convolution layer,BN and ReLU operations are applied.We then upsample the output of every block to a fixed size and concatanate to construct the high resolution feature map. Finally, this feature map is mapped to the desired learning targets:(1) a probability score map and (2) a regression map.\uff32\uff30\uff2e\u7684\u8f93\u5165\u662f\u5377\u79ef\u4e2d\u95f4\u5c42\u63d0\u4f9b\u7684\u7279\u5f81\u56fe\u3002\u8be5\u7f51\u7edc\u7684\u4f53\u7cfb\u7ed3\u6784\u5982\u56fe\uff14\u6240\u793a\u3002\u8be5\u7f51\u7edc\u5177\u6709\u4e09\u4e2a\u5b8c\u5168\u5377\u57fa\u5c42\u7684\u5757\u3002\u6bcf\u4e2a\u5757\u7684\u7b2c\u4e00\u5c42\u901a\u8fc7\u6b65\u5e45\u4e3a\uff12\u7684\u5377\u79ef\u5bf9\u7279\u5f81\u56fe\u8fdb\u884c\u4e00\u534a\u4e0b\u91c7\u6837\uff0c\u7136\u540e\u662f\u6b65\u5e45\uff11\u7684\u5377\u79ef\u5e8f\u5217\uff08xq \u8868\u793a\u6ee4\u6ce2\u5668\u7684\uff51\u4e2a\u5e94\u7528\uff09\u3002\u5728\u6bcf\u4e2a\u5377\u57fa\u5c42\u4e4b\u540e\uff0cBN \u548cReLU \u64cd\u4f5c\u88ab\u5e94\u7528\u3002\u7136\u540e\uff0c\u6211\u4eec\u5c06\u6bcf\u4e2a\u5757\u7684\u8f93\u51fa\u4e0a\u91c7\u6837\u5230\u56fa\u5b9a\u5927\u5c0f\uff0c\u5e76\u6c47\u603b\u4ee5\u6784\u5efa\u9ad8\u5206\u8fa8\u7387\u7279\u5f81\u56fe\u3002\u6700\u540e\uff0c\u5c06\u6b64\u7279\u5f81\u56fe\u6620\u5c04\u5230\u6240\u9700\u7684\u5b66\u4e60\u76ee\u6807\uff1a\uff08\uff11\uff09\u6982\u7387\u5206\u6570\u56fe\u548c\uff08\uff12\uff09\u56de\u5f52\u56fe\u3002","title":"Region Proposal Network"},{"location":"voxelnet/#loss-function","text":"Let be the set of positive anchors\u951a\u70b9 and be the set of negative anchors. We parameterize a 3D ground truth box as ,where represent the center location, are length ,width,height of the box, and is the yaw rotation around Z-axis. To retrieve\u627e\u56de the ground truth box from a matching positive anchorparameterized as ,we define the residual vector\u6b8b\u5dee\u5411\u91cf containing the 7 regression targets corresponding to center location \u25b3x,\u25b3y,\u25b3z,three dimensions \u25b3l, \u25b3w, \u25b3h, and the rotation \u25b3\u0398, which are computed as :\\ \\ where is the diagonal\u5bf9\u89d2\u7ebf of the base of the anchor box.Here, we aim to directly estimate the oriented\u5b9a\u5411\u7684 3D box and normallize \u0394x and \u0394y homogeneously\u5747\u5300\u7684 with the diagonal\u5bf9\u89d2\u7ebf ,We define the loss function as follows:\\ \\ where and represent the softmax output for positive anchor and negative anchor respectively,while and are the regression\u56de\u5f52 output and ground truth for positive anchor .The first two terms are the normalized classification loss for and , where the stands for binary cross entropy loss\u4e8c\u5143\u4ea4\u53c9\u71b5\u635f\u5931 and \u03b1\uff0c\u03b2 are postive constants balancing the relative importance. The last term is the regression loss, where we use the SmoothL1 function.","title":"Loss Function"},{"location":"voxelnet/#efficient-implementation","text":"Gpus are optimized\u4f18\u5316 for processing dense\u7a20\u5bc6 tensor structures\uff0eThe problem with working directly with the point cloud is that the points are sparsely distributed across sapce and each voxel has a variable number of points.We devised\u8bbe\u8ba1\u7684 a method that converts the point cloud into a dense tensor structure where stacked VFE operations can be processed in parallel across points and voxels.\\ \\ The method is summarized in Figure 5. We initialize a KxTx7 dimensional tensor structure to store the voxel input feature where K is the maximum number of non-empty voxels, T is the maximum number of points per voxel, and 7 is the input encoding dimension for each point.The points are randomized\u968f\u673a\u7684 before processing.For each point in pointcloud, we check if the corresponding voxel already exists. This lookup operation is done efficiently in O(1) using a hash table where the voxel coordinate is used as the hash key.If the voxel is already initialized we insert the point to voxel location if there are less than T points, otherwise the point is ignored.If the voxel is not initialized, we initialize a new voxel, store its coordinate in the voxel coordinate buffer, and insert the point to this voxel location.The voxel input feature and coordinate buffers can be constructed\u5efa via a signle pass over the point list, therefore its complexity is O(n).\u4f53\u7d20\u8f93\u5165\u7279\u5f81\u548c\u5750\u6807\u7f13\u51b2\u533a\u53ef\u4ee5\u901a\u8fc7\u5bf9\u70b9\u5217\u8868\u7684\u4e00\u6b21\u904d\u5386\u6765\u6784\u9020\uff0c\u56e0\u6b64\u5176\u590d\u6742\u5ea6\u4e3aO\uff08n\uff09\u3002To further improve the memory/compute efficiency it is possible to only store a limited number of voxels(K) and ignore points coming from voxels with few points. After the voxel input buffer is constructed,the stacked VFE only involves\u6d89\u53ca point level and voxel level dense operations which can be computed on a GPU in parallel.Note that, after concatenation\u7ea7\u8054 operations in VFE, we reset the features corresponding to empty points to zero such that they do not affect the computed voxel features.Finally, using the stored coordinate buffer we reorganize\u6539\u7ec4 the computed sparse voxel-wise structures to the dense voxel grid.\u6700\u540e\uff0c\u4f7f\u7528\u5b58\u50a8\u7684\u5750\u6807\u7f13\u51b2\u533a\uff0c\u6211\u4eec\u5c06\u8ba1\u7b97\u7684\u7a00\u758f\u4f53\u7d20\u7ed3\u6784\u91cd\u7ec4\u4e3a\u5bc6\u96c6\u4f53\u7d20\u7f51\u683c.The following convolutional middle layers and RPN operations work on a dense voxel grid which can be efficiently implemented on a GPU.","title":"Efficient Implementation"},{"location":"voxelnet/#training-details","text":"","title":"Training Details"},{"location":"voxelnet/#network-details","text":"Our experimental setup\u5b9e\u9a8c\u8bbe\u7f6e is based on the LiDAR specifications of the KITTI dataset.\\ Car Detection For this task,we consider point clouds within the range of [-3,1]x[-40,40]x[0,70.4]meters along Z,Y,X axis respectively.Points taht are Points that are projected outside of image boundaries are removed.We choose a voxel size of meters,which leads to D'=10,H'=400,W'=352. We set T = 35 as the maximum number of randomly sampled points in each non-empty voxel.We use two VFE layers VFE-1(7,32) and VFE-2(32,128).The final FCN maps VFE-2 output to .Thus our feature learning net generate a sparse tensor of shape 128x10x400x352.To aggregate voxel-wise features, we employ three convolution middle layers sequentially as Conv3D(128,64,3,(2,1,1),(1,1,1)),Conv3D(64,64,3,(1,1,1),(0,1,1)),and Conv3D(64,64,3,(2,1,1),(1,1,1)), which yields a 4D tensor of size 64x2x400x352.After reshaping, the input to RPN is a feature map of size 128x400x352,where the dimensions correspond to channel,height, and width of the 3D tensor.Figure 4 illustrates the detailed network architecture for this task.Unlike, we use only one anchor size, meters, centered at\u96c6\u4e2d\u4e8e meters with two rotations, 0 and 90 degrees.Our anchor matching criteria\u5339\u914d\u6807\u51c6 as follows: An anchor is considered as positive if it has the highest Intersection over Union(IoU) with a ground truth or its IoU with ground truth is above 0.6(in bird's eye view).An anchor is considered as negative if the IoU between it and all ground true boxes is less than 0.45. We treat anchors as don't care if they have 0.45\u2264IoU\u22640.6 with any ground truth.We set \u03b1\uff1d1.5 and \u03b2=1 in Eqn.2. Pedestrain and Cyclist Detection The input range is [-3,1]x[-20,20]x[0,48] meters along Z,Y,X axis respectively.We use the same voxel size as for car detection,which yields D=10, H=200, W=240.We set T=45 in order to obtain more LiDAR points for better capturing shape information.The feature learning network and convolutional middle layers ate identical\u76f8\u540c to the networks used in car detection task.For the RPN, we make one modification to block 1 in Figure 4 by changing the stride size in the first 2D convolution from 2 to 1. This allows finer resolution in anchor matching, which is necessary for detecting pedestrains and cyclists.We use anchor size meters centered at with 0 and 90 degrees rotation for pedestrain detection and use anchor size meters centered at with 0 and 90 degrees rotation for cyclist detecction.The specific anchor matching criteria is as follows:We assign an anchor as positive if it has the highest IoU with a ground truth, or its IoU with ground truth is above 0.5. An anchor is considered as negative if its IoU with every ground truth is less than 0.35.For anchors having 0.35\u2264IoU\u22640.5 with any ground truth,we treat them as don't care. During training, we use stochastic\u968f\u673a gradient descent(SGD) with learning rate 0.01 for the first 150 epochs and decrease the learning rate to 0.001 for the last 10 epochs.We use a batchsize of 16 point clouds.","title":"Network Details"},{"location":"voxelnet/#data-augmentation","text":"\uff37ith less than 4000 training point clouds, training our network from scratch will inevitably \u4e0d\u53ef\u907f\u514d suffer from overfitting. To reduce this issue, we introduce three different forms of data augmentation. The augmented training data are generated on-the-fly\u5373\u65f6\u3000without the need to be stored on disk. Define set as the whole point cloud, consisting of N points.We parameterize a 3D bounding box ,where are center locations, l,w,h are length, width, height, and \u03b8 is the yaw rotation around Z-axis.We define as the set containing all LiDAR points within , where p=[x,y,z,r] denotes a particular LiDAR point in the whole set M. The first form of data augmentation applies perturbation\u6444\u52a8 independently to each ground truth 3D bounding box together with those LiDAR points within the box.Specifically,around Z-axis we rotate and the associated with respect to ( ) by a uniformally\u7edf\u4e00\u7684 distributed random variable . Then we add a translation (\u0394x,\u0394y,\u0394z) to the XYZ components of and to each point in ,where \u0394x,\u0394y,\u0394z are drawn independently from a Gaussian distribution with mean zero and standard deviation 1.0. To avoid physically impossible outcomes, we perform a collision\u78b0\u649e test between any two boxes after the perturbation and revert\u8fd8\u539f to orignal if a collision is detected.Since the perturbation is applied to each ground truth box and the associated LiDAR points independently, the network is able to learn from substantially\u5b9e\u8d28\u4e0a more variations than from the orignal training data. Secondly,we apply global scaling to all ground truth boxes and to whole point cloud M. Specifically, we multiply the XYZ coordinates and the three dimensions of each , and the XYZ coordinates of all points in M with a random variable drawn from uniform distribution [0.95,1.05]. Introducing global scale augementation improves robustness of the network for detecting objects with various sizes and distances as shown in image-based classification and detection tasks. Finally, we apply global rotation to all ground truth boxes and to the whole point cloud M. The rotation is applied along Z-axis and around(0,0,0). The global rotation offset is determined by sampling from uniform distribution [-\u03c0/4,\u03c0/4].By rotating the entire point cloud, we simulate the vehicle making a turn.","title":"Data Augmentation \u6570\u636e\u6269\u5c55"},{"location":"%E6%91%84%E5%83%8F%E6%9C%BA%E5%92%8C%E6%AF%AB%E7%B1%B3%E6%B3%A2%E5%9D%90%E6%A0%87%E7%B3%BB%E5%85%B3%E7%B3%BB/","text":"\u6444\u50cf\u673a\u5750\u6807\u7cfb \u6444\u50cf\u673a\u9ed8\u8ba4\u7684\u662f\u4e0e\u4e16\u754c\u5750\u6807\u7cfb\u5bf9\u9f50\u7684\u4f46\u662f\u76f8\u673a\u7684z\u8f74\u65b9\u5411\u6307\u5411\u76f8\u673a\u5185\u90e8\u3002 \u6beb\u7c73\u6ce2\u5750\u6807\u7cfb \u7ea2\u8272\u4e3ax\u8f74\uff0c\u7eff\u8272\u4e3ay\u8f74\uff0c\u84dd\u8272\u4e3az\u8f74 \u7ed5x\u8f74\u7684\u65cb\u8f6c\u77e9\u9635 R_x(\\theta)=\\begin{bmatrix} 1&0&0\\\\ 0&cos\\theta&-sin\\theta\\\\ 0&sin\\theta&cos\\theta \\end{bmatrix} \u7ed5y\u8f74\u7684\u65cb\u8f6c\u77e9\u9635 R_y(\\theta)=\\begin{bmatrix} cos\\theta&0&sin\\theta\\\\ 0&1&0\\\\ -sin\\theta&0&cos\\theta \\end{bmatrix} \u7ed5z\u8f74\u65cb\u8f6c\u77e9\u9635 R_z(\\theta)=\\begin{bmatrix} cos\\theta&-sin\\theta&0\\\\ sin\\theta&cos\\theta&0\\\\ 0&0&1 \\end{bmatrix} \u901a\u7528\u65cb\u8f6c\u77e9\u9635 R=R_z(\\alpha)R_y(\\beta)R_z(\\gamma)\\\\=\\begin{bmatrix} cos\\alpha cos\\beta&cos\\alpha sin\\beta sin\\gamma -sin\\alpha cos\\gamma&cos\\alpha sin\\beta cos\\gamma+sin\\alpha sin\\gamma\\\\ sin\\alpha cos\\beta&sin\\alpha sin\\beta sin\\gamma + cos\\alpha cos\\gamma&sin\\alpha sin\\beta cos\\gamma -cos\\alpha sin\\gamma\\\\ -sin\\beta&cos\\beta sin\\gamma&cos\\beta cos\\gamma \\end{bmatrix} \u53d8\u6362\u77e9\u9635 T=\\begin{bmatrix} cos\\alpha cos\\beta&cos\\alpha sin\\beta sin\\gamma -sin\\alpha cos\\gamma&cos\\alpha sin\\beta cos\\gamma+sin\\alpha sin\\gamma&x\\\\ sin\\alpha cos\\beta&sin\\alpha sin\\beta sin\\gamma + cos\\alpha cos\\gamma&sin\\alpha sin\\beta cos\\gamma -cos\\alpha sin\\gamma&y\\\\ -sin\\beta&cos\\beta sin\\gamma&cos\\beta cos\\gamma&z\\\\ 0&0&0&1 \\end{bmatrix} /** * x :\u6beb\u7c73\u6ce2\u6570\u636e\u7684x\u5750\u6807\u503c * y :\u6beb\u7c73\u6ce2\u6570\u636e\u7684y\u5750\u6807\u503c * radar_x :\u6beb\u7c73\u6ce2\u76f8\u5bf9\u4e8e\u76f8\u673a\u5750\u6807\u7cfbx\u8f74\u7684\u5750\u6807\u4f4d\u7f6e * radar_y \uff1a\u6beb\u7c73\u6ce2\u76f8\u5bf9\u4e8e\u76f8\u673a\u5750\u6807\u7cfby\u8f74\u7684\u5750\u6807\u4f4d\u7f6e * radar_z \uff1a\u6beb\u7c73\u6ce2\u76f8\u5bf9\u4e8e\u76f8\u673a\u5750\u6807\u7cfbz\u8f74\u7684\u5750\u6807\u4f4d\u7f6e * a : \u6beb\u7c73\u6ce2\u5bf9\u4e8e\u76f8\u673a\u7ed5x\u8f74\u65cb\u8f6c\u89d2\u5ea6,\u9006\u65f6\u9488\u4e3a\u6b63 * b : \u6beb\u7c73\u6ce2\u5bf9\u4e8e\u76f8\u673a\u7ed5y\u8f74\u65cb\u8f6c\u89d2\u5ea6,\u9006\u65f6\u9488\u4e3a\u6b63 * c : \u6beb\u7c73\u6ce2\u76f8\u5bf9\u4e8e\u76f8\u673a\u7ed5z\u8f74\u65cb\u8f6c\u89d2\u5ea6,\u9006\u65f6\u9488\u4e3a\u6b63 **/ Eigen::Vector3d radar2camera(double x, double y, double radar_x=0.,double radar_y=0., double radar_z=0.,double a=0.,double b=0., double c=0.) { Eigen::Isometry3d T=Eigen::Isometry3d::Identity(); T.pretranslate ( Eigen::Vector3d (radar_x,radar_y,radar_z ) ); Eigen::AngleAxisd rotation_vector_x (a, Eigen::Vector3d ( 1,0,0 )); Eigen::AngleAxisd rotation_vector_y (b, Eigen::Vector3d ( 0,1,0 )); Eigen::AngleAxisd rotation_vector_z (c, Eigen::Vector3d ( 0,0,1 )); T.rotate(rotation_vector_x); T.rotate(rotation_vector_y); T.rotate(rotation_vector_z); Eigen::Vector3d vtmp(x,y,0.0); vtmp = T*vtmp; return vtmp; }","title":"\u6444\u50cf\u673a\u548c\u6beb\u7c73\u6ce2\u5750\u6807\u7cfb\u5173\u7cfb"},{"location":"%E6%91%84%E5%83%8F%E6%9C%BA%E5%92%8C%E6%AF%AB%E7%B1%B3%E6%B3%A2%E5%9D%90%E6%A0%87%E7%B3%BB%E5%85%B3%E7%B3%BB/#_1","text":"\u6444\u50cf\u673a\u9ed8\u8ba4\u7684\u662f\u4e0e\u4e16\u754c\u5750\u6807\u7cfb\u5bf9\u9f50\u7684\u4f46\u662f\u76f8\u673a\u7684z\u8f74\u65b9\u5411\u6307\u5411\u76f8\u673a\u5185\u90e8\u3002","title":"\u6444\u50cf\u673a\u5750\u6807\u7cfb"},{"location":"%E6%91%84%E5%83%8F%E6%9C%BA%E5%92%8C%E6%AF%AB%E7%B1%B3%E6%B3%A2%E5%9D%90%E6%A0%87%E7%B3%BB%E5%85%B3%E7%B3%BB/#_2","text":"\u7ea2\u8272\u4e3ax\u8f74\uff0c\u7eff\u8272\u4e3ay\u8f74\uff0c\u84dd\u8272\u4e3az\u8f74 \u7ed5x\u8f74\u7684\u65cb\u8f6c\u77e9\u9635 R_x(\\theta)=\\begin{bmatrix} 1&0&0\\\\ 0&cos\\theta&-sin\\theta\\\\ 0&sin\\theta&cos\\theta \\end{bmatrix} \u7ed5y\u8f74\u7684\u65cb\u8f6c\u77e9\u9635 R_y(\\theta)=\\begin{bmatrix} cos\\theta&0&sin\\theta\\\\ 0&1&0\\\\ -sin\\theta&0&cos\\theta \\end{bmatrix} \u7ed5z\u8f74\u65cb\u8f6c\u77e9\u9635 R_z(\\theta)=\\begin{bmatrix} cos\\theta&-sin\\theta&0\\\\ sin\\theta&cos\\theta&0\\\\ 0&0&1 \\end{bmatrix} \u901a\u7528\u65cb\u8f6c\u77e9\u9635 R=R_z(\\alpha)R_y(\\beta)R_z(\\gamma)\\\\=\\begin{bmatrix} cos\\alpha cos\\beta&cos\\alpha sin\\beta sin\\gamma -sin\\alpha cos\\gamma&cos\\alpha sin\\beta cos\\gamma+sin\\alpha sin\\gamma\\\\ sin\\alpha cos\\beta&sin\\alpha sin\\beta sin\\gamma + cos\\alpha cos\\gamma&sin\\alpha sin\\beta cos\\gamma -cos\\alpha sin\\gamma\\\\ -sin\\beta&cos\\beta sin\\gamma&cos\\beta cos\\gamma \\end{bmatrix} \u53d8\u6362\u77e9\u9635 T=\\begin{bmatrix} cos\\alpha cos\\beta&cos\\alpha sin\\beta sin\\gamma -sin\\alpha cos\\gamma&cos\\alpha sin\\beta cos\\gamma+sin\\alpha sin\\gamma&x\\\\ sin\\alpha cos\\beta&sin\\alpha sin\\beta sin\\gamma + cos\\alpha cos\\gamma&sin\\alpha sin\\beta cos\\gamma -cos\\alpha sin\\gamma&y\\\\ -sin\\beta&cos\\beta sin\\gamma&cos\\beta cos\\gamma&z\\\\ 0&0&0&1 \\end{bmatrix} /** * x :\u6beb\u7c73\u6ce2\u6570\u636e\u7684x\u5750\u6807\u503c * y :\u6beb\u7c73\u6ce2\u6570\u636e\u7684y\u5750\u6807\u503c * radar_x :\u6beb\u7c73\u6ce2\u76f8\u5bf9\u4e8e\u76f8\u673a\u5750\u6807\u7cfbx\u8f74\u7684\u5750\u6807\u4f4d\u7f6e * radar_y \uff1a\u6beb\u7c73\u6ce2\u76f8\u5bf9\u4e8e\u76f8\u673a\u5750\u6807\u7cfby\u8f74\u7684\u5750\u6807\u4f4d\u7f6e * radar_z \uff1a\u6beb\u7c73\u6ce2\u76f8\u5bf9\u4e8e\u76f8\u673a\u5750\u6807\u7cfbz\u8f74\u7684\u5750\u6807\u4f4d\u7f6e * a : \u6beb\u7c73\u6ce2\u5bf9\u4e8e\u76f8\u673a\u7ed5x\u8f74\u65cb\u8f6c\u89d2\u5ea6,\u9006\u65f6\u9488\u4e3a\u6b63 * b : \u6beb\u7c73\u6ce2\u5bf9\u4e8e\u76f8\u673a\u7ed5y\u8f74\u65cb\u8f6c\u89d2\u5ea6,\u9006\u65f6\u9488\u4e3a\u6b63 * c : \u6beb\u7c73\u6ce2\u76f8\u5bf9\u4e8e\u76f8\u673a\u7ed5z\u8f74\u65cb\u8f6c\u89d2\u5ea6,\u9006\u65f6\u9488\u4e3a\u6b63 **/ Eigen::Vector3d radar2camera(double x, double y, double radar_x=0.,double radar_y=0., double radar_z=0.,double a=0.,double b=0., double c=0.) { Eigen::Isometry3d T=Eigen::Isometry3d::Identity(); T.pretranslate ( Eigen::Vector3d (radar_x,radar_y,radar_z ) ); Eigen::AngleAxisd rotation_vector_x (a, Eigen::Vector3d ( 1,0,0 )); Eigen::AngleAxisd rotation_vector_y (b, Eigen::Vector3d ( 0,1,0 )); Eigen::AngleAxisd rotation_vector_z (c, Eigen::Vector3d ( 0,0,1 )); T.rotate(rotation_vector_x); T.rotate(rotation_vector_y); T.rotate(rotation_vector_z); Eigen::Vector3d vtmp(x,y,0.0); vtmp = T*vtmp; return vtmp; }","title":"\u6beb\u7c73\u6ce2\u5750\u6807\u7cfb"},{"location":"%E6%AF%AB%E7%B1%B3%E6%B3%A2%E5%92%8C%E6%91%84%E5%83%8F%E6%9C%BA%E8%9E%8D%E5%90%88/","text":"\u73af\u5883\u8981\u6c42 Ubuntu 18.04 Cuda 10.0 ROS melodic Qt5 \u8f6f\u4ef6\u73af\u5883\u90e8\u7f72 QT\u5b89\u88c5 sudo apt-get install qttools5-dev-tools libqt5svg5-dev qtmultimedia5-dev ROS melodic \u5b89\u88c5 ROS\u5b98\u7f51 Cuda10.0 \uff0ccudnn\u5b89\u88c5 \u5b89\u88c5\u6307\u5357 \u628a\u7a0b\u5e8f\u5305\u91cc\u7684 radar \u6587\u4ef6\u5939\u62f7\u8d1d\u5230\u672c\u5730 \u5728\u6b64\u76ee\u5f55\u4e0b\uff0c\u6267\u884c catkin_make \u6307\u4ee4 \u7f16\u8bd1\u5b8c\u6210\u540e\uff0c\u6307\u4ee4\u8f93\u5165 sudo ip link set can0 up type can bitrate 500000 roslaunch ars_40X ars_40X.launch \u5728\u8bbe\u5907\u94fe\u63a5\u6b63\u5e38\u7684\u60c5\u51b5\u4e0b\u5c31\u53ef\u770b\u5230\u8f93\u51fa ARS404\u7684\u5e94\u7528 cansend can0 200#F8000000089C0000 // Objects detection with all extended properties cansend can0 200#F8000000109C0000 // Clusters detection with all extended properties \u8fd9\u4e24\u6761\u6307\u4ee4\u5206\u522b\u542f\u52a8radar\u7684 Cluster\u6a21\u5f0f\u548cObject\u6a21\u5f0f\uff0c\u6211\u4eec\u4f7f\u7528Object\u6a21\u5f0f\uff0c\uff08cluster\u6a21\u5f0f\uff09 \u6beb\u7c73\u6ce2\u96f7\u8fbe\u6570\u636e\u662f\u4e8c\u7ef4\u6570\u636e\uff0c\u53ea\u6709x, y \u5750\u6807\u4fe1\u606f\uff0c \u663e\u793a\u5982\u4e0a\u56fe \u6beb\u7c73\u6ce2\u96f7\u8fbe\u6570\u636e\u8bfb\u53d6\uff0c\u6211\u4eec\u4f7f\u7528 ars_40X \u7a0b\u5e8f \u6570\u636e\u663e\u793a\u548c\u56fe\u50cf\u663e\u793a\uff0c\u6211\u4eec\u4f7f\u7528 ros2qt \u7a0b\u5e8f \u56fe\u50cf\u7269\u4f53\u8bc6\u522b\uff0c\u6211\u4eec\u4f7f\u7528 vision_darknet_detect \u7a0b\u5e8f \u6beb\u7c73\u6ce2\u548c\u56fe\u50cf\u7684\u878d\u5408 \u9996\u5148\u5bf9\u6444\u50cf\u5934\u8fdb\u884c\u6807\u5b9a \u83b7\u53d6\u5185\u53c2\u77e9\u9635 \u901a\u8fc7\u5185\u53c2\u77e9\u9635\u628a\u6beb\u7c73\u6ce2\u6570\u636e\u6620\u5c04\u5230\u56fe\u50cf\u4e0a \u901a\u8fc7yolo\u8bc6\u522b\uff0c\u5e76\u63d0\u53d6\u51fa\u7269\u4f53\u8ddd\u79bb\uff0c\u5982\u4e0b\u56fe \u6444\u50cf\u5934\u6807\u5b9a \u6444\u50cf\u5934\u6807\u5b9a\u53ef\u4ee5\u4f7f\u7528opencv\u63d0\u4f9b\u7684\u6807\u5b9a\u65b9\u6cd5\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528matlab\u63d0\u4f9b\u7684\u6444\u50cf\u5934\u6807\u5b9a\u63d2\u4ef6\u3002 \u6211\u4eec\u4ecb\u7ecd\u4e0bmatlab\u6444\u50cf\u5934\u6807\u5b9a\u65b9\u6cd5\u3002\u4e3a\u4e86\u65b9\u4fbf\u6807\u5b9a\uff0c\u6211\u4eec\u5236\u4f5c\u4e86\u4e24\u4e2a\u201d\u6807\u5b9a\u76d2\u5b50\u201d \u901a\u8fc7\u8be5\u6444\u50cf\u5934\u4f9d\u6b21\u91c7\u96c6\u5982\u4e0b\u4f4d\u7f6e\u56fe\u7247\u4f4d\u4e8eimage\u6587\u4ef6\u5939 \u5728matlab \u5e94\u7528\u7a0b\u5e8f\u4e0b\u627e\u5230Camera Calibration\u5de5\u5177\u7bb1 \u52a0\u8f7d\u5f85\u6807\u5b9a\u7684\u56fe\u50cf \u586b\u5199\u68cb\u76d8\u683c\u6bcf\u4e2a\u683c\u5b50\u8fb9\u957f\u7684\u771f\u5b9e\u503c \u53ef\u4ee5\u9884\u89c8\u6210\u529f\u68c0\u6d4b\u51fa\u68cb\u76d8\u683c\u7684\u56fe\u50cf\uff0c\u7136\u540e\u5f00\u59cb\u6807\u5b9a\uff0c\u70b9\u51fbCalibrate \u5e73\u5747\u8bef\u5dee\u5c0f\u4e8e0.5\u5373\u53ef \u5bfc\u51fa\u76f8\u673a\u6807\u5b9a\u53c2\u6570 \u6211\u4eec\u53ea\u9700\u8981\u7528\u5230 IntrinsicMatrix cv::Point World2Image(Eigen::Vector3d Pw) { static Eigen::Matrix3d intric = (Eigen::Matrix3d() << 465.2203, 0, 351.1325, 0, 463.8023, 256.4198, 0, 0, 1).finished(); static Eigen::Matrix3d mi = (Eigen::Matrix3d() << 1, 0, 0, 0, -1,0, 0, 0, 1).finished(); Pw = mi*Pw; Eigen::VectorXd result(2); result = intric*Pw/Pw.z(); cv::Point P (result(0),result(1)); return P; } \u901a\u8fc7world2image\u51fd\u6570\u628a\u6beb\u7c73\u6ce2\u5750\u6807\u7cfb\u8f6c\u6362\u5230\u56fe\u50cf\u5750\u6807\u7cfb \u8fd9\u6837\u5728\u56fe\u50cf\u4e2d\uff0c\u6211\u4eec\u5c31\u6709\u4e86\u6df1\u5ea6\u4fe1\u606f\u3002 \u7136\u540e\u518d\u901a\u8fc7yolo\u7684darknet\u8fdb\u884c\u7269\u4f53\u8bc6\u522b\uff0c\u5c31\u53ef\u5f97\u5230\uff0c\u969c\u788d\u7269\u7684\u7c7b\u522b\u548c\u8ddd\u79bb\u4fe1\u606f\u3002 \u8fd9\u91cc\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u6444\u50cf\u5934\u548c\u6beb\u7c73\u6ce2\u5b89\u88c5\u4f4d\u7f6e\u5173\u7cfb\uff0c\u7531\u4e8e\u6beb\u7c73\u6ce2\u53ea\u6709x\uff0cy\u4fe1\u606f\uff0c\u6ca1\u6709\u9ad8\u5ea6\u4fe1\u606f\uff0c\u6240\u4ee5\u6211\u4eec\u5c3d\u91cf\u4fdd\u6301\u6444\u50cf\u5934\u548c\u6beb\u7c73\u6ce2\u7ad6\u76f4\u65b9\u5411\u4e0a\u4e2d\u5fc3\u7ebf\u91cd\u5408\u3002 \u5907\u6ce8\uff0c\u5982\u679c\u60f3\u8981\u66f4\u6539\u6444\u50cf\u5934\u548c\u6beb\u7c73\u6ce2\u7684\u76f8\u5bf9\u4f4d\u7f6e\uff0c\u53ea\u9700\u901a\u8fc7\u521a\u6027\u53d8\u6362\u77e9\u9635\uff0c\u8fdb\u884c\u5750\u6807\u7cfb\u8f6c\u6362 $ \\begin{bmatrix}cos(\\theta)&-sin(\\theta)&0.0&x\\\\sin(\\theta)&cos(\\theta)&0.0&y\\\\0.0&0.0&1.0&z\\\\0.0&0.0&0.0&1.0\\end{bmatrix} $ \u53d8\u6362\u77e9\u9635\u91cc$ \\begin{bmatrix}cos(\\theta)&-sin(\\theta)&0.0\\\\sin(\\theta)&cos(\\theta)&0.0\\\\0.0&0.0&1.0\\end{bmatrix} $\u662f3x3\u65cb\u8f6c\u77e9\u9635\uff0cxyz\u662f\u5e73\u79fb\u77e9\u9635\u3002 \u53ea\u9700\u66f4\u6539\u5bf9\u5e94\u7684\u65cb\u8f6c\u5e73\u79fb\u53c2\u6570\uff0c\u5373\u53ef\u8fdb\u884c\u5750\u6807\u7cfb\u8f6c\u6362\u3002 darknet Yolo\u7269\u4f53\u68c0\u6d4b \u6211\u4eec\u5728\u7a0b\u5e8f\u91cc\u4f7f\u7528\u4e86yolo v3\u8fdb\u884c\u7269\u4f53\u8bc6\u522b \u5982\u4f55\u63d0\u53d6\u51fa\u8ddd\u79bb\u4fe1\u606f \u6211\u4eec\u5df2\u7ecf\u628a\u6beb\u7c73\u6ce2\u7684\u8ddd\u79bb\u4fe1\u606f\u6295\u5f71\u5230\u4e86\u56fe\u7247\u4e0a\uff0c\u540c\u65f6\uff0c\u6211\u4eec\u7528yolo\u5bf9\u969c\u788d\u7269\u8fdb\u884c\u4e86\u8bc6\u522b\u5206\u7c7b\uff0c\u753b\u51fa\u4e86\u8fb9\u754c\u6846\u3002 \u6211\u4eec\u53ea\u9700\u8981\u628a\u8fb9\u754c\u6846\u5185\u90e8\u7684\u8ddd\u79bb\u4fe1\u606f\u63d0\u53d6\u51fa\u6765\u5373\u53ef\u3002\u7531\u4e8e\u6beb\u7c73\u6ce2\u6709\u8bef\u5dee\uff0c\u6240\u4ee5\u6211\u4eec\u9700\u8981\u5bf9\u8fd9\u4e9b\u8fb9\u754c\u6846\u91cc\u7684\u4fe1\u606f\u8fdb\u884c\u7b5b\u9009\u3002\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u6444\u50cf\u5934\u53ef\u4ee5\u76f4\u63a5\u68c0\u6d4b\u5230\u7684\u7269\u4f53\uff0c\u8bf4\u660e\u6b64\u7269\u4f53\u524d\u9762\u6ca1\u6709\u969c\u788d\u7269\uff0c\u56e0\u4e3a\u5149\u662f\u76f4\u7ebf\u4f20\u64ad\u7684\u3002\u4e5f\u5c31\u662f\u8bf4\u5982\u679c\u969c\u788d\u7269\u524d\u65b9\u8fd8\u6709\u6709\u969c\u788d\u7269\uff0c\u6444\u50cf\u5934\u5f88\u5927\u53ef\u80fd\u5c31\u770b\u4e0d\u5230\u540e\u9762\u90a3\u4e00\u4e2a\uff0c\u4f46\u662f\u6beb\u7c73\u6ce2\u53ef\u80fd\u901a\u8fc7\u6f2b\u53cd\u5c04\u770b\u5230\u4e86\uff0c\u4ea7\u751f\u591a\u4e2a\u8ddd\u79bb\u4fe1\u606f\u3002\u6240\u4ee5\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u63d0\u53d6\u8fb9\u754c\u6846\u91cc\u7684\u6700\u5c0f\u8ddd\u79bb\u5f53\u505a\u8bc6\u522b\u969c\u788d\u7269\u7684\u8ddd\u79bb\u3002\u8fd9\u540c\u6837\u662f\u51fa\u4e8e\u5b89\u5168\u8003\u8651\uff0c\u9700\u8981\u9009\u62e9\u8ddd\u79bb\u6700\u8fd1\u7684\u8ddd\u79bb\u3002 \u6211\u4eec\u53ef\u4ee5\u6839\u636e\u6027\u80fd\u8981\u6c42\uff0c\u63d0\u51fa\u66f4\u597d\u7684\u63d0\u53d6\u6df1\u5ea6\u4fe1\u606f\u65b9\u6cd5\u3002 \u56fe\u4e2d\u7070\u8272\u7684\u70b9\u662f\u6beb\u7c73\u6ce2\u6295\u5c04\u5230\u56fe\u50cf\u4e0a\u7684\u70b9\uff0c\u9ec4\u8272\u7684\u6846\u662fyolo\u8bc6\u522b\u7684\u8fb9\u754c\u6846 \u6574\u4e2a\u7b97\u6cd5\u7684\u5177\u4f53\u5b9e\u73b0\u903b\u8f91\u53ef\u4ee5\u9605\u8bfb\u7a0b\u5e8f\u5305\u91cc\u5b8c\u6574\u7684\u7a0b\u5e8f\u3002 \u9884\u6d4b\u78b0\u649e\u65f6\u95f4\uff0cudp\u3002 \u6beb\u7c73\u6ce2\u5355\u76ee\u6df1\u5ea6\u878d\u5408(\u6df1\u5ea6\u5b66\u4e60) \u73af\u5883 * Ubuntu 18.04 * Cuda 10.0 * Pytorch 1.3.1 * Python 3.6.5 \u4f7f\u7528 radar_depth \u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc,\u53c2\u89c1 \u7f51\u5740 \u73af\u5883\u642d\u5efa \u6df1\u5ea6\u878d\u5408\u7684\u57fa\u7840\u662f\u6211\u4eec\u4e0a\u9762\u7684\u6570\u636e\u7ea7\u878d\u5408\u3002 \u7b2c\u4e00\u6b65\uff0c\u628a\u7a0b\u5e8f\u5305\u91cc\u7684 radar_depth \u6587\u4ef6\u5939\u62f7\u8d1d\u5230\u672c\u5730 cd radar_depth pip install -r requirement.txt \u5b89\u88c5\u8fc7\u7a0b\u4e2d\u5982\u679c\u51fa\u73b0\u8981\u6c42\u7248\u672c\u7684\u5e93\u7f3a\u5931\uff0c\u4f8b\u5982 nuscenes-devkit 1.1.5 \u53ef\u4ee5\u5355\u72ec\u5b89\u88c5 pip install nuscenes-devkit==1.1.5 \u73af\u5883\u642d\u5efa\u5b8c\u540e\uff0c\u66f4\u6539config/config_nuscenes.py. \u5904\u7684\u9879\u76ee\u8bbe\u7f6e PROJECT_ROOT = \"YOUR_PATH/radar_depth\" DATASET_ROOT = \"DATASET_PATH\" \u53ef\u4ee5\u5bf9\u8be5\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8fdb\u884c\u6df1\u5165\u7814\u7a76 pdf \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u9700\u8981\u51c6\u5907\u795e\u7ecf\u7f51\u7edc\u8f93\u5165\u6570\u636e\uff08\u56fe\u7247\u548c\u96f7\u8fbe\u6570\u636e\uff09\u8fd0\u884c\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\uff0c\u5f97\u5230\u8f93\u51fa\u6570\u636e\u3002 \u8f93\u5165 + \u96f7\u8fbe\u6570\u636e \u3002\u3002\u3002 \u8f93\u5165\u6570\u636e\u51c6\u5907\uff0c\u9700\u8981\u5229\u7528\u7a0b\u5e8f\u5305\u91cc radar/src/ars_40X/src/ros/deal_data4net.cpp \u8282\u70b9\u6765\u5236\u9020\u795e\u7ecf\u7f51\u7edc\u8f93\u5165\u6570\u636e\u3002 \u5148\u542f\u52a8\u6570\u636e\u91c7\u96c6 sudo ip link set can0 up type can bitrate 500000 roslaunch ars_40X ars_40X.launch rosrun ars_40X deal_data4net \u8fd0\u884c\u4f1a\u5b58\u6863\u4e00\u5f20\u56fe\u7247 pic.jpg \u548c \u96f7\u8fbe\u6570\u636e result.csv \u795e\u7ecf\u7f51\u7edc\u63a8\u7406 \u6211\u4eec\u4f7f\u7528 MyFun.py\u811a\u672c\u8fdb\u884c\u63a8\u7406 im = Image.open('/home/promote/pic.jpg') # \u4e3a\u56fe\u50cf\u8bfb\u53d6\u63a5\u53e3 \u3002\u3002\u3002 with open('/home/promote/result.csv', newline='') as f: #\u4e3a\u96f7\u8fbe\u8bfb\u53d6\u63a5\u53e3 \u53ea\u9700\u4fee\u6539\u4e3a\u672c\u5730\u6b63\u786e\u7684\u8def\u5f84 \u63a8\u7406 python MyFun.py \\ --evaluate /home/username/radar_depth/pretrained/resnet18_multistage.pth.tar \\ --data nuscenes \\ --arch resnet18_multistage_uncertainty_fixs \\ --modality rgbd \\ --sparsifier radar \\ --decoder upproj \u8f93\u51fa","title":"\u6beb\u7c73\u6ce2\u548c\u6444\u50cf\u673a\u878d\u5408"},{"location":"%E6%AF%AB%E7%B1%B3%E6%B3%A2%E5%92%8C%E6%91%84%E5%83%8F%E6%9C%BA%E8%9E%8D%E5%90%88/#_1","text":"Ubuntu 18.04 Cuda 10.0 ROS melodic Qt5","title":"\u73af\u5883\u8981\u6c42"},{"location":"%E6%AF%AB%E7%B1%B3%E6%B3%A2%E5%92%8C%E6%91%84%E5%83%8F%E6%9C%BA%E8%9E%8D%E5%90%88/#_2","text":"","title":"\u8f6f\u4ef6\u73af\u5883\u90e8\u7f72"},{"location":"%E6%AF%AB%E7%B1%B3%E6%B3%A2%E5%92%8C%E6%91%84%E5%83%8F%E6%9C%BA%E8%9E%8D%E5%90%88/#qt","text":"sudo apt-get install qttools5-dev-tools libqt5svg5-dev qtmultimedia5-dev","title":"QT\u5b89\u88c5"},{"location":"%E6%AF%AB%E7%B1%B3%E6%B3%A2%E5%92%8C%E6%91%84%E5%83%8F%E6%9C%BA%E8%9E%8D%E5%90%88/#ros-melodic","text":"ROS\u5b98\u7f51","title":"ROS melodic \u5b89\u88c5"},{"location":"%E6%AF%AB%E7%B1%B3%E6%B3%A2%E5%92%8C%E6%91%84%E5%83%8F%E6%9C%BA%E8%9E%8D%E5%90%88/#cuda100-cudnn","text":"\u5b89\u88c5\u6307\u5357 \u628a\u7a0b\u5e8f\u5305\u91cc\u7684 radar \u6587\u4ef6\u5939\u62f7\u8d1d\u5230\u672c\u5730 \u5728\u6b64\u76ee\u5f55\u4e0b\uff0c\u6267\u884c catkin_make \u6307\u4ee4 \u7f16\u8bd1\u5b8c\u6210\u540e\uff0c\u6307\u4ee4\u8f93\u5165 sudo ip link set can0 up type can bitrate 500000 roslaunch ars_40X ars_40X.launch \u5728\u8bbe\u5907\u94fe\u63a5\u6b63\u5e38\u7684\u60c5\u51b5\u4e0b\u5c31\u53ef\u770b\u5230\u8f93\u51fa","title":"Cuda10.0 \uff0ccudnn\u5b89\u88c5"},{"location":"%E6%AF%AB%E7%B1%B3%E6%B3%A2%E5%92%8C%E6%91%84%E5%83%8F%E6%9C%BA%E8%9E%8D%E5%90%88/#ars404","text":"cansend can0 200#F8000000089C0000 // Objects detection with all extended properties cansend can0 200#F8000000109C0000 // Clusters detection with all extended properties \u8fd9\u4e24\u6761\u6307\u4ee4\u5206\u522b\u542f\u52a8radar\u7684 Cluster\u6a21\u5f0f\u548cObject\u6a21\u5f0f\uff0c\u6211\u4eec\u4f7f\u7528Object\u6a21\u5f0f\uff0c\uff08cluster\u6a21\u5f0f\uff09 \u6beb\u7c73\u6ce2\u96f7\u8fbe\u6570\u636e\u662f\u4e8c\u7ef4\u6570\u636e\uff0c\u53ea\u6709x, y \u5750\u6807\u4fe1\u606f\uff0c \u663e\u793a\u5982\u4e0a\u56fe \u6beb\u7c73\u6ce2\u96f7\u8fbe\u6570\u636e\u8bfb\u53d6\uff0c\u6211\u4eec\u4f7f\u7528 ars_40X \u7a0b\u5e8f \u6570\u636e\u663e\u793a\u548c\u56fe\u50cf\u663e\u793a\uff0c\u6211\u4eec\u4f7f\u7528 ros2qt \u7a0b\u5e8f \u56fe\u50cf\u7269\u4f53\u8bc6\u522b\uff0c\u6211\u4eec\u4f7f\u7528 vision_darknet_detect \u7a0b\u5e8f","title":"ARS404\u7684\u5e94\u7528"},{"location":"%E6%AF%AB%E7%B1%B3%E6%B3%A2%E5%92%8C%E6%91%84%E5%83%8F%E6%9C%BA%E8%9E%8D%E5%90%88/#_3","text":"\u9996\u5148\u5bf9\u6444\u50cf\u5934\u8fdb\u884c\u6807\u5b9a \u83b7\u53d6\u5185\u53c2\u77e9\u9635 \u901a\u8fc7\u5185\u53c2\u77e9\u9635\u628a\u6beb\u7c73\u6ce2\u6570\u636e\u6620\u5c04\u5230\u56fe\u50cf\u4e0a \u901a\u8fc7yolo\u8bc6\u522b\uff0c\u5e76\u63d0\u53d6\u51fa\u7269\u4f53\u8ddd\u79bb\uff0c\u5982\u4e0b\u56fe","title":"\u6beb\u7c73\u6ce2\u548c\u56fe\u50cf\u7684\u878d\u5408"},{"location":"%E6%AF%AB%E7%B1%B3%E6%B3%A2%E5%92%8C%E6%91%84%E5%83%8F%E6%9C%BA%E8%9E%8D%E5%90%88/#_4","text":"\u6444\u50cf\u5934\u6807\u5b9a\u53ef\u4ee5\u4f7f\u7528opencv\u63d0\u4f9b\u7684\u6807\u5b9a\u65b9\u6cd5\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528matlab\u63d0\u4f9b\u7684\u6444\u50cf\u5934\u6807\u5b9a\u63d2\u4ef6\u3002 \u6211\u4eec\u4ecb\u7ecd\u4e0bmatlab\u6444\u50cf\u5934\u6807\u5b9a\u65b9\u6cd5\u3002\u4e3a\u4e86\u65b9\u4fbf\u6807\u5b9a\uff0c\u6211\u4eec\u5236\u4f5c\u4e86\u4e24\u4e2a\u201d\u6807\u5b9a\u76d2\u5b50\u201d \u901a\u8fc7\u8be5\u6444\u50cf\u5934\u4f9d\u6b21\u91c7\u96c6\u5982\u4e0b\u4f4d\u7f6e\u56fe\u7247\u4f4d\u4e8eimage\u6587\u4ef6\u5939 \u5728matlab \u5e94\u7528\u7a0b\u5e8f\u4e0b\u627e\u5230Camera Calibration\u5de5\u5177\u7bb1 \u52a0\u8f7d\u5f85\u6807\u5b9a\u7684\u56fe\u50cf \u586b\u5199\u68cb\u76d8\u683c\u6bcf\u4e2a\u683c\u5b50\u8fb9\u957f\u7684\u771f\u5b9e\u503c \u53ef\u4ee5\u9884\u89c8\u6210\u529f\u68c0\u6d4b\u51fa\u68cb\u76d8\u683c\u7684\u56fe\u50cf\uff0c\u7136\u540e\u5f00\u59cb\u6807\u5b9a\uff0c\u70b9\u51fbCalibrate \u5e73\u5747\u8bef\u5dee\u5c0f\u4e8e0.5\u5373\u53ef \u5bfc\u51fa\u76f8\u673a\u6807\u5b9a\u53c2\u6570 \u6211\u4eec\u53ea\u9700\u8981\u7528\u5230 IntrinsicMatrix cv::Point World2Image(Eigen::Vector3d Pw) { static Eigen::Matrix3d intric = (Eigen::Matrix3d() << 465.2203, 0, 351.1325, 0, 463.8023, 256.4198, 0, 0, 1).finished(); static Eigen::Matrix3d mi = (Eigen::Matrix3d() << 1, 0, 0, 0, -1,0, 0, 0, 1).finished(); Pw = mi*Pw; Eigen::VectorXd result(2); result = intric*Pw/Pw.z(); cv::Point P (result(0),result(1)); return P; } \u901a\u8fc7world2image\u51fd\u6570\u628a\u6beb\u7c73\u6ce2\u5750\u6807\u7cfb\u8f6c\u6362\u5230\u56fe\u50cf\u5750\u6807\u7cfb \u8fd9\u6837\u5728\u56fe\u50cf\u4e2d\uff0c\u6211\u4eec\u5c31\u6709\u4e86\u6df1\u5ea6\u4fe1\u606f\u3002 \u7136\u540e\u518d\u901a\u8fc7yolo\u7684darknet\u8fdb\u884c\u7269\u4f53\u8bc6\u522b\uff0c\u5c31\u53ef\u5f97\u5230\uff0c\u969c\u788d\u7269\u7684\u7c7b\u522b\u548c\u8ddd\u79bb\u4fe1\u606f\u3002 \u8fd9\u91cc\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u6444\u50cf\u5934\u548c\u6beb\u7c73\u6ce2\u5b89\u88c5\u4f4d\u7f6e\u5173\u7cfb\uff0c\u7531\u4e8e\u6beb\u7c73\u6ce2\u53ea\u6709x\uff0cy\u4fe1\u606f\uff0c\u6ca1\u6709\u9ad8\u5ea6\u4fe1\u606f\uff0c\u6240\u4ee5\u6211\u4eec\u5c3d\u91cf\u4fdd\u6301\u6444\u50cf\u5934\u548c\u6beb\u7c73\u6ce2\u7ad6\u76f4\u65b9\u5411\u4e0a\u4e2d\u5fc3\u7ebf\u91cd\u5408\u3002 \u5907\u6ce8\uff0c\u5982\u679c\u60f3\u8981\u66f4\u6539\u6444\u50cf\u5934\u548c\u6beb\u7c73\u6ce2\u7684\u76f8\u5bf9\u4f4d\u7f6e\uff0c\u53ea\u9700\u901a\u8fc7\u521a\u6027\u53d8\u6362\u77e9\u9635\uff0c\u8fdb\u884c\u5750\u6807\u7cfb\u8f6c\u6362 $ \\begin{bmatrix}cos(\\theta)&-sin(\\theta)&0.0&x\\\\sin(\\theta)&cos(\\theta)&0.0&y\\\\0.0&0.0&1.0&z\\\\0.0&0.0&0.0&1.0\\end{bmatrix} $ \u53d8\u6362\u77e9\u9635\u91cc$ \\begin{bmatrix}cos(\\theta)&-sin(\\theta)&0.0\\\\sin(\\theta)&cos(\\theta)&0.0\\\\0.0&0.0&1.0\\end{bmatrix} $\u662f3x3\u65cb\u8f6c\u77e9\u9635\uff0cxyz\u662f\u5e73\u79fb\u77e9\u9635\u3002 \u53ea\u9700\u66f4\u6539\u5bf9\u5e94\u7684\u65cb\u8f6c\u5e73\u79fb\u53c2\u6570\uff0c\u5373\u53ef\u8fdb\u884c\u5750\u6807\u7cfb\u8f6c\u6362\u3002","title":"\u6444\u50cf\u5934\u6807\u5b9a"},{"location":"%E6%AF%AB%E7%B1%B3%E6%B3%A2%E5%92%8C%E6%91%84%E5%83%8F%E6%9C%BA%E8%9E%8D%E5%90%88/#darknet","text":"Yolo\u7269\u4f53\u68c0\u6d4b \u6211\u4eec\u5728\u7a0b\u5e8f\u91cc\u4f7f\u7528\u4e86yolo v3\u8fdb\u884c\u7269\u4f53\u8bc6\u522b","title":"darknet"},{"location":"%E6%AF%AB%E7%B1%B3%E6%B3%A2%E5%92%8C%E6%91%84%E5%83%8F%E6%9C%BA%E8%9E%8D%E5%90%88/#_5","text":"\u6211\u4eec\u5df2\u7ecf\u628a\u6beb\u7c73\u6ce2\u7684\u8ddd\u79bb\u4fe1\u606f\u6295\u5f71\u5230\u4e86\u56fe\u7247\u4e0a\uff0c\u540c\u65f6\uff0c\u6211\u4eec\u7528yolo\u5bf9\u969c\u788d\u7269\u8fdb\u884c\u4e86\u8bc6\u522b\u5206\u7c7b\uff0c\u753b\u51fa\u4e86\u8fb9\u754c\u6846\u3002 \u6211\u4eec\u53ea\u9700\u8981\u628a\u8fb9\u754c\u6846\u5185\u90e8\u7684\u8ddd\u79bb\u4fe1\u606f\u63d0\u53d6\u51fa\u6765\u5373\u53ef\u3002\u7531\u4e8e\u6beb\u7c73\u6ce2\u6709\u8bef\u5dee\uff0c\u6240\u4ee5\u6211\u4eec\u9700\u8981\u5bf9\u8fd9\u4e9b\u8fb9\u754c\u6846\u91cc\u7684\u4fe1\u606f\u8fdb\u884c\u7b5b\u9009\u3002\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u6444\u50cf\u5934\u53ef\u4ee5\u76f4\u63a5\u68c0\u6d4b\u5230\u7684\u7269\u4f53\uff0c\u8bf4\u660e\u6b64\u7269\u4f53\u524d\u9762\u6ca1\u6709\u969c\u788d\u7269\uff0c\u56e0\u4e3a\u5149\u662f\u76f4\u7ebf\u4f20\u64ad\u7684\u3002\u4e5f\u5c31\u662f\u8bf4\u5982\u679c\u969c\u788d\u7269\u524d\u65b9\u8fd8\u6709\u6709\u969c\u788d\u7269\uff0c\u6444\u50cf\u5934\u5f88\u5927\u53ef\u80fd\u5c31\u770b\u4e0d\u5230\u540e\u9762\u90a3\u4e00\u4e2a\uff0c\u4f46\u662f\u6beb\u7c73\u6ce2\u53ef\u80fd\u901a\u8fc7\u6f2b\u53cd\u5c04\u770b\u5230\u4e86\uff0c\u4ea7\u751f\u591a\u4e2a\u8ddd\u79bb\u4fe1\u606f\u3002\u6240\u4ee5\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u63d0\u53d6\u8fb9\u754c\u6846\u91cc\u7684\u6700\u5c0f\u8ddd\u79bb\u5f53\u505a\u8bc6\u522b\u969c\u788d\u7269\u7684\u8ddd\u79bb\u3002\u8fd9\u540c\u6837\u662f\u51fa\u4e8e\u5b89\u5168\u8003\u8651\uff0c\u9700\u8981\u9009\u62e9\u8ddd\u79bb\u6700\u8fd1\u7684\u8ddd\u79bb\u3002 \u6211\u4eec\u53ef\u4ee5\u6839\u636e\u6027\u80fd\u8981\u6c42\uff0c\u63d0\u51fa\u66f4\u597d\u7684\u63d0\u53d6\u6df1\u5ea6\u4fe1\u606f\u65b9\u6cd5\u3002 \u56fe\u4e2d\u7070\u8272\u7684\u70b9\u662f\u6beb\u7c73\u6ce2\u6295\u5c04\u5230\u56fe\u50cf\u4e0a\u7684\u70b9\uff0c\u9ec4\u8272\u7684\u6846\u662fyolo\u8bc6\u522b\u7684\u8fb9\u754c\u6846 \u6574\u4e2a\u7b97\u6cd5\u7684\u5177\u4f53\u5b9e\u73b0\u903b\u8f91\u53ef\u4ee5\u9605\u8bfb\u7a0b\u5e8f\u5305\u91cc\u5b8c\u6574\u7684\u7a0b\u5e8f\u3002 \u9884\u6d4b\u78b0\u649e\u65f6\u95f4\uff0cudp\u3002","title":"\u5982\u4f55\u63d0\u53d6\u51fa\u8ddd\u79bb\u4fe1\u606f"},{"location":"%E6%AF%AB%E7%B1%B3%E6%B3%A2%E5%92%8C%E6%91%84%E5%83%8F%E6%9C%BA%E8%9E%8D%E5%90%88/#_6","text":"\u73af\u5883 * Ubuntu 18.04 * Cuda 10.0 * Pytorch 1.3.1 * Python 3.6.5 \u4f7f\u7528 radar_depth \u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc,\u53c2\u89c1 \u7f51\u5740 \u73af\u5883\u642d\u5efa \u6df1\u5ea6\u878d\u5408\u7684\u57fa\u7840\u662f\u6211\u4eec\u4e0a\u9762\u7684\u6570\u636e\u7ea7\u878d\u5408\u3002 \u7b2c\u4e00\u6b65\uff0c\u628a\u7a0b\u5e8f\u5305\u91cc\u7684 radar_depth \u6587\u4ef6\u5939\u62f7\u8d1d\u5230\u672c\u5730 cd radar_depth pip install -r requirement.txt \u5b89\u88c5\u8fc7\u7a0b\u4e2d\u5982\u679c\u51fa\u73b0\u8981\u6c42\u7248\u672c\u7684\u5e93\u7f3a\u5931\uff0c\u4f8b\u5982 nuscenes-devkit 1.1.5 \u53ef\u4ee5\u5355\u72ec\u5b89\u88c5 pip install nuscenes-devkit==1.1.5 \u73af\u5883\u642d\u5efa\u5b8c\u540e\uff0c\u66f4\u6539config/config_nuscenes.py. \u5904\u7684\u9879\u76ee\u8bbe\u7f6e PROJECT_ROOT = \"YOUR_PATH/radar_depth\" DATASET_ROOT = \"DATASET_PATH\" \u53ef\u4ee5\u5bf9\u8be5\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8fdb\u884c\u6df1\u5165\u7814\u7a76 pdf \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u9700\u8981\u51c6\u5907\u795e\u7ecf\u7f51\u7edc\u8f93\u5165\u6570\u636e\uff08\u56fe\u7247\u548c\u96f7\u8fbe\u6570\u636e\uff09\u8fd0\u884c\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\uff0c\u5f97\u5230\u8f93\u51fa\u6570\u636e\u3002","title":"\u6beb\u7c73\u6ce2\u5355\u76ee\u6df1\u5ea6\u878d\u5408(\u6df1\u5ea6\u5b66\u4e60)"},{"location":"%E6%AF%AB%E7%B1%B3%E6%B3%A2%E5%92%8C%E6%91%84%E5%83%8F%E6%9C%BA%E8%9E%8D%E5%90%88/#_7","text":"+ \u96f7\u8fbe\u6570\u636e \u3002\u3002\u3002 \u8f93\u5165\u6570\u636e\u51c6\u5907\uff0c\u9700\u8981\u5229\u7528\u7a0b\u5e8f\u5305\u91cc radar/src/ars_40X/src/ros/deal_data4net.cpp \u8282\u70b9\u6765\u5236\u9020\u795e\u7ecf\u7f51\u7edc\u8f93\u5165\u6570\u636e\u3002 \u5148\u542f\u52a8\u6570\u636e\u91c7\u96c6 sudo ip link set can0 up type can bitrate 500000 roslaunch ars_40X ars_40X.launch rosrun ars_40X deal_data4net \u8fd0\u884c\u4f1a\u5b58\u6863\u4e00\u5f20\u56fe\u7247 pic.jpg \u548c \u96f7\u8fbe\u6570\u636e result.csv","title":"\u8f93\u5165"},{"location":"%E6%AF%AB%E7%B1%B3%E6%B3%A2%E5%92%8C%E6%91%84%E5%83%8F%E6%9C%BA%E8%9E%8D%E5%90%88/#_8","text":"\u6211\u4eec\u4f7f\u7528 MyFun.py\u811a\u672c\u8fdb\u884c\u63a8\u7406 im = Image.open('/home/promote/pic.jpg') # \u4e3a\u56fe\u50cf\u8bfb\u53d6\u63a5\u53e3 \u3002\u3002\u3002 with open('/home/promote/result.csv', newline='') as f: #\u4e3a\u96f7\u8fbe\u8bfb\u53d6\u63a5\u53e3 \u53ea\u9700\u4fee\u6539\u4e3a\u672c\u5730\u6b63\u786e\u7684\u8def\u5f84 \u63a8\u7406 python MyFun.py \\ --evaluate /home/username/radar_depth/pretrained/resnet18_multistage.pth.tar \\ --data nuscenes \\ --arch resnet18_multistage_uncertainty_fixs \\ --modality rgbd \\ --sparsifier radar \\ --decoder upproj","title":"\u795e\u7ecf\u7f51\u7edc\u63a8\u7406"},{"location":"%E6%AF%AB%E7%B1%B3%E6%B3%A2%E5%92%8C%E6%91%84%E5%83%8F%E6%9C%BA%E8%9E%8D%E5%90%88/#_9","text":"","title":"\u8f93\u51fa"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/","text":"\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50\u5c01\u88c5\uff561.0 \u786c\u4ef6\u73af\u5883\u7ec4\u6210 \u9002\u7528\u4e8eAGX\u63a7\u5236\u5668 velodyne\u6fc0\u5149\u96f7\u8fbe\uff0c \u6ce8\uff1a\u5176\u4ed6\u54c1\u724c\u96f7\u8fbe\u53ea\u9700\u66f4\u6362\u9a71\u52a8\u548c\u5bf9\u5e94\u7684\u70b9\u4e91topic \u8f6f\u4ef6\u73af\u5883\u90e8\u7f72 ubuntu18.04\u64cd\u4f5c\u7cfb\u7edf \u6839\u636eAGX\u63d0\u4f9b\u7684\u7cfb\u7edf\u5305Jetpack\u5b89\u88c5 Cuda\u548ccudnn\u5b89\u88c5 \u901a\u8fc7Jetpack\u5b89\u88c5 ROS\u5b89\u88c5 \u4e0b\u8f7d git clone https://github.com/jetsonhacks/installROSXavier.git \u5207\u6362\u76ee\u5f55 cd installROSXavier \u5b89\u88c5 ./installROS.sh -p ros-melodic-desktop -p ros-melodic-rgbd-launch \u63d2\u4ef6\u5b89\u88c5 sudo apt-get install ros-melodic-jsk-rviz-plugins caffe \u90e8\u7f72 \u4e0b\u8f7d git clone https://github.com/BVLC/caffe.git \u5b89\u88c5\u4f9d\u8d56 sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler sudo apt-get install --no-install-recommends libboost-all-dev sudo apt-get install libopenblas-dev sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev \u5207\u6362\u76ee\u5f55 cd caffe \u65b0\u5efaMakefile.config\u6587\u4ef6 ,\u5e76\u7c98\u8d34\u4e0b\u9762\u7684\u5185\u5bb9 ``` \uff41## Refer to http://caffe.berkeleyvision.org/installation.html # Contributions simplifying and improving our build system are welcome! cuDNN acceleration switch (uncomment to build with cuDNN). USE_CUDNN := 1 CPU-only switch (uncomment to build without GPU support). CPU_ONLY := 1 uncomment to disable IO dependencies and corresponding data layers USE_OPENCV := 0 USE_LEVELDB := 0 USE_LMDB := 0 uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary) You should not set this flag if you will be reading LMDBs with any possibility of simultaneous read and write ALLOW_LMDB_NOLOCK := 1 Uncomment if you're using OpenCV 3 OPENCV_VERSION := 3 To customize your choice of compiler, uncomment and set the following. N.B. the default for Linux is g++ and the default for OSX is clang++ CUSTOM_CXX := g++ CUDA directory contains bin/ and lib/ directories that we need. CUDA_DIR := /usr/local/cuda On Ubuntu 14.04, if cuda tools are installed via \"sudo apt-get install nvidia-cuda-toolkit\" then use this instead: CUDA_DIR := /usr CUDA architecture setting: going with all of them. For CUDA < 6.0, comment the lines after *_35 for compatibility. CUDA_ARCH := -gencode arch=compute_30,code=sm_30 \\ -gencode arch=compute_35,code=sm_35 \\ -gencode arch=compute_50,code=sm_50 \\ -gencode arch=compute_52,code=sm_52 \\ -gencode arch=compute_61,code=sm_61 \\ -gencode arch=compute_61,code=compute_61 # -gencode arch=compute_20,code=sm_20 \\ # -gencode arch=compute_20,code=sm_21 \\ BLAS choice: atlas for ATLAS (default) mkl for MKL open for OpenBlas BLAS := atlas BLAS := open Custom (MKL/ATLAS/OpenBLAS) include and lib directories. Leave commented to accept the defaults for your choice of BLAS (which should work)! BLAS_INCLUDE := /path/to/your/blas BLAS_LIB := /path/to/your/blas Homebrew puts openblas in a directory that is not on the standard search path BLAS_INCLUDE := $(shell brew --prefix openblas)/include BLAS_LIB := $(shell brew --prefix openblas)/lib This is required only if you will compile the matlab interface. MATLAB directory should contain the mex binary in /bin. MATLAB_DIR := /usr/local MATLAB_DIR := /Applications/MATLAB_R2012b.app NOTE: this is required only if you will compile the python interface. We need to be able to find Python.h and numpy/arrayobject.h. PYTHON_INCLUDE := /usr/include/python2.7 \\ /usr/lib/python2.7/dist-packages/numpy/core/include Anaconda Python distribution is quite popular. Include path: Verify anaconda location, sometimes it's in root. ANACONDA_HOME := $(HOME)/anaconda2 # PYTHON_INCLUDE := $(ANACONDA_HOME)/include \\ $(ANACONDA_HOME)/include/python2.7 \\ $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include \\ Uncomment to use Python 3 (default is Python 2) PYTHON_LIBRARIES := boost_python3 python3.5m # PYTHON_INCLUDE := /usr/include/python3.5m \\ /usr/lib/python3.5/dist-packages/numpy/core/include We need to be able to find libpythonX.X.so or .dylib. PYTHON_LIB := /usr/lib PYTHON_LIB := $(ANACONDA_HOME)/lib Homebrew installs numpy in a non standard path (keg only) PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core. file )'))/include PYTHON_LIB += $(shell brew --prefix numpy)/lib Uncomment to support layers written in Python (will link against Python libs) WITH_PYTHON_LAYER := 1 Whatever else you find you need goes here. INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial/ LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/aarch64-linux-gnu/hdf5/serial If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies INCLUDE_DIRS += $(shell brew --prefix)/include LIBRARY_DIRS += $(shell brew --prefix)/lib Uncomment to use pkg-config to specify OpenCV library paths. (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.) USE_PKG_CONFIG := 1 N.B. both build and distribute dirs are cleared on make clean BUILD_DIR := build DISTRIBUTE_DIR := distribute Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171 DEBUG := 1 The ID of the GPU that 'make runtest' will use to run unit tests. TEST_GPUID := 0 enable pretty build (comment to see full commands) Q ?= @ ``` * \u7f16\u8bd1 * make -j 8 * sudo make distribute qt\u5b89\u88c5 sudo apt-get install qttools5-dev-tools libqt5svg5-dev qtmultimedia5-dev \u7b97\u6cd5\u76d2\u5b50\u7ec4\u6210\u90e8\u5206 \u70b9\u4e91\u6807\u5b9a\u529f\u80fd \u70b9\u4e91\u5efa\u56fe\u529f\u80fd \u70b9\u4e91\u805a\u7c7b\u529f\u80fd \u70b9\u4e91\u667a\u80fd\u8bc6\u522b\u529f\u80fd \u7a0b\u5e8f\u6587\u4ef6\u5939\u76ee\u5f55\u7ed3\u6784 \u7a0b\u5e8f catkin_ws src common detected_objects_visualizer lidar_cnn_seg_detect lidar_demo ndt_mapping rockauto_msgs ros2qt velodyne CMakeLists.txt \u6211\u4eec\u9700\u8981\u628acatkin_ws\u6587\u4ef6\u5939\u62f7\u8d1d\u5230\u5de5\u63a7\u673a\u7684home\u76ee\u5f55\u4e0b\u3002 \u7f16\u8bd1\u7a0b\u5e8f \u5207\u6362\u76ee\u5f55 cd catkin_ws \u7f16\u8bd1 catkin_make source \u73af\u5883 source devel/setup.bash \u7a0b\u5e8f\u542f\u52a8 rosrun ros2qt ros2qt \u5e94\u7528\u8bb2\u89e3 \u7a0b\u5e8f\u542f\u52a8\u540e\u5f39\u51fa\u4e0b\u56fe\u754c\u9762 \u5982\u4e0b\u56fe\u914d\u7f6e Fixed Frame \u4e3a velodyne ,\u52fe\u9009 pointCloud2 \u590d\u9009\u6846\uff0c\u8bbe\u7f6e\u5176 Topic \u4e3a point_raw \u53ef\u89c2\u770b\u96f7\u8fbe\u6570\u636e \u542f\u7528\uff33\uff2c\uff21\uff2d\u529f\u80fd\uff0c\u5219\u52fe\u9009 SLAM \u590d\u9009\u6846,\u5c06\u5176 Topic \u8bbe\u7f6e\u4e3a point_raw \uff0c\u540c\u65f6\u5c06 pointCloud2 \u7684 Topic \u8bbe\u7f6e\u4e3a cloud \u3002 Slam\u63d0\u4f9b\u4e86\u53ef\u8c03\u53c2\u6570\uff1a Topic:\u8bbe\u7f6e\u6784\u56fe\u4e3b\u9898 Size:\u8bbe\u7f6e\u70b9\u4e91\u5730\u56fe\u4e2d\u70b9\u7684\u663e\u793a\u5927\u5c0f Color:\u70b9\u4e91\u5730\u56fe\u7684\u989c\u8272 Res:\u5206\u8fa8\u7387 Step_size:\u6b65\u5e45 Trans_epsilon:\u6536\u655b\u5747\u65b9\u5dee Max_iter:\u6700\u5927\u8fed\u4ee3\u6b21\u6570 Voxel_leaf_size:Voxel\u5c3a\u5bf8 Min_scan_range:\u6700\u5c0f\u626b\u63cf\u8303\u56f4 Max_scan_range:\u6700\u5927\u626b\u63cf\u8303\u56f4 Scan_rate:\u626b\u63cf\u5468\u671f \u663e\u793a\u969c\u788d\u7269\uff0c\u5219\u53d6\u6d88 SLAM \u52fe\u9009\uff0c\u5c06 pointCloud2 \u7684 Topic \u66f4\u6539\u56de point_raw .\u52fe\u9009 \uff22oundingBox \u590d\u9009\u6846\u3002 \u663e\u793a\u4f4d\u7f6e\uff0c\u901f\u5ea6\uff0c\u52a0\u901f\u5ea6\u548c\u8def\u5f84 \u663e\u793a\u805a\u7c7b\u969c\u788d\u7269\uff0c\u5219\u52fe\u9009'BoundingBox2'\u590d\u9009\u6846\u3002 \u8fd9\u91cc\u67093\u4e2a\u53c2\u6570: Cluster_D:\u805a\u7c7b\u76f4\u5f84 Cluster_Min:\u6700\u5c0f\u805a\u7c7b\u70b9\u6570 Cluster_Max:\u6700\u5927\u805a\u7c7b\u70b9\u6570 \u70b9\u4e91\u6807\u5b9a \u9996\u5148\u8981\u52fe\u9009 PointCloud2 \u663e\u793a\u70b9\u4e91\uff0c\u518d\u52fe\u9009 \u6807\u5b9a \u53ef\u79fb\u52a8xyz\u4e09\u8f74\uff0c\u4e5f\u53ef\u7ed5\u4e09\u8f74\u65cb\u8f6c\u3002 \u7a0b\u5e8f\u8bb2\u89e3 \u70b9\u4e91\u6807\u5b9a\u529f\u80fd \u6807\u5b9a\u7a0b\u5e8f\u529f\u80fd\u4ee3\u7801\u6bb5\u4f4d\u4e8e\"ros2qt\\src\\qnode.cpp\"\u6587\u4ef6 void QNode::biaodingCallback(const PointCloud::ConstPtr& msg) { //\u70b9\u4e91\u65cb\u8f6c Eigen::Affine3f transform_2 = Eigen::Affine3f::Identity(); transform_2.translation() << x_pianyi, y_pianyi, z_pianyi; float theta_2 = 0*M_PI/180; transform_2.rotate (Eigen::AngleAxisf (theta_2, Eigen::Vector3f::UnitY())); pcl::PointCloud<pcl::PointXYZ>::Ptr transformed_cloud_2 (new pcl::PointCloud<pcl::PointXYZ> ()); pcl::transformPointCloud (*msg, *transformed_cloud_2, transform_2); Eigen::Affine3f transform = Eigen::Affine3f::Identity(); transform.translation() << 0.0, 0.0, 0.0; float theta = x_xuanzhuan*M_PI/180; transform.rotate (Eigen::AngleAxisf (theta, Eigen::Vector3f::UnitX())); pcl::PointCloud<pcl::PointXYZ>::Ptr transformed_cloud (new pcl::PointCloud<pcl::PointXYZ> ()); pcl::transformPointCloud (*transformed_cloud_2, *transformed_cloud, transform); Eigen::Affine3f transform_3 = Eigen::Affine3f::Identity(); transform_3.translation() << 0.0, 0.0, 0.0; float theta_3 = z_xuanzhuan*M_PI/180; transform_3.rotate (Eigen::AngleAxisf (theta_3, Eigen::Vector3f::UnitZ())); pcl::PointCloud<pcl::PointXYZ>::Ptr transformed_cloud_3 (new pcl::PointCloud<pcl::PointXYZ> ()); pcl::transformPointCloud (*transformed_cloud, *transformed_cloud_3, transform_3); Eigen::Affine3f transform_4 = Eigen::Affine3f::Identity(); transform_4.translation() << 0.0, 0.0, 0.0; float theta_4 = y_xuanzhuan*M_PI/180; transform_4.rotate (Eigen::AngleAxisf (theta_4, Eigen::Vector3f::UnitY())); pcl::PointCloud<pcl::PointXYZ>::Ptr transformed_cloud_4 (new pcl::PointCloud<pcl::PointXYZ> ()); pcl::transformPointCloud (*transformed_cloud_3, *transformed_cloud_4, transform_4); transformed_cloud_4->header.frame_id = frame_id; pub.publish(transformed_cloud_4); } \u70b9\u4e91\u7684\u6807\u5b9a\uff0c\u4e3b\u8981\u662f\u70b9\u4e91\u7684\u65cb\u8f6c\u548c\u5e73\u79fb\u529f\u80fd\uff0c\u53ef\u4ee5\u53c2\u8003pcl\uff0c\u8fd9\u91cc\u7b80\u5355\u8bb2\u89e3\u4e00\u4e0b\u65cb\u8f6c\u5e73\u79fb\u529f\u80fd \u9996\u5148\u5b9a\u4e49\u4e00\u4e2aAffine3f\u7ed3\u6784\u4f53\uff0c\u7528\u6765\u5b58\u50a8\u70b9\u4e91\u65cb\u8f6c\u548c\u5e73\u79fb\u4fe1\u606f Eigen::Affine3f transform_2 = Eigen::Affine3f::Identity(); \u518d\u628a\uff58\uff0c\uff59\uff0c\uff5a\u8f74\u7684\u5e73\u79fb\u91cfx_pianyi, y_pianyi, z_pianyi\u4f20\u9012\u8fdbAffine3f\u7ed3\u6784\u4f53 transform_2.translation() << x_pianyi, y_pianyi, z_pianyi; \u5b9a\u4e49\u4e00\u4e2a\u65cb\u8f6c\u89d2\u5ea6 float theta_2 = 0*M_PI/180; \u628a\u7ed5\uff39\u8f74\u65cb\u8f6c\u89d2\u5ea6\uff0c\u4f20\u5165Affin3f\u7ed3\u6784\u4f53\uff0c\u65cb\u8f6c\u8f74Eigen::Vector3f::UnitY() transform_2.rotate (Eigen::AngleAxisf (theta_2, Eigen::Vector3f::UnitY())); \u5b9a\u4e49\u65cb\u8f6c\u540e\u7684\u70b9\u4e91 pcl::PointCloudpcl::pointxyz::Ptr transformed_cloud_2 (new pcl::PointCloudpcl::pointxyz ()); \u6267\u884c\u70b9\u4e91\u7684\u5e73\u79fb\u548c\u65cb\u8f6c\uff0c\u5e76\u628a\u7ed3\u679c\u5b58\u5165 transformed_cloud_2 pcl::transformPointCloud (*msg, *transformed_cloud_2, transform_2); \u70b9\u4e91\u5efa\u56fe\u529f\u80fd \u5efa\u56fe\u7a0b\u5e8f\u5305\u5373ndt_mapping\u6587\u4ef6\u5939,\u91cd\u70b9\u8fd0\u7b97\u903b\u8f91\u5728ndt_mapping.cpp\u6587\u4ef6\u5185\uff0c\u8fd9\u91cc\u89e3\u6790\u4e00\u4e0b ndt_mapping::ndt_mapping() { transform_pub = nh_.advertise<sensor_msgs::PointCloud2> (\"/cloud\", 1, false); points_sub_ = nh_.subscribe(slam_topic, 100000, &ndt_mapping::points_callback,this); ndt_map_pub_ = nh_.advertise<sensor_msgs::PointCloud2>(\"/ndt_map\", 1000); current_pose_pub_ = nh_.advertise<geometry_msgs::PoseStamped>(\"/current_pose\", 1000); max_iter_ = max_iter_1; ndt_res_ = ndt_res_1; step_size_ = step_size_1; trans_eps_ = trans_eps_1; voxel_leaf_size_ = voxel_leaf_size_1; scan_rate_ = scan_rate_1; min_scan_range_ = min_scan_range_1; max_scan_range_ = max_scan_range_1; min_add_scan_shift_ = min_add_scan_shift_1; initial_scan_loaded = 0; min_add_scan_shift_ = 1.0; _tf_x=0.0, _tf_y=0.0, _tf_z=0.0, _tf_roll=0.0, _tf_pitch=0.0, _tf_yaw=0.0; Eigen::Translation3f tl_btol(_tf_x, _tf_y, _tf_z); Eigen::AngleAxisf rot_x_btol(_tf_roll, Eigen::Vector3f::UnitX()); Eigen::AngleAxisf rot_y_btol(_tf_pitch, Eigen::Vector3f::UnitY()); Eigen::AngleAxisf rot_z_btol(_tf_yaw, Eigen::Vector3f::UnitZ()); tf_btol_ = (tl_btol * rot_z_btol * rot_y_btol * rot_x_btol).matrix(); tf_ltob_ = tf_btol_.inverse(); map_.header.frame_id = \"velodyne\"; current_pose_.x = current_pose_.y = current_pose_.z = 0.0;current_pose_.roll = current_pose_.pitch = current_pose_.yaw = 0.0; previous_pose_.x = previous_pose_.y = previous_pose_.z = 0.0;previous_pose_.roll = previous_pose_.pitch = previous_pose_.yaw = 0.0; voxel_grid_filter_.setLeafSize(voxel_leaf_size_, voxel_leaf_size_, voxel_leaf_size_); ndt.setTransformationEpsilon(trans_eps_); ndt.setStepSize(step_size_); ndt.setResolution(ndt_res_); ndt.setMaximumIterations(max_iter_); is_first_map_ = true; }; ndt_mapping::ndt_mapping()\u51fd\u6570\uff0c\u4e3b\u8981\u8fdb\u884c\u4e86\u4e00\u4e9b\u53c2\u6570\u8bbe\u5b9a\uff0c\u548ctopic\u8bbe\u5b9a\u3002 void ndt_mapping::points_callback(const sensor_msgs::PointCloud2::ConstPtr& input) { pcl::PointCloud<pcl::PointXYZI> tmp, scan; pcl::PointCloud<pcl::PointXYZI>::Ptr filtered_scan_ptr(new pcl::PointCloud<pcl::PointXYZI>()); pcl::PointCloud<pcl::PointXYZI>::Ptr transformed_scan_ptr(new pcl::PointCloud<pcl::PointXYZI>()); tf::Quaternion q; Eigen::Matrix4f t_localizer(Eigen::Matrix4f::Identity()); Eigen::Matrix4f t_base_link(Eigen::Matrix4f::Identity()); static tf::TransformBroadcaster br_; tf::Transform transform; pcl::fromROSMsg(*input, tmp); double r; Eigen::Vector3d point_pos; pcl::PointXYZI p; for (pcl::PointCloud<pcl::PointXYZI>::const_iterator item = tmp.begin(); item != tmp.end(); item++) { use_imu_ = false; if(use_imu_){ // deskew(TODO:inplement of predicting pose by imu) point_pos.x() = (double)item->x; point_pos.y() = (double)item->y; point_pos.z() = (double)item->z; double s = scan_rate_ * (double(item->intensity) - int(item->intensity)); point_pos.x() -= s * current_pose_msg_.pose.position.x;//current_pose_imu_ point_pos.y() -= s * current_pose_msg_.pose.position.y; point_pos.z() -= s * current_pose_msg_.pose.position.z; Eigen::Quaterniond start_quat, end_quat, mid_quat; mid_quat.setIdentity(); end_quat = Eigen::Quaterniond( current_pose_msg_.pose.orientation.w, current_pose_msg_.pose.orientation.x, current_pose_msg_.pose.orientation.y, current_pose_msg_.pose.orientation.z); start_quat = mid_quat.slerp(s, end_quat); point_pos = start_quat.conjugate() * start_quat * point_pos; point_pos.x() += current_pose_msg_.pose.position.x; point_pos.y() += current_pose_msg_.pose.position.y; point_pos.z() += current_pose_msg_.pose.position.z; p.x = point_pos.x(); p.y = point_pos.y(); p.z = point_pos.z(); } else{ p.x = (double)item->x; p.y = (double)item->y; p.z = (double)item->z; } p.intensity = (double)item->intensity; r = sqrt(pow(p.x, 2.0) + pow(p.y, 2.0)); if (min_scan_range_ < r && r < max_scan_range_) { scan.push_back(p); } } pcl::PointCloud<pcl::PointXYZI>::Ptr scan_ptr(new pcl::PointCloud<pcl::PointXYZI>(scan)); if (initial_scan_loaded == 0) { pcl::transformPointCloud(*scan_ptr, *transformed_scan_ptr, tf_btol_); map_ += *transformed_scan_ptr; initial_scan_loaded = 1; } voxel_grid_filter_.setInputCloud(scan_ptr); voxel_grid_filter_.filter(*filtered_scan_ptr); ndt.setInputSource(filtered_scan_ptr); pcl::PointCloud<pcl::PointXYZI>::Ptr map_ptr(new pcl::PointCloud<pcl::PointXYZI>(map_)); if (is_first_map_ == true){ ndt.setInputTarget(map_ptr); is_first_map_ = false; } Eigen::Translation3f init_translation(current_pose_.x, current_pose_.y, current_pose_.z); Eigen::AngleAxisf init_rotation_x(current_pose_.roll, Eigen::Vector3f::UnitX()); Eigen::AngleAxisf init_rotation_y(current_pose_.pitch, Eigen::Vector3f::UnitY()); Eigen::AngleAxisf init_rotation_z(current_pose_.yaw, Eigen::Vector3f::UnitZ()); Eigen::Matrix4f init_guess = (init_translation * init_rotation_z * init_rotation_y * init_rotation_x).matrix() * tf_btol_; pcl::PointCloud<pcl::PointXYZI>::Ptr output_cloud(new pcl::PointCloud<pcl::PointXYZI>); ndt.align(*output_cloud, init_guess); t_localizer = ndt.getFinalTransformation(); t_base_link = t_localizer * tf_ltob_; pcl::transformPointCloud(*scan_ptr, *transformed_scan_ptr, t_localizer); sensor_msgs::PointCloud2::Ptr tt(new sensor_msgs::PointCloud2); pcl::toROSMsg(*transformed_scan_ptr, *tt); tt->header.frame_id = \"velodyne\"; transform_pub.publish(tt); tf::Matrix3x3 mat_b; mat_b.setValue(static_cast<double>(t_base_link(0, 0)), static_cast<double>(t_base_link(0, 1)), static_cast<double>(t_base_link(0, 2)), static_cast<double>(t_base_link(1, 0)), static_cast<double>(t_base_link(1, 1)), static_cast<double>(t_base_link(1, 2)), static_cast<double>(t_base_link(2, 0)), static_cast<double>(t_base_link(2, 1)), static_cast<double>(t_base_link(2, 2))); current_pose_.x = t_base_link(0, 3);current_pose_.y = t_base_link(1, 3);current_pose_.z = t_base_link(2, 3); mat_b.getRPY(current_pose_.roll, current_pose_.pitch, current_pose_.yaw, 1); transform.setOrigin(tf::Vector3(current_pose_.x, current_pose_.y, current_pose_.z)); q.setRPY(current_pose_.roll, current_pose_.pitch, current_pose_.yaw); transform.setRotation(q); br_.sendTransform(tf::StampedTransform(transform, input->header.stamp, \"velodyne\", \"base_link\")); double shift = sqrt(pow(current_pose_.x - previous_pose_.x, 2.0) + pow(current_pose_.y - previous_pose_.y, 2.0)); if (shift >= min_add_scan_shift_) { map_ += *transformed_scan_ptr; previous_pose_.x = current_pose_.x;previous_pose_.y = current_pose_.y;previous_pose_.z = current_pose_.z; previous_pose_.roll = current_pose_.roll;previous_pose_.pitch = current_pose_.pitch;previous_pose_.yaw = current_pose_.yaw; ndt.setInputTarget(map_ptr); sensor_msgs::PointCloud2::Ptr map_msg_ptr(new sensor_msgs::PointCloud2); pcl::toROSMsg(*map_ptr, *map_msg_ptr); ndt_map_pub_.publish(*map_msg_ptr); } current_pose_msg_.header.frame_id = \"velodyne\"; current_pose_msg_.header.stamp = input->header.stamp; current_pose_msg_.pose.position.x = current_pose_.x;current_pose_msg_.pose.position.y = current_pose_.y;current_pose_msg_.pose.position.z = current_pose_.z; current_pose_msg_.pose.orientation.x = q.x();current_pose_msg_.pose.orientation.y = q.y();current_pose_msg_.pose.orientation.z = q.z();current_pose_msg_.pose.orientation.w = q.w(); current_pose_pub_.publish(current_pose_msg_); std::cout << \"-----------------------------------------------------------------\" << std::endl; std::cout << \"\u6784\u5efa\u5730\u56fe\" << std::endl; std::cout << \"-----------------------------------------------------------------\" << std::endl; } ndt_mapping::points_callback\u51fd\u6570\u5373\u8fdb\u884c\u6784\u56fe\uff0c\u5c06\u521d\u59cb\u5316\u70b9\u4e91\u52a0\u5165\u81f3\u5730\u56fe\uff0c\u82e5\u70b9\u4e91\u5730\u56fe\u6ca1\u6709\u521d\u59cb\u5316\u8f7d\u5165\uff0c\u5219\u5c06\u7b2c\u4e00\u5e27\u56fe\u50cf\u4f5c\u4e3a\u521d\u59cb\u56fe\u50cf\uff0c\u7136\u540e\u5c06\u914d\u51c6\u4e4b\u540e\u7684\u56fe\u50cf\u9010\u5e27\u52a0\u5165map\u3002\u901a\u8fc7tf_btol\u53d8\u6362\u77e9\u9635\u5c06\u539f\u59cb\u70b9\u4e91\u8fdb\u884c\u8f6c\u5316\u3002tf_btol\u662f\u8f66\u8f86\u5728\u8d77\u59cb\u4f4d\u7f6e\u662f\u4e0d\u5728\u5168\u5c40\u5730\u56fe\u539f\u70b9\u65f6\u7684\u53d8\u6362\u77e9\u9635\u3002\u7136\u540e\u5bf9\u539f\u59cb\u8f93\u5165\u70b9\u4e91\u8fdb\u884c\u4f53\u7d20\u8fc7\u6ee4\uff0c\u9009\u62e9\u4e0d\u540c\u7684\u65b9\u6cd5\u8fdb\u884c\u53c2\u6570\u8bbe\u7f6e init_guess\u662fndt\u914d\u51c6\u65f6\u5019\u7684\u521d\u59cb\u4f4d\u7f6e\uff0c\u8be5\u4f4d\u7f6e\u4e00\u822c\u7531\u524d\u4e00\u5e27\u4f4d\u7f6e\u52a0\u4e0a\u5fae\u5c0f\u65f6\u95f4\u6bb5\u5185\u7684\u53d8\u5316\uff0c\u5f53\u91c7\u7528imu\u6216odom\u65f6\u53ef\u4ee5\u5229\u7528\u5176\u8fdb\u884c\u8f85\u52a9\u7cbe\u786e\u5b9a\u4f4d\u521d\u59cb\u4f4d\u7f6e\u3002\u5982\u679c\u672a\u4f7f\u7528imu\u4ee5\u53caodom\u5219\u4f7f\u7528\u539f\u6765\u7684init_guess \u70b9\u4e91\u805a\u7c7b\u529f\u80fd \u70b9\u4e91\u805a\u7c7b\u529f\u80fd\u5728\"lidar_demo/src/lidar_demo.cpp\"\u6587\u4ef6 void callback(const boost::shared_ptr<const sensor_msgs::PointCloud2>& msg) { ros::NodeHandle n; n.getParam(\"Cluster_D\", Cluster_D); n.getParam(\"Cluster_Max\", Cluster_Max); n.getParam(\"Cluster_Min\", Cluster_Min); pcl::PCLPointCloud2 pcl_pc2; pcl_conversions::toPCL(*msg,pcl_pc2); pcl::PointCloud<pcl::PointXYZ>::Ptr temp_cloud(new pcl::PointCloud<pcl::PointXYZ>); pcl::fromPCLPointCloud2(pcl_pc2,*temp_cloud); std::vector<int> mapping; pcl::removeNaNFromPointCloud(*temp_cloud, *temp_cloud, mapping); pub2.publish(temp_cloud); pcl::PointIndices::Ptr inliers (new pcl::PointIndices); pcl::ModelCoefficients::Ptr coefficients (new pcl::ModelCoefficients); pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_plane (new pcl::PointCloud<pcl::PointXYZ> ()); pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_f (new pcl::PointCloud<pcl::PointXYZ>); if (temp_cloud->points.size() == 0) { std::cout << \"cloud in ROI is empty\" << std::endl; return; } pcl::search::KdTree<pcl::PointXYZ>::Ptr tree (new pcl::search::KdTree<pcl::PointXYZ>); tree->setInputCloud (temp_cloud); std::vector<pcl::PointIndices> cluster_indices; pcl::EuclideanClusterExtraction<pcl::PointXYZ> ec; cout << Cluster_D << \":\" << Cluster_Min << \":\" << Cluster_Max<<endl; ec.setClusterTolerance (Cluster_D); ec.setMinClusterSize (Cluster_Min); ec.setMaxClusterSize (Cluster_Max); ec.setSearchMethod (tree); ec.setInputCloud (temp_cloud); ec.extract (cluster_indices); jsk_recognition_msgs::BoundingBoxArray BOXS; int j = 0; vector<Eigen::Vector3f> center; for (std::vector<pcl::PointIndices>::const_iterator it = cluster_indices.begin (); it != cluster_indices.end (); ++it) { pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_cluster (new pcl::PointCloud<pcl::PointXYZ>); for (std::vector<int>::const_iterator pit = it->indices.begin (); pit != it->indices.end (); ++pit) cloud_cluster->push_back ((*temp_cloud)[*pit]); cloud_cluster->width = cloud_cluster->size (); cloud_cluster->height = 1; cloud_cluster->is_dense = true; std::cout << \"Cluster has : \" << cloud_cluster->size () << \" data points.\" << std::endl; j++; jsk_recognition_msgs::BoundingBox box2; Eigen::Vector3f mass_center; pcl::MomentOfInertiaEstimation<pcl::PointXYZ> feature_extractor; feature_extractor.setInputCloud(cloud_cluster); feature_extractor.compute(); pcl::PointXYZ min_point_OBB; pcl::PointXYZ max_point_OBB; pcl::PointXYZ position_OBB; Eigen::Matrix3f rotational_matrix_OBB; feature_extractor.getOBB(min_point_OBB, max_point_OBB, position_OBB, rotational_matrix_OBB); Eigen::Quaternionf quat (rotational_matrix_OBB); feature_extractor.getMassCenter (mass_center); center.push_back(mass_center); pcl::PointXYZ min; pcl::PointXYZ max; pcl::getMinMax3D(*cloud_cluster,min,max); max.x = max.x; box2.label = j+1; box2.pose.position.x = (max.x + min.x) / 2; box2.pose.position.y = (max.y + min.y) / 2; box2.pose.position.z = (max.z + min.z) / 2; box2.dimensions.x = (max.x - min.x); box2.dimensions.y = (max.y - min.y); box2.dimensions.z = (max.z - min.z); box2.header.frame_id = frame_id; BOXS.boxes.push_back(box2); } BOXS.header.frame_id = frame_id; pub.publish(BOXS); } \u70b9\u4e91\u805a\u7c7b\u4e3b\u8981\u4f7f\u7528pcl\u5e93\u805a\u7c7b\u7b97\u6cd5 pcl::EuclideanClusterExtractionpcl::pointxyz ec; \u53ea\u9700\u8bbe\u5b9a\u53c2\u6570\uff1a Cluster_D:\u805a\u7c7b\u76f4\u5f84 Cluster_Min:\u6700\u5c0f\u805a\u7c7b\u70b9\u6570 Cluster_Max:\u6700\u5927\u805a\u7c7b\u70b9\u6570 \u70b9\u4e91\u667a\u80fd\u8bc6\u522b\u529f\u80fd \u4e3b\u8981\u4ee3\u7801\u5206\u5e03\u4f4d\u4e8elidar_cnn_seg_detect\uff0cdetected_objects_visualizer\uff0ccommon\uff0crockauto_msgs\u6587\u4ef6\u5939\u5185 \u70b9\u4e91\u667a\u80fd\u8bc6\u522b\u529f\u80fd\uff0c\u91c7\u7528\u7684\u662fapolo\u7684\u795e\u7ecf\u7f51\u7edc\u8bc6\u522b\u7b97\u6cd5\uff0c\u53ef\u5230\u8be5\u7b97\u6cd5\u7684 \u7ef4\u62a4\u5e73\u53f0 \u8fdb\u884c\u6df1\u5165\u7814\u7a76\uff0c\u8be5\u7b97\u6cd5\u4e0evoxelnet\u7f51\u7edc\u76f8\u4f3c\uff0c\u4e5f\u53ef\u53c2\u8003 github \u6211\u4eec\u5728\u667a\u80fd\u8bc6\u522b\u7684\u57fa\u7840\u4e0a\u6dfb\u52a0\u4e86\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\u663e\u793a VisualizeDetectedObjects::ObjectsToLabels(const rockauto_msgs::DetectedObjectArray &in_objects) { visualization_msgs::MarkerArray label_markers; for (auto const &object: in_objects.objects) { if (IsObjectValid(object)) { visualization_msgs::Marker label_marker; label_marker.lifetime = ros::Duration(marker_display_duration_); label_marker.header = in_objects.header; label_marker.ns = ros_namespace_ + \"/label_markers\"; label_marker.action = visualization_msgs::Marker::ADD; label_marker.type = visualization_msgs::Marker::TEXT_VIEW_FACING; label_marker.scale.x = 1.5; label_marker.scale.y = 1.5; label_marker.scale.z = 1.5; label_marker.color = label_color_; label_marker.id = marker_id_++; if(!object.label.empty() && object.label != \"unknown\") label_marker.text = object.label + \" \"; //Object Class if available float velocity_x = 0.0, velocity_y = 0.0, a_x =0.0, a_y = 0.0; if(past_x.size() ==0) { velocity_x = 0.0; velocity_y = 0.0; } else{ int before = 0; float past_d =100.0; for(int i=0;i<past_x.size();i++) { float d = sqrt(pow((past_x[i] - object.pose.position.x),2)+pow((past_y[i] - object.pose.position.y),2)); if(d < past_d) { past_d = d; before = i; } } velocity_x = (-past_x[before] + object.pose.position.x)/(0.1); velocity_y = (-past_y[before] + object.pose.position.y)/(0.1); a_x = (-past_vx[before] + velocity_x)/(0.1); a_y = (-past_vy[before] + velocity_y)/(0.1); } std::stringstream distance_stream; distance_stream << std::fixed << std::setprecision(1) << sqrt((object.pose.position.x * object.pose.position.x) + (object.pose.position.y * object.pose.position.y)); std::stringstream velocity_stream; velocity_stream <<std::fixed<<std::setprecision(2)<<\"v_x:\"<<velocity_x <<\"m/s a_x:\"<<a_x<<\"m/s2\\n\" <<\"v_y:\"<<velocity_y<<\"m/s a_y:\"<<a_y<<\"m/s2\"; std::string distance_str = distance_stream.str() + \" m\\n\" + velocity_stream.str(); label_marker.text += distance_str; if (object.velocity_reliable) { double velocity = object.velocity.linear.x; if (velocity < -0.1) { velocity *= -1; } if (abs(velocity) < object_speed_threshold_) { velocity = 0.0; } tf::Quaternion q(object.pose.orientation.x, object.pose.orientation.y, object.pose.orientation.z, object.pose.orientation.w); double roll, pitch, yaw; tf::Matrix3x3(q).getRPY(roll, pitch, yaw); // convert m/s to km/h std::stringstream kmh_velocity_stream; kmh_velocity_stream << std::fixed << std::setprecision(1) << (velocity * 3.6); std::string text = \"\\n<\" + std::to_string(object.id) + \"> \" + kmh_velocity_stream.str() + \" km/h\"; label_marker.text += text; } label_marker.pose.position.x = object.pose.position.x; label_marker.pose.position.y = object.pose.position.y; label_marker.pose.position.z = label_height_; label_marker.scale.z = 1.0; if (!label_marker.text.empty()) label_markers.markers.push_back(label_marker); past_x.push_back(object.pose.position.x); past_y.push_back(object.pose.position.y); past_vx.push_back(velocity_x); past_vy.push_back(velocity_y); } } // end in_objects.objects loop return label_markers; }//ObjectsToLabels \u6ce8\u610f\u4e8b\u9879 \u7a0b\u5e8f\u8981\u653e\u5728\u82f1\u6587\u8def\u5f84\u4e0b\uff0c\u6700\u597d\u4e0d\u8981\u653e\u5728\u4e2d\u6587\u76ee\u5f55\u4e0b\uff0c\u56e0\u4e3a\u5bb9\u6613\u51fa\u73b0\u4e2d\u6587\u4e71\u7801\u95ee\u9898\u800c\u5f71\u54cd\u7a0b\u5e8f\u8fd0\u884c","title":"\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50\u5c01\u88c5\uff561.0"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#v10","text":"","title":"\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50\u5c01\u88c5\uff561.0"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#_1","text":"\u9002\u7528\u4e8eAGX\u63a7\u5236\u5668 velodyne\u6fc0\u5149\u96f7\u8fbe\uff0c \u6ce8\uff1a\u5176\u4ed6\u54c1\u724c\u96f7\u8fbe\u53ea\u9700\u66f4\u6362\u9a71\u52a8\u548c\u5bf9\u5e94\u7684\u70b9\u4e91topic","title":"\u786c\u4ef6\u73af\u5883\u7ec4\u6210"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#_2","text":"","title":"\u8f6f\u4ef6\u73af\u5883\u90e8\u7f72"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#ubuntu1804","text":"\u6839\u636eAGX\u63d0\u4f9b\u7684\u7cfb\u7edf\u5305Jetpack\u5b89\u88c5","title":"ubuntu18.04\u64cd\u4f5c\u7cfb\u7edf"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#cudacudnn","text":"\u901a\u8fc7Jetpack\u5b89\u88c5","title":"Cuda\u548ccudnn\u5b89\u88c5"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#ros","text":"\u4e0b\u8f7d git clone https://github.com/jetsonhacks/installROSXavier.git \u5207\u6362\u76ee\u5f55 cd installROSXavier \u5b89\u88c5 ./installROS.sh -p ros-melodic-desktop -p ros-melodic-rgbd-launch \u63d2\u4ef6\u5b89\u88c5 sudo apt-get install ros-melodic-jsk-rviz-plugins","title":"ROS\u5b89\u88c5"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#caffe","text":"\u4e0b\u8f7d git clone https://github.com/BVLC/caffe.git \u5b89\u88c5\u4f9d\u8d56 sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler sudo apt-get install --no-install-recommends libboost-all-dev sudo apt-get install libopenblas-dev sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev \u5207\u6362\u76ee\u5f55 cd caffe \u65b0\u5efaMakefile.config\u6587\u4ef6 ,\u5e76\u7c98\u8d34\u4e0b\u9762\u7684\u5185\u5bb9 ``` \uff41## Refer to http://caffe.berkeleyvision.org/installation.html # Contributions simplifying and improving our build system are welcome!","title":"caffe \u90e8\u7f72"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#cudnn-acceleration-switch-uncomment-to-build-with-cudnn","text":"","title":"cuDNN acceleration switch (uncomment to build with cuDNN)."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#use_cudnn-1","text":"","title":"USE_CUDNN := 1"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#cpu-only-switch-uncomment-to-build-without-gpu-support","text":"","title":"CPU-only switch (uncomment to build without GPU support)."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#cpu_only-1","text":"","title":"CPU_ONLY := 1"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#uncomment-to-disable-io-dependencies-and-corresponding-data-layers","text":"","title":"uncomment to disable IO dependencies and corresponding data layers"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#use_opencv-0","text":"","title":"USE_OPENCV := 0"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#use_leveldb-0","text":"","title":"USE_LEVELDB := 0"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#use_lmdb-0","text":"","title":"USE_LMDB := 0"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#uncomment-to-allow-mdb_nolock-when-reading-lmdb-files-only-if-necessary","text":"","title":"uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#you-should-not-set-this-flag-if-you-will-be-reading-lmdbs-with-any","text":"","title":"You should not set this flag if you will be reading LMDBs with any"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#possibility-of-simultaneous-read-and-write","text":"","title":"possibility of simultaneous read and write"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#allow_lmdb_nolock-1","text":"","title":"ALLOW_LMDB_NOLOCK := 1"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#uncomment-if-youre-using-opencv-3","text":"OPENCV_VERSION := 3","title":"Uncomment if you're using OpenCV 3"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#to-customize-your-choice-of-compiler-uncomment-and-set-the-following","text":"","title":"To customize your choice of compiler, uncomment and set the following."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#nb-the-default-for-linux-is-g-and-the-default-for-osx-is-clang","text":"","title":"N.B. the default for Linux is g++ and the default for OSX is clang++"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#custom_cxx-g","text":"","title":"CUSTOM_CXX := g++"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#cuda-directory-contains-bin-and-lib-directories-that-we-need","text":"CUDA_DIR := /usr/local/cuda","title":"CUDA directory contains bin/ and lib/ directories that we need."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#on-ubuntu-1404-if-cuda-tools-are-installed-via","text":"","title":"On Ubuntu 14.04, if cuda tools are installed via"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#sudo-apt-get-install-nvidia-cuda-toolkit-then-use-this-instead","text":"","title":"\"sudo apt-get install nvidia-cuda-toolkit\" then use this instead:"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#cuda_dir-usr","text":"","title":"CUDA_DIR := /usr"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#cuda-architecture-setting-going-with-all-of-them","text":"","title":"CUDA architecture setting: going with all of them."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#for-cuda-60-comment-the-lines-after-_35-for-compatibility","text":"CUDA_ARCH := -gencode arch=compute_30,code=sm_30 \\ -gencode arch=compute_35,code=sm_35 \\ -gencode arch=compute_50,code=sm_50 \\ -gencode arch=compute_52,code=sm_52 \\ -gencode arch=compute_61,code=sm_61 \\ -gencode arch=compute_61,code=compute_61 # -gencode arch=compute_20,code=sm_20 \\ # -gencode arch=compute_20,code=sm_21 \\","title":"For CUDA &lt; 6.0, comment the lines after *_35 for compatibility."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#blas-choice","text":"","title":"BLAS choice:"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#atlas-for-atlas-default","text":"","title":"atlas for ATLAS (default)"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#mkl-for-mkl","text":"","title":"mkl for MKL"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#open-for-openblas","text":"","title":"open for OpenBlas"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#blas-atlas","text":"BLAS := open","title":"BLAS := atlas"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#custom-mklatlasopenblas-include-and-lib-directories","text":"","title":"Custom (MKL/ATLAS/OpenBLAS) include and lib directories."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#leave-commented-to-accept-the-defaults-for-your-choice-of-blas","text":"","title":"Leave commented to accept the defaults for your choice of BLAS"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#which-should-work","text":"","title":"(which should work)!"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#blas_include-pathtoyourblas","text":"","title":"BLAS_INCLUDE := /path/to/your/blas"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#blas_lib-pathtoyourblas","text":"","title":"BLAS_LIB := /path/to/your/blas"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#homebrew-puts-openblas-in-a-directory-that-is-not-on-the-standard-search-path","text":"","title":"Homebrew puts openblas in a directory that is not on the standard search path"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#blas_include-shell-brew-prefix-openblasinclude","text":"","title":"BLAS_INCLUDE := $(shell brew --prefix openblas)/include"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#blas_lib-shell-brew-prefix-openblaslib","text":"","title":"BLAS_LIB := $(shell brew --prefix openblas)/lib"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#this-is-required-only-if-you-will-compile-the-matlab-interface","text":"","title":"This is required only if you will compile the matlab interface."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#matlab-directory-should-contain-the-mex-binary-in-bin","text":"","title":"MATLAB directory should contain the mex binary in /bin."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#matlab_dir-usrlocal","text":"","title":"MATLAB_DIR := /usr/local"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#matlab_dir-applicationsmatlab_r2012bapp","text":"","title":"MATLAB_DIR := /Applications/MATLAB_R2012b.app"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#note-this-is-required-only-if-you-will-compile-the-python-interface","text":"","title":"NOTE: this is required only if you will compile the python interface."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#we-need-to-be-able-to-find-pythonh-and-numpyarrayobjecth","text":"PYTHON_INCLUDE := /usr/include/python2.7 \\ /usr/lib/python2.7/dist-packages/numpy/core/include","title":"We need to be able to find Python.h and numpy/arrayobject.h."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#anaconda-python-distribution-is-quite-popular-include-path","text":"","title":"Anaconda Python distribution is quite popular. Include path:"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#verify-anaconda-location-sometimes-its-in-root","text":"","title":"Verify anaconda location, sometimes it's in root."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#anaconda_home-homeanaconda2","text":"# PYTHON_INCLUDE := $(ANACONDA_HOME)/include \\ $(ANACONDA_HOME)/include/python2.7 \\ $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include \\","title":"ANACONDA_HOME := $(HOME)/anaconda2"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#uncomment-to-use-python-3-default-is-python-2","text":"","title":"Uncomment to use Python 3 (default is Python 2)"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#python_libraries-boost_python3-python35m","text":"# PYTHON_INCLUDE := /usr/include/python3.5m \\","title":"PYTHON_LIBRARIES := boost_python3 python3.5m"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#usrlibpython35dist-packagesnumpycoreinclude","text":"","title":"/usr/lib/python3.5/dist-packages/numpy/core/include"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#we-need-to-be-able-to-find-libpythonxxso-or-dylib","text":"PYTHON_LIB := /usr/lib","title":"We need to be able to find libpythonX.X.so or .dylib."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#python_lib-anaconda_homelib","text":"","title":"PYTHON_LIB := $(ANACONDA_HOME)/lib"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#homebrew-installs-numpy-in-a-non-standard-path-keg-only","text":"","title":"Homebrew installs numpy in a non standard path (keg only)"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#python_include-dir-shell-python-c-import-numpycore-printnumpycorefileinclude","text":"","title":"PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.file)'))/include"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#python_lib-shell-brew-prefix-numpylib","text":"","title":"PYTHON_LIB += $(shell brew --prefix numpy)/lib"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#uncomment-to-support-layers-written-in-python-will-link-against-python-libs","text":"","title":"Uncomment to support layers written in Python (will link against Python libs)"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#with_python_layer-1","text":"","title":"WITH_PYTHON_LAYER := 1"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#whatever-else-you-find-you-need-goes-here","text":"INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial/ LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/aarch64-linux-gnu/hdf5/serial","title":"Whatever else you find you need goes here."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#if-homebrew-is-installed-at-a-non-standard-location-for-example-your-home-directory-and-you-use-it-for-general-dependencies","text":"","title":"If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#include_dirs-shell-brew-prefixinclude","text":"","title":"INCLUDE_DIRS += $(shell brew --prefix)/include"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#library_dirs-shell-brew-prefixlib","text":"","title":"LIBRARY_DIRS += $(shell brew --prefix)/lib"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#uncomment-to-use-pkg-config-to-specify-opencv-library-paths","text":"","title":"Uncomment to use pkg-config to specify OpenCV library paths."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#usually-not-necessary-opencv-libraries-are-normally-installed-in-one-of-the-above-library_dirs","text":"","title":"(Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#use_pkg_config-1","text":"","title":"USE_PKG_CONFIG := 1"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#nb-both-build-and-distribute-dirs-are-cleared-on-make-clean","text":"BUILD_DIR := build DISTRIBUTE_DIR := distribute","title":"N.B. both build and distribute dirs are cleared on make clean"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#uncomment-for-debugging-does-not-work-on-osx-due-to-httpsgithubcombvlccaffeissues171","text":"","title":"Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#debug-1","text":"","title":"DEBUG := 1"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#the-id-of-the-gpu-that-make-runtest-will-use-to-run-unit-tests","text":"TEST_GPUID := 0","title":"The ID of the GPU that 'make runtest' will use to run unit tests."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#enable-pretty-build-comment-to-see-full-commands","text":"Q ?= @ ``` * \u7f16\u8bd1 * make -j 8 * sudo make distribute","title":"enable pretty build (comment to see full commands)"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#qt","text":"sudo apt-get install qttools5-dev-tools libqt5svg5-dev qtmultimedia5-dev","title":"qt\u5b89\u88c5"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#_3","text":"\u70b9\u4e91\u6807\u5b9a\u529f\u80fd \u70b9\u4e91\u5efa\u56fe\u529f\u80fd \u70b9\u4e91\u805a\u7c7b\u529f\u80fd \u70b9\u4e91\u667a\u80fd\u8bc6\u522b\u529f\u80fd \u7a0b\u5e8f\u6587\u4ef6\u5939\u76ee\u5f55\u7ed3\u6784 \u7a0b\u5e8f catkin_ws src common detected_objects_visualizer lidar_cnn_seg_detect lidar_demo ndt_mapping rockauto_msgs ros2qt velodyne CMakeLists.txt \u6211\u4eec\u9700\u8981\u628acatkin_ws\u6587\u4ef6\u5939\u62f7\u8d1d\u5230\u5de5\u63a7\u673a\u7684home\u76ee\u5f55\u4e0b\u3002","title":"\u7b97\u6cd5\u76d2\u5b50\u7ec4\u6210\u90e8\u5206"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#_4","text":"\u5207\u6362\u76ee\u5f55 cd catkin_ws \u7f16\u8bd1 catkin_make source \u73af\u5883 source devel/setup.bash","title":"\u7f16\u8bd1\u7a0b\u5e8f"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#_5","text":"rosrun ros2qt ros2qt","title":"\u7a0b\u5e8f\u542f\u52a8"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#_6","text":"\u7a0b\u5e8f\u542f\u52a8\u540e\u5f39\u51fa\u4e0b\u56fe\u754c\u9762 \u5982\u4e0b\u56fe\u914d\u7f6e Fixed Frame \u4e3a velodyne ,\u52fe\u9009 pointCloud2 \u590d\u9009\u6846\uff0c\u8bbe\u7f6e\u5176 Topic \u4e3a point_raw \u53ef\u89c2\u770b\u96f7\u8fbe\u6570\u636e \u542f\u7528\uff33\uff2c\uff21\uff2d\u529f\u80fd\uff0c\u5219\u52fe\u9009 SLAM \u590d\u9009\u6846,\u5c06\u5176 Topic \u8bbe\u7f6e\u4e3a point_raw \uff0c\u540c\u65f6\u5c06 pointCloud2 \u7684 Topic \u8bbe\u7f6e\u4e3a cloud \u3002 Slam\u63d0\u4f9b\u4e86\u53ef\u8c03\u53c2\u6570\uff1a Topic:\u8bbe\u7f6e\u6784\u56fe\u4e3b\u9898 Size:\u8bbe\u7f6e\u70b9\u4e91\u5730\u56fe\u4e2d\u70b9\u7684\u663e\u793a\u5927\u5c0f Color:\u70b9\u4e91\u5730\u56fe\u7684\u989c\u8272 Res:\u5206\u8fa8\u7387 Step_size:\u6b65\u5e45 Trans_epsilon:\u6536\u655b\u5747\u65b9\u5dee Max_iter:\u6700\u5927\u8fed\u4ee3\u6b21\u6570 Voxel_leaf_size:Voxel\u5c3a\u5bf8 Min_scan_range:\u6700\u5c0f\u626b\u63cf\u8303\u56f4 Max_scan_range:\u6700\u5927\u626b\u63cf\u8303\u56f4 Scan_rate:\u626b\u63cf\u5468\u671f \u663e\u793a\u969c\u788d\u7269\uff0c\u5219\u53d6\u6d88 SLAM \u52fe\u9009\uff0c\u5c06 pointCloud2 \u7684 Topic \u66f4\u6539\u56de point_raw .\u52fe\u9009 \uff22oundingBox \u590d\u9009\u6846\u3002 \u663e\u793a\u4f4d\u7f6e\uff0c\u901f\u5ea6\uff0c\u52a0\u901f\u5ea6\u548c\u8def\u5f84 \u663e\u793a\u805a\u7c7b\u969c\u788d\u7269\uff0c\u5219\u52fe\u9009'BoundingBox2'\u590d\u9009\u6846\u3002 \u8fd9\u91cc\u67093\u4e2a\u53c2\u6570: Cluster_D:\u805a\u7c7b\u76f4\u5f84 Cluster_Min:\u6700\u5c0f\u805a\u7c7b\u70b9\u6570 Cluster_Max:\u6700\u5927\u805a\u7c7b\u70b9\u6570 \u70b9\u4e91\u6807\u5b9a \u9996\u5148\u8981\u52fe\u9009 PointCloud2 \u663e\u793a\u70b9\u4e91\uff0c\u518d\u52fe\u9009 \u6807\u5b9a \u53ef\u79fb\u52a8xyz\u4e09\u8f74\uff0c\u4e5f\u53ef\u7ed5\u4e09\u8f74\u65cb\u8f6c\u3002","title":"\u5e94\u7528\u8bb2\u89e3"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#_7","text":"","title":"\u7a0b\u5e8f\u8bb2\u89e3"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#_8","text":"\u6807\u5b9a\u7a0b\u5e8f\u529f\u80fd\u4ee3\u7801\u6bb5\u4f4d\u4e8e\"ros2qt\\src\\qnode.cpp\"\u6587\u4ef6 void QNode::biaodingCallback(const PointCloud::ConstPtr& msg) { //\u70b9\u4e91\u65cb\u8f6c Eigen::Affine3f transform_2 = Eigen::Affine3f::Identity(); transform_2.translation() << x_pianyi, y_pianyi, z_pianyi; float theta_2 = 0*M_PI/180; transform_2.rotate (Eigen::AngleAxisf (theta_2, Eigen::Vector3f::UnitY())); pcl::PointCloud<pcl::PointXYZ>::Ptr transformed_cloud_2 (new pcl::PointCloud<pcl::PointXYZ> ()); pcl::transformPointCloud (*msg, *transformed_cloud_2, transform_2); Eigen::Affine3f transform = Eigen::Affine3f::Identity(); transform.translation() << 0.0, 0.0, 0.0; float theta = x_xuanzhuan*M_PI/180; transform.rotate (Eigen::AngleAxisf (theta, Eigen::Vector3f::UnitX())); pcl::PointCloud<pcl::PointXYZ>::Ptr transformed_cloud (new pcl::PointCloud<pcl::PointXYZ> ()); pcl::transformPointCloud (*transformed_cloud_2, *transformed_cloud, transform); Eigen::Affine3f transform_3 = Eigen::Affine3f::Identity(); transform_3.translation() << 0.0, 0.0, 0.0; float theta_3 = z_xuanzhuan*M_PI/180; transform_3.rotate (Eigen::AngleAxisf (theta_3, Eigen::Vector3f::UnitZ())); pcl::PointCloud<pcl::PointXYZ>::Ptr transformed_cloud_3 (new pcl::PointCloud<pcl::PointXYZ> ()); pcl::transformPointCloud (*transformed_cloud, *transformed_cloud_3, transform_3); Eigen::Affine3f transform_4 = Eigen::Affine3f::Identity(); transform_4.translation() << 0.0, 0.0, 0.0; float theta_4 = y_xuanzhuan*M_PI/180; transform_4.rotate (Eigen::AngleAxisf (theta_4, Eigen::Vector3f::UnitY())); pcl::PointCloud<pcl::PointXYZ>::Ptr transformed_cloud_4 (new pcl::PointCloud<pcl::PointXYZ> ()); pcl::transformPointCloud (*transformed_cloud_3, *transformed_cloud_4, transform_4); transformed_cloud_4->header.frame_id = frame_id; pub.publish(transformed_cloud_4); } \u70b9\u4e91\u7684\u6807\u5b9a\uff0c\u4e3b\u8981\u662f\u70b9\u4e91\u7684\u65cb\u8f6c\u548c\u5e73\u79fb\u529f\u80fd\uff0c\u53ef\u4ee5\u53c2\u8003pcl\uff0c\u8fd9\u91cc\u7b80\u5355\u8bb2\u89e3\u4e00\u4e0b\u65cb\u8f6c\u5e73\u79fb\u529f\u80fd \u9996\u5148\u5b9a\u4e49\u4e00\u4e2aAffine3f\u7ed3\u6784\u4f53\uff0c\u7528\u6765\u5b58\u50a8\u70b9\u4e91\u65cb\u8f6c\u548c\u5e73\u79fb\u4fe1\u606f Eigen::Affine3f transform_2 = Eigen::Affine3f::Identity(); \u518d\u628a\uff58\uff0c\uff59\uff0c\uff5a\u8f74\u7684\u5e73\u79fb\u91cfx_pianyi, y_pianyi, z_pianyi\u4f20\u9012\u8fdbAffine3f\u7ed3\u6784\u4f53 transform_2.translation() << x_pianyi, y_pianyi, z_pianyi; \u5b9a\u4e49\u4e00\u4e2a\u65cb\u8f6c\u89d2\u5ea6 float theta_2 = 0*M_PI/180; \u628a\u7ed5\uff39\u8f74\u65cb\u8f6c\u89d2\u5ea6\uff0c\u4f20\u5165Affin3f\u7ed3\u6784\u4f53\uff0c\u65cb\u8f6c\u8f74Eigen::Vector3f::UnitY() transform_2.rotate (Eigen::AngleAxisf (theta_2, Eigen::Vector3f::UnitY())); \u5b9a\u4e49\u65cb\u8f6c\u540e\u7684\u70b9\u4e91 pcl::PointCloudpcl::pointxyz::Ptr transformed_cloud_2 (new pcl::PointCloudpcl::pointxyz ()); \u6267\u884c\u70b9\u4e91\u7684\u5e73\u79fb\u548c\u65cb\u8f6c\uff0c\u5e76\u628a\u7ed3\u679c\u5b58\u5165 transformed_cloud_2 pcl::transformPointCloud (*msg, *transformed_cloud_2, transform_2);","title":"\u70b9\u4e91\u6807\u5b9a\u529f\u80fd"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#_9","text":"\u5efa\u56fe\u7a0b\u5e8f\u5305\u5373ndt_mapping\u6587\u4ef6\u5939,\u91cd\u70b9\u8fd0\u7b97\u903b\u8f91\u5728ndt_mapping.cpp\u6587\u4ef6\u5185\uff0c\u8fd9\u91cc\u89e3\u6790\u4e00\u4e0b ndt_mapping::ndt_mapping() { transform_pub = nh_.advertise<sensor_msgs::PointCloud2> (\"/cloud\", 1, false); points_sub_ = nh_.subscribe(slam_topic, 100000, &ndt_mapping::points_callback,this); ndt_map_pub_ = nh_.advertise<sensor_msgs::PointCloud2>(\"/ndt_map\", 1000); current_pose_pub_ = nh_.advertise<geometry_msgs::PoseStamped>(\"/current_pose\", 1000); max_iter_ = max_iter_1; ndt_res_ = ndt_res_1; step_size_ = step_size_1; trans_eps_ = trans_eps_1; voxel_leaf_size_ = voxel_leaf_size_1; scan_rate_ = scan_rate_1; min_scan_range_ = min_scan_range_1; max_scan_range_ = max_scan_range_1; min_add_scan_shift_ = min_add_scan_shift_1; initial_scan_loaded = 0; min_add_scan_shift_ = 1.0; _tf_x=0.0, _tf_y=0.0, _tf_z=0.0, _tf_roll=0.0, _tf_pitch=0.0, _tf_yaw=0.0; Eigen::Translation3f tl_btol(_tf_x, _tf_y, _tf_z); Eigen::AngleAxisf rot_x_btol(_tf_roll, Eigen::Vector3f::UnitX()); Eigen::AngleAxisf rot_y_btol(_tf_pitch, Eigen::Vector3f::UnitY()); Eigen::AngleAxisf rot_z_btol(_tf_yaw, Eigen::Vector3f::UnitZ()); tf_btol_ = (tl_btol * rot_z_btol * rot_y_btol * rot_x_btol).matrix(); tf_ltob_ = tf_btol_.inverse(); map_.header.frame_id = \"velodyne\"; current_pose_.x = current_pose_.y = current_pose_.z = 0.0;current_pose_.roll = current_pose_.pitch = current_pose_.yaw = 0.0; previous_pose_.x = previous_pose_.y = previous_pose_.z = 0.0;previous_pose_.roll = previous_pose_.pitch = previous_pose_.yaw = 0.0; voxel_grid_filter_.setLeafSize(voxel_leaf_size_, voxel_leaf_size_, voxel_leaf_size_); ndt.setTransformationEpsilon(trans_eps_); ndt.setStepSize(step_size_); ndt.setResolution(ndt_res_); ndt.setMaximumIterations(max_iter_); is_first_map_ = true; }; ndt_mapping::ndt_mapping()\u51fd\u6570\uff0c\u4e3b\u8981\u8fdb\u884c\u4e86\u4e00\u4e9b\u53c2\u6570\u8bbe\u5b9a\uff0c\u548ctopic\u8bbe\u5b9a\u3002 void ndt_mapping::points_callback(const sensor_msgs::PointCloud2::ConstPtr& input) { pcl::PointCloud<pcl::PointXYZI> tmp, scan; pcl::PointCloud<pcl::PointXYZI>::Ptr filtered_scan_ptr(new pcl::PointCloud<pcl::PointXYZI>()); pcl::PointCloud<pcl::PointXYZI>::Ptr transformed_scan_ptr(new pcl::PointCloud<pcl::PointXYZI>()); tf::Quaternion q; Eigen::Matrix4f t_localizer(Eigen::Matrix4f::Identity()); Eigen::Matrix4f t_base_link(Eigen::Matrix4f::Identity()); static tf::TransformBroadcaster br_; tf::Transform transform; pcl::fromROSMsg(*input, tmp); double r; Eigen::Vector3d point_pos; pcl::PointXYZI p; for (pcl::PointCloud<pcl::PointXYZI>::const_iterator item = tmp.begin(); item != tmp.end(); item++) { use_imu_ = false; if(use_imu_){ // deskew(TODO:inplement of predicting pose by imu) point_pos.x() = (double)item->x; point_pos.y() = (double)item->y; point_pos.z() = (double)item->z; double s = scan_rate_ * (double(item->intensity) - int(item->intensity)); point_pos.x() -= s * current_pose_msg_.pose.position.x;//current_pose_imu_ point_pos.y() -= s * current_pose_msg_.pose.position.y; point_pos.z() -= s * current_pose_msg_.pose.position.z; Eigen::Quaterniond start_quat, end_quat, mid_quat; mid_quat.setIdentity(); end_quat = Eigen::Quaterniond( current_pose_msg_.pose.orientation.w, current_pose_msg_.pose.orientation.x, current_pose_msg_.pose.orientation.y, current_pose_msg_.pose.orientation.z); start_quat = mid_quat.slerp(s, end_quat); point_pos = start_quat.conjugate() * start_quat * point_pos; point_pos.x() += current_pose_msg_.pose.position.x; point_pos.y() += current_pose_msg_.pose.position.y; point_pos.z() += current_pose_msg_.pose.position.z; p.x = point_pos.x(); p.y = point_pos.y(); p.z = point_pos.z(); } else{ p.x = (double)item->x; p.y = (double)item->y; p.z = (double)item->z; } p.intensity = (double)item->intensity; r = sqrt(pow(p.x, 2.0) + pow(p.y, 2.0)); if (min_scan_range_ < r && r < max_scan_range_) { scan.push_back(p); } } pcl::PointCloud<pcl::PointXYZI>::Ptr scan_ptr(new pcl::PointCloud<pcl::PointXYZI>(scan)); if (initial_scan_loaded == 0) { pcl::transformPointCloud(*scan_ptr, *transformed_scan_ptr, tf_btol_); map_ += *transformed_scan_ptr; initial_scan_loaded = 1; } voxel_grid_filter_.setInputCloud(scan_ptr); voxel_grid_filter_.filter(*filtered_scan_ptr); ndt.setInputSource(filtered_scan_ptr); pcl::PointCloud<pcl::PointXYZI>::Ptr map_ptr(new pcl::PointCloud<pcl::PointXYZI>(map_)); if (is_first_map_ == true){ ndt.setInputTarget(map_ptr); is_first_map_ = false; } Eigen::Translation3f init_translation(current_pose_.x, current_pose_.y, current_pose_.z); Eigen::AngleAxisf init_rotation_x(current_pose_.roll, Eigen::Vector3f::UnitX()); Eigen::AngleAxisf init_rotation_y(current_pose_.pitch, Eigen::Vector3f::UnitY()); Eigen::AngleAxisf init_rotation_z(current_pose_.yaw, Eigen::Vector3f::UnitZ()); Eigen::Matrix4f init_guess = (init_translation * init_rotation_z * init_rotation_y * init_rotation_x).matrix() * tf_btol_; pcl::PointCloud<pcl::PointXYZI>::Ptr output_cloud(new pcl::PointCloud<pcl::PointXYZI>); ndt.align(*output_cloud, init_guess); t_localizer = ndt.getFinalTransformation(); t_base_link = t_localizer * tf_ltob_; pcl::transformPointCloud(*scan_ptr, *transformed_scan_ptr, t_localizer); sensor_msgs::PointCloud2::Ptr tt(new sensor_msgs::PointCloud2); pcl::toROSMsg(*transformed_scan_ptr, *tt); tt->header.frame_id = \"velodyne\"; transform_pub.publish(tt); tf::Matrix3x3 mat_b; mat_b.setValue(static_cast<double>(t_base_link(0, 0)), static_cast<double>(t_base_link(0, 1)), static_cast<double>(t_base_link(0, 2)), static_cast<double>(t_base_link(1, 0)), static_cast<double>(t_base_link(1, 1)), static_cast<double>(t_base_link(1, 2)), static_cast<double>(t_base_link(2, 0)), static_cast<double>(t_base_link(2, 1)), static_cast<double>(t_base_link(2, 2))); current_pose_.x = t_base_link(0, 3);current_pose_.y = t_base_link(1, 3);current_pose_.z = t_base_link(2, 3); mat_b.getRPY(current_pose_.roll, current_pose_.pitch, current_pose_.yaw, 1); transform.setOrigin(tf::Vector3(current_pose_.x, current_pose_.y, current_pose_.z)); q.setRPY(current_pose_.roll, current_pose_.pitch, current_pose_.yaw); transform.setRotation(q); br_.sendTransform(tf::StampedTransform(transform, input->header.stamp, \"velodyne\", \"base_link\")); double shift = sqrt(pow(current_pose_.x - previous_pose_.x, 2.0) + pow(current_pose_.y - previous_pose_.y, 2.0)); if (shift >= min_add_scan_shift_) { map_ += *transformed_scan_ptr; previous_pose_.x = current_pose_.x;previous_pose_.y = current_pose_.y;previous_pose_.z = current_pose_.z; previous_pose_.roll = current_pose_.roll;previous_pose_.pitch = current_pose_.pitch;previous_pose_.yaw = current_pose_.yaw; ndt.setInputTarget(map_ptr); sensor_msgs::PointCloud2::Ptr map_msg_ptr(new sensor_msgs::PointCloud2); pcl::toROSMsg(*map_ptr, *map_msg_ptr); ndt_map_pub_.publish(*map_msg_ptr); } current_pose_msg_.header.frame_id = \"velodyne\"; current_pose_msg_.header.stamp = input->header.stamp; current_pose_msg_.pose.position.x = current_pose_.x;current_pose_msg_.pose.position.y = current_pose_.y;current_pose_msg_.pose.position.z = current_pose_.z; current_pose_msg_.pose.orientation.x = q.x();current_pose_msg_.pose.orientation.y = q.y();current_pose_msg_.pose.orientation.z = q.z();current_pose_msg_.pose.orientation.w = q.w(); current_pose_pub_.publish(current_pose_msg_); std::cout << \"-----------------------------------------------------------------\" << std::endl; std::cout << \"\u6784\u5efa\u5730\u56fe\" << std::endl; std::cout << \"-----------------------------------------------------------------\" << std::endl; } ndt_mapping::points_callback\u51fd\u6570\u5373\u8fdb\u884c\u6784\u56fe\uff0c\u5c06\u521d\u59cb\u5316\u70b9\u4e91\u52a0\u5165\u81f3\u5730\u56fe\uff0c\u82e5\u70b9\u4e91\u5730\u56fe\u6ca1\u6709\u521d\u59cb\u5316\u8f7d\u5165\uff0c\u5219\u5c06\u7b2c\u4e00\u5e27\u56fe\u50cf\u4f5c\u4e3a\u521d\u59cb\u56fe\u50cf\uff0c\u7136\u540e\u5c06\u914d\u51c6\u4e4b\u540e\u7684\u56fe\u50cf\u9010\u5e27\u52a0\u5165map\u3002\u901a\u8fc7tf_btol\u53d8\u6362\u77e9\u9635\u5c06\u539f\u59cb\u70b9\u4e91\u8fdb\u884c\u8f6c\u5316\u3002tf_btol\u662f\u8f66\u8f86\u5728\u8d77\u59cb\u4f4d\u7f6e\u662f\u4e0d\u5728\u5168\u5c40\u5730\u56fe\u539f\u70b9\u65f6\u7684\u53d8\u6362\u77e9\u9635\u3002\u7136\u540e\u5bf9\u539f\u59cb\u8f93\u5165\u70b9\u4e91\u8fdb\u884c\u4f53\u7d20\u8fc7\u6ee4\uff0c\u9009\u62e9\u4e0d\u540c\u7684\u65b9\u6cd5\u8fdb\u884c\u53c2\u6570\u8bbe\u7f6e init_guess\u662fndt\u914d\u51c6\u65f6\u5019\u7684\u521d\u59cb\u4f4d\u7f6e\uff0c\u8be5\u4f4d\u7f6e\u4e00\u822c\u7531\u524d\u4e00\u5e27\u4f4d\u7f6e\u52a0\u4e0a\u5fae\u5c0f\u65f6\u95f4\u6bb5\u5185\u7684\u53d8\u5316\uff0c\u5f53\u91c7\u7528imu\u6216odom\u65f6\u53ef\u4ee5\u5229\u7528\u5176\u8fdb\u884c\u8f85\u52a9\u7cbe\u786e\u5b9a\u4f4d\u521d\u59cb\u4f4d\u7f6e\u3002\u5982\u679c\u672a\u4f7f\u7528imu\u4ee5\u53caodom\u5219\u4f7f\u7528\u539f\u6765\u7684init_guess","title":"\u70b9\u4e91\u5efa\u56fe\u529f\u80fd"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#_10","text":"\u70b9\u4e91\u805a\u7c7b\u529f\u80fd\u5728\"lidar_demo/src/lidar_demo.cpp\"\u6587\u4ef6 void callback(const boost::shared_ptr<const sensor_msgs::PointCloud2>& msg) { ros::NodeHandle n; n.getParam(\"Cluster_D\", Cluster_D); n.getParam(\"Cluster_Max\", Cluster_Max); n.getParam(\"Cluster_Min\", Cluster_Min); pcl::PCLPointCloud2 pcl_pc2; pcl_conversions::toPCL(*msg,pcl_pc2); pcl::PointCloud<pcl::PointXYZ>::Ptr temp_cloud(new pcl::PointCloud<pcl::PointXYZ>); pcl::fromPCLPointCloud2(pcl_pc2,*temp_cloud); std::vector<int> mapping; pcl::removeNaNFromPointCloud(*temp_cloud, *temp_cloud, mapping); pub2.publish(temp_cloud); pcl::PointIndices::Ptr inliers (new pcl::PointIndices); pcl::ModelCoefficients::Ptr coefficients (new pcl::ModelCoefficients); pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_plane (new pcl::PointCloud<pcl::PointXYZ> ()); pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_f (new pcl::PointCloud<pcl::PointXYZ>); if (temp_cloud->points.size() == 0) { std::cout << \"cloud in ROI is empty\" << std::endl; return; } pcl::search::KdTree<pcl::PointXYZ>::Ptr tree (new pcl::search::KdTree<pcl::PointXYZ>); tree->setInputCloud (temp_cloud); std::vector<pcl::PointIndices> cluster_indices; pcl::EuclideanClusterExtraction<pcl::PointXYZ> ec; cout << Cluster_D << \":\" << Cluster_Min << \":\" << Cluster_Max<<endl; ec.setClusterTolerance (Cluster_D); ec.setMinClusterSize (Cluster_Min); ec.setMaxClusterSize (Cluster_Max); ec.setSearchMethod (tree); ec.setInputCloud (temp_cloud); ec.extract (cluster_indices); jsk_recognition_msgs::BoundingBoxArray BOXS; int j = 0; vector<Eigen::Vector3f> center; for (std::vector<pcl::PointIndices>::const_iterator it = cluster_indices.begin (); it != cluster_indices.end (); ++it) { pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_cluster (new pcl::PointCloud<pcl::PointXYZ>); for (std::vector<int>::const_iterator pit = it->indices.begin (); pit != it->indices.end (); ++pit) cloud_cluster->push_back ((*temp_cloud)[*pit]); cloud_cluster->width = cloud_cluster->size (); cloud_cluster->height = 1; cloud_cluster->is_dense = true; std::cout << \"Cluster has : \" << cloud_cluster->size () << \" data points.\" << std::endl; j++; jsk_recognition_msgs::BoundingBox box2; Eigen::Vector3f mass_center; pcl::MomentOfInertiaEstimation<pcl::PointXYZ> feature_extractor; feature_extractor.setInputCloud(cloud_cluster); feature_extractor.compute(); pcl::PointXYZ min_point_OBB; pcl::PointXYZ max_point_OBB; pcl::PointXYZ position_OBB; Eigen::Matrix3f rotational_matrix_OBB; feature_extractor.getOBB(min_point_OBB, max_point_OBB, position_OBB, rotational_matrix_OBB); Eigen::Quaternionf quat (rotational_matrix_OBB); feature_extractor.getMassCenter (mass_center); center.push_back(mass_center); pcl::PointXYZ min; pcl::PointXYZ max; pcl::getMinMax3D(*cloud_cluster,min,max); max.x = max.x; box2.label = j+1; box2.pose.position.x = (max.x + min.x) / 2; box2.pose.position.y = (max.y + min.y) / 2; box2.pose.position.z = (max.z + min.z) / 2; box2.dimensions.x = (max.x - min.x); box2.dimensions.y = (max.y - min.y); box2.dimensions.z = (max.z - min.z); box2.header.frame_id = frame_id; BOXS.boxes.push_back(box2); } BOXS.header.frame_id = frame_id; pub.publish(BOXS); } \u70b9\u4e91\u805a\u7c7b\u4e3b\u8981\u4f7f\u7528pcl\u5e93\u805a\u7c7b\u7b97\u6cd5 pcl::EuclideanClusterExtractionpcl::pointxyz ec; \u53ea\u9700\u8bbe\u5b9a\u53c2\u6570\uff1a Cluster_D:\u805a\u7c7b\u76f4\u5f84 Cluster_Min:\u6700\u5c0f\u805a\u7c7b\u70b9\u6570 Cluster_Max:\u6700\u5927\u805a\u7c7b\u70b9\u6570","title":"\u70b9\u4e91\u805a\u7c7b\u529f\u80fd"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#_11","text":"\u4e3b\u8981\u4ee3\u7801\u5206\u5e03\u4f4d\u4e8elidar_cnn_seg_detect\uff0cdetected_objects_visualizer\uff0ccommon\uff0crockauto_msgs\u6587\u4ef6\u5939\u5185 \u70b9\u4e91\u667a\u80fd\u8bc6\u522b\u529f\u80fd\uff0c\u91c7\u7528\u7684\u662fapolo\u7684\u795e\u7ecf\u7f51\u7edc\u8bc6\u522b\u7b97\u6cd5\uff0c\u53ef\u5230\u8be5\u7b97\u6cd5\u7684 \u7ef4\u62a4\u5e73\u53f0 \u8fdb\u884c\u6df1\u5165\u7814\u7a76\uff0c\u8be5\u7b97\u6cd5\u4e0evoxelnet\u7f51\u7edc\u76f8\u4f3c\uff0c\u4e5f\u53ef\u53c2\u8003 github \u6211\u4eec\u5728\u667a\u80fd\u8bc6\u522b\u7684\u57fa\u7840\u4e0a\u6dfb\u52a0\u4e86\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\u663e\u793a VisualizeDetectedObjects::ObjectsToLabels(const rockauto_msgs::DetectedObjectArray &in_objects) { visualization_msgs::MarkerArray label_markers; for (auto const &object: in_objects.objects) { if (IsObjectValid(object)) { visualization_msgs::Marker label_marker; label_marker.lifetime = ros::Duration(marker_display_duration_); label_marker.header = in_objects.header; label_marker.ns = ros_namespace_ + \"/label_markers\"; label_marker.action = visualization_msgs::Marker::ADD; label_marker.type = visualization_msgs::Marker::TEXT_VIEW_FACING; label_marker.scale.x = 1.5; label_marker.scale.y = 1.5; label_marker.scale.z = 1.5; label_marker.color = label_color_; label_marker.id = marker_id_++; if(!object.label.empty() && object.label != \"unknown\") label_marker.text = object.label + \" \"; //Object Class if available float velocity_x = 0.0, velocity_y = 0.0, a_x =0.0, a_y = 0.0; if(past_x.size() ==0) { velocity_x = 0.0; velocity_y = 0.0; } else{ int before = 0; float past_d =100.0; for(int i=0;i<past_x.size();i++) { float d = sqrt(pow((past_x[i] - object.pose.position.x),2)+pow((past_y[i] - object.pose.position.y),2)); if(d < past_d) { past_d = d; before = i; } } velocity_x = (-past_x[before] + object.pose.position.x)/(0.1); velocity_y = (-past_y[before] + object.pose.position.y)/(0.1); a_x = (-past_vx[before] + velocity_x)/(0.1); a_y = (-past_vy[before] + velocity_y)/(0.1); } std::stringstream distance_stream; distance_stream << std::fixed << std::setprecision(1) << sqrt((object.pose.position.x * object.pose.position.x) + (object.pose.position.y * object.pose.position.y)); std::stringstream velocity_stream; velocity_stream <<std::fixed<<std::setprecision(2)<<\"v_x:\"<<velocity_x <<\"m/s a_x:\"<<a_x<<\"m/s2\\n\" <<\"v_y:\"<<velocity_y<<\"m/s a_y:\"<<a_y<<\"m/s2\"; std::string distance_str = distance_stream.str() + \" m\\n\" + velocity_stream.str(); label_marker.text += distance_str; if (object.velocity_reliable) { double velocity = object.velocity.linear.x; if (velocity < -0.1) { velocity *= -1; } if (abs(velocity) < object_speed_threshold_) { velocity = 0.0; } tf::Quaternion q(object.pose.orientation.x, object.pose.orientation.y, object.pose.orientation.z, object.pose.orientation.w); double roll, pitch, yaw; tf::Matrix3x3(q).getRPY(roll, pitch, yaw); // convert m/s to km/h std::stringstream kmh_velocity_stream; kmh_velocity_stream << std::fixed << std::setprecision(1) << (velocity * 3.6); std::string text = \"\\n<\" + std::to_string(object.id) + \"> \" + kmh_velocity_stream.str() + \" km/h\"; label_marker.text += text; } label_marker.pose.position.x = object.pose.position.x; label_marker.pose.position.y = object.pose.position.y; label_marker.pose.position.z = label_height_; label_marker.scale.z = 1.0; if (!label_marker.text.empty()) label_markers.markers.push_back(label_marker); past_x.push_back(object.pose.position.x); past_y.push_back(object.pose.position.y); past_vx.push_back(velocity_x); past_vy.push_back(velocity_y); } } // end in_objects.objects loop return label_markers; }//ObjectsToLabels","title":"\u70b9\u4e91\u667a\u80fd\u8bc6\u522b\u529f\u80fd"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#_12","text":"\u7a0b\u5e8f\u8981\u653e\u5728\u82f1\u6587\u8def\u5f84\u4e0b\uff0c\u6700\u597d\u4e0d\u8981\u653e\u5728\u4e2d\u6587\u76ee\u5f55\u4e0b\uff0c\u56e0\u4e3a\u5bb9\u6613\u51fa\u73b0\u4e2d\u6587\u4e71\u7801\u95ee\u9898\u800c\u5f71\u54cd\u7a0b\u5e8f\u8fd0\u884c","title":"\u6ce8\u610f\u4e8b\u9879"},{"location":"%E7%AC%9B%E5%8D%A1%E5%B0%94%E7%A7%AF/","text":"${X_i|i\\in I}$ \\prod_{x\\in I}X_i=\\{f:I\\rightarrow\\bigcup_{x\\in I}X_i|(\\forall_i)(f(i)\\in X_i)\\} \\pi_j(f)=f(j) \u7b2cj\u6295\u5f71\u6620\u5c04 $$ \\pi_j:\\prod_{i\\in I}X_i\\rightarrow X_j $$ $$ \\prod_{n=1}^{\\infty}\\mathbb{R}=\\mathbb{R}^{\\omega}=\\mathbb{R}\\times\\mathbb{R}\\cdots $$ \u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u5e94\u7528\u4e8e\u7ebf\u6027\u56de\u5f52 \u6781\u5927\u4f3c\u7136\u6cd5\u662f\u4e00\u79cd\u7528\u4e8e\u4f30\u8ba1\u6a21\u578b\u53c2\u6570\u7684\u7edf\u8ba1\u65b9\u6cd5\u3002 \u5728\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u4e2d\uff0c\u9009\u62e9\u53c2\u6570\u4ee5\u4f7f\u5047\u8bbe\u7684\u6a21\u578b\u4ea7\u751f\u89c2\u6d4b\u6570\u636e\u7684\u53ef\u80fd\u6027\u6700\u5927\u3002 \u5047\u8bbe\u6211\u4eec\u6709\u4e2a\u6a21\u578b\uff0c\u4e5f\u79f0\u4e3a\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u3002 \u80fd\u591f\u4e3a\u6211\u4eec\u7684\u6570\u636e\u5bfc\u51fa\u4f3c\u7136\u51fd\u6570\u3002 \u4e00\u65e6\u5bfc\u51fa\u4f3c\u7136\u51fd\u6570\uff0c\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u4ec5\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u4f18\u5316\u95ee\u9898\u3002 \u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u4f18\u52bf\uff1a \u5982\u679c\u6b63\u786e\u8bbe\u5b9a\u4e86\u6a21\u578b\uff0c\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5668\u5c06\u662f\u6700\u6709\u6548\u7684\u4f30\u8ba1\u5668 \u5b83\u63d0\u4f9b\u4e86\u4e00\u81f4\u4f46\u7075\u6d3b\u7684\u65b9\u6cd5\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u5404\u79cd\u5e94\u7528\u7a0b\u5e8f\uff0c\u5305\u62ec\u5176\u4ed6\u8bbe\u5b9a\u6a21\u578b\u65e0\u6548\u7684\u60c5\u51b5\u3002 \u5728\u8f83\u5927\u6a21\u578b\u4e2d\u5f97\u51fa\u65e0\u504f\u4f30\u8ba1 \u6548\u7387\u662f\u4f30\u8ba1\u5668\u8d28\u91cf\u7684\u4e00\u79cd\u5ea6\u91cf\uff0c\u6709\u6548\u7684\u4f30\u8ba1\u5668\u5177\u6709\u8f83\u5c0f\u7684\u65b9\u5dee\u6216\u5747\u65b9\u8bef\u5dee\u3002 \u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u7f3a\u70b9\uff1a \u5b83\u4f9d\u8d56\u4e8e\u6a21\u578b\u7684\u8bbe\u5b9a\u4ee5\u53ca\u4f3c\u7136\u51fd\u6570\u7684\u63a8\u5bfc\uff0c\u800c\u8fd9\u79cd\u63a8\u5bfc\u5e76\u4e0d\u603b\u662f\u90a3\u4e48\u5bb9\u6613 \u50cf\u5176\u4ed6\u4f18\u5316\u95ee\u9898\u4e00\u6837\uff0c\u6700\u5927 \u4f3c\u7136\u4f30\u8ba1\u53ef\u80fd\u5bf9\u521d\u59cb\u503c\u7684\u9009\u62e9\u5f88\u654f\u611f\u3002 \u53d6\u51b3\u4e8e\u4f3c\u7136\u51fd\u6570\u7684\u590d\u6742\u5ea6\uff0c\u6570\u503c\u4f30\u8ba1\u5728\u8ba1\u7b97\u4e0a\u53ef\u80fd\u662f\u6602\u8d35\u7684\u3002 \u5c0f\u6837\u672c\u7684\u4f30\u8ba1\u503c\u53ef\u80fd\u6709\u504f\u5dee\u3002 \u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u53d6\u51b3\u4e8e\u4f3c\u7136\u51fd\u6570\u7684\u63a8\u5bfc\u3002\u56e0\u6b64\u91cd\u8981\u7684\u662f\u8981\u5bf9\u4f3c\u7136\u51fd\u6570\u662f\u4ec0\u4e48\u4ee5\u53ca\u5b83\u6765\u81ea\u4f55\u5904\u6709\u4e00\u4e2a\u5f88\u597d\u7684\u4e86\u89e3\u3002 \u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u7b2c\u4e00\u6b65\u662f\u5047\u8bbe\u6570\u636e\u7684\u6982\u7387\u5206\u5e03\uff0c\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u5728\u7ed9\u5b9a\u4e00\u7ec4\u57fa\u7840\u6a21\u578b\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u6d4b\u91cf\u89c2\u5bdf\u6570\u636e\u7684\u6982\u7387\u3002 \u6211\u4eec\u5047\u8bbe\u6211\u4eec\u7684\u6570\u636e\u5177\u6709\u6f5c\u5728\u7684\u6cca\u677e\u5206\u5e03\uff0c\u8fd9\u662f\u4e00\u4e2a\u666e\u904d\u7684\u5047\u8bbe\uff0c\u5c24\u5176\u5bf9\u4e8e\u975e\u8d1f\u8ba1\u6570\u6570\u636e\u3002 \u5355\u4e2a\u89c2\u6d4b\u503c$y_i$\u7684\u6cca\u677e\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u7531\u4e0b\u5f0f\u7ed9\u51fa\uff1a $$ f(y_i|\\theta)=\\frac{e^{-\\theta}\\theta^{y_i}}{y_i!} $$ \u7531\u4e8e\u6211\u4eec\u6837\u672c\u4e2d\u7684\u89c2\u6d4b\u503c\u662f\u72ec\u7acb\u7684\uff0c\u56e0\u6b64\u53ef\u4ee5\u901a\u8fc7\u53d6\u5404\u4e2a\u89c2\u6d4b\u503c\u7684\u6982\u7387\u4e58\u79ef\u6765\u6c42\u51fa\u6211\u4eec\u89c2\u6d4b\u6837\u672c\u7684 \u6982\u7387\u5bc6\u5ea6\uff1a $$ f(y_1,y_2,\\cdots,y_{10}|\\theta)=\\prod_{i=1}^{10}\\frac{e^{-\\theta}\\theta^{y_i}}{y_i!}=\\frac{e^{-10\\theta}\\theta\\sum_{i=1}^{10}y_i}{\\prod_{i=1}^{10}y_i!} $$ \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u6982\u7387\u5bc6\u5ea6\u6765\u56de\u7b54\u5728\u7ed9\u5b9a\u7279\u5b9a\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u6570\u636e\u53d1\u751f\u7684\u53ef\u80fd\u6027\u3002 \u4f3c\u7136\u51fd\u6570\u548c\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u7684\u4e0d\u540c\u662f\u7ec6\u5fae\u7684\u4f46\u662f\u662f\u91cd\u8981\u7684 \u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u5f3a\u8c03\u5728\u7ed9\u5b9a\u57fa\u7840\u5206\u5e03\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u89c2\u5bdf\u6211\u4eec\u6570\u636e\u7684\u6982\u7387\uff0c\u524d\u63d0\u662f\u5047\u8bbe\u53c2\u6570\u5df2\u77e5\u3002 \u4f3c\u7136\u51fd\u6570\u5f3a\u8c03\u89c2\u6d4b\u6570\u636e\u65f6\u51fa\u73b0\u53c2\u6570\u503c\u7684\u53ef\u80fd\u6027\uff0c\u8fd9\u91cc\u53c2\u6570\u662f\u672a\u77e5\u7684\u3002 \u6570\u5b66\u4e0a\u4f3c\u7136\u51fd\u6570\u548c\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u5f88\u50cf $$ L(\\theta|y_1,y_2,\\cdots,y_{10})=f(y_1,y_2,\\cdots,y_{10}|\\theta) $$ \u5bf9\u4e8e\u6211\u4eec\u7684\u6cca\u677e\u5206\u5e03\u6570\u636e\uff0c\u6211\u4eec\u76f8\u5f53\u5bb9\u6613\u7684\u5f97\u5230\u4f3c\u7136\u51fd\u6570 $$ L(\\theta|y_1,y_2,\\cdots,y_{10})=\\frac{e^{-10\\theta}\\theta\\sum_{i=1}^{10}y_i}{\\prod_{i=1}^{10}y_i!}=\\frac{e^{-10\\theta}\\theta^{20}}{207360} $$ $$ \\prod_{k=3}^{7}k=3\\times4\\times5\\times6\\times7 $$ \u672a\u77e5\u53c2\u6570$\\theta$\u7684\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\uff0c\u662f\u4f7f\u8fd9\u79cd\u53ef\u80fd\u6027\u6700\u5927\u5316\u7684\u503c\u3002 \u5bf9\u6570\u4f3c\u7136\u51fd\u6570 \u5728\u5b9e\u9645\u4e2d\uff0c\u8054\u5408\u5206\u914d\u51fd\u6570\u96be\u4ee5\u4f7f\u7528\uff0c\u4e00\u822c\u7528$ln$\u4f3c\u7136\u51fd\u6570\u4ee3\u66ff\u3002\u5728\u6211\u4eec\u7684\u6cca\u677e\u6570\u636e\u91cc\u5bf9\u6570\u4f3c\u7136\u51fd\u6570\u662f\uff1a ln(L(\\theta|y)=-n\\theta+ln\\sum_{i=1}^{n}y_i-ln\\theta\\sum_{i=1}^{n}y_i!=-10\\theta+20ln(\\theta)-ln(207360) \u5728\u7ebf\u6027\u56de\u5f52\uff0c\u6211\u4eec\u5047\u8bbe\u6a21\u578b\u7684\u6b8b\u5dee\u662f\u72ec\u7279\u7684\u5b8c\u5168\u6b63\u6001\u5206\u5e03\u3002 \\epsilon=y-\\hat{\\beta}x\\quad\\sim N(0,\\delta^2) \u5bf9\u4e8e\u8fd9\u79cd\u60c5\u51b5\uff0c\u672a\u77e5\u53c2\u6570\u5411\u91cf$\\theta={\\beta,\\delta^2}$,\u89c2\u5bdf\u6570\u636e\u7684\u6761\u4ef6y\u548cx\u5df2\u77e5\uff0c\u5bf9\u6570\u4f3c\u7136\u51fd\u6570\u662f\uff1a lnL(\\theta|y,x)=-\\frac{1}{2}\\sum_{i=1}^{n}\\left[ln\\delta^2+ln(2\\pi)+\\frac{y-\\hat{\\beta}x}{\\delta^2}\\right] $\\beta$\u548c$\\delta^2$\u7684\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u5c31\u662f\u4f7f\u53ef\u80fd\u6700\u5927\u5316\u7684\u503c\u3002 \u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u548c\u6982\u7387\u6a21\u578b y^*=x\\theta+\\epsilon\\quad\\epsilon\\sim N(0,1) y_i=\\left\\{\\begin{matrix}0\\quad if\\quad y^{*}_{i}\\leqslant0 &\\\\1\\quad if\\quad y^{*}_i>0\\end{matrix}\\right. P(y_i=1|X_i)=P(y^*_i>0|X_i)=P(x\\theta+\\epsilon>0|X_i)=P(\\epsilon>-x\\theta|X_i)=1-\\Phi(-\\theta x)=\\Phi(x\\theta) $\\Phi$\u4ee3\u8868\u6b63\u6001\u7d2f\u8ba1\u5206\u5e03\u3002 \u6bcf\u4e2a$y_i$\u662f\u72ec\u7acb\u7684\u6240\u4ee5 f(y=(y_1,y_2,\\cdots,y_n|w)=f_1(y_1|w)f_2(y_2|w)\\cdots f_n(y_n|w) log\\pi_\\theta(\\alpha|s)=log[P_\\theta(s)]_\\alpha \u91c7\u6837 \u5df2\u77e5mean action $\\mu_\\theta(s)$\u548c\u6807\u51c6\u504f\u5dee$\\delta_\\theta(s)$\u548c\u7403\u5f62\u9ad8\u65af\u566a\u97f3\u7684\u5411\u91cfz\uff08$z\\sim N(0,1)$\uff09\u53ef\u4ee5\u8ba1\u7b97\u51fa\u4e00\u4e2aaction sample \\alpha=\\mu_\\theta(s)+\\delta_\\theta(s)\\bigodot z $\\bigodot$\u4ee3\u8868\u4e24\u4e2a\u5411\u91cf\u5143\u7d20\u4e58\u79ef\u3002 \u5bf9\u4e8e\u5747\u503c$\\mu=\\mu_\\theta(s),\u6807\u51c6\u504f\u5dee$$\\delta=\\delta_\\theta(s)$\u7684\u5bf9\u89d2\u9ad8\u65af\uff0c$k$\u7ef4action $\\alpha$ \u7684\u5bf9\u6570\u4f3c\u7136\u7531\u4ee5\u4e0b\u7ed9\u51fa\uff1a log\\pi_\\theta(\\alpha|s)=-\\frac{1}{2}\\left(\\sum_{i=1}^{k}\\left(\\frac{(\\alpha_i-\\mu_i)^2}{\\delta^2_i}+2log\\delta_i\\right)+klog2\\pi\\right) Trajectories \u4e00\u4e2a\u8f68\u8ff9$\\tau$\u662fworld\u91cc\u4e00\u7cfb\u5217\u72b6\u6001\u548caction\u3002 \\tau=(s_0,\\alpha_0,s_1,\\alpha_1,\\cdots) s_{t+1}=f(s_t,\\alpha_t) Or s_{t+1}\\sim P(\\cdot|s_t,\\alpha_t) actions\u662f\u4ee3\u7406\u901a\u8fc7\u7b56\u7565\u4ea7\u751f\u7684 \u5956\u52b1\u548c\u53cd\u9988 \u5956\u52b1\u53d6\u51b3\u4e8e\u5f53\u524dworld\u7684\u72b6\u6001\uff0c\u521a\u521a\u91c7\u53d6\u7684action\u548cworld\u7684\u4e0b\u4e00\u4e2a\u72b6\u6001 r_t=R(s_t,a_t,s_{t+1}) \u6709\u65f6\u4f1a\u7b80\u5316\u4e3a$r_t=R(s_t)$\u6216\u8005state-action $r_t=R(s_t,a_t)$ \u4ee3\u7406\u7684\u76ee\u6807\u662f\u4f7f\u67d0\u4e2a\u8f68\u8ff9\u4e0a\u7684\u67d0\u4e2a\u6982\u5ff5\u7684\u7d2f\u8ba1\u5956\u52b1\u6700\u5927\u5316\u3002 \u6211\u4eec\u5c06\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u8bb0\u5f55\u6240\u6709\u7684\u60c5\u51b5$R(\\tau)$ \u4e00\u79cd\u53cd\u9988\u662f\u6709\u9650\u6c34\u5e73\u7684\u65e0\u6298\u73b0\u53cd\u9988\uff0c\u5c31\u662f\u6240\u6709\u5956\u52b1\u7684\u548c R(\\tau)=\\sum_{t=0}^{T}r_t \u53e6\u4e00\u79cd\u53cd\u9988\u662f\u6709\u9650\u6c34\u5e73\u7684\u6298\u73b0\u53cd\u9988\uff0c\u4ee3\u7406\u83b7\u5f97\u7684\u5956\u52b1\u4e4b\u548c\uff0c\u6298\u73b0\u5c06\u6765\u4ee3\u7406\u83b7\u5f97\u7684how far off.\u8fd9\u4e2a\u65b9\u7a0b\u5f0f\u7684\u5956\u52b1\u5305\u542b\u4e00\u4e2a\u6298\u73b0\u56e0\u5b50$\\gamma\\in(0,1)$ R(\\tau)=\\sum_{t=o}^{\\infty}\\gamma^t\\tau_t \u4e3a\u4ec0\u4e48\u6211\u4eec\u60f3\u8981\u4e00\u4e2a\u6298\u73b0\u7684\u53cd\u9988\uff0c\u800c\u4e0d\u662f\u6240\u6709\u7684\u53cd\u9988\uff1f\u6211\u4eec\u7684\u786e\u60f3\u8981\u6240\u6709\u53cd\u9988\uff0c\u4f46\u662f\u6298\u73b0\u56e0\u5b50\u5728\u76f4\u89c2\u4e0a\u5373\u5438\u5f15\u4eba\uff0c\u6570\u5b66\u5b9e\u73b0\u8d77\u6765\u53c8\u65b9\u4fbf\u3002\u4e00\u79cd\u76f4\u89c9\uff1a\u73b0\u5728\u7684\u73b0\u91d1\u6bd4\u4ee5\u540e\u7684\u73b0\u91d1\u597d\u3002\u6570\u5b66\u4e0a\uff1a\u65e0\u9650\u6c34\u5e73\u7684\u5956\u52b1\u4e4b\u548c\u53ef\u80fd\u4e0d\u4f1a\u6536\u655b\u5230\u6709\u9650\u7684\u503c\uff0c\u5e76\u4e14\u5f88\u96be\u7528\u65b9\u7a0b\u5f0f\u5904\u7406\u3002\u4f46\u662f\u5728\u6709\u6298\u73b0\u56e0\u5b50\u7684\u60c5\u51b5\u4e0b\uff0c\u5e76\u5728\u5408\u7406\u7684\u6761\u4ef6\u4e0b\uff0c\u65e0\u7a77\u5927\u662f\u6536\u655b\u7684\u3002 Deep RL \u7ec3\u4e60\u5f80\u5f80\u4f1a\u4f7f\u8fd9\u6761\u7ebf\u6a21\u7cca\u4e0d\u6e05\uff0c\u4f8b\u5982\uff0c\u6211\u4eec\u7ecf\u5e38\u8bbe\u7f6e\u7b97\u6cd5\u4ee5\u4f18\u5316\u672a\u6298\u73b0\u7684\u53cd\u9988\uff0c\u4f46\u5728\u8bc4\u4ef7value function \u7684\u65f6\u5019\u4f7f\u7528\u6298\u73b0\u56e0\u5b50\u3002 \u65e0\u8bba\u9009\u62e9\u54ea\u79cd\u53cd\u9988\u65b9\u6cd5\uff0c\u9009\u62e9\u54ea\u79cd\u7b56\u7565\uff0cRL\u7684\u76ee\u6807\u662f\u9009\u62e9\u4e00\u4e2a\u7b56\u7565\u4f7f\u4ee3\u7406\u8fd0\u884c\u6b64\u7b56\u7565\u4f7f\u671f\u671b\u7684\u53cd\u9988\u6700\u5927\u5316\u3002 \u8981\u8ba8\u8bba\u671f\u671b\u7684\u53cd\u9988\uff0c\u6211\u4eec\u5148\u6765\u8ba8\u8bba\u4e00\u4e0b\u8f68\u8ff9\u4e0a\u7684\u6982\u7387\u5206\u5e03\u3002 \u8ba9\u6211\u5047\u8bbe\u73af\u5883\u8f6c\u53d8\u548c\u7b56\u7565\u90fd\u662f\u968f\u673a\u7684\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u8f68\u8ff9T-step\u7684\u6982\u7387\u662f\uff1a P(T|\\pi)=\\rho_0(s_0)\\prod_{t=0}^{T-1}P(s_{t+1}|s_t,\\alpha_t)\\pi(s_t,\\alpha_t) \u671f\u671b\u7684\u53cd\u9988\u8868\u793a\u4e3a J(\\pi)=\\int_{r}P(\\tau|\\pi)R(\\tau)=\\underset{\\tau\\sim\\pi}{E}[R(\\tau)] RL\u4e2d\u7684\u96c6\u4e2d\u4f18\u5316\u95ee\u9898\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a \\pi^*=arg\\quad\\underset{\\pi}{max}J(\\pi) $\\pi^ $\u662f\u6700\u4f73\u7b56\u7565\u3002 Value Function \u77e5\u9053value of state \u6216\u8005 state-action \u901a\u5e38\u662f\u6709\u7528\u7684\u3002 \u8fd9\u91cc\u6709\u56db\u79cdvalue function. 1. \u57fa\u4e8e\u7b56\u7565\u7684value function.$V^\\pi(s)$,\u5f53state s,\u6267\u884c\u7b56\u7565$\\pi$\u65f6\uff0c V^\\pi(s)=\\underset{\\tau\\in\\pi}{E}[R(\\tau)|s_0=s] 2. \u57fa\u4e8e\u7b56\u7565\u7684 action-value function. $Q^\\pi(s,\\alpha)$, Q^\\pi(s,\\alpha)=\\underset{\\tau\\in\\pi}{E}[R(\\tau)|s_0=s,a_0=a] 3. \u6700\u4f18value function\uff0c $V^ (s)$ V^*(s)=\\underset{\\pi}{max}\\underset{\\tau\\in\\pi}{E}[R(\\tau)|s_0=s] 4. \u6700\u4f73action-value function, $Q^*(s,\\alpha)$ Q^*(s,\\alpha)=\\underset{\\pi}{max}\\underset{\\tau\\in\\pi}{E}[R(\\tau)|s_0=s,\\alpha_0=\\alpha] \u5728value function \u548c action-value function \u4e4b\u95f4\u6709\u4e24\u4e2a\u5173\u952e\u7684\u94fe\u63a5 V^\\pi(s)=\\underset{\\alpha\\sim\\pi}{E}[Q^\\pi(s,\\alpha)] \u548c V^*(s)=\\underset{\\alpha}{max}Q^*(s,\\alpha) \\alpha^*(s)=arg\\quad\\underset{\\alpha}{max}Q^*(s,\\alpha) \u8d1d\u5c14\u66fc\u65b9\u7a0b \u8fd94\u4e2avalue function\u90fd\u9075\u5faa\u7684\u7279\u6b8a\u81ea\u6cbb\u65b9\u7a0b,\u79f0\u4e3aBellman equations Bellman equation\u540e\u9762\u7684\u57fa\u672c\u601d\u60f3\u662f\uff1a \u8d77\u70b9\u5904\u7684value\uff0c\u662f\u4f60\u671f\u671b\u4ece\u90a3\u91cc\u5f97\u5230\u7684reward\uff0c\u518d\u52a0\u4e0a\u4e0b\u4e00\u6b65\u5230\u8fbe\u5730\u65b9\u7684value\u3002 \u57fa\u4e8e\u7b56\u7565\u7684value function \u7684Bellman equations V^\\pi(s)=\\underset{s'\\sim P}{\\underset{\\alpha\\sim\\pi}{E}}[\\tau(s,\\alpha)+\\gamma V^\\pi(s')] Q^\\pi(s,\\alpha)=\\underset{s'\\sim P}{E}\\left[\\tau(s,\\alpha)+\\gamma\\underset{a'\\sim\\pi}{E}[Q^\\pi(s',\\alpha')]\\right] $s'\\sim P$\u662f$s'\\sim P(\\cdot|s,\\alpha)$\u7684\u7f29\u5199\uff0c\u6307\u793a\u4e0b\u4e00\u72b6\u6001$s'$\u4ece\u73af\u5883\u7684\u8fc7\u5ea6\u89c4\u5219\u4e2d\u91c7\u6837\uff1b$\\alpha\\sim\\pi$\u662f$\\alpha\\sim\\pi(\\cdot|s)$\u7684\u7f29\u5199\uff1b$\\alpha'\\sim\\pi$\u662f$\\alpha'\\sim\\pi(\\cdot|s')$\u7684\u7f29\u5199\u3002 \u6700\u4f73value function\u7684Bellman equation\u662f V^*(s)=\\underset{\\alpha}{max}\\quad\\underset{s'\\sim P}{E}[\\tau(s,\\alpha)+\\gamma V^*(s')] Q^*(s,\\alpha)=\\underset{s'\\sim P}{E}\\left[\\tau(s,\\alpha)+\\gamma\\underset{\\alpha'}{max}Q^*(s',\\alpha')\\right] Bellman equation: reward-plus-next-value Advantage Functions A^\\pi(s,\\alpha)=Q^\\pi(s,\\alpha)-V^\\pi(s) Markov Decision Processes An MDP is a 5-tuple,$(S,A,R,P,\\rho_0)$ S is the set of all valid states. A is the set of all valid actions. R:$S\\times A\\times S\\rightarrow \\mathbb{R}$ is the reward function , with $\\tau_t=R(s_t,\\alpha_t,s_{t+1})$ P:$S\\times A \\rightarrow P(S)$ is the transition probability function, with $P(s'|s,\\alpha)$ being the probability of transitioning into state $s'$,if you start in state s and take action $\\alpha$ $\\rho_0$ is the starting state distribution.","title":"\u7b1b\u5361\u5c14\u79ef"}]}