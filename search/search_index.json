{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to ZGF Wiki \u867d\u53e4\u72b9\u4e0d\u53ca \u5b50\u66f0\uff1a\u201c\u5b66\u800c\u65f6\u4e60\u4e4b\uff0c\u4e0d\u4ea6\u8bf4\u4e4e\uff1f\u6709\u670b\u81ea\u8fdc\u65b9\u6765\uff0c\u4e0d\u4ea6\u4e50\u4e4e\uff1f\u4eba\u4e0d\u77e5\u800c\u4e0d\u6120\uff0c\u4e0d\u4ea6\u541b\u5b50\u4e4e\uff1f\u201d \u66fe\u5b50\u66f0\uff1a\u201c\u543e\u65e5\u4e09\u7701\u543e\u8eab\uff1a\u4e3a\u4eba\u8c0b\u800c\u4e0d\u5fe0\u4e4e\uff1f\u4e0e\u670b\u53cb\u4ea4\u800c\u4e0d\u4fe1\u4e4e\uff1f\u4f20\u4e0d\u4e60\u4e4e\uff1f\u201d \u5b50\u66f0\uff1a\u201c\u6e29\u6545\u800c\u77e5\u65b0\uff0c\u53ef\u4ee5\u4e3a\u5e08\u77e3\u3002\u201d \u5b50\u66f0\uff1a\u201c\u5b66\u800c\u4e0d\u601d\u5219\u7f54\uff0c\u601d\u800c\u4e0d\u5b66\u5219\u6b86\u3002\u201d \u5b50\u66f0\uff1a\u201c\u7531\uff0c\u8bf2\u5973\u77e5\u4e4b\u4e4e\uff01\u77e5\u4e4b\u4e3a\u77e5\u4e4b\uff0c\u4e0d\u77e5\u4e3a\u4e0d\u77e5\uff0c\u662f\u77e5\u4e5f\u3002\u201d \u5b50\u66f0\uff1a\u201c\u89c1\u8d24\u601d\u9f50\u7109\uff0c\u89c1\u4e0d\u8d24\u800c\u5185\u81ea\u7701\u4e5f\u3002\u201d \u5b50\u66f0\uff1a\u201c\u4e09\u4eba\u884c\uff0c\u5fc5\u6709\u6211\u5e08\u7109\u3002\u62e9\u5176\u5584\u8005\u800c\u4ece\u4e4b\uff0c\u5176\u4e0d\u5584\u8005\u800c\u6539\u4e4b\u3002\u201d \u66fe\u5b50\u66f0\uff1a\u201c\u58eb\u4e0d\u53ef\u4ee5\u4e0d\u5f18\u6bc5\uff0c\u4efb\u91cd\u800c\u9053\u8fdc\u3002\u4ec1\u4ee5\u4e3a\u5df1\u4efb\uff0c\u4e0d\u4ea6\u91cd\u4e4e\uff1f\u6b7b\u800c\u540e\u5df2\uff0c\u4e0d\u4ea6\u8fdc\u4e4e\uff1f\u201d \u5b50\u66f0\uff1a\u201c\u5c81\u5bd2\uff0c\u7136\u540e\u77e5\u677e\u67cf\u4e4b\u540e\u51cb\u4e5f\u3002\u201d \u5b50\u8d21\u95ee\u66f0\uff1a\u201c\u6709\u4e00\u8a00\u800c\u53ef\u4ee5\u7ec8\u8eab\u884c\u4e4b\u8005\u4e4e\uff1f\u201d \u5b50\u66f0\uff1a\u201c\u5176\u6055\u4e4e\uff01\u5df1\u6240\u4e0d\u6b32\uff0c\u52ff\u65bd\u4e8e\u4eba\u3002\u201d","title":"Home"},{"location":"#welcome-to-zgf-wiki","text":"\u867d\u53e4\u72b9\u4e0d\u53ca \u5b50\u66f0\uff1a\u201c\u5b66\u800c\u65f6\u4e60\u4e4b\uff0c\u4e0d\u4ea6\u8bf4\u4e4e\uff1f\u6709\u670b\u81ea\u8fdc\u65b9\u6765\uff0c\u4e0d\u4ea6\u4e50\u4e4e\uff1f\u4eba\u4e0d\u77e5\u800c\u4e0d\u6120\uff0c\u4e0d\u4ea6\u541b\u5b50\u4e4e\uff1f\u201d \u66fe\u5b50\u66f0\uff1a\u201c\u543e\u65e5\u4e09\u7701\u543e\u8eab\uff1a\u4e3a\u4eba\u8c0b\u800c\u4e0d\u5fe0\u4e4e\uff1f\u4e0e\u670b\u53cb\u4ea4\u800c\u4e0d\u4fe1\u4e4e\uff1f\u4f20\u4e0d\u4e60\u4e4e\uff1f\u201d \u5b50\u66f0\uff1a\u201c\u6e29\u6545\u800c\u77e5\u65b0\uff0c\u53ef\u4ee5\u4e3a\u5e08\u77e3\u3002\u201d \u5b50\u66f0\uff1a\u201c\u5b66\u800c\u4e0d\u601d\u5219\u7f54\uff0c\u601d\u800c\u4e0d\u5b66\u5219\u6b86\u3002\u201d \u5b50\u66f0\uff1a\u201c\u7531\uff0c\u8bf2\u5973\u77e5\u4e4b\u4e4e\uff01\u77e5\u4e4b\u4e3a\u77e5\u4e4b\uff0c\u4e0d\u77e5\u4e3a\u4e0d\u77e5\uff0c\u662f\u77e5\u4e5f\u3002\u201d \u5b50\u66f0\uff1a\u201c\u89c1\u8d24\u601d\u9f50\u7109\uff0c\u89c1\u4e0d\u8d24\u800c\u5185\u81ea\u7701\u4e5f\u3002\u201d \u5b50\u66f0\uff1a\u201c\u4e09\u4eba\u884c\uff0c\u5fc5\u6709\u6211\u5e08\u7109\u3002\u62e9\u5176\u5584\u8005\u800c\u4ece\u4e4b\uff0c\u5176\u4e0d\u5584\u8005\u800c\u6539\u4e4b\u3002\u201d \u66fe\u5b50\u66f0\uff1a\u201c\u58eb\u4e0d\u53ef\u4ee5\u4e0d\u5f18\u6bc5\uff0c\u4efb\u91cd\u800c\u9053\u8fdc\u3002\u4ec1\u4ee5\u4e3a\u5df1\u4efb\uff0c\u4e0d\u4ea6\u91cd\u4e4e\uff1f\u6b7b\u800c\u540e\u5df2\uff0c\u4e0d\u4ea6\u8fdc\u4e4e\uff1f\u201d \u5b50\u66f0\uff1a\u201c\u5c81\u5bd2\uff0c\u7136\u540e\u77e5\u677e\u67cf\u4e4b\u540e\u51cb\u4e5f\u3002\u201d \u5b50\u8d21\u95ee\u66f0\uff1a\u201c\u6709\u4e00\u8a00\u800c\u53ef\u4ee5\u7ec8\u8eab\u884c\u4e4b\u8005\u4e4e\uff1f\u201d \u5b50\u66f0\uff1a\u201c\u5176\u6055\u4e4e\uff01\u5df1\u6240\u4e0d\u6b32\uff0c\u52ff\u65bd\u4e8e\u4eba\u3002\u201d","title":"Welcome to ZGF Wiki"},{"location":"Atlas200%E9%AA%8C%E6%94%B6%E6%8A%A5%E5%91%8A/","text":"Atlas200\u9a8c\u6536\u62a5\u544a \u5728\u7ebf\u652f\u6301 - \u665f\u817e\u5b66\u9662 - Atlas 200 DK \u6027\u80fd\u7279\u70b9 - \u53ef\u63d0\u4f9b16TOPS (INT8) \u7684\u5cf0\u503c\u8ba1\u7b97\u80fd\u529b\u3002 - \u652f\u6301\u4e24\u8defCamera\u8f93\u5165\uff0c\u4e24\u8defISP\u56fe\u50cf\u5904\u7406\uff0c\u652f\u6301HDR10\u9ad8\u52a8\u6001\u8303\u56f4\u6280\u672f\u6807\u51c6 - \u652f\u63011000M\u4ee5\u592a\u7f51\u5bf9\u5916\u63d0\u4f9b\u9ad8\u901f\u7f51\u7edc\u8fde\u63a5\uff0c\u5339\u914d\u5f3a\u52b2\u8ba1\u7b97\u80fd\u529b - \u901a\u7528\u768440-pin\u6269\u5c55\u63a5\u53e3\uff0c\u65b9\u4fbf\u4ea7\u54c1\u539f\u578b\u8bbe\u8ba1 - \u652f\u63015v~28v\u5bbd\u8303\u56f4\u76f4\u6d41\u7535\u6e90\u8f93\u5165 \u9700\u8981\u914d\u4ef6 - micro sd\u5361 32G - typeC usb \u7ebf - \u7f51\u7ebf - \u8bfb\u5361\u5668 - Ubuntu\u670d\u52a1\u5668 \u5236\u4f5cUsb\u7cfb\u7edf\u542f\u52a8\u76d8 - \u5c06SD\u5361\u653e\u5165\u8bfb\u5361\u5668\uff0c\u5c06\u8bfb\u5361\u5668\u4e0eUbuntu\u670d\u52a1\u5668\u7684USB\u8fde\u63a5\uff0c\u7136\u540e\u901a\u8fc7\u5236\u5361\u811a\u672c\u8fdb\u884cSD\u5361\u7684\u5236\u4f5c - \u8f6f\u4ef6\u5305\u51c6\u5907 - \u5236\u5361\u5165\u53e3\u811a\u672c make_sd_card.py - \u5236\u4f5cSD\u5361\u64cd\u4f5c\u7cfb\u7edf\u811a\u672c make_ubuntu_sd.sh - \u8bf7\u9009\u62e9\u914d\u5957\u7684CANN\u7248\u672c\uff1a20.1\u3002 - \u201c\u4ea7\u54c1\u7cfb\u5217\u201d\u8bf7\u9009\u62e9\u201cAtlas 200 DK\u201d\u3002 - \u201c\u4ea7\u54c1\u578b\u53f7\u201d\u8bf7\u9009\u62e9\u201cAtlas 200 DK \u5f00\u53d1\u8005\u5957\u4ef6\u201d\u3002 - Ubuntu\u64cd\u4f5c\u7cfb\u7edf\u955c\u50cf\u5305 - ubuntu-18.04.xx-server-arm64.iso - \u5f00\u53d1\u8005\u677f\u9a71\u52a8\u5305 - A200dk-npu-driver-{software version}-ubuntu18.04-aarch64-minirc.tar.gz - \u8bf7\u9009\u62e9\u914d\u5957\u7684CANN\u7248\u672c\uff1a20.1\u3002 - \u201c\u4ea7\u54c1\u7cfb\u5217\u201d\u8bf7\u9009\u62e9\u201cAtlas 200 DK\u201d\u3002 - \u201c\u4ea7\u54c1\u578b\u53f7\u201d\u8bf7\u9009\u62e9\u201cAtlas 200 DK \u5f00\u53d1\u8005\u5957\u4ef6\u201d\u3002 - \u5f00\u53d1\u8005\u677f\u8fd0\u884c\u5305 - Ascend-cann-minirc_{software version}_ubuntu18.04-aarch64.zip - \u8bf7\u4ece\u201cCANN\u8f6f\u4ef6\u5305\u201d\u4e2d\u9009\u62e9\u5bf9\u5e94\u7248\u672c\u7684\u8f6f\u4ef6\u5305\u4e0b\u8f7d\u3002 \u64cd\u4f5c\u6b65\u9aa4 1. \u8bf7\u5c06SD\u5361\u653e\u5165\u8bfb\u5361\u5668\uff0c\u5e76\u5c06\u8bfb\u5361\u5668\u4e0eUbuntu\u670d\u52a1\u5668\u7684USB\u63a5\u53e3\u8fde\u63a5\u3002 2. sudo apt-get install qemu-user-static binfmt-support python3-yaml gcc-aarch64-linux-gnu g++-aarch64-linux-gnu 3. \u628a\u51c6\u5907\u597d\u7684\u6587\u4ef6\u5305\u653e\u5728/home/username/mksd\u76ee\u5f55\u4e0b\uff0c cd /home/username/mksd 4. \u6267\u884c fdisk -l \u67e5\u770bsd\u5361\u6240\u5728usb\u8bbe\u5907\u540d\u79f0\u4e3a\u201c/dev/sda\u201d\uff0c\u53ef\u901a\u8fc7\u63d2\u62d4SD\u5361\u7684\u65b9\u5f0f\u786e\u5b9a\u8bbe\u5907\u540d\u79f0\u3002 5. \u8fd0\u884cSD\u5236\u5361\u811a\u672c\u201cmake_sd_card.py\u201d\u3002 1. python3 make_sd_card.py local /dev/sda 2. \u201clocal\u201d\u8868\u793a\u4f7f\u7528\u672c\u5730\u65b9\u5f0f\u5236\u4f5cSD\u5361\u3002 3. \u201c/dev/sda\u201d\u4e3aSD\u5361\u6240\u5728\u7684USB\u8bbe\u5907\u540d\u79f0\u3002 \u9047\u5230\u95ee\u9898 \uff1a\u5236\u4f5csd\u5361\u4e0d\u6210\u529f\uff0c\u7ecf\u8fc7\u53cd\u590d\u534f\u52a9\u534e\u4e3a\u65b9\u6280\u672f\u4eba\u5458\uff0c\u53d1\u73b0\u534e\u4e3a\u5b98\u65b9\u63d0\u4f9b\u7684\u811a\u672c\u6587\u4ef6 make_ubuntu_sd.sh \u6709bug\u3002\u4f7f\u7528\u4f53\u9a8c\u5dee\u3002\u534e\u4e3a\u65b9\u89e3\u51b3\u95ee\u9898\u5468\u671f\u957f\uff0c\u8fd9\u4e2a\u95ee\u9898\u7528\u4e86\u4e24\u5929\u624d\u89e3\u51b3\u3002\u540e\u671f\u5e94\u7528\u8fc7\u7a0b\u4e2d\u534e\u4e3aatlas\u5e73\u53f0\u4e5f\u4f1a\u5b58\u5728\u4e00\u4e9bbug\uff0c\u5f00\u53d1\u901f\u5ea6\u53d7\u9650\u4e8e\u534e\u4e3a\u65b9\u89e3\u51b3atlas\u4ea7\u54c1bug\u7684\u901f\u5ea6\u3002\u9047\u5230Atlas\u5e73\u53f0\u95ee\u9898\uff0c\u53ea\u80fd\u5728\u534e\u4e3a\u4e91\u8bba\u575b\u4e0a\u8fdb\u884c\u63d0\u95ee\uff0c\u89e3\u51b3\u95ee\u9898\u5468\u671f\u957f\u3002 \u83b7\u5f97\u4fee\u6b63\u540e\u7684 make_ubuntu_sd.sh \u811a\u672c\u6587\u4ef6\u5b89\u88c5\u6210\u529f\u3002 Atlas200 \u5f00\u673a\u542f\u52a8\u6210\u529f\uff0c\u8fdb\u5165\u7cfb\u7edf\u3002\\ \u8fdb\u884c\u670d\u52a1\u5668ip\u8bbe\u7f6e \uff1a\\ ip:192.168.0.3\\ netmask:255.255.0.0\\ gateway:192.168.0.1\\ ssh\u767b\u5165Atlas200 :\\ ssh HwHiAiUser@192.168.0.2 \\ \u5bc6\u7801:\\ Mind@123 \\","title":"Atlas200\u9a8c\u6536\u62a5\u544a"},{"location":"Atlas200%E9%AA%8C%E6%94%B6%E6%8A%A5%E5%91%8A/#atlas200","text":"\u5728\u7ebf\u652f\u6301 - \u665f\u817e\u5b66\u9662 - Atlas 200 DK \u6027\u80fd\u7279\u70b9 - \u53ef\u63d0\u4f9b16TOPS (INT8) \u7684\u5cf0\u503c\u8ba1\u7b97\u80fd\u529b\u3002 - \u652f\u6301\u4e24\u8defCamera\u8f93\u5165\uff0c\u4e24\u8defISP\u56fe\u50cf\u5904\u7406\uff0c\u652f\u6301HDR10\u9ad8\u52a8\u6001\u8303\u56f4\u6280\u672f\u6807\u51c6 - \u652f\u63011000M\u4ee5\u592a\u7f51\u5bf9\u5916\u63d0\u4f9b\u9ad8\u901f\u7f51\u7edc\u8fde\u63a5\uff0c\u5339\u914d\u5f3a\u52b2\u8ba1\u7b97\u80fd\u529b - \u901a\u7528\u768440-pin\u6269\u5c55\u63a5\u53e3\uff0c\u65b9\u4fbf\u4ea7\u54c1\u539f\u578b\u8bbe\u8ba1 - \u652f\u63015v~28v\u5bbd\u8303\u56f4\u76f4\u6d41\u7535\u6e90\u8f93\u5165 \u9700\u8981\u914d\u4ef6 - micro sd\u5361 32G - typeC usb \u7ebf - \u7f51\u7ebf - \u8bfb\u5361\u5668 - Ubuntu\u670d\u52a1\u5668 \u5236\u4f5cUsb\u7cfb\u7edf\u542f\u52a8\u76d8 - \u5c06SD\u5361\u653e\u5165\u8bfb\u5361\u5668\uff0c\u5c06\u8bfb\u5361\u5668\u4e0eUbuntu\u670d\u52a1\u5668\u7684USB\u8fde\u63a5\uff0c\u7136\u540e\u901a\u8fc7\u5236\u5361\u811a\u672c\u8fdb\u884cSD\u5361\u7684\u5236\u4f5c - \u8f6f\u4ef6\u5305\u51c6\u5907 - \u5236\u5361\u5165\u53e3\u811a\u672c make_sd_card.py - \u5236\u4f5cSD\u5361\u64cd\u4f5c\u7cfb\u7edf\u811a\u672c make_ubuntu_sd.sh - \u8bf7\u9009\u62e9\u914d\u5957\u7684CANN\u7248\u672c\uff1a20.1\u3002 - \u201c\u4ea7\u54c1\u7cfb\u5217\u201d\u8bf7\u9009\u62e9\u201cAtlas 200 DK\u201d\u3002 - \u201c\u4ea7\u54c1\u578b\u53f7\u201d\u8bf7\u9009\u62e9\u201cAtlas 200 DK \u5f00\u53d1\u8005\u5957\u4ef6\u201d\u3002 - Ubuntu\u64cd\u4f5c\u7cfb\u7edf\u955c\u50cf\u5305 - ubuntu-18.04.xx-server-arm64.iso - \u5f00\u53d1\u8005\u677f\u9a71\u52a8\u5305 - A200dk-npu-driver-{software version}-ubuntu18.04-aarch64-minirc.tar.gz - \u8bf7\u9009\u62e9\u914d\u5957\u7684CANN\u7248\u672c\uff1a20.1\u3002 - \u201c\u4ea7\u54c1\u7cfb\u5217\u201d\u8bf7\u9009\u62e9\u201cAtlas 200 DK\u201d\u3002 - \u201c\u4ea7\u54c1\u578b\u53f7\u201d\u8bf7\u9009\u62e9\u201cAtlas 200 DK \u5f00\u53d1\u8005\u5957\u4ef6\u201d\u3002 - \u5f00\u53d1\u8005\u677f\u8fd0\u884c\u5305 - Ascend-cann-minirc_{software version}_ubuntu18.04-aarch64.zip - \u8bf7\u4ece\u201cCANN\u8f6f\u4ef6\u5305\u201d\u4e2d\u9009\u62e9\u5bf9\u5e94\u7248\u672c\u7684\u8f6f\u4ef6\u5305\u4e0b\u8f7d\u3002 \u64cd\u4f5c\u6b65\u9aa4 1. \u8bf7\u5c06SD\u5361\u653e\u5165\u8bfb\u5361\u5668\uff0c\u5e76\u5c06\u8bfb\u5361\u5668\u4e0eUbuntu\u670d\u52a1\u5668\u7684USB\u63a5\u53e3\u8fde\u63a5\u3002 2. sudo apt-get install qemu-user-static binfmt-support python3-yaml gcc-aarch64-linux-gnu g++-aarch64-linux-gnu 3. \u628a\u51c6\u5907\u597d\u7684\u6587\u4ef6\u5305\u653e\u5728/home/username/mksd\u76ee\u5f55\u4e0b\uff0c cd /home/username/mksd 4. \u6267\u884c fdisk -l \u67e5\u770bsd\u5361\u6240\u5728usb\u8bbe\u5907\u540d\u79f0\u4e3a\u201c/dev/sda\u201d\uff0c\u53ef\u901a\u8fc7\u63d2\u62d4SD\u5361\u7684\u65b9\u5f0f\u786e\u5b9a\u8bbe\u5907\u540d\u79f0\u3002 5. \u8fd0\u884cSD\u5236\u5361\u811a\u672c\u201cmake_sd_card.py\u201d\u3002 1. python3 make_sd_card.py local /dev/sda 2. \u201clocal\u201d\u8868\u793a\u4f7f\u7528\u672c\u5730\u65b9\u5f0f\u5236\u4f5cSD\u5361\u3002 3. \u201c/dev/sda\u201d\u4e3aSD\u5361\u6240\u5728\u7684USB\u8bbe\u5907\u540d\u79f0\u3002 \u9047\u5230\u95ee\u9898 \uff1a\u5236\u4f5csd\u5361\u4e0d\u6210\u529f\uff0c\u7ecf\u8fc7\u53cd\u590d\u534f\u52a9\u534e\u4e3a\u65b9\u6280\u672f\u4eba\u5458\uff0c\u53d1\u73b0\u534e\u4e3a\u5b98\u65b9\u63d0\u4f9b\u7684\u811a\u672c\u6587\u4ef6 make_ubuntu_sd.sh \u6709bug\u3002\u4f7f\u7528\u4f53\u9a8c\u5dee\u3002\u534e\u4e3a\u65b9\u89e3\u51b3\u95ee\u9898\u5468\u671f\u957f\uff0c\u8fd9\u4e2a\u95ee\u9898\u7528\u4e86\u4e24\u5929\u624d\u89e3\u51b3\u3002\u540e\u671f\u5e94\u7528\u8fc7\u7a0b\u4e2d\u534e\u4e3aatlas\u5e73\u53f0\u4e5f\u4f1a\u5b58\u5728\u4e00\u4e9bbug\uff0c\u5f00\u53d1\u901f\u5ea6\u53d7\u9650\u4e8e\u534e\u4e3a\u65b9\u89e3\u51b3atlas\u4ea7\u54c1bug\u7684\u901f\u5ea6\u3002\u9047\u5230Atlas\u5e73\u53f0\u95ee\u9898\uff0c\u53ea\u80fd\u5728\u534e\u4e3a\u4e91\u8bba\u575b\u4e0a\u8fdb\u884c\u63d0\u95ee\uff0c\u89e3\u51b3\u95ee\u9898\u5468\u671f\u957f\u3002 \u83b7\u5f97\u4fee\u6b63\u540e\u7684 make_ubuntu_sd.sh \u811a\u672c\u6587\u4ef6\u5b89\u88c5\u6210\u529f\u3002 Atlas200 \u5f00\u673a\u542f\u52a8\u6210\u529f\uff0c\u8fdb\u5165\u7cfb\u7edf\u3002\\ \u8fdb\u884c\u670d\u52a1\u5668ip\u8bbe\u7f6e \uff1a\\ ip:192.168.0.3\\ netmask:255.255.0.0\\ gateway:192.168.0.1\\ ssh\u767b\u5165Atlas200 :\\ ssh HwHiAiUser@192.168.0.2 \\ \u5bc6\u7801:\\ Mind@123 \\","title":"Atlas200\u9a8c\u6536\u62a5\u544a"},{"location":"Autoware.auto/","text":"Autoware.Auto Install ubuntu 20.04 ROS2_FOXY sudo apt update && sudo apt install locales sudo locale-gen en_US en_US.UTF-8 sudo update-locale LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8 export LANG=en_US.UTF-8 sudo apt install curl sudo apt update && sudo apt install curl gnupg2 lsb-release curl -s https://raw.githubusercontent.com/ros/rosdistro/master/ros.asc | sudo apt-key add - sudo sh -c 'echo \"deb [arch=$(dpkg --print-architecture)] http://packages.ros.org/ros2/ubuntu $(lsb_release -cs) main\" > /etc/apt/sources.list.d/ros2-latest.list' sudo apt update && sudo apt install -y \\ build-essential \\ cmake \\ git \\ libbullet-dev \\ python3-colcon-common-extensions \\ python3-flake8 \\ python3-pip \\ python3-pytest-cov \\ python3-rosdep \\ python3-setuptools \\ python3-vcstool \\ wget # install some pip packages needed for testing python3 -m pip install -U \\ argcomplete \\ flake8-blind-except \\ flake8-builtins \\ flake8-class-newline \\ flake8-comprehensions \\ flake8-deprecated \\ flake8-docstrings \\ flake8-import-order \\ flake8-quotes \\ pytest-repeat \\ pytest-rerunfailures \\ pytest # install Fast-RTPS dependencies sudo apt install --no-install-recommends -y \\ libasio-dev \\ libtinyxml2-dev # install Cyclone DDS dependencies sudo apt install --no-install-recommends -y \\ libcunit1-dev \u83b7\u53d6ros2\u4ee3\u7801 mkdir -p ~/ros2_foxy/src cd ~/ros2_foxy wget https://raw.githubusercontent.com/ros2/ros2/foxy/ros2.repos vcs import src < ros2.repos \u4f7f\u7528rosdep\u5b89\u88c5\u4f9d\u8d56 sudo rosdep init rosdep update rosdep install --from-paths src --ignore-src --rosdistro foxy -y --skip-keys \"console_bridge fastcdr fastrtps rti-connext-dds-5.3.1 urdfdom_headers\" Build cd ~/ros2_foxy colcon build --symlink-install build\u8fc7\u7a0b\u4e2d\u4f1a\u51fa\u73b0\u4e0b\u8f7d\u9519\u8bef\uff0c\u4f7f\u7528hub.fastgit.org\u4ee3\u66ffgithub.com","title":"Autoware.Auto"},{"location":"Autoware.auto/#autowareauto","text":"","title":"Autoware.Auto"},{"location":"Autoware.auto/#install","text":"ubuntu 20.04 ROS2_FOXY sudo apt update && sudo apt install locales sudo locale-gen en_US en_US.UTF-8 sudo update-locale LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8 export LANG=en_US.UTF-8 sudo apt install curl sudo apt update && sudo apt install curl gnupg2 lsb-release curl -s https://raw.githubusercontent.com/ros/rosdistro/master/ros.asc | sudo apt-key add - sudo sh -c 'echo \"deb [arch=$(dpkg --print-architecture)] http://packages.ros.org/ros2/ubuntu $(lsb_release -cs) main\" > /etc/apt/sources.list.d/ros2-latest.list' sudo apt update && sudo apt install -y \\ build-essential \\ cmake \\ git \\ libbullet-dev \\ python3-colcon-common-extensions \\ python3-flake8 \\ python3-pip \\ python3-pytest-cov \\ python3-rosdep \\ python3-setuptools \\ python3-vcstool \\ wget # install some pip packages needed for testing python3 -m pip install -U \\ argcomplete \\ flake8-blind-except \\ flake8-builtins \\ flake8-class-newline \\ flake8-comprehensions \\ flake8-deprecated \\ flake8-docstrings \\ flake8-import-order \\ flake8-quotes \\ pytest-repeat \\ pytest-rerunfailures \\ pytest # install Fast-RTPS dependencies sudo apt install --no-install-recommends -y \\ libasio-dev \\ libtinyxml2-dev # install Cyclone DDS dependencies sudo apt install --no-install-recommends -y \\ libcunit1-dev \u83b7\u53d6ros2\u4ee3\u7801 mkdir -p ~/ros2_foxy/src cd ~/ros2_foxy wget https://raw.githubusercontent.com/ros2/ros2/foxy/ros2.repos vcs import src < ros2.repos \u4f7f\u7528rosdep\u5b89\u88c5\u4f9d\u8d56 sudo rosdep init rosdep update rosdep install --from-paths src --ignore-src --rosdistro foxy -y --skip-keys \"console_bridge fastcdr fastrtps rti-connext-dds-5.3.1 urdfdom_headers\"","title":"Install"},{"location":"Autoware.auto/#build","text":"cd ~/ros2_foxy colcon build --symlink-install build\u8fc7\u7a0b\u4e2d\u4f1a\u51fa\u73b0\u4e0b\u8f7d\u9519\u8bef\uff0c\u4f7f\u7528hub.fastgit.org\u4ee3\u66ffgithub.com","title":"Build"},{"location":"Ros/","text":"\u5f15\u7528\u5176\u4ed6\u5305\u7684\uff4d\uff53\uff47 \u9996\u5148cmakelists.txt find_package(catkin REQUIRED COMPONENTS message_generation pcl_ros roscpp rospy std_msgs wjj ) catkin_package( INCLUDE_DIRS include CATKIN_DEPENDS message_runtime pcl_ros roscpp rospy std_msgs wjj ) add_dependencies(teaching ${catkin_EXPORTED_TARGETS} wjj_gencpp) \u518d\u8005package.xml <build_depend>wjj</build_depend> <exec_depend>wjj</exec_depend> \u6700\u540e\u4f7f\u7528 #include <wjj/SaeJ1939.h> sudo rosdep init &rosdep update\u5931\u8d25 \u8be5\u89e3\u51b3\u65b9\u6848\u662f\u9488\u5bf9\u7531\u4e8e\u4ee5\u4e0b\u4e24\u4e2a\u65e0\u6cd5\u6b63\u5e38\u8bbf\u95ee\uff0c\u4f46\u53ef\u4ee5ping\u901a\uff0c\u4e8e\u662f\u4fee\u6539hosts\u6587\u4ef6\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e24\u4e2a\u7f51\u5740\u7684IP\u5730\u5740\u5b9e\u73b0\u8bbf\u95ee\u3002 'sudo gedit /etc/hosts' \u6dfb\u52a0 199.232.28.133 raw.githubusercontent.com 151.101.228.133 raw.github.com \u4fee\u6539\u5b8c\u6210\u540e\uff0c\u5728\u7ec8\u7aef\u6267\u884c sudo rosdep init redep update vscode python ros debug \u9996\u5148 catkin_make -DCMAKE_BUILD_TYPE=DEBUG \u5176\u6b21\u70b9\u51fbdebug\u6309\u94ae\uff0c\u9009\u62e9\u751f\u6210\u65b0\u7684launch\u6587\u4ef6\u3002 \uff44ebug\u65f6\u9047\u5230\u8def\u5f84\u95ee\u9898,\u4f8b\u5982rosmsg\u8def\u5f84\uff0c\u6700\u597d\u5728\u6587\u4ef6\u5c5e\u6027\u91cc\u590d\u5236\u8def\u5f84\uff0c\u4e0d\u7136\u5bb9\u6613\u51fa\u9519 import sys sys.path.append(\"/home/pmjd/Downloads/catkin_ws/devel/lib/python2.7/dist-packages\") Error: package 'teleop_twist_keyboard' not found You need to download the teleop_twist_keyboard from the github to your ~/catkin_ws/src folder. Steps: 1) cd ~/catkin_ws/src 2) git clone https://github.com/ros-teleop/teleop_twist_keyboard Spawn service failed. Exiting. export ROS_MASTER_URI=http://promote-OMEN-by-HP-Laptop-17-cb1xxx:11311/ createQuaternionFromRPY static geometry_msgs::Quaternion createQuaternionFromRPY(double roll, double pitch, double yaw) { geometry_msgs::Quaternion q; double t0 = cos(yaw * 0.5); double t1 = sin(yaw * 0.5); double t2 = cos(roll * 0.5); double t3 = sin(roll * 0.5); double t4 = cos(pitch * 0.5); double t5 = sin(pitch * 0.5); q.w = t0 * t2 * t4 + t1 * t3 * t5; q.x = t0 * t3 * t4 - t1 * t2 * t5; q.y = t0 * t2 * t5 + t1 * t3 * t4; q.z = t1 * t2 * t4 - t0 * t3 * t5; return q; } launch \u542f\u52a8 rviz <launch> <node type=\"rviz\" name=\"rviz\" pkg=\"rviz\" args=\"-d $(find package_name)/rviz/config_file.rviz\" /> </launch> rosdep\u5931\u8d25 \u9996\u5148 sudo rosdep init \u8fd9\u4e00\u6b65\u4f1a\u5728 /etc/ros/rosdep/sources.list.d/ \u76ee\u5f55\u4e0b\u65b0\u5efa 20-default.list # os-specific listings first yaml https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/osx-homebrew.yaml osx # generic yaml https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/base.yaml yaml https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/python.yaml yaml https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/ruby.yaml gbpdistro https://raw.githubusercontent.com/ros/rosdistro/master/releases/fuerte.yaml fuerte # newer distributions (Groovy, Hydro, ...) must not be listed anymore, they are being fetched from the rosdistro index.yaml instead \u6211\u4eec\u9700\u8981\u4f9d\u6b21\u4e0b\u8f7d osx-homebrew.yaml \u7b49\u8fd9\u51e0\u4e2a yaml \u6587\u4ef6, \u4e0b\u8f7d\u5de5\u5177 , \u5b58\u653e\u5728/home/promote/Downloads\u76ee\u5f55\u4e0b \u7136\u540e\u66f4\u6539 20-default.list \u4e3a # os-specific listings first yaml file:///home/promote/Downloads/2021-04-01-14-01-21-master-osx-homebrew.yaml osx # generic yaml file:///home/promote/Downloads/2021-04-01-14-02-25-master-base.yaml yaml file:///home/promote/Downloads/2021-04-01-14-05-35-master-python.yaml yaml file:///home/promote/Downloads/2021-04-01-14-06-50-master-ruby.yaml gbpdistro file:///home/promote/Downloads/2021-04-01-14-07-41-master-fuerte.yaml fuerte # newer distributions (Groovy, Hydro, ...) must not be listed anymore, they are being fetched from the rosdistro index.yaml instead \u8fd8\u9700\u8981\u4f7f\u7528 mate-search-tool \u5728 /usr/lib/ \u76ee\u5f55\u4e0b\u627e\u5230\u5305\u542b DEFAULT_INDEX_URL \u7684 py \u6587\u4ef6\u3002 /usr/lib/python2.7/dist-packages/rosdistro/__init__.py \u627e\u5230\u4ee3\u7801\u884c DEFAULT_INDEX_URL = 'https://raw.githubusercontent.com/ros/rosdistro/master/index-v4.yaml' \u540c\u6837\u6211\u4eec\u4e0b\u8f7d index-v4.yaml \u6587\u4ef6\u81f3 /home/promote/Downloads \uff0c\u628a\u6b64\u884c\u4ee3\u7801\u6539\u4e3a # DEFAULT_INDEX_URL = 'https://raw.githubusercontent.com/ros/rosdistro/master/index-v4.yaml' DEFAULT_INDEX_URL = 'file:/home/promote/Downloads/2021-04-01-14-51-42-rosdistro-index-v4.yaml' \u8fd9\u65f6\u518d\u8fd0\u884c rosdep update \u4f1a\u63d0\u793a No such file or directory: '/home/promote/Downloads/dashing/distribution.yaml' \u6211\u4eec\u9700\u8981\u4e0b\u8f7d https://raw.githubusercontent.com/ros/rosdistro/master/dashing/distribution.yaml \u5728 /home/promote/Downloads \u76ee\u5f55\u4e0b\u65b0\u5efa\u6587\u4ef6\u5939 dashing \uff0c\u5e76\u628a\u4e0b\u8f7d\u7684 yaml \u6587\u4ef6\u653e\u5165 dashing \u6587\u4ef6\u5939\u4e0b\u91cd\u547d\u540d\u4e3a distribution.yaml \u91cd\u590d\u4e0a\u4e00\u6b65\uff0c\u4f9d\u6b21\u4e0b\u8f7d\u5b8c dashing, kinetic, melodic, rolling, noetic, foxy \u7b49 \u518d\u8fd0\u884c rosdep update \u5c31\u6210\u529f\u4e86 ros node \u6253\u5305 \u5b89\u88c5\u4f9d\u8d56 Install bloom : sudo apt-get install python-bloom or (recommended) sudo pip install -U bloom Install fakeroot: sudo apt-get install fakeroot \u51c6\u5907 To make a debian folder structure from the ROS package you must cd into the package to be in the same folder where package.xml file is. \u751f\u6210debian\u5305 bloom-generate rosdebian --os-name ubuntu --os-version trusty --ros-distro indigo","title":"Ros"},{"location":"Ros/#msg","text":"\u9996\u5148cmakelists.txt find_package(catkin REQUIRED COMPONENTS message_generation pcl_ros roscpp rospy std_msgs wjj ) catkin_package( INCLUDE_DIRS include CATKIN_DEPENDS message_runtime pcl_ros roscpp rospy std_msgs wjj ) add_dependencies(teaching ${catkin_EXPORTED_TARGETS} wjj_gencpp) \u518d\u8005package.xml <build_depend>wjj</build_depend> <exec_depend>wjj</exec_depend> \u6700\u540e\u4f7f\u7528 #include <wjj/SaeJ1939.h>","title":"\u5f15\u7528\u5176\u4ed6\u5305\u7684\uff4d\uff53\uff47"},{"location":"Ros/#sudo-rosdep-init-rosdep-update","text":"\u8be5\u89e3\u51b3\u65b9\u6848\u662f\u9488\u5bf9\u7531\u4e8e\u4ee5\u4e0b\u4e24\u4e2a\u65e0\u6cd5\u6b63\u5e38\u8bbf\u95ee\uff0c\u4f46\u53ef\u4ee5ping\u901a\uff0c\u4e8e\u662f\u4fee\u6539hosts\u6587\u4ef6\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e24\u4e2a\u7f51\u5740\u7684IP\u5730\u5740\u5b9e\u73b0\u8bbf\u95ee\u3002 'sudo gedit /etc/hosts' \u6dfb\u52a0 199.232.28.133 raw.githubusercontent.com 151.101.228.133 raw.github.com \u4fee\u6539\u5b8c\u6210\u540e\uff0c\u5728\u7ec8\u7aef\u6267\u884c sudo rosdep init redep update","title":"sudo rosdep init &amp;rosdep update\u5931\u8d25"},{"location":"Ros/#vscode-python-ros-debug","text":"\u9996\u5148 catkin_make -DCMAKE_BUILD_TYPE=DEBUG \u5176\u6b21\u70b9\u51fbdebug\u6309\u94ae\uff0c\u9009\u62e9\u751f\u6210\u65b0\u7684launch\u6587\u4ef6\u3002 \uff44ebug\u65f6\u9047\u5230\u8def\u5f84\u95ee\u9898,\u4f8b\u5982rosmsg\u8def\u5f84\uff0c\u6700\u597d\u5728\u6587\u4ef6\u5c5e\u6027\u91cc\u590d\u5236\u8def\u5f84\uff0c\u4e0d\u7136\u5bb9\u6613\u51fa\u9519 import sys sys.path.append(\"/home/pmjd/Downloads/catkin_ws/devel/lib/python2.7/dist-packages\")","title":"vscode python ros debug"},{"location":"Ros/#error-package-teleop_twist_keyboard-not-found","text":"You need to download the teleop_twist_keyboard from the github to your ~/catkin_ws/src folder. Steps: 1) cd ~/catkin_ws/src 2) git clone https://github.com/ros-teleop/teleop_twist_keyboard","title":"Error: package 'teleop_twist_keyboard' not found"},{"location":"Ros/#spawn-service-failed-exiting","text":"export ROS_MASTER_URI=http://promote-OMEN-by-HP-Laptop-17-cb1xxx:11311/","title":"Spawn service failed. Exiting."},{"location":"Ros/#createquaternionfromrpy","text":"static geometry_msgs::Quaternion createQuaternionFromRPY(double roll, double pitch, double yaw) { geometry_msgs::Quaternion q; double t0 = cos(yaw * 0.5); double t1 = sin(yaw * 0.5); double t2 = cos(roll * 0.5); double t3 = sin(roll * 0.5); double t4 = cos(pitch * 0.5); double t5 = sin(pitch * 0.5); q.w = t0 * t2 * t4 + t1 * t3 * t5; q.x = t0 * t3 * t4 - t1 * t2 * t5; q.y = t0 * t2 * t5 + t1 * t3 * t4; q.z = t1 * t2 * t4 - t0 * t3 * t5; return q; }","title":"createQuaternionFromRPY"},{"location":"Ros/#launch-rviz","text":"<launch> <node type=\"rviz\" name=\"rviz\" pkg=\"rviz\" args=\"-d $(find package_name)/rviz/config_file.rviz\" /> </launch>","title":"launch \u542f\u52a8 rviz"},{"location":"Ros/#rosdep","text":"\u9996\u5148 sudo rosdep init \u8fd9\u4e00\u6b65\u4f1a\u5728 /etc/ros/rosdep/sources.list.d/ \u76ee\u5f55\u4e0b\u65b0\u5efa 20-default.list # os-specific listings first yaml https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/osx-homebrew.yaml osx # generic yaml https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/base.yaml yaml https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/python.yaml yaml https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/ruby.yaml gbpdistro https://raw.githubusercontent.com/ros/rosdistro/master/releases/fuerte.yaml fuerte # newer distributions (Groovy, Hydro, ...) must not be listed anymore, they are being fetched from the rosdistro index.yaml instead \u6211\u4eec\u9700\u8981\u4f9d\u6b21\u4e0b\u8f7d osx-homebrew.yaml \u7b49\u8fd9\u51e0\u4e2a yaml \u6587\u4ef6, \u4e0b\u8f7d\u5de5\u5177 , \u5b58\u653e\u5728/home/promote/Downloads\u76ee\u5f55\u4e0b \u7136\u540e\u66f4\u6539 20-default.list \u4e3a # os-specific listings first yaml file:///home/promote/Downloads/2021-04-01-14-01-21-master-osx-homebrew.yaml osx # generic yaml file:///home/promote/Downloads/2021-04-01-14-02-25-master-base.yaml yaml file:///home/promote/Downloads/2021-04-01-14-05-35-master-python.yaml yaml file:///home/promote/Downloads/2021-04-01-14-06-50-master-ruby.yaml gbpdistro file:///home/promote/Downloads/2021-04-01-14-07-41-master-fuerte.yaml fuerte # newer distributions (Groovy, Hydro, ...) must not be listed anymore, they are being fetched from the rosdistro index.yaml instead \u8fd8\u9700\u8981\u4f7f\u7528 mate-search-tool \u5728 /usr/lib/ \u76ee\u5f55\u4e0b\u627e\u5230\u5305\u542b DEFAULT_INDEX_URL \u7684 py \u6587\u4ef6\u3002 /usr/lib/python2.7/dist-packages/rosdistro/__init__.py \u627e\u5230\u4ee3\u7801\u884c DEFAULT_INDEX_URL = 'https://raw.githubusercontent.com/ros/rosdistro/master/index-v4.yaml' \u540c\u6837\u6211\u4eec\u4e0b\u8f7d index-v4.yaml \u6587\u4ef6\u81f3 /home/promote/Downloads \uff0c\u628a\u6b64\u884c\u4ee3\u7801\u6539\u4e3a # DEFAULT_INDEX_URL = 'https://raw.githubusercontent.com/ros/rosdistro/master/index-v4.yaml' DEFAULT_INDEX_URL = 'file:/home/promote/Downloads/2021-04-01-14-51-42-rosdistro-index-v4.yaml' \u8fd9\u65f6\u518d\u8fd0\u884c rosdep update \u4f1a\u63d0\u793a No such file or directory: '/home/promote/Downloads/dashing/distribution.yaml' \u6211\u4eec\u9700\u8981\u4e0b\u8f7d https://raw.githubusercontent.com/ros/rosdistro/master/dashing/distribution.yaml \u5728 /home/promote/Downloads \u76ee\u5f55\u4e0b\u65b0\u5efa\u6587\u4ef6\u5939 dashing \uff0c\u5e76\u628a\u4e0b\u8f7d\u7684 yaml \u6587\u4ef6\u653e\u5165 dashing \u6587\u4ef6\u5939\u4e0b\u91cd\u547d\u540d\u4e3a distribution.yaml \u91cd\u590d\u4e0a\u4e00\u6b65\uff0c\u4f9d\u6b21\u4e0b\u8f7d\u5b8c dashing, kinetic, melodic, rolling, noetic, foxy \u7b49 \u518d\u8fd0\u884c rosdep update \u5c31\u6210\u529f\u4e86","title":"rosdep\u5931\u8d25"},{"location":"Ros/#ros-node","text":"","title":"ros node \u6253\u5305"},{"location":"Ros/#_1","text":"Install bloom : sudo apt-get install python-bloom or (recommended) sudo pip install -U bloom Install fakeroot: sudo apt-get install fakeroot","title":"\u5b89\u88c5\u4f9d\u8d56"},{"location":"Ros/#_2","text":"To make a debian folder structure from the ROS package you must cd into the package to be in the same folder where package.xml file is.","title":"\u51c6\u5907"},{"location":"Ros/#debian","text":"bloom-generate rosdebian --os-name ubuntu --os-version trusty --ros-distro indigo","title":"\u751f\u6210debian\u5305"},{"location":"Ros%E5%BB%BA%E6%A8%A1/","text":"Ros\u5efa\u6a21 robot_model Create your own urdf file description: in this tutorial you start creating your own urdf robot description file. Create the tree structure\\ in this tutorial we will create the URDF description of the \"robot\" shown in the image below. \\ The robot in the image is a tree structure.Let's start very simple,and create a description of that tree structure,without worrying about the dimensions etc.Fire up your favorite text editor,and create a file called my_robot.urdf: 1 <robot name=\"test_robot\"> 2 <link name=\"link1\" /> 3 <link name=\"link2\" /> 4 <link name=\"link3\" /> 5 <link name=\"link4\" /> 6 7 <joint name=\"joint1\" type=\"continuous\"> 8 <parent link=\"link1\"/> 9 <child link=\"link2\"/> 10 </joint> 11 12 <joint name=\"joint2\" type=\"continuous\"> 13 <parent link=\"link1\"/> 14 <child link=\"link3\"/> 15 </joint> 16 17 <joint name=\"joint3\" type=\"continuous\"> 18 <parent link=\"link3\"/> 19 <child link=\"link4\"/> 20 </joint> 21 </robot> So,just creating the structure is very simple.Now let's see if can get this urdf file parsed.There is a simple command line tool that will parse a urdf file for you, and tell you if the syntax is correct.\\ you might need to install, urdfdom as an upstream,ROS independent package:\\ sudo apt-get install liburdfdom-tools Now run the check command: rosmake urdfdom_model check_urdf my_robot.urdf Add the dimensions So now that we have the basic tree structure.let's add the appropriate dimensions.As you notice in the robot image,the reference frame of each link (in green) is located at the bottom of the link.and is identical to the reference frame of the joint.So,to add dimensions to our tree,all we habe to specify is the offset from a link to the joint(s) of its children.To accomplish this,we will add the field to each of the joints.\\ Let's look at the second joint.Joint2 is offset in the Y-direction from link1,a little offset in the negative X-direction from link1, and it is rotated 90 degrees around the Z-axis.So,we need to add the following element: <origin xyz=\"-2 5 0\" rpy=\"0 0 1.57\"/> If you repeat this for all the elements our URDF will look like this: 1 <robot name=\"test_robot\"> 2 <link name=\"link1\" /> 3 <link name=\"link2\" /> 4 <link name=\"link3\" /> 5 <link name=\"link4\" /> 6 7 8 <joint name=\"joint1\" type=\"continuous\"> 9 <parent link=\"link1\"/> 10 <child link=\"link2\"/> 11 <origin xyz=\"5 3 0\" rpy=\"0 0 0\" /> 12 </joint> 13 14 <joint name=\"joint2\" type=\"continuous\"> 15 <parent link=\"link1\"/> 16 <child link=\"link3\"/> 17 <origin xyz=\"-2 5 0\" rpy=\"0 0 1.57\" /> 18 </joint> 19 20 <joint name=\"joint3\" type=\"continuous\"> 21 <parent link=\"link3\"/> 22 <child link=\"link4\"/> 23 <origin xyz=\"5 0 0\" rpy=\"0 0 -1.57\" /> 24 </joint> 25 </robot> Completing the Kinematics What we didn't specify yet is around which axis the points rotate.Once we add that,we actually have a full kinematic model of this robot!All we need to do is add the element to each joint.The axis specifies the rotational axis in the local frame.\\ So, If you look at joint2, you see it rotates around the positive Y-axis.So, simple add the following xml to the joint element: <axis xyz=\"0 1 0\" /> Similarly,joint1 is rotating around the following axis: <axis xyz=\"-0.707 0.707 0\"/> Note that it is a good idea to normalize the axis. If we add this to all the joints of the robot, our URDF looks like this: 1 <robot name=\"test_robot\"> 2 <link name=\"link1\" /> 3 <link name=\"link2\" /> 4 <link name=\"link3\" /> 5 <link name=\"link4\" /> 6 7 <joint name=\"joint1\" type=\"continuous\"> 8 <parent link=\"link1\"/> 9 <child link=\"link2\"/> 10 <origin xyz=\"5 3 0\" rpy=\"0 0 0\" /> 11 <axis xyz=\"-0.9 0.15 0\" /> 12 </joint> 13 14 <joint name=\"joint2\" type=\"continuous\"> 15 <parent link=\"link1\"/> 16 <child link=\"link3\"/> 17 <origin xyz=\"-2 5 0\" rpy=\"0 0 1.57\" /> 18 <axis xyz=\"-0.707 0.707 0\" /> 19 </joint> 20 21 <joint name=\"joint3\" type=\"continuous\"> 22 <parent link=\"link3\"/> 23 <child link=\"link4\"/> 24 <origin xyz=\"5 0 0\" rpy=\"0 0 -1.57\" /> 25 <axis xyz=\"0.707 -0.707 0\" /> 26 </joint> 27 </robot> Update your file my_robot.urdf and run it through the parser. check_urdf my_robot.urdf That's it.you created your first URDF robot description!Now you can try to visualize the URDF using graphiz: urdf_to_graphiz my_robot.urdf and open the generated file with your favorite pdf viewer: evince test_robot.pdf Parse a urdf file Description: This tutorial teachs you how to use the urdf parser Reading a URDF file This tutorial starts off where the previous one ended.You should still habe your my_robot.urdf file with a description of the robot shown before below.\\ Let's first create a package with a dependency on the urdf parser in our snadbox: cd ~/catkin_ws/src catkin_create_pkg robot_description urdf roscpp rospy tf sensor_msgs std_msgs cd robot_description Now create a /urdf folder to store the urdf file we just created: mkdir urdf cd urdf This follows the convention of always storing your robot's URDF file in a ROS package named MYROBOT_description and within a subfolder named /urdf.Other standard subfolders of your robot's description package include /meshes, /media and /cad,like so:\\ /MYROBOT_description package.xml CMakeLists.txt /urdf /meshes /materials /cad Using the robot state publisher on your own robot Description: This tutorial explains how you can publish the state of your robot to tf, using the robot state publisher. When you are working with a robot that has many relevant frames.it becomes quite a task to publish them all to tf.The robot state publisher is a tool that will do this job for you.\\ \\ The robot state publisher helps you to broadcast the state of your robot to the tf transform library.The robot state publisher internally has a kinematic\u8fd0\u52a8\u5b66 model of the robot; so given the joint positions of the robot,the robot state publisher can compute and broadcast the 3D pose of each link in the robot. You can use the robot state publisher as a standalone ROS node or as a library: 1.Running as a ROS node 1.1 robot_state_pubisher The easiest way to run the robot state publisher is as a node.For normal users,this is the recommanded usage.You need two things to run the robot state publisher: a urdf xml robot description loaded on the Parameter Server. A source that publishes the joint positions as a sensor_msgs/JointState 1.1.1 Subscribed topics joint_states(Sensor_msgs/JointState) joint position information 1.1.2 Parameters robot_description(urdf map)\\ tf_prefix(string) Set the tf prefix for namespace-aware publishing of transform publish_frequency(double) Publish frequency of state publisher,default:50Hz 1.2 Example launch file <launch> <param name = \"robot_description\" textfile = \"$(find mypackage)/urdf/robotmodel.xml\"/> <node pkg = \"robot_state_publisher\" type=\"robot_state_publisher\" name = \"rob_st_pub\"> <remap from=\"robot_state_publisher\" to=\"my_robot_description\"/> <remap from=\"joint_states\" to=\"different_joint_states\"/> </node> </launch> 2.Runing as a library Advanced users can also run the robot state publisher as a library,from within their own c++ code.After you include the header: #include <robot_state_publisher/robot_state_publisher.h> \\ all you need is the constructor which takes in a KDL tree\\ RobotStatePublisher(const KDL::Tree& tree); \\ and now, everytime you want to publish the state of your robot, you call the publishTransforms functions: //Publish moving joints void publishTransfroms(const std::map<std::string, double>& joint_positions, const ros::Time& time); //publish fixed joints void publishFixedTransforms(); The first argument is a map with joint names and joint positions, and the second argument is the time at which the joint positions were recorded. It is okay if the map does not contain all the joint names. It is also okay if the map contains some joints names that are not part of the kinematic model. But note if you don't tell the joint state publisher about some of the joints in your kinematic model, then your tf tree will not be complete. Using urdf with robot_state_publisher Create the URDF File Publishing the State cd %TOP_DIR_YOUR_CATKIN_WS%/src Then fire your favourite editor and paste the following code into the src/state_publisher.cpp file: 1 #include <string> 2 #include <ros/ros.h> 3 #include <sensor_msgs/JointState.h> 4 #include <tf/transform_broadcaster.h> 5 6 int main(int argc, char** argv) { 7 ros::init(argc, argv, \"state_publisher\"); 8 ros::NodeHandle n; 9 ros::Publisher joint_pub = n.advertise<sensor_msgs::JointState>(\"joint_states\", 1); 10 tf::TransformBroadcaster broadcaster; 11 ros::Rate loop_rate(30); 12 13 const double degree = M_PI/180; 14 15 // robot state 16 double tilt = 0, tinc = degree, swivel=0, angle=0, height=0, hinc=0.005; 17 18 // message declarations 19 geometry_msgs::TransformStamped odom_trans; 20 sensor_msgs::JointState joint_state; 21 odom_trans.header.frame_id = \"odom\"; 22 odom_trans.child_frame_id = \"axis\"; 23 24 while (ros::ok()) { 25 //update joint_state 26 joint_state.header.stamp = ros::Time::now(); 27 joint_state.name.resize(3); 28 joint_state.position.resize(3); 29 joint_state.name[0] =\"swivel\"; 30 joint_state.position[0] = swivel; 31 joint_state.name[1] =\"tilt\"; 32 joint_state.position[1] = tilt; 33 joint_state.name[2] =\"periscope\"; 34 joint_state.position[2] = height; 35 36 37 // update transform 38 // (moving in a circle with radius=2) 39 odom_trans.header.stamp = ros::Time::now(); 40 odom_trans.transform.translation.x = cos(angle)*2; 41 odom_trans.transform.translation.y = sin(angle)*2; 42 odom_trans.transform.translation.z = .7; 43 odom_trans.transform.rotation = tf::createQuaternionMsgFromYaw(angle+M_PI/2); 44 45 //send the joint state and transform 46 joint_pub.publish(joint_state); 47 broadcaster.sendTransform(odom_trans); 48 49 // Create new robot state 50 tilt += tinc; 51 if (tilt<-.5 || tilt>0) tinc *= -1; 52 height += hinc; 53 if (height>.2 || height<0) hinc *= -1; 54 swivel += degree; 55 angle += degree/4; 56 57 // This will adjust as needed per iteration 58 loop_rate.sleep(); 59 } 60 61 62 return 0; 63 } Launch File This launch file assumes you are using the package name \"r2d2\" and node name \"state_publisher\" and you have saved this urdf to the \"r2d2\" package. 1 <launch> 2 <param name=\"robot_description\" command=\"cat $(find r2d2)/model.xml\" /> 3 <node name=\"robot_state_publisher\" pkg=\"robot_state_publisher\" type=\"robot_state_publisher\" /> 4 <node name=\"state_publisher\" pkg=\"robot_description\" type=\"state_publisher\" /> 5 </launch> Viewing the Reult First we have to edit the CMakeLists.txt in the package where we saved the above source code.Make sure to add the tf dependency in addition to the other dependencies: find_package(catkin REQUIRED COMPONENTS roscpp rospy std_msgs tf) Notice that roscpp is used to parse the code that we wrote and generate the state_publisher node. We also have to add the following to the end of the CMakelists.txt in order to generate the state_publisher node: include_directories(include ${catkin_INCLUDE_DIRS})\\ add_executable(state_publisher src/state_publisher.cpp)\\ target_link_libraries(state_publisher ${catkin_LIBRARIES}) Now we should go to the directory of the workspace and build it using: catkin_make Now launch the package (assuming that our launch file is named display.launch): roslaunch r2d2 display.launch Run rviz in a new terminal using: rosrun rviz rviz Choose odom as your fixed frame (under Global Options). Then choose \"Add Display\" and add a Robot Model Display and a TF Display\\ Building a Visual Robot Model with URDF from Scratch Description:learn how to build a visual model of a robot that you can view in rviz Before continuing,make sure you have the joint_state_publisher package installed. sudo apt-get install ros-melodic-joint-state-publisher 1.One Shape First, we're just going to explore one simple shape.Here's about as simple as a urdf as you can make. <?xml version=\"1.0\"?> <robot name=\"myfirst\"> <link name=\"base_link\"> <visual> <geometry> <cylinder length=\"0.6\" radius=\"0.2\"/> </geometry> </visual> </link> </robot> To translate the XML into English, this is a robot with the name myfirst, that contains only one link (a.k.a. part), whose visual component is just a cylinder 0.6 meters long with a 0.2 meter radius. This may seem like a lot of enclosing tags for a simple \u201chello world\u201d type example, but it will get more complicated, trust me.\\ Gazebo \u5b89\u88c5 sudo apt-get install ros-melodic-gazebo-ros-pkgs ros-melodic-gazebo-ros-control ros link \u63d0\u793a\u6ca1\u6709\u5305\u542b\u5728gazebo \u53ef\u80fd\u6709\u4e24\u4e2a\u539f\u56e0\uff0c\u4e00\u4e2a\u662f\u5b83\u81ea\u8eab\u6ca1\u6709\u7269\u7406\u5c5e\u6027\uff0c\u53e6\u5916\u4e00\u4e2a\u662f\u5b83\u7684\u8fde\u63a5\u5bf9\u8c61\u6ca1\u6709\u7269\u7406\u5c5e\u6027 Denavit-Hartenberg parameters In mechanical engineering, the Denavit-Hartenberg parameters(also called DH parameters) are the four parameters associated with a particular convention for attaching reference frames to the links of a spatial kinematic chain or robot manipulator. Jacques Denavit and Richard Hartenberg introduced this convention in 1955 in order to standardlize the coordinate frames for spatial linkages. Denavit-Hartenberg convention \\ A commonly used convention for selecting frames of reference in robotics applications is Denavit and Hartenberg(D-H) convention. In this convention, coordinate frames are attached to the joints between two links such that one transformation is associated with the joint,[Z], and the second is associated with the link,[X]. The coordinate transformations along a serial robot consisting of n links from the kinematics equations of the robot. where [T] is the transformation locating the end-link. In order to determine the coordinate transformations [Z] and [X], the joints connecting the links are modeled as either hinged or sliding joints, each of which have a unique line S in space that forms the joint axis and define the relative movement of the two links.A typical serial robot is characterized by a sequence of six lines Si,i=1,......6, one for each joint in the robot.For each sequence of lines Si and Si+1, there is a common normal line Ai,j+1.The system of six joint axes Si","title":"Ros\u5efa\u6a21"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#ros","text":"robot_model","title":"Ros\u5efa\u6a21"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#create-your-own-urdf-file","text":"","title":"Create your own urdf file"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#description-in-this-tutorial-you-start-creating-your-own-urdf-robot-description-file","text":"Create the tree structure\\ in this tutorial we will create the URDF description of the \"robot\" shown in the image below. \\ The robot in the image is a tree structure.Let's start very simple,and create a description of that tree structure,without worrying about the dimensions etc.Fire up your favorite text editor,and create a file called my_robot.urdf: 1 <robot name=\"test_robot\"> 2 <link name=\"link1\" /> 3 <link name=\"link2\" /> 4 <link name=\"link3\" /> 5 <link name=\"link4\" /> 6 7 <joint name=\"joint1\" type=\"continuous\"> 8 <parent link=\"link1\"/> 9 <child link=\"link2\"/> 10 </joint> 11 12 <joint name=\"joint2\" type=\"continuous\"> 13 <parent link=\"link1\"/> 14 <child link=\"link3\"/> 15 </joint> 16 17 <joint name=\"joint3\" type=\"continuous\"> 18 <parent link=\"link3\"/> 19 <child link=\"link4\"/> 20 </joint> 21 </robot> So,just creating the structure is very simple.Now let's see if can get this urdf file parsed.There is a simple command line tool that will parse a urdf file for you, and tell you if the syntax is correct.\\ you might need to install, urdfdom as an upstream,ROS independent package:\\ sudo apt-get install liburdfdom-tools Now run the check command: rosmake urdfdom_model check_urdf my_robot.urdf","title":"description: in this tutorial you start creating your own urdf robot description file."},{"location":"Ros%E5%BB%BA%E6%A8%A1/#add-the-dimensions","text":"So now that we have the basic tree structure.let's add the appropriate dimensions.As you notice in the robot image,the reference frame of each link (in green) is located at the bottom of the link.and is identical to the reference frame of the joint.So,to add dimensions to our tree,all we habe to specify is the offset from a link to the joint(s) of its children.To accomplish this,we will add the field to each of the joints.\\ Let's look at the second joint.Joint2 is offset in the Y-direction from link1,a little offset in the negative X-direction from link1, and it is rotated 90 degrees around the Z-axis.So,we need to add the following element: <origin xyz=\"-2 5 0\" rpy=\"0 0 1.57\"/> If you repeat this for all the elements our URDF will look like this: 1 <robot name=\"test_robot\"> 2 <link name=\"link1\" /> 3 <link name=\"link2\" /> 4 <link name=\"link3\" /> 5 <link name=\"link4\" /> 6 7 8 <joint name=\"joint1\" type=\"continuous\"> 9 <parent link=\"link1\"/> 10 <child link=\"link2\"/> 11 <origin xyz=\"5 3 0\" rpy=\"0 0 0\" /> 12 </joint> 13 14 <joint name=\"joint2\" type=\"continuous\"> 15 <parent link=\"link1\"/> 16 <child link=\"link3\"/> 17 <origin xyz=\"-2 5 0\" rpy=\"0 0 1.57\" /> 18 </joint> 19 20 <joint name=\"joint3\" type=\"continuous\"> 21 <parent link=\"link3\"/> 22 <child link=\"link4\"/> 23 <origin xyz=\"5 0 0\" rpy=\"0 0 -1.57\" /> 24 </joint> 25 </robot>","title":"Add the dimensions"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#completing-the-kinematics","text":"What we didn't specify yet is around which axis the points rotate.Once we add that,we actually have a full kinematic model of this robot!All we need to do is add the element to each joint.The axis specifies the rotational axis in the local frame.\\ So, If you look at joint2, you see it rotates around the positive Y-axis.So, simple add the following xml to the joint element: <axis xyz=\"0 1 0\" /> Similarly,joint1 is rotating around the following axis: <axis xyz=\"-0.707 0.707 0\"/> Note that it is a good idea to normalize the axis. If we add this to all the joints of the robot, our URDF looks like this: 1 <robot name=\"test_robot\"> 2 <link name=\"link1\" /> 3 <link name=\"link2\" /> 4 <link name=\"link3\" /> 5 <link name=\"link4\" /> 6 7 <joint name=\"joint1\" type=\"continuous\"> 8 <parent link=\"link1\"/> 9 <child link=\"link2\"/> 10 <origin xyz=\"5 3 0\" rpy=\"0 0 0\" /> 11 <axis xyz=\"-0.9 0.15 0\" /> 12 </joint> 13 14 <joint name=\"joint2\" type=\"continuous\"> 15 <parent link=\"link1\"/> 16 <child link=\"link3\"/> 17 <origin xyz=\"-2 5 0\" rpy=\"0 0 1.57\" /> 18 <axis xyz=\"-0.707 0.707 0\" /> 19 </joint> 20 21 <joint name=\"joint3\" type=\"continuous\"> 22 <parent link=\"link3\"/> 23 <child link=\"link4\"/> 24 <origin xyz=\"5 0 0\" rpy=\"0 0 -1.57\" /> 25 <axis xyz=\"0.707 -0.707 0\" /> 26 </joint> 27 </robot> Update your file my_robot.urdf and run it through the parser. check_urdf my_robot.urdf That's it.you created your first URDF robot description!Now you can try to visualize the URDF using graphiz: urdf_to_graphiz my_robot.urdf and open the generated file with your favorite pdf viewer: evince test_robot.pdf","title":"Completing the Kinematics"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#parse-a-urdf-file","text":"","title":"Parse a urdf file"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#description-this-tutorial-teachs-you-how-to-use-the-urdf-parser","text":"","title":"Description: This tutorial teachs you how to use the urdf parser"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#reading-a-urdf-file","text":"This tutorial starts off where the previous one ended.You should still habe your my_robot.urdf file with a description of the robot shown before below.\\ Let's first create a package with a dependency on the urdf parser in our snadbox: cd ~/catkin_ws/src catkin_create_pkg robot_description urdf roscpp rospy tf sensor_msgs std_msgs cd robot_description Now create a /urdf folder to store the urdf file we just created: mkdir urdf cd urdf This follows the convention of always storing your robot's URDF file in a ROS package named MYROBOT_description and within a subfolder named /urdf.Other standard subfolders of your robot's description package include /meshes, /media and /cad,like so:\\ /MYROBOT_description package.xml CMakeLists.txt /urdf /meshes /materials /cad","title":"Reading a URDF file"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#using-the-robot-state-publisher-on-your-own-robot","text":"","title":"Using the robot state publisher on your own robot"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#description-this-tutorial-explains-how-you-can-publish-the-state-of-your-robot-to-tf-using-the-robot-state-publisher","text":"When you are working with a robot that has many relevant frames.it becomes quite a task to publish them all to tf.The robot state publisher is a tool that will do this job for you.\\ \\ The robot state publisher helps you to broadcast the state of your robot to the tf transform library.The robot state publisher internally has a kinematic\u8fd0\u52a8\u5b66 model of the robot; so given the joint positions of the robot,the robot state publisher can compute and broadcast the 3D pose of each link in the robot. You can use the robot state publisher as a standalone ROS node or as a library:","title":"Description: This tutorial explains how you can publish the state of your robot to tf, using the robot state publisher."},{"location":"Ros%E5%BB%BA%E6%A8%A1/#1running-as-a-ros-node","text":"","title":"1.Running as a ROS node"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#11-robot_state_pubisher","text":"The easiest way to run the robot state publisher is as a node.For normal users,this is the recommanded usage.You need two things to run the robot state publisher: a urdf xml robot description loaded on the Parameter Server. A source that publishes the joint positions as a sensor_msgs/JointState","title":"1.1 robot_state_pubisher"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#111-subscribed-topics","text":"joint_states(Sensor_msgs/JointState) joint position information","title":"1.1.1 Subscribed topics"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#112-parameters","text":"robot_description(urdf map)\\ tf_prefix(string) Set the tf prefix for namespace-aware publishing of transform publish_frequency(double) Publish frequency of state publisher,default:50Hz","title":"1.1.2 Parameters"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#12-example-launch-file","text":"<launch> <param name = \"robot_description\" textfile = \"$(find mypackage)/urdf/robotmodel.xml\"/> <node pkg = \"robot_state_publisher\" type=\"robot_state_publisher\" name = \"rob_st_pub\"> <remap from=\"robot_state_publisher\" to=\"my_robot_description\"/> <remap from=\"joint_states\" to=\"different_joint_states\"/> </node> </launch>","title":"1.2 Example launch file"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#2runing-as-a-library","text":"Advanced users can also run the robot state publisher as a library,from within their own c++ code.After you include the header: #include <robot_state_publisher/robot_state_publisher.h> \\ all you need is the constructor which takes in a KDL tree\\ RobotStatePublisher(const KDL::Tree& tree); \\ and now, everytime you want to publish the state of your robot, you call the publishTransforms functions: //Publish moving joints void publishTransfroms(const std::map<std::string, double>& joint_positions, const ros::Time& time); //publish fixed joints void publishFixedTransforms(); The first argument is a map with joint names and joint positions, and the second argument is the time at which the joint positions were recorded. It is okay if the map does not contain all the joint names. It is also okay if the map contains some joints names that are not part of the kinematic model. But note if you don't tell the joint state publisher about some of the joints in your kinematic model, then your tf tree will not be complete.","title":"2.Runing as a library"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#using-urdf-with-robot_state_publisher","text":"","title":"Using urdf with robot_state_publisher"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#create-the-urdf-file","text":"","title":"Create the URDF File"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#publishing-the-state","text":"cd %TOP_DIR_YOUR_CATKIN_WS%/src Then fire your favourite editor and paste the following code into the src/state_publisher.cpp file: 1 #include <string> 2 #include <ros/ros.h> 3 #include <sensor_msgs/JointState.h> 4 #include <tf/transform_broadcaster.h> 5 6 int main(int argc, char** argv) { 7 ros::init(argc, argv, \"state_publisher\"); 8 ros::NodeHandle n; 9 ros::Publisher joint_pub = n.advertise<sensor_msgs::JointState>(\"joint_states\", 1); 10 tf::TransformBroadcaster broadcaster; 11 ros::Rate loop_rate(30); 12 13 const double degree = M_PI/180; 14 15 // robot state 16 double tilt = 0, tinc = degree, swivel=0, angle=0, height=0, hinc=0.005; 17 18 // message declarations 19 geometry_msgs::TransformStamped odom_trans; 20 sensor_msgs::JointState joint_state; 21 odom_trans.header.frame_id = \"odom\"; 22 odom_trans.child_frame_id = \"axis\"; 23 24 while (ros::ok()) { 25 //update joint_state 26 joint_state.header.stamp = ros::Time::now(); 27 joint_state.name.resize(3); 28 joint_state.position.resize(3); 29 joint_state.name[0] =\"swivel\"; 30 joint_state.position[0] = swivel; 31 joint_state.name[1] =\"tilt\"; 32 joint_state.position[1] = tilt; 33 joint_state.name[2] =\"periscope\"; 34 joint_state.position[2] = height; 35 36 37 // update transform 38 // (moving in a circle with radius=2) 39 odom_trans.header.stamp = ros::Time::now(); 40 odom_trans.transform.translation.x = cos(angle)*2; 41 odom_trans.transform.translation.y = sin(angle)*2; 42 odom_trans.transform.translation.z = .7; 43 odom_trans.transform.rotation = tf::createQuaternionMsgFromYaw(angle+M_PI/2); 44 45 //send the joint state and transform 46 joint_pub.publish(joint_state); 47 broadcaster.sendTransform(odom_trans); 48 49 // Create new robot state 50 tilt += tinc; 51 if (tilt<-.5 || tilt>0) tinc *= -1; 52 height += hinc; 53 if (height>.2 || height<0) hinc *= -1; 54 swivel += degree; 55 angle += degree/4; 56 57 // This will adjust as needed per iteration 58 loop_rate.sleep(); 59 } 60 61 62 return 0; 63 }","title":"Publishing the State"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#launch-file","text":"This launch file assumes you are using the package name \"r2d2\" and node name \"state_publisher\" and you have saved this urdf to the \"r2d2\" package. 1 <launch> 2 <param name=\"robot_description\" command=\"cat $(find r2d2)/model.xml\" /> 3 <node name=\"robot_state_publisher\" pkg=\"robot_state_publisher\" type=\"robot_state_publisher\" /> 4 <node name=\"state_publisher\" pkg=\"robot_description\" type=\"state_publisher\" /> 5 </launch>","title":"Launch File"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#viewing-the-reult","text":"First we have to edit the CMakeLists.txt in the package where we saved the above source code.Make sure to add the tf dependency in addition to the other dependencies: find_package(catkin REQUIRED COMPONENTS roscpp rospy std_msgs tf) Notice that roscpp is used to parse the code that we wrote and generate the state_publisher node. We also have to add the following to the end of the CMakelists.txt in order to generate the state_publisher node: include_directories(include ${catkin_INCLUDE_DIRS})\\ add_executable(state_publisher src/state_publisher.cpp)\\ target_link_libraries(state_publisher ${catkin_LIBRARIES}) Now we should go to the directory of the workspace and build it using: catkin_make Now launch the package (assuming that our launch file is named display.launch): roslaunch r2d2 display.launch Run rviz in a new terminal using: rosrun rviz rviz Choose odom as your fixed frame (under Global Options). Then choose \"Add Display\" and add a Robot Model Display and a TF Display\\","title":"Viewing the Reult"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#building-a-visual-robot-model-with-urdf-from-scratch","text":"","title":"Building a Visual Robot Model with URDF from Scratch"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#descriptionlearn-how-to-build-a-visual-model-of-a-robot-that-you-can-view-in-rviz","text":"Before continuing,make sure you have the joint_state_publisher package installed. sudo apt-get install ros-melodic-joint-state-publisher","title":"Description:learn how to build a visual model of a robot that you can view in rviz"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#1one-shape","text":"First, we're just going to explore one simple shape.Here's about as simple as a urdf as you can make. <?xml version=\"1.0\"?> <robot name=\"myfirst\"> <link name=\"base_link\"> <visual> <geometry> <cylinder length=\"0.6\" radius=\"0.2\"/> </geometry> </visual> </link> </robot> To translate the XML into English, this is a robot with the name myfirst, that contains only one link (a.k.a. part), whose visual component is just a cylinder 0.6 meters long with a 0.2 meter radius. This may seem like a lot of enclosing tags for a simple \u201chello world\u201d type example, but it will get more complicated, trust me.\\","title":"1.One Shape"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#gazebo","text":"sudo apt-get install ros-melodic-gazebo-ros-pkgs ros-melodic-gazebo-ros-control","title":"Gazebo \u5b89\u88c5"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#ros-link-gazebo","text":"\u53ef\u80fd\u6709\u4e24\u4e2a\u539f\u56e0\uff0c\u4e00\u4e2a\u662f\u5b83\u81ea\u8eab\u6ca1\u6709\u7269\u7406\u5c5e\u6027\uff0c\u53e6\u5916\u4e00\u4e2a\u662f\u5b83\u7684\u8fde\u63a5\u5bf9\u8c61\u6ca1\u6709\u7269\u7406\u5c5e\u6027","title":"ros link \u63d0\u793a\u6ca1\u6709\u5305\u542b\u5728gazebo"},{"location":"Ros%E5%BB%BA%E6%A8%A1/#denavit-hartenberg-parameters","text":"In mechanical engineering, the Denavit-Hartenberg parameters(also called DH parameters) are the four parameters associated with a particular convention for attaching reference frames to the links of a spatial kinematic chain or robot manipulator. Jacques Denavit and Richard Hartenberg introduced this convention in 1955 in order to standardlize the coordinate frames for spatial linkages. Denavit-Hartenberg convention \\ A commonly used convention for selecting frames of reference in robotics applications is Denavit and Hartenberg(D-H) convention. In this convention, coordinate frames are attached to the joints between two links such that one transformation is associated with the joint,[Z], and the second is associated with the link,[X]. The coordinate transformations along a serial robot consisting of n links from the kinematics equations of the robot. where [T] is the transformation locating the end-link. In order to determine the coordinate transformations [Z] and [X], the joints connecting the links are modeled as either hinged or sliding joints, each of which have a unique line S in space that forms the joint axis and define the relative movement of the two links.A typical serial robot is characterized by a sequence of six lines Si,i=1,......6, one for each joint in the robot.For each sequence of lines Si and Si+1, there is a common normal line Ai,j+1.The system of six joint axes Si","title":"Denavit-Hartenberg parameters"},{"location":"c%2B%2B/","text":"string \u524d\u8865\u96f6 include include include using namespace std; void main() { int num = 1024; stringstream ss; ss << setw(5) << setfill('0') << num ; string str; ss >> str; //\u5c06\u5b57\u7b26\u6d41\u4f20\u7ed9 str //str = ss.str(); //\u4e5f\u53ef\u4ee5 cout << str; } \u5f97\u5230\u5f53\u524d\u65f6\u95f4 #include <iostream> #include <ctime> int main () { time_t rawtime; struct tm * timeinfo; char buffer[80]; time (&rawtime); timeinfo = localtime(&rawtime); strftime(buffer,sizeof(buffer),\"%d-%m-%Y %H:%M:%S\",timeinfo); std::string str(buffer); std::cout << str; return 0; } \u5199\u5165\u6587\u4ef6append #include <fstream> int main() { std::ofstream outfile; outfile.open(\"test.txt\", std::ios_base::app); // append instead of overwrite outfile << \"Data\"; return 0; } float to string to_string(float) string to float stof(str) \u8bfb\u53d6\u6587\u6863\u6bcf\u4e00\u884c\u5185\u5bb9 fstream newfile; newfile.open(learning_file,ios::in); if (newfile.is_open()) { string tp; while(getline(newfile, tp)) { cout << tp << \"\\n\"; } newfile.close(); } \u6587\u4ef6\u6e05\u7a7a fstream newfile; newfile.open(learning_file,ios::out); newfile.close(); \u5b57\u7b26\u4e32\uff53\uff50\uff4c\uff49\uff54 #include <boost/algorithm/string.hpp> std::string text = \"Let me split this into words\"; std::vector<std::string> results; boost::split(results, text, [](char c){return c == ' ';}); CMakeLists.txt \u5f15\u5165\uff42\uff4f\uff4f\uff53\uff54 find_package(Boost COMPONENTS program_options filesystem REQUIRED ) include_directories(${Boost_INCLUDE_DIRS}) link_directories(${Boost_LIBRARY_DIRS}) target_link_libraries(learning ${catkin_LIBRARIES} ${Boost_LIBRARIES}) explicit #include <iostream> using namespace std; class Complex { private: double real; double imag; public: // Default constructor Complex(double r = 0.0, double i = 0.0) : real(r), imag(i) {} // A method to compare two Complex numbers bool operator == (Complex rhs) { return (real == rhs.real && imag == rhs.imag)? true : false; } }; int main() { // a Complex object Complex com1(3.0, 0.0); if (com1 == 3.0) cout << \"Same\"; else cout << \"Not Same\"; return 0; } \u7a0b\u5e8f\u8f93\u51fa\u3000 Same \u5728C ++\u4e2d\uff0c\u5982\u679c\u7c7b\u5177\u6709\u53ef\u4ee5\u7528\u5355\u4e2a\u53c2\u6570\u8c03\u7528\u7684\u6784\u9020\u51fd\u6570\uff0c\u5219\u6b64\u6784\u9020\u51fd\u6570\u5c06\u6210\u4e3a\u8f6c\u6362\u6784\u9020\u51fd\u6570\uff0c\u56e0\u4e3a\u8fd9\u6837\u7684\u6784\u9020\u51fd\u6570\u5141\u8bb8\u5c06\u5355\u4e2a\u53c2\u6570\u8f6c\u6362\u4e3a\u6b63\u5728\u6784\u9020\u7684\u7c7b\u3002 \u6211\u4eec\u53ef\u4ee5\u907f\u514d\u8fd9\u79cd\u9690\u5f0f\u8f6c\u6362\uff0c\u56e0\u4e3a\u5b83\u4eec\u53ef\u80fd\u5bfc\u81f4\u610f\u5916\u7684\u7ed3\u679c\u3002\u6211\u4eec\u53ef\u4ee5\u5728explicit\u5173\u952e\u5b57\u7684\u5e2e\u52a9\u4e0b\u4f7f\u6784\u9020\u51fd\u6570\u663e\u5f0f\u5316\u3002\\ using namespace std; class Complex { private: double real; double imag; public: // Default constructor explicit Complex(double r = 0.0, double i = 0.0) : real(r), imag(i) {} // A method to compare two Complex numbers bool operator== (Complex rhs) { return (real == rhs.real && imag == rhs.imag)? true : false; } }; int main() { // a Complex object Complex com1(3.0, 0.0); if (com1 == (Complex)3.0) cout << \"Same\"; else cout << \"Not Same\"; return 0; } Use Eigen in cmake program ```bash find_package(Eigen3 3.3 REQUIRED) ... include_directories(${EIGEN3_INCLUDE_DIR}) ... target_link_libraries(example Eigen3::Eigen)","title":"C++"},{"location":"c%2B%2B/#string","text":"","title":"string \u524d\u8865\u96f6"},{"location":"c%2B%2B/#include","text":"","title":"include "},{"location":"c%2B%2B/#include_1","text":"","title":"include "},{"location":"c%2B%2B/#include_2","text":"using namespace std; void main() { int num = 1024; stringstream ss; ss << setw(5) << setfill('0') << num ; string str; ss >> str; //\u5c06\u5b57\u7b26\u6d41\u4f20\u7ed9 str //str = ss.str(); //\u4e5f\u53ef\u4ee5 cout << str; }","title":"include "},{"location":"c%2B%2B/#_1","text":"#include <iostream> #include <ctime> int main () { time_t rawtime; struct tm * timeinfo; char buffer[80]; time (&rawtime); timeinfo = localtime(&rawtime); strftime(buffer,sizeof(buffer),\"%d-%m-%Y %H:%M:%S\",timeinfo); std::string str(buffer); std::cout << str; return 0; }","title":"\u5f97\u5230\u5f53\u524d\u65f6\u95f4"},{"location":"c%2B%2B/#append","text":"#include <fstream> int main() { std::ofstream outfile; outfile.open(\"test.txt\", std::ios_base::app); // append instead of overwrite outfile << \"Data\"; return 0; }","title":"\u5199\u5165\u6587\u4ef6append"},{"location":"c%2B%2B/#float-to-string","text":"to_string(float)","title":"float to string"},{"location":"c%2B%2B/#string-to-float","text":"stof(str)","title":"string to float"},{"location":"c%2B%2B/#_2","text":"fstream newfile; newfile.open(learning_file,ios::in); if (newfile.is_open()) { string tp; while(getline(newfile, tp)) { cout << tp << \"\\n\"; } newfile.close(); }","title":"\u8bfb\u53d6\u6587\u6863\u6bcf\u4e00\u884c\u5185\u5bb9"},{"location":"c%2B%2B/#_3","text":"fstream newfile; newfile.open(learning_file,ios::out); newfile.close();","title":"\u6587\u4ef6\u6e05\u7a7a"},{"location":"c%2B%2B/#split","text":"#include <boost/algorithm/string.hpp> std::string text = \"Let me split this into words\"; std::vector<std::string> results; boost::split(results, text, [](char c){return c == ' ';});","title":"\u5b57\u7b26\u4e32\uff53\uff50\uff4c\uff49\uff54"},{"location":"c%2B%2B/#cmakeliststxt-boost","text":"find_package(Boost COMPONENTS program_options filesystem REQUIRED ) include_directories(${Boost_INCLUDE_DIRS}) link_directories(${Boost_LIBRARY_DIRS}) target_link_libraries(learning ${catkin_LIBRARIES} ${Boost_LIBRARIES})","title":"CMakeLists.txt \u5f15\u5165\uff42\uff4f\uff4f\uff53\uff54"},{"location":"c%2B%2B/#explicit","text":"#include <iostream> using namespace std; class Complex { private: double real; double imag; public: // Default constructor Complex(double r = 0.0, double i = 0.0) : real(r), imag(i) {} // A method to compare two Complex numbers bool operator == (Complex rhs) { return (real == rhs.real && imag == rhs.imag)? true : false; } }; int main() { // a Complex object Complex com1(3.0, 0.0); if (com1 == 3.0) cout << \"Same\"; else cout << \"Not Same\"; return 0; } \u7a0b\u5e8f\u8f93\u51fa\u3000 Same \u5728C ++\u4e2d\uff0c\u5982\u679c\u7c7b\u5177\u6709\u53ef\u4ee5\u7528\u5355\u4e2a\u53c2\u6570\u8c03\u7528\u7684\u6784\u9020\u51fd\u6570\uff0c\u5219\u6b64\u6784\u9020\u51fd\u6570\u5c06\u6210\u4e3a\u8f6c\u6362\u6784\u9020\u51fd\u6570\uff0c\u56e0\u4e3a\u8fd9\u6837\u7684\u6784\u9020\u51fd\u6570\u5141\u8bb8\u5c06\u5355\u4e2a\u53c2\u6570\u8f6c\u6362\u4e3a\u6b63\u5728\u6784\u9020\u7684\u7c7b\u3002 \u6211\u4eec\u53ef\u4ee5\u907f\u514d\u8fd9\u79cd\u9690\u5f0f\u8f6c\u6362\uff0c\u56e0\u4e3a\u5b83\u4eec\u53ef\u80fd\u5bfc\u81f4\u610f\u5916\u7684\u7ed3\u679c\u3002\u6211\u4eec\u53ef\u4ee5\u5728explicit\u5173\u952e\u5b57\u7684\u5e2e\u52a9\u4e0b\u4f7f\u6784\u9020\u51fd\u6570\u663e\u5f0f\u5316\u3002\\ using namespace std; class Complex { private: double real; double imag; public: // Default constructor explicit Complex(double r = 0.0, double i = 0.0) : real(r), imag(i) {} // A method to compare two Complex numbers bool operator== (Complex rhs) { return (real == rhs.real && imag == rhs.imag)? true : false; } }; int main() { // a Complex object Complex com1(3.0, 0.0); if (com1 == (Complex)3.0) cout << \"Same\"; else cout << \"Not Same\"; return 0; }","title":"explicit"},{"location":"c%2B%2B/#use-eigen-in-cmake-program","text":"```bash find_package(Eigen3 3.3 REQUIRED) ... include_directories(${EIGEN3_INCLUDE_DIR}) ... target_link_libraries(example Eigen3::Eigen)","title":"Use Eigen in cmake program"},{"location":"caffe/","text":"caffe paython \u73af\u5883\u914d\u7f6e export PYTHONPATH=$HOME/Downloads/caffe/python:$PYTHONPATH caffe \u5b89\u88c5 git clone https://github.com/BVLC/caffe.git \u5b89\u88c5\u4f9d\u8d56 sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler sudo apt-get install --no-install-recommends libboost-all-dev sudo apt-get install libopenblas-dev sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev Makefile.config ## Refer to http://caffe.berkeleyvision.org/installation.html # Contributions simplifying and improving our build system are welcome! # cuDNN acceleration switch (uncomment to build with cuDNN). # USE_CUDNN := 1 # CPU-only switch (uncomment to build without GPU support). # CPU_ONLY := 1 # uncomment to disable IO dependencies and corresponding data layers # USE_OPENCV := 0 # USE_LEVELDB := 0 # USE_LMDB := 0 # uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary) # You should not set this flag if you will be reading LMDBs with any # possibility of simultaneous read and write # ALLOW_LMDB_NOLOCK := 1 # Uncomment if you're using OpenCV 3 OPENCV_VERSION := 3 # To customize your choice of compiler, uncomment and set the following. # N.B. the default for Linux is g++ and the default for OSX is clang++ # CUSTOM_CXX := g++ # CUDA directory contains bin/ and lib/ directories that we need. CUDA_DIR := /usr/local/cuda # On Ubuntu 14.04, if cuda tools are installed via # \"sudo apt-get install nvidia-cuda-toolkit\" then use this instead: # CUDA_DIR := /usr # CUDA architecture setting: going with all of them. # For CUDA < 6.0, comment the lines after *_35 for compatibility. CUDA_ARCH := -gencode arch=compute_30,code=sm_30 \\ -gencode arch=compute_35,code=sm_35 \\ -gencode arch=compute_50,code=sm_50 \\ -gencode arch=compute_52,code=sm_52 \\ -gencode arch=compute_61,code=sm_61 \\ -gencode arch=compute_61,code=compute_61 # -gencode arch=compute_20,code=sm_20 \\ # -gencode arch=compute_20,code=sm_21 \\ # BLAS choice: # atlas for ATLAS (default) # mkl for MKL # open for OpenBlas # BLAS := atlas BLAS := open # Custom (MKL/ATLAS/OpenBLAS) include and lib directories. # Leave commented to accept the defaults for your choice of BLAS # (which should work)! # BLAS_INCLUDE := /path/to/your/blas # BLAS_LIB := /path/to/your/blas # Homebrew puts openblas in a directory that is not on the standard search path # BLAS_INCLUDE := $(shell brew --prefix openblas)/include # BLAS_LIB := $(shell brew --prefix openblas)/lib # This is required only if you will compile the matlab interface. # MATLAB directory should contain the mex binary in /bin. # MATLAB_DIR := /usr/local # MATLAB_DIR := /Applications/MATLAB_R2012b.app # NOTE: this is required only if you will compile the python interface. # We need to be able to find Python.h and numpy/arrayobject.h. PYTHON_INCLUDE := /usr/include/python2.7 \\ /usr/lib/python2.7/dist-packages/numpy/core/include # Anaconda Python distribution is quite popular. Include path: # Verify anaconda location, sometimes it's in root. # ANACONDA_HOME := $(HOME)/anaconda2 # PYTHON_INCLUDE := $(ANACONDA_HOME)/include \\ $(ANACONDA_HOME)/include/python2.7 \\ $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include \\ # Uncomment to use Python 3 (default is Python 2) # PYTHON_LIBRARIES := boost_python3 python3.5m # PYTHON_INCLUDE := /usr/include/python3.5m \\ # /usr/lib/python3.5/dist-packages/numpy/core/include # We need to be able to find libpythonX.X.so or .dylib. PYTHON_LIB := /usr/lib # PYTHON_LIB := $(ANACONDA_HOME)/lib # Homebrew installs numpy in a non standard path (keg only) # PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include # PYTHON_LIB += $(shell brew --prefix numpy)/lib # Uncomment to support layers written in Python (will link against Python libs) # WITH_PYTHON_LAYER := 1 # Whatever else you find you need goes here. INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial/ LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu/hdf5/serial # If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies # INCLUDE_DIRS += $(shell brew --prefix)/include # LIBRARY_DIRS += $(shell brew --prefix)/lib # Uncomment to use `pkg-config` to specify OpenCV library paths. # (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.) # USE_PKG_CONFIG := 1 # N.B. both build and distribute dirs are cleared on `make clean` BUILD_DIR := build DISTRIBUTE_DIR := distribute # Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171 # DEBUG := 1 # The ID of the GPU that 'make runtest' will use to run unit tests. TEST_GPUID := 0 # enable pretty build (comment to see full commands) Q ?= @ \u5b89\u88c5 sudo make -j8\\ sudo make distribute \u5e94\u7528 CMakeLists.txt set(CAFFE_PATH \"$ENV{HOME}/Disk/caffe/distribute\") if (EXISTS \"${CAFFE_PATH}\") include_directories( include ${catkin_INCLUDE_DIRS} ${OpenCV_INCLUDE_DIRS} ${CAFFE_PATH}/include ) target_link_libraries(lidar_cnn_seg_detect ${catkin_LIBRARIES} ${OpenCV_LIBRARIES} ${CUDA_LIBRARIES} ${CAFFE_PATH}/lib/libcaffe.so glog ) caffe python layer layer tuozhan The models are defined in plaintext protocol buffer schema (prototxt) while the learned models are serialized as binary protocol buffer (binaryproto) .caffemodel files. caffe \u8bad\u7ec3\u9700\u8981\u6570\u636e\u96c6\uff0c\u7f51\u7edc.prototxt \u548c solver.prototxt caffe \u7f51\u7edc\u53ef\u89c6\u5316 python draw_net.py --rankdir TB test.prototxt test.png","title":"Caffe"},{"location":"caffe/#caffe-paython","text":"export PYTHONPATH=$HOME/Downloads/caffe/python:$PYTHONPATH","title":"caffe paython \u73af\u5883\u914d\u7f6e"},{"location":"caffe/#caffe","text":"git clone https://github.com/BVLC/caffe.git","title":"caffe \u5b89\u88c5"},{"location":"caffe/#_1","text":"sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler sudo apt-get install --no-install-recommends libboost-all-dev sudo apt-get install libopenblas-dev sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev","title":"\u5b89\u88c5\u4f9d\u8d56"},{"location":"caffe/#makefileconfig","text":"## Refer to http://caffe.berkeleyvision.org/installation.html # Contributions simplifying and improving our build system are welcome! # cuDNN acceleration switch (uncomment to build with cuDNN). # USE_CUDNN := 1 # CPU-only switch (uncomment to build without GPU support). # CPU_ONLY := 1 # uncomment to disable IO dependencies and corresponding data layers # USE_OPENCV := 0 # USE_LEVELDB := 0 # USE_LMDB := 0 # uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary) # You should not set this flag if you will be reading LMDBs with any # possibility of simultaneous read and write # ALLOW_LMDB_NOLOCK := 1 # Uncomment if you're using OpenCV 3 OPENCV_VERSION := 3 # To customize your choice of compiler, uncomment and set the following. # N.B. the default for Linux is g++ and the default for OSX is clang++ # CUSTOM_CXX := g++ # CUDA directory contains bin/ and lib/ directories that we need. CUDA_DIR := /usr/local/cuda # On Ubuntu 14.04, if cuda tools are installed via # \"sudo apt-get install nvidia-cuda-toolkit\" then use this instead: # CUDA_DIR := /usr # CUDA architecture setting: going with all of them. # For CUDA < 6.0, comment the lines after *_35 for compatibility. CUDA_ARCH := -gencode arch=compute_30,code=sm_30 \\ -gencode arch=compute_35,code=sm_35 \\ -gencode arch=compute_50,code=sm_50 \\ -gencode arch=compute_52,code=sm_52 \\ -gencode arch=compute_61,code=sm_61 \\ -gencode arch=compute_61,code=compute_61 # -gencode arch=compute_20,code=sm_20 \\ # -gencode arch=compute_20,code=sm_21 \\ # BLAS choice: # atlas for ATLAS (default) # mkl for MKL # open for OpenBlas # BLAS := atlas BLAS := open # Custom (MKL/ATLAS/OpenBLAS) include and lib directories. # Leave commented to accept the defaults for your choice of BLAS # (which should work)! # BLAS_INCLUDE := /path/to/your/blas # BLAS_LIB := /path/to/your/blas # Homebrew puts openblas in a directory that is not on the standard search path # BLAS_INCLUDE := $(shell brew --prefix openblas)/include # BLAS_LIB := $(shell brew --prefix openblas)/lib # This is required only if you will compile the matlab interface. # MATLAB directory should contain the mex binary in /bin. # MATLAB_DIR := /usr/local # MATLAB_DIR := /Applications/MATLAB_R2012b.app # NOTE: this is required only if you will compile the python interface. # We need to be able to find Python.h and numpy/arrayobject.h. PYTHON_INCLUDE := /usr/include/python2.7 \\ /usr/lib/python2.7/dist-packages/numpy/core/include # Anaconda Python distribution is quite popular. Include path: # Verify anaconda location, sometimes it's in root. # ANACONDA_HOME := $(HOME)/anaconda2 # PYTHON_INCLUDE := $(ANACONDA_HOME)/include \\ $(ANACONDA_HOME)/include/python2.7 \\ $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include \\ # Uncomment to use Python 3 (default is Python 2) # PYTHON_LIBRARIES := boost_python3 python3.5m # PYTHON_INCLUDE := /usr/include/python3.5m \\ # /usr/lib/python3.5/dist-packages/numpy/core/include # We need to be able to find libpythonX.X.so or .dylib. PYTHON_LIB := /usr/lib # PYTHON_LIB := $(ANACONDA_HOME)/lib # Homebrew installs numpy in a non standard path (keg only) # PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include # PYTHON_LIB += $(shell brew --prefix numpy)/lib # Uncomment to support layers written in Python (will link against Python libs) # WITH_PYTHON_LAYER := 1 # Whatever else you find you need goes here. INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial/ LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu/hdf5/serial # If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies # INCLUDE_DIRS += $(shell brew --prefix)/include # LIBRARY_DIRS += $(shell brew --prefix)/lib # Uncomment to use `pkg-config` to specify OpenCV library paths. # (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.) # USE_PKG_CONFIG := 1 # N.B. both build and distribute dirs are cleared on `make clean` BUILD_DIR := build DISTRIBUTE_DIR := distribute # Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171 # DEBUG := 1 # The ID of the GPU that 'make runtest' will use to run unit tests. TEST_GPUID := 0 # enable pretty build (comment to see full commands) Q ?= @","title":"Makefile.config"},{"location":"caffe/#_2","text":"sudo make -j8\\ sudo make distribute","title":"\u5b89\u88c5"},{"location":"caffe/#_3","text":"CMakeLists.txt set(CAFFE_PATH \"$ENV{HOME}/Disk/caffe/distribute\") if (EXISTS \"${CAFFE_PATH}\") include_directories( include ${catkin_INCLUDE_DIRS} ${OpenCV_INCLUDE_DIRS} ${CAFFE_PATH}/include ) target_link_libraries(lidar_cnn_seg_detect ${catkin_LIBRARIES} ${OpenCV_LIBRARIES} ${CUDA_LIBRARIES} ${CAFFE_PATH}/lib/libcaffe.so glog )","title":"\u5e94\u7528"},{"location":"caffe/#caffe-python-layer","text":"layer tuozhan","title":"caffe python layer"},{"location":"caffe/#the-models-are-defined-in-plaintext-protocol-buffer-schema-prototxt-while-the-learned-models-are-serialized-as-binary-protocol-buffer-binaryproto-caffemodel-files","text":"caffe \u8bad\u7ec3\u9700\u8981\u6570\u636e\u96c6\uff0c\u7f51\u7edc.prototxt \u548c solver.prototxt","title":"The models are defined in plaintext protocol buffer schema (prototxt) while the learned models are serialized as binary protocol buffer (binaryproto) .caffemodel files."},{"location":"caffe/#caffe_1","text":"python draw_net.py --rankdir TB test.prototxt test.png","title":"caffe \u7f51\u7edc\u53ef\u89c6\u5316"},{"location":"deeplearning/","text":"LeNet use convolution to extract spatial features. subsample using spatial average of maps. non-linearity in the form of tanh or sigmoids. multi-layer neural network(MLP) as final classifier sparse connection matrix between layers to avoid large computational cost AlexNet use of rectified linear units(ReLU) as non-linearities. use of dropout technique to selectively ignore single neurons during training , a way to avoid overfitting of the model. overlapping max pooling, avoiding the averaging effects of average pooling. using GPUs to reduce training time xception Computing the mean and std of dataset import tensorflow as tf from PIL import ImageStat class Stats(ImageStat.Stat): def __add__(self, other): return Stats(list(map(add, self.h, other.h))) loader = DataLoader(dataset, batch_size=10, num_workers=5) statistics = None for data in loader: for b in range(data.shape[0]): if statistics is None: statistics = Stats(tf.to_pil_image(data[b])) else: statistics += Stats(tf.to_pil_image(data[b])) print(f'mean:{statistics.mean}, std:{statistics.stddev}') pyx to so python setup.py build_ext --inplace \u5f7b\u5e95\u5220\u9664cuda sudo apt-get purge nvidia* sudo apt-get autoremove sudo apt-get autoclean sudo rm -rf /usr/local/cuda* Steps to Install PyTorch With CUDA 10.0 conda install pytorch torchvision cudatoolkit=10.0 -c pytorch \\ pip install torchsummary conda install matplotlib conda install -c conda-forge matplotlib conda install tensorflow object_detection api conda install -c conda-forge tf_object_detection ModuleNotFoundError: No module named 'deployment' from slim.deployment import model_deploy ModuleNotFoundError: No module named 'slim' conda install -c conda-forge tf-slim ModuleNotFoundError: No module named 'nets' change frome nets to from slim.nets creating trainval.txt import os import random import sys if len(sys.argv) < 2: print(\"no directory specified, please input target directory\") exit() root_path = sys.argv[1] xmlfilepath = root_path + '/Annotations' txtsavepath = root_path + '/ImageSets/Main' if not os.path.exists(root_path): print(\"cannot find such directory: \" + root_path) exit() if not os.path.exists(txtsavepath): os.makedirs(txtsavepath) trainval_percent = 0.9 train_percent = 0.8 total_xml = os.listdir(xmlfilepath) num = len(total_xml) list = range(num) tv = int(num * trainval_percent) tr = int(tv * train_percent) trainval = random.sample(list, tv) train = random.sample(trainval, tr) print(\"train and val size:\", tv) print(\"train size:\", tr) ftrainval = open(txtsavepath + '/trainval.txt', 'w') ftest = open(txtsavepath + '/test.txt', 'w') ftrain = open(txtsavepath + '/train.txt', 'w') fval = open(txtsavepath + '/val.txt', 'w') for i in list: name = total_xml[i][:-4] + '\\n' if i in trainval: ftrainval.write(name) if i in train: ftrain.write(name) else: fval.write(name) else: ftest.write(name) ftrainval.close() ftrain.close() fval.close() ftest.close() creating TFR datasets import hashlib import io import logging import os import random import re from lxml import etree import PIL.Image import tensorflow as tf from object_detection.utils import dataset_util from object_detection.utils import label_map_util flags = tf.app.flags flags.DEFINE_string('data_dir', '', 'Root directory to raw pet dataset.') flags.DEFINE_string('output_dir', '', 'Path to directory to output TFRecords.') flags.DEFINE_string('label_map_path', 'data/pet_label_map.pbtxt', 'Path to label map proto') FLAGS = flags.FLAGS def get_class_name_from_filename(file_name): \"\"\"Gets the class name from a file. Args: file_name: The file name to get the class name from. ie. \"american_pit_bull_terrier_105.jpg\" Returns: A string of the class name. \"\"\" print(file_name) match = re.match(r'([A-Za-z_]+)(_[0-9]+\\.jpg)', file_name, re.I) return match.groups()[0] def dict_to_tf_example(data, label_map_dict, image_subdirectory, ignore_difficult_instances=False): \"\"\"Convert XML derived dict to tf.Example proto. Notice that this function normalizes the bounding box coordinates provided by the raw data. Args: data: dict holding PASCAL XML fields for a single image (obtained by running dataset_util.recursive_parse_xml_to_dict) label_map_dict: A map from string label names to integers ids. image_subdirectory: String specifying subdirectory within the Pascal dataset directory holding the actual image data. ignore_difficult_instances: Whether to skip difficult instances in the dataset (default: False). Returns: example: The converted tf.Example. Raises: ValueError: if the image pointed to by data['filename'] is not a valid JPEG \"\"\" img_path = os.path.join(image_subdirectory, data['filename']) with tf.gfile.GFile(img_path, 'rb') as fid: encoded_jpg = fid.read() encoded_jpg_io = io.BytesIO(encoded_jpg) image = PIL.Image.open(encoded_jpg_io) if image.format != 'JPEG': raise ValueError('Image format not JPEG') key = hashlib.sha256(encoded_jpg).hexdigest() width = int(data['size']['width']) height = int(data['size']['height']) xmin = [] ymin = [] xmax = [] ymax = [] classes = [] classes_text = [] truncated = [] poses = [] difficult_obj = [] for obj in data['object']: difficult = bool(int(obj['difficult'])) if ignore_difficult_instances and difficult: continue difficult_obj.append(int(difficult)) xmin.append(float(obj['bndbox']['xmin']) / width) ymin.append(float(obj['bndbox']['ymin']) / height) xmax.append(float(obj['bndbox']['xmax']) / width) ymax.append(float(obj['bndbox']['ymax']) / height) class_name = get_class_name_from_filename(data['filename']) classes_text.append(class_name.encode('utf8')) classes.append(label_map_dict[class_name]) truncated.append(int(obj['truncated'])) poses.append(obj['pose'].encode('utf8')) example = tf.train.Example(features=tf.train.Features(feature={ 'image/height': dataset_util.int64_feature(height), 'image/width': dataset_util.int64_feature(width), 'image/filename': dataset_util.bytes_feature( data['filename'].encode('utf8')), 'image/source_id': dataset_util.bytes_feature( data['filename'].encode('utf8')), 'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')), 'image/encoded': dataset_util.bytes_feature(encoded_jpg), 'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')), 'image/object/bbox/xmin': dataset_util.float_list_feature(xmin), 'image/object/bbox/xmax': dataset_util.float_list_feature(xmax), 'image/object/bbox/ymin': dataset_util.float_list_feature(ymin), 'image/object/bbox/ymax': dataset_util.float_list_feature(ymax), 'image/object/class/text': dataset_util.bytes_list_feature(classes_text), 'image/object/class/label': dataset_util.int64_list_feature(classes), 'image/object/difficult': dataset_util.int64_list_feature(difficult_obj), 'image/object/truncated': dataset_util.int64_list_feature(truncated), 'image/object/view': dataset_util.bytes_list_feature(poses), })) return example def create_tf_record(output_filename, label_map_dict, annotations_dir, image_dir, examples): \"\"\"Creates a TFRecord file from examples. Args: output_filename: Path to where output file is saved. label_map_dict: The label map dictionary. annotations_dir: Directory where annotation files are stored. image_dir: Directory where image files are stored. examples: Examples to parse and save to tf record. \"\"\" writer = tf.python_io.TFRecordWriter(output_filename) for idx, example in enumerate(examples): if idx % 100 == 0: logging.info('On image %d of %d', idx, len(examples)) # path = os.path.join(annotations_dir, 'xmls', example + '.xml') path = os.path.join(annotations_dir, example + '.xml') if not os.path.exists(path): logging.warning('Could not find %s, ignoring example.', path) continue with tf.gfile.GFile(path, 'r') as fid: xml_str = fid.read() xml = etree.fromstring(xml_str) data = dataset_util.recursive_parse_xml_to_dict(xml)['annotation'] tf_example = dict_to_tf_example(data, label_map_dict, image_dir) writer.write(tf_example.SerializeToString()) writer.close() # TODO: Add test for pet/PASCAL main files. def main(_): data_dir = FLAGS.data_dir label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path) logging.info('Reading from Pet dataset.') image_dir = os.path.join(data_dir, 'images') annotations_dir = os.path.join(data_dir, 'annotations') examples_path = os.path.join(annotations_dir, 'trainval.txt') examples_list = dataset_util.read_examples_list(examples_path) # Test images are not included in the downloaded data set, so we shall perform # our own split. random.seed(42) random.shuffle(examples_list) num_examples = len(examples_list) num_train = int(0.7 * num_examples) train_examples = examples_list[:num_train] val_examples = examples_list[num_train:] logging.info('%d training and %d validation examples.', len(train_examples), len(val_examples)) train_output_path = os.path.join(FLAGS.output_dir, 'pet_train.record') val_output_path = os.path.join(FLAGS.output_dir, 'pet_val.record') create_tf_record(train_output_path, label_map_dict, annotations_dir, image_dir, train_examples) create_tf_record(val_output_path, label_map_dict, annotations_dir, image_dir, val_examples) if __name__ == '__main__': tf.app.run() python object_detection/dataset_tools/create_pet_tf_record.py \\ --label_map_path=object_detection/data/pet_label_map.pbtxt \\ --data_dir=`pwd` \\ --output_dir=`pwd` create model config file # Faster R-CNN with Resnet-101 (v1), configuration for MSCOCO Dataset. # Users should configure the fine_tune_checkpoint field in the train config as # well as the label_map_path and input_path fields in the train_input_reader and # eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that # should be configured. model { faster_rcnn { num_classes: 90 image_resizer { keep_aspect_ratio_resizer { min_dimension: 600 max_dimension: 1024 } } feature_extractor { type: 'faster_rcnn_resnet101' first_stage_features_stride: 16 } first_stage_anchor_generator { grid_anchor_generator { scales: [0.25, 0.5, 1.0, 2.0] aspect_ratios: [0.5, 1.0, 2.0] height_stride: 16 width_stride: 16 } } first_stage_box_predictor_conv_hyperparams { op: CONV regularizer { l2_regularizer { weight: 0.0 } } initializer { truncated_normal_initializer { stddev: 0.01 } } } first_stage_nms_score_threshold: 0.0 first_stage_nms_iou_threshold: 0.7 first_stage_max_proposals: 300 first_stage_localization_loss_weight: 2.0 first_stage_objectness_loss_weight: 1.0 initial_crop_size: 14 maxpool_kernel_size: 2 maxpool_stride: 2 second_stage_box_predictor { mask_rcnn_box_predictor { use_dropout: false dropout_keep_probability: 1.0 fc_hyperparams { op: FC regularizer { l2_regularizer { weight: 0.0 } } initializer { variance_scaling_initializer { factor: 1.0 uniform: true mode: FAN_AVG } } } } } second_stage_post_processing { batch_non_max_suppression { score_threshold: 0.0 iou_threshold: 0.6 max_detections_per_class: 100 max_total_detections: 300 } score_converter: SOFTMAX } second_stage_localization_loss_weight: 2.0 second_stage_classification_loss_weight: 1.0 } } train_config: { batch_size: 1 optimizer { momentum_optimizer: { learning_rate: { manual_step_learning_rate { initial_learning_rate: 0.0003 schedule { step: 900000 learning_rate: .00003 } schedule { step: 1200000 learning_rate: .000003 } } } momentum_optimizer_value: 0.9 } use_moving_average: false } gradient_clipping_by_norm: 10.0 fine_tune_checkpoint: \"PATH_TO_BE_CONFIGURED/model.ckpt\" from_detection_checkpoint: true data_augmentation_options { random_horizontal_flip { } } } train_input_reader: { tf_record_input_reader { input_path: \"PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100\" } label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\" } eval_config: { num_examples: 8000 # Note: The below line limits the evaluation process to 10 evaluations. # Remove the below line to evaluate indefinitely. max_evals: 10 } eval_input_reader: { tf_record_input_reader { input_path: \"PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010\" } label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\" shuffle: false num_readers: 1 } train python train.py --logtostderr --train_dir=/home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/train_dir --pipeline_config_path=/home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/net.config tensorboard --logdir /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/train_dir/ evaluator python new_eval.py --logtostderr --checkpoint_dir /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/train_dir --eval_dir /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/eval_dir --pipeline_config_path /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/net.config tensorboard --logdir /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/eval_dir/ # new_eval.py import functools import os import tensorflow.compat.v1 as tf from tensorflow.python.util.deprecation import deprecated from object_detection.builders import dataset_builder from object_detection.builders import graph_rewriter_builder from object_detection.builders import model_builder from object_detection.legacy import evaluator from object_detection.utils import config_util from object_detection.utils import label_map_util from tensorflow.compat.v1 import ConfigProto from tensorflow.compat.v1 import InteractiveSession import keras.backend as K config = ConfigProto() config.gpu_options.allow_growth = True session = InteractiveSession(config=config) K.set_session(session) tf.logging.set_verbosity(tf.logging.INFO) flags = tf.app.flags flags.DEFINE_boolean('eval_training_data', False, 'If training data should be evaluated for this job.') flags.DEFINE_string( 'checkpoint_dir', '', 'Directory containing checkpoints to evaluate, typically ' 'set to `train_dir` used in the training job.') flags.DEFINE_string('eval_dir', '', 'Directory to write eval summaries to.') flags.DEFINE_string( 'pipeline_config_path', '', 'Path to a pipeline_pb2.TrainEvalPipelineConfig config ' 'file. If provided, other configs are ignored') flags.DEFINE_string('eval_config_path', '', 'Path to an eval_pb2.EvalConfig config file.') flags.DEFINE_string('input_config_path', '', 'Path to an input_reader_pb2.InputReader config file.') flags.DEFINE_string('model_config_path', '', 'Path to a model_pb2.DetectionModel config file.') flags.DEFINE_boolean( 'run_once', False, 'Option to only run a single pass of ' 'evaluation. Overrides the `max_evals` parameter in the ' 'provided config.') FLAGS = flags.FLAGS @deprecated(None, 'Use object_detection/model_main.py.') def main(unused_argv): assert FLAGS.checkpoint_dir, '`checkpoint_dir` is missing.' assert FLAGS.eval_dir, '`eval_dir` is missing.' tf.gfile.MakeDirs(FLAGS.eval_dir) if FLAGS.pipeline_config_path: configs = config_util.get_configs_from_pipeline_file( FLAGS.pipeline_config_path) tf.gfile.Copy( FLAGS.pipeline_config_path, os.path.join(FLAGS.eval_dir, 'pipeline.config'), overwrite=True) else: configs = config_util.get_configs_from_multiple_files( model_config_path=FLAGS.model_config_path, eval_config_path=FLAGS.eval_config_path, eval_input_config_path=FLAGS.input_config_path) for name, config in [('model.config', FLAGS.model_config_path), ('eval.config', FLAGS.eval_config_path), ('input.config', FLAGS.input_config_path)]: tf.gfile.Copy(config, os.path.join(FLAGS.eval_dir, name), overwrite=True) model_config = configs['model'] eval_config = configs['eval_config'] input_config = configs['eval_input_config'] if FLAGS.eval_training_data: input_config = configs['train_input_config'] model_fn = functools.partial( model_builder.build, model_config=model_config, is_training=False) def get_next(config): return dataset_builder.make_initializable_iterator( dataset_builder.build(config)).get_next() create_input_dict_fn = functools.partial(get_next, input_config) categories = label_map_util.create_categories_from_labelmap( input_config.label_map_path) if FLAGS.run_once: eval_config.max_evals = 1 graph_rewriter_fn = None if 'graph_rewriter_config' in configs: graph_rewriter_fn = graph_rewriter_builder.build( configs['graph_rewriter_config'], is_training=False) evaluator.evaluate( create_input_dict_fn, model_fn, eval_config, categories, FLAGS.checkpoint_dir, FLAGS.eval_dir, graph_hook_fn=graph_rewriter_fn) if __name__ == '__main__': tf.app.run() from object_detection import evaluator ImportError: cannot import name 'evaluator' \\ from object_detection.legacy import evaluator control trainning steps num_steps:600 tensorflow freeze # From tensorflow/models/research/ INPUT_TYPE=image_tensor PIPELINE_CONFIG_PATH={path to pipeline config file} TRAINED_CKPT_PREFIX={path to model.ckpt} EXPORT_DIR={path to folder that will be used for export} python object_detection/export_inference_graph.py \\ --input_type=${INPUT_TYPE} \\ --pipeline_config_path=${PIPELINE_CONFIG_PATH} \\ --trained_checkpoint_prefix=${TRAINED_CKPT_PREFIX} \\ --output_directory=${EXPORT_DIR} model:save and load model.fit(x_train, y_train, epochs = 150, batch_size = 32,callbacks=[tensorboard_callback]) model.save('./models/model.h5') model.save_weights('./models/weights.h5') model_path = './models/model.h5' model_weights_path = './models/weights.h5' model = load_model(model_path) model.load_weights(model_weights_path) array = model.predict(point_set) or tf.keras.models.save_model( model, 'models/mymode', overwrite=True, include_optimizer=True ) model = tf.keras.models.load_model('./models/mymode') cudnn\u5931\u8d25 from tensorflow.compat.v1 import ConfigProto from tensorflow.compat.v1 import InteractiveSession import keras.backend as K K.set_image_data_format('channels_last') K.set_learning_phase(1) config = ConfigProto() config.gpu_options.allow_growth = True session = InteractiveSession(config=config) K.set_session(session) pb:save and load def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True): graph = session.graph with graph.as_default(): freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or [])) output_names = output_names or [] output_names += [v.op.name for v in tf.global_variables()] input_graph_def = graph.as_graph_def() if clear_devices: for node in input_graph_def.node: node.device = '' frozen_graph = tf.graph_util.convert_variables_to_constants( session, input_graph_def, output_names, freeze_var_names) return frozen_graph frozen_graph = freeze_session(K.get_session(), output_names=[out.op.name for out in model.outputs]) tf.io.write_graph(frozen_graph, './models', 'xor.pbtxt', as_text=True) tf.io.write_graph(frozen_graph, './models', 'xor.pb', as_text=False) detection_graph = tf.Graph() with detection_graph.as_default(): od_graph_def = tf.GraphDef() with tf.gfile.GFile('xor.pb', 'rb') as fid: serialized_graph = fid.read() od_graph_def.ParseFromString(serialized_graph) tf.import_graph_def(od_graph_def, name='') input = tf.get_default_graph().get_tensor_by_name('input_1:0') output = tf.get_default_graph().get_tensor_by_name('fc2/Softmax:0') with detection_graph.as_default(): with tf.Session() as sess: values =sess.run(output, feed_dict={input: point_set}) print(values) \u67e5\u770b\u6a21\u578b\u7684\u8f93\u5165\u5c42\u548c\u8f93\u51fa\u5c42 print('model.inputs :',model.inputs) print('model.outputs : ',model.outputs) output model.inputs : [<tf.Tensor 'input_1:0' shape=(?, 2600, 3) dtype=float32>] model.outputs : [<tf.Tensor 'fc2/Softmax:0' shape=(?, 2) dtype=float32>] \u6240\u4ee5\u8f93\u5165\u5c42\u662f input_1:0 \u8f93\u51fa\u5c42\u662f fc2/Softmax:0 \u5b89\u88c5\uff54\uff45\uff4e\uff53\uff4f\uff52RT \u4e0b\u8f7d\u4e0e\u60a8\u4f7f\u7528\u7684Ubuntu\u7248\u672c\u548cCPU\u67b6\u6784\u5339\u914d\u7684TensorRT\u672c\u5730repo\u6587\u4ef6\u3002 \u4eceDebian\u672c\u5730repo\u8f6f\u4ef6\u5305\u5b89\u88c5TensorRT\u3002 os=\"ubuntu1x04\" tag=\"cudax.x-trt7.x.x.x-ga-yyyymmdd\" sudo dpkg -i nv-tensorrt-repo-${os}-${tag}_1-1_amd64.deb sudo apt-key add /var/nv-tensorrt-repo-${tag}/7fa2af80.pub sudo apt-get update sudo apt-get install tensorrt \u5982\u679c\u4f7f\u7528Python 2.7\uff1a\\ sudo apt-get install python-libnvinfer-dev \\ \u5c06\u5b89\u88c5\u4ee5\u4e0b\u9644\u52a0\u8f6f\u4ef6\u5305\uff1a\\ python-libnvinfer \\ \u5982\u679c\u4f7f\u7528Python 3.x\uff1a\\ sudo apt-get install python3-libnvinfer-dev \\ \u5c06\u5b89\u88c5\u4ee5\u4e0b\u9644\u52a0\u8f6f\u4ef6\u5305\uff1a\\ python3-libnvinfer \\ \u5982\u679c\u60a8\u6253\u7b97\u5c06TensorRT\u4e0eTensorFlow\u7ed3\u5408\u4f7f\u7528\uff1a\\ sudo apt-get install uff-converter-tf \\ \u5982\u679c\u60a8\u8981\u8fd0\u884c\u9700\u8981ONNX\u7684\u793a\u4f8b \u56fe\u5f62\u5916\u79d1\u533b\u751f \u6216\u5c06Python\u6a21\u5757\u7528\u4e8e\u60a8\u81ea\u5df1\u7684\u9879\u76ee\uff0c\u8fd0\u884c\uff1a\\ sudo apt-get install onnx-graphsurgeon \\ anaconda tensorRT \u4e0b\u8f7dtar\u6587\u4ef6 TensorRT-7.0.0.11.Ubuntu-18.04.x86_64-gnu.cuda-10.0.cudnn7.6.tar.gz (wind1) star@xmatrix:~$ (wind1) star@xmatrix:~$ (wind1) star@xmatrix:~$ cd TensorRT (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ ls TensorRT TensorRT_1 TensorRT-7.0.0.11 TensorRT-7.0.0.11.Ubuntu-16.04.x86_64-gnu.cuda-10.0.cudnn7.6.tar.gz TensorRT-7.0.0(1).tar.gz (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ cd TensorRT-7.0.0.11 (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ ls bin data doc graphsurgeon include lib python samples targets TensorRT-Release-Notes.pdf uff (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ cd python (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ ls tensorrt-7.0.0.11-cp27-none-linux_x86_64.whl tensorrt-7.0.0.11-cp35-none-linux_x86_64.whl tensorrt-7.0.0.11-cp37-none-linux_x86_64.whl tensorrt-7.0.0.11-cp34-none-linux_x86_64.whl tensorrt-7.0.0.11-cp36-none-linux_x86_64.whl (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ pip --version pip 19.3.1 from /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages/pip (python 3.6) (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ pip3 --version pip 19.3.1 from /usr/local/lib/python3.5/dist-packages/pip (python 3.5) (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ python -m pip install tensorrt-7.0.0.11-cp36-none-linux_x86_64.whl Processing ./tensorrt-7.0.0.11-cp36-none-linux_x86_64.whl Installing collected packages: tensorrt Successfully installed tensorrt-7.0.0.11 (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ python Python 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) [GCC 7.3.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> >>> >>> import tensorrt >>> >>> exit(); (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ cd ../ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ ls bin data doc graphsurgeon include lib python samples targets TensorRT-Release-Notes.pdf uff (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ cd uff (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ ls uff-0.6.5-py2.py3-none-any.whl (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ python -m pip install uff-0.6.5-py2.py3-none-any.whl Processing ./uff-0.6.5-py2.py3-none-any.whl Requirement already satisfied: numpy>=1.11.0 in /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages (from uff==0.6.5) (1.16.4) Requirement already satisfied: protobuf>=3.3.0 in /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages (from uff==0.6.5) (3.11.2) Requirement already satisfied: six>=1.9 in /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages (from protobuf>=3.3.0->uff==0.6.5) (1.13.0) Requirement already satisfied: setuptools in /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages (from protobuf>=3.3.0->uff==0.6.5) (42.0.2.post20191203) Installing collected packages: uff Successfully installed uff-0.6.5 (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ python Python 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) [GCC 7.3.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> >>> >>> import tensorrt >>> import uff WARNING:tensorflow:From /home/star/anaconda3/envs/wind1/lib/python3.6 7.0.0.11 (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ Using UFF converter to convert the frozen tensorflow model to a UFF file conda activate wjj \\ pip install nvidia-pyindex \\ pip install uff \\ you need to find your uff installed path. \\ import uff print(uff.__path__) And after locating it , in it\u2019s bin folder there should be a script named as convert_to_uff.py . And now you need to open the terminal and simply type python3 convert_to_uff.py < path to the saved model >\\ In my case-->\\ python3 convert_to_uff.py /home/models/catsAndDogs.pb \\ And it will simply save the converted .uff in your .pb model location. And then this is how the next script should be done. NVIDIA\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6 NVIDIA\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6 Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR Use allow_growth memory option in TensorFlow and Keras, before your code. For Keras import tensorflow as tf config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True config.log_device_placement = True sess = tf.compat.v1.Session(config=config) tf.compat.v1.keras.backend.set_session(sess) For TensorFlow import tensorflow as tf config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True config.log_device_placement = True sess = tf.compat.v1.Session(config=config) \u8fd8\u6709\u4e00\u79cd\u60c5\u51b5,\u51cf\u5c11batch size Anchor boxes One of the hardest concepts to grasp\u628a\u63e1 when learning about Convolutional Neural Networks for object detection is the idea of anchor boxes.It is also one of the most important parameters you can tune for improved performance on your dataset.In fact,if anchor oxes are not tuned correctly,your neural network will never even know that certain\u67d0\u4e9b small,large or irregular\u4e0d\u89c4\u5219 objects exist and will never have a chance to detect them.Luckily, there are some simple steps you can take to make sure you do not fall into this trap\u9677\u9631. what are anchor boxes? \\ when you use a neural network like yolo or ssd to predict multiple objects in a picture, the network is actually making thousands of predictions and only showing the ones that it decided were an object.The multiple predictions are output the following format:\\ Prediction 1: (X,Y,Height,Width),Class\\ ...\\ Prediction ~8000: (X,Y,Height,Width),Class Where the (X,Y,Height,Width) is called the \"bounding box\", or box surrounding the objects.This box and the object class are labelled manually by human annotators. In an extremely simplified example, imagine that we have a model that has two predictions and receives the following image:\\ We need to tell our network if each of its predictions is correct or not in order for it to be able to learn.But what do we tell the neural network it prediction should be? Should the predicted class be:\\ Prediction 1:Pear\\ Prediction 2:Apple Or should it be:\\ Prediction 1:Apple\\ Prediction 2:Pear What if the network predicts:\\ Prediction 1:Apple\\ Prediction 2:Apple We need our network's two predictors to be able to tell whether it is their job to predict the pear or the apple.To do this there are a several tools.Predictors can specialize in certain size objects, objects with a certain aspect ratio(tall vs. wide),or objects in different parts of the image.Most networks use all three criteria\u6807\u51c6.In our example of the pear/apple image,we could have Prediction 1 be for objects on the left and Prediction 2 for objects on the right side of the image.Then we would have our answer for what the network should be predict:\\ Prediction 1:Pear\\ Prediction 2:Apple Anchor Boxes in Practice\\ State of the art\u6700\u5148\u8fdb\u7684 object detection systems currently do the following:\\ 1. Create thousands of \"anchor boxes\" or \"prior boxes\" for each predictor that represent the ideal location, shape and size of the object it specializes\u4e13 in predicting. 2. For each anchor box,calculate which object's bounding box has the highest overlap divided by non-overlap.This is called Intersection Over Union or IOU. 3. If the highest IOU is greater than 50%, tell the anchor box that it should detect the object that gave the highest IOU. 4. Otherwise if the IOU is greater than 40%, tell the neural network that the true detection is ambiguous and not to learn from that example. 5. If the highest IOU is less than 40%,then the anchor box should predict that there is no object. This works well in practice and the thousands of predictors do a very good job of deciding whether their type of object apperars in an image. Using the default anchor box configuration can create predictors that are too specialized and ojects that appear in the image may not achieve an IOU of 50% with any of the anchor boxes.In this case, the neural network will never know these objects existed and will never learn to predict them. We can tweak\u8c03\u6574 our anchor boxes to be much smaller In xx net configuration, the smallest anchor box size is 32x32.This means that many objects smaller than this will go undetected. We can fix this by changing our default anchor box configurations. Reducing the smallest anchor box size, all of the things line up with at least one of our anchor boxes and our neural network can learn to detect them! Improving Anchor Box Configuration \\ As a general rule,you should ask yourself the following questions about your dataset before diving into training your model: 1. What is the smallest size box I want to be able to detect? 2. What is the largest size box I want to be able to detect? 3. What are the shapes the box can take? For example, a car detector might have short and wide anchor boxes as long as there is no chance of the car or the camera being turned on its side. You can get a rough estimate of these by actually calculating the most extreme\u6781\u7aef sizes\u3000and aspect ratios in the dataset.Yolo V3 uses K-means to estimate the ideal bounding boxes.Another option is to learn the anchor box configuration. Once you have thought through these questions you can start designing your anchor boxes. Be sure to test them by encoding your ground truth bounding boxes\u5730\u9762\u771f\u503c\u8fb9\u754c\u6846 and then decoding them as though\u5c31\u50cf they were predictions from your model.You should be able to recover the ground truth bounding boxes. Autonomous self-learning systems Preface This report describes the various processes that are part of the work on the main project at Oslo and Akershus University College(HIOA), department of engineering education,spring 2015. The report deals with the development of a self-learning algorithm and a demonstrator in the form of a robot that will avoid static and dynamic objects in an environment that is constantly changing.The thesis\u8bba\u6587 is given by HIOA.The report addresses the theory behind the most well-known and used self-learning algorithms,and discusses the advantages,disadvantages and uses of theese.It also contains a description of the technical solution for the demonstrator, and the method used in this project. The report is written within a topic that is considered new technical and is therefore assumed to be able to be used for futher research and/or learning within autonomous self-learning system. The reader is expected to have basic knowledge in electronics and information technology. We would like to thank our employer.Oslo and Akershus University College,for the opportunity to carry out the project and for financial support.We would also like to thank supervisor EZ for a good collaboration, as well as important and constructive guidance throughout the project period. Summary In today's society, self-learning systems are an increasingly relevant topic.Systems that are not explicity programmed to perform a specific task, but are even able to adapt,can be very useful. The system described in this thesis is realized with Q-learning by both the table method and the neural network.Q-learning is a learning algorithm based on experience.The algorithm involves an agent exploring his environment, where the environment is represented by a number of states. The agent experiments withs the environment by performing an action, and then observes the consequence of that action.The consequence is given in the form of a positive or negative reward. The goal of the method is to maximize the accumulated reward over time. Autonomous self-learning systems are becoming increasingly relevant because the system is able to adapt to partially or completely unknown situations.It learns from experience and needs less information at start-up as it acquires information along the way.In autonomous self-learning systems and self-propelled robots, avoiding obstacles is a key task.This report addresses a demonstrator of such a system, realized with Q-learning presented later in the reportk, and provides a description of the algorithm and results. Theory Before looking at the structure of a self-learning system, one can advantageouly look at what the concept of learning is.Learning is often defined as a lasting change in behavior as a result of experience(St. Olavs Hospital,undated).The property of organisms\u751f\u7269\u4f53 that is defined as learning is one of the cornerstones of what is called intelligence which, among other things, is defined as an ability to acquire and apply knowledge and skills.Humans and animals are considered intelligent, among other things,based on their ability to learn from experience. machine learning This subchapter is based on the theory of S.M,2009.Machine learning is a form of artifical intelligence that focuses on the development of self-learning algorithms. In most cases,self-learning systems deal with parts of natural intellignece, including memory,adaptation and generalization\u6982\u62ec.Unlike traditional non-learning systems, the method makes it possible to construct a system that is able to expand , adapt to new information and learn a given task without being specifically programmed for this.For machine learning,this system is called an agent.By using menmory,an agent can recognize the last time it was in a similiar situation and what action it took.Based on the outcome from the previous time, it can, if it was correct,choose to repeat the action, or try something new.By generallizing, the agent can recognize similarities in different situations, and thus use experienct from one situation and apply this experience in another. In order to realize\u5b9e\u73b0 this concept, machine learning uses principles\u539f\u7406 from statistics, mathematics, physics, neurology and biology. \uff37hen talking about machine learning and self-learning systems,algorithms are mainly the main product.The actual process in these algorithms can be compared to data mining\u6316\u6398.Data mining is a process that analyzes data from different perspectives and summarizes if into useful information.Both methods go through data to find patterns\u6a21\u5f0f,but instead of extractiong data for human interpretation,the information is used to improve the agent's understanding.For the agent to be able to learn, it must know how to improve , and whether it will get better or not.There are 15 more methods to solve this, which in turn provide servel main categories within machine learning:Supervised learning,unsupervised learning and reinforcement learning. Supervised Learning An agent is given a training set with ,for example,pictures of a face and pictures without a face.The agent then prepared through a training process where it gives a forecast of what the picture is of. Whenever the forecast is incorrect, the agent is corrected.This process continues untill the model achieves a desired level of accuracy. Since the algorithm does not have a specific definition of what is a face and what is not, it must therefore learn this using examples.A good algorithm will eventually be able to estimate whether an image is of a face or not.The learning methods are best explained by examples: Unsupervised Learning When learning without supervision, information is not marked.This is ,the system is not told what is the image of a face and what is not.As a result, there is no correction or reward to indicate a potential solution, but the algorithm tries to identify the similarities between the images, swfor then categorize them and divide them into groups. Reinforcement Learning In reinforcement learning,the algorithm is told when the answer it gives is incorrect, but receives no suggestions on how to correct this.It must explore and try out different solutions untill it finds out how it gets the right answers.This is a kind of middle ground of supervised learning and unsupervised learning.Examples could be learning to play a board game or a robot that is going to learn to walk.Each time an agent performs an action,he or she recieves a reward or a penalty, based on how desirable\u53ef\u53d6\u7684 the outcome of the action is.For example.when an agent is trained to play a board game, he gets a positive reward for winning, and a negative reward for losing.All other cases give no reward.\\ \\ The activity mentioned above can be represent as a sequence of state-action reward:\\ \\ This means that the agent was in state s0, performed action a0,which resulted in it receiving reward r1 and ending up in state s1.Furthermore, it performed action a1, received reward r2, and ended up in state s2, and so on.\\ \uff34his sequence is made up of experiences where experience is given as:\\ \\ Experience shows that the agent was in state s,performed action a,received reward r, and ended up in state s', and represented by (s,a,s',r) In order for the agent to be able to learn from the sequences mentioned and thus call it an experience.It has a table called Q-table which acts as its memory.All data points stored in this table are called Q-values and represent how desirable\u53ef\u53d6\u7684 it is to perform a specific action in a specific state. One experience adds one data point Q(s,a) in the table that represents the agents current estimate\u4f30\u8ba1 of the optimal\u6700\u4f73 Q value. It is this information that the agent uses to learn an optimal pattern of action.The size of the table depends on how many conditions and actions are included in the problem you are trying to solve, where the number of conditions gives the number of rows ,while the number of actions gives the number of colums.For example if you have 20 states and 3 actions, you will get a table of 20x3. Before the agent begins to experiment, it knows nothing else what actions it is capable of performing, and the Q table is consequently empty.This is, all Q values are equal to 0. The key to the method described above is to update the Q value that is applied to the agent when it performs an action in a given state in the current data point when it gains an experience. This value is given by the Q function\\ \\ or more clearly:\\ \u03b1\uff0d\uff0dlearning rate\\ \u0393\uff0d\uff0ddiscount factor\\ r\uff0d\uff0dreward\\ The learning rate dicates\u6307\u793a how much of previouly\u5148\u524d acquired learning should be replaced with new information. By \u03b1=1,previous values are replaced with new information.while \u03b1=0 ,corresponds to no update.In other words,the agent will konw by \u03b1=1,assume that the last reward and resulting state are representative of future values. The discount factor indicates the weight of all futher step rewards.If \u0393=0,the agent will only consider current rewards, while it will think more long-term\u957f\u671f and strive\u52aa\u529b for higher future rewards as \u0393 approaches 1. The reward is defined when you create the program in the form of reward functions\u5956\u52b1\u662f\u5728\u521b\u5efa\u7a0b\u5e8f\u65f6\u4ee5\u5956\u52b1\u51fd\u6570\u7684\u5f62\u5f0f\u5b9a\u4e49\u7684,and can be positive or negative. What this reward value is defined as is not critical.On the other hand, It is important that there is a clear distinction\u533a\u522b between the reward for good deeds\u884c\u4e3a and the reward for bad deeds. When a new experience is added to the algorithm, a new Q value is estimated, and the old value of the Q table for the last experience is updated with the new one. Assume that the agent is a mouse that is placed in a room divded into six states, see below Figure.In state s6 there is a piece of cheese,while in state s5 there is a cat.If the agent finds the piece of cheese, it receives a reward of 10, while it receives a reward of -10 if it moves to the state where the cat is.These two states are called terminal states.The agent explores the environment untill one of the terminal states is reached,before starting a new iteration(attempt)\\ \uff34he agent can perform four actions: up,down,left,right.If it moves in a direction where there is a wall, it gets a reward of -1, and it remains in the same state. All other conditions have a value equal to 0.This is implemented in the algorithm using reward functions.\\ \\ The agent has no information about his surroundings other than that there are six states and four acitons, what state he is in at any given time and any rewards it receives as actions are performed.Nor does it know what a reward or punishment is.The Q table is illustrated in Table 1. By assuming the following sequences of experiences,(s,a,r,s') one can illustrate how a sequence updates the Q-table. For the sake of illustration, the learning rate \u03b1 and the discount factor \u0393 set equal to 1 and all Q-values have an initial value equal to 0.\\ After this sequence of experiences, the Q table with updated values will look like this:\\ \\ opp=up ned=down venstre=left h\u00d8yre=right Each experience will update the value of the state table for the performed state and action combination, and the Q values will converge\u6c47\u805a to optimal values over time.The more experiments the agent conducts\u884c\u4e3a, the better the estimates in the Qtable.In theory, the agent will eventually achieve an optimized Q-table, and will be able to choose the shortest path to the cheese each time, regardless of the starting state. Exploration or Utilize \u52d8\u63a2\u6216\u5f00\u53d1 One of the challenges with the Q-learning algorithm is that it does not directly tell what the agent should do, but rather functions as a tool that shows the agent what the optimal action is in a given condition.This is a challenge because the agent must explore the environment enough times to be able to build a solid foundation\u575a\u5b9e\u7684\u57fa\u7840 that provides a good estimate of the Q values.To address this, an exploration feature is implemented that determines the strategy\u6218\u7565 the agent will use to select actions, either explore or exploit. Exploration:The agent is bold\u80c6\u5927 and does not necessary choose the best course of action, with the intension of establishing a better estimate of the Q values. Utilize: The agent utilizes the experience it has already built up, and selects the optimal action for the condition it is in. That is, the action that gives the highest Q value. It is said that the agent is greedy\u8d2a\u5a6a The purpose of this feature is to establish a relationship between the two strategies. It is important to explore enough so that the agent builds a solid foundation of the Q-values, but it is also important to utilize the already acquired knowledge to ensure the highest possible reward. There are a number of ways to accomplish this, but two of the most popular methods are the greedy feature and the Boltzmann distribution, popularly called \"softmax\": \u03b5-greedy \u03b5-greedy is a strategy based on always choosing the optimal action except for \u03b5 of the aisles and choosing a random action of the aisles.\\ \\ there\\ a* is optimal action with probability 1-\u03b5\\ ar is random action with probability \u03b5\\ where\\ 0< \u03b5 < 1 It is possible to change over time. This allows the agent to choose a more random strategy at the beginning of the learning process, and then become more greedy as it acquires more knowledge. One of the challenges with greed is that it considers all actions except the optimal one as equal.\u5b83\u5c06\u6700\u4f73\u884c\u52a8\u4e4b\u5916\u7684\u6240\u6709\u884c\u52a8\u90fd\u89c6\u4e3a\u5e73\u7b49 If, in addition to an optimal action, there are two good actions as well as additional actions that look less promising,\u8fd8\u6709\u4e24\u4e2a\u597d\u7684\u52a8\u4f5c\u4ee5\u53ca\u770b\u8d77\u6765\u4e0d\u592a\u6709\u524d\u9014\u7684\u5176\u4ed6\u52a8\u4f5c\uff0c it will make sense\u6709\u610f\u4e49 for the agent to choose one of the two better ones instead of spending time exploring the bad actions. Using greed is as likely to explore a bad action as to explore one of the two better ones. One way to solve this is to use the Boltzmann distribution 2.3.2 Boltzmann distribution This strategy is known as \"softmax\" action selection and involves selecting an action with a probability that depends on the Q value. The probability of selecting action a in state s is proportional to\u3000 ,which means that the agent in state s chooses action a with probability \\ The parameter T specifies how random values are to be selected.\u53c2\u6570T\u6307\u5b9a\u5982\u4f55\u9009\u62e9\u968f\u673a\u503c\u3002 When T has a high value, actions are chosen to approximately the same extent.\u9009\u62e9\u7684\u52a8\u4f5c\u7684\u7a0b\u5ea6\u5927\u81f4\u76f8\u540c As T decreases, the actions with the highest Q value are more likely to be selected, while the best action is always selected when T-> 0 One of the advantages of the Q-learning algorithm is that it is exploration insensitive\u5bf9\u63a2\u7d22\u4e0d\u654f\u611f. That is, the Q values will converge to optimal values regardless of the agent's behavior as long as information is collected. This is one of the reasons why Q-learning is one of the most widely used and effective methods in reinforcement learning. On the other hand, it is a prerequisite that all combinations of condition / action are tried out many enough times, which in most cases means that the exploration and exploitation aspect must be addressed as it is not always possible for an agent to experiment infinitely. There are several methods for setting up an artificial neural network.One of the most common,which is also used in this project, is called a feedforward neural network, which means that the information is always fed from the input page and supplied further through the network.In other words,output from one layer + bias is used as input in the next. Fuzzy Logic\u6a21\u7cca\u7406\u8bba The theory of fuzzy logic is based on S.K.J(2007)\\ Fuzzy logic is a decision-making or management model based on diffuse\u3000\u6269\u6563\uff0c\u6563\u53d1,and input data classified as degrees of membership in a diffuse quantity. A diffuse set has no sharp boundaries, and is a set of values that belong to the same definition, where the definition is a diffuse term often described by an adjective\u5f62\u5bb9\u8bcd. For example: big potatoes or tall people.A diffuse set is also called a semantically\u8bed\u4e49\u4e0a variable or set. Fuzzy logic classifies input data according to how large a membership the input data has to a set.The degree of membership is graded\u5206\u7ea7 between 0 and 1,and determined on the basis of a membership function\u6839\u636e\u96b6\u5c5e\u51fd\u6570\u786e\u5b9a.\uff34he member ship function is determined on the basis of knowledge of the field, and can be non-linear.Input data can have membership in servel different sets, but can only have full membership in one set. Fuzzy logic behavior Diffusion\u6269\u6563: input data is graded according to the diffused quantities,and the diffused quantities are sent to subprocess 2 Logical rules:the diffused quantites are tested against rules.The degree of fulfillment \u5c65\u884c from each rule is sent to sub-process 3.The rules can be implented with {and, if, or},or other methods such as an impact\u5f71\u54cd matrix.The rules describe the relationship between the diffused input and output. Implication: the received data from sub-process 2 is tested againist rules that grade the impact on the overall consequence of each of the received data.The ratings are then sent to sub-process 4. Aggregation\u805a\u5408:all the consequence\u540e\u679c quantities are compiled into an aggregate quantity\u603b\u91cf,which is the union of the quantities from subprocess 3. A vdiffusion: The most representative element is extracted from the aggregate amount.This is usually the center of gravity\u91cd\u529b.That value will then be the control signal. The Fuzzy logic method has a number of advantages in that it is able to process imprecise\u4e0d\u7cbe\u786e data and model non-linear relationships between input and output.The principle enables an accurate and realistic\u73b0\u5b9e\u7684 classification of data, as data is rarely\u5f88\u5c11 only true or false. Technical solution Agents Two versions of the agent were developed.The first version,which was later replaced, was built on the basis of drivetrain\u52a8\u529b\u603b\u6210 and mechanics\u673a\u68b0\u5e08 taken from 'WLtoys' model 'A969'. The radio-controlled car in scale 1:18 was delivered\u5df2\u4ea4\u4ed8 to drive with ratio and battery.The car measured 285x140x105mm(LxWxH) and could run for about 10 minutes on a fully charged battery. Since the car was to be equiped with an Xbee module and communicate via bluetooth.the car's original reciever was removed.The steering servo\u8f6c\u5411\u4f3a\u670d was adapted to the car's receiver and was therefore replaced in favour of a PWM-controllable servo.To improve the agent's running time, the original battery(7.4V/1100mAh) was replaced with a 7.4V/5200mAh \"Fire Bull\" type battery, which would thoretically provide approximately five times as long. To make room for all the electronics, a plate 1.5mm aluminum\u94dd was cut out and mounted on top of the car.The motor driver, microcontroller board and Xbee module were placed together with the motor and servo on the classis itself, while the battery, sensors and voltage regulator were mounted\u5b89\u88c5 on the new plate for easier access. After the mentioned changes, the agent had improved battery life, adjustable speed and the correct physical size, but it turned out not to have a good enough turning radius. It had particular problems in situations where it had to react in one direction, and then change direction. The car's turning radius was tested before the conversion was started, and it was measured that the car need 70cm to turn 90 degrees to the left or right(figure 3-4).It thus needed an area of 140cm in diameter to turn 180 degrees.It was therefore assumed that a round arena\u7ade\u6280\u573a with a diameter of about four meters was needed.The area was built,and it was found that four meters was enough to be able to drive around without obstacles,but that it became too samll when car had to avoid objects. The ultrasonic sensors are limited, through discretion\u901a\u8fc7\u659f\u914c,to integer values between 1cm and 120cm, If one is to create a look-up table for the entire spectrum\u8303\u56f4, the table will consist of 120^5 different combinations, and would with three possible actions end up with a look-up table of 120^5x3 number of places. This is a less good solution as the table would be very large(74 billion seats).A large lookup table means that the agent spends much more time learning all the combinations of conditions, and actions for these.In addition, such a solution is far too large to be implemented on a normal computer.This chanllenge is solved by discretizing\u79bb\u6563\u5316 or dividing, the continuous sensor area into smaller and more managerable state spaces. 4.2.1 Condition space model All sensors can measure distances up to 400 cm. To avoid a large state table and to streamline the learning process, the sensor values are discretized down to 144 states. For the project described in this report, this also simplifies the reward function. Each snsor is divided into four zones based on distance in centimeters. The division is shown in Figure 4.4\\ \\ A condition consists of four parameters. Two for the left side and two for the right side of the agent's field of view.A state is a combination of distance to the nearest obstacle and in which sector the obstacle is located .a sector describes the angle at which an object is located, relative to the agent. The four parameter are defined by:\\ k1:zone\u533a left side \u533a\u57df\u5de6\u4fa7\\ k2:zone right side \u533a\u57df\u53f3\u4fa7\\ k3:sector\u90e8\u95e8 left side \u6247\u533a\u5de6\u4fa7\\ k4:sector right side \u6247\u533a\u53f3\u4fa7\\ k5:observation of dynamic or static obstacle left side \u89c2\u5bdf\u52a8\u6001\u6216\u9759\u6001\u969c\u788d\u7269\u5de6\u4fa7\\ k6:observation of dynamic or static obstacle right side \u89c2\u5bdf\u52a8\u6001\u6216\u9759\u6001\u969c\u788d\u7269\u53f3\u4fa7\\ S(state space model) = k1 x k2 x k3 x k4 x (k5 x k6)\\ The agent can perceive\u611f\u77e5 two obstacles simultaneously, but not on the same page.With two obstacles on the same side,the algorithm will prioritize\u4f18\u5148 the obstacle that is in the zone with the lowest numbering. The division of the sectors is illustrated in Figure below\\ \\ sector\u6247\u533a\\ The sectors have four different values, 0,1,2,3.The sensors perceive\u611f\u77e5 an object when the zone value is less than 2.The sectors are symmetrically\u5bf9\u79f0\u5730 distributed\u5206\u5e03.so that the agent's right field of view is discretized in the same way as the left.Sensors 1,2 and 3 on the left side correspond to sensors 3,4 and 5 for the right side. 4.2.11 Discretion for system with dynamic obstacles In the simulation for a system with dynamic obstacles, the state space model is expanded with two additional parameters,k5 and k6. k5 indicates with the value 1 or 0 whether the obstacle detected on the left side is dynamic, while k6 does the same for the right side.The number 1 indicates a dynamic obstacle,while 0 indicates static.The state table has thus benn expanded to 576 states.,An object os classified as a dynamic obstacle in that the agent first, while standing still,performs two measurements in a row. Then the two measurements are compared, if the obstacle moves away from the agent, the first measurement will be positive number, and classified as a static obstacle.If on the other hand, the same calculation operation gives a negative number, the obstacle is classified as a dynamic obstacle and the agent must react accordingly. 4.2.1.2 the state table The state table is structured as a table where all combinations of states and actions are represented as a table point.The state table for the physical model has 144 rows, and three columns that present each of the possible actions(left,right, straightfoward). A section of the condition table is shown in Table 7. 4.2.2 Reward function The reward feature determines the agent's immmediate reward after each experience.When implementing the reward function, a distinction is made between good actions that give a positive reward value,and bad actions that give a negative reward value.The reward function used for Q-learning with the table method is inspired by and shown under equation.\\ The reward function, r1 is defined to give a positive reward value when the agent drives straight ahead, while for turns a small negative reward value is given. The negative value is given to prioritize \u4f18\u5148 driving straight ahead. The parameter r2 gives a positve reward when the total change in the sensor values is positive. That is , the agent moves away from an obstacle. If the agent moves towards an obstacle , it becomes total the change in sensor values is negatie, and the agent recieve a negative reward. The function r3 gives the agent a negative reward value if it first turns to right and then to the left, or first to the left and then to right one after the other. This feature allows the agent to avoid an indecisive\u4f18\u67d4\u5be1\u65ad\u7684 sequence, where it only swings\u6447\u6446 back and forth. \uff34he function r gives the agent a reward value equal to the sum of r1,r2 and r3 if it does not collide, while in the event of a collision it gets a negative reward value of -100. 4.2.3 Exploration function The exploration function determines the agent's strategy for choosing actions. It can either explore the state space or take advantage of the experiences it already has. The function ensures an appropriate balance between these strategies (see chapter 2.3). The simulations for the exploration methods show that softmax gives the best results in the simulated model, and is thus used in the simulation. The parameter T is set equal to 24, and is reduced by 5% of its total value for each individual experiment.In the physical model, epsilon is used because it provides a better overview of how much coincidence there is for the agent to take an arbitrary action, and epsilon is set at a 5% chance of doing a random action. 4.2.4 Implementation The implementation of Q-learning with the table method is based on the same algorithm and reward function in the simulation and for the physical model.The different between the simulation and the physical agent is the exploration function and an additional extension of the state space in the simulation(see the discretization\u79bb\u6563\u5316 chapter).In the simulation, you have the choice between static and dynamic obstacles, and to adapt the agent to dynamic obstacle, the Q table is expanded. 4.2.4.1 The Q-Learning algorithm with the table method The Q-Learning algorithm with the table method At startup: - initialize state space S - initialize the action list A - initialize the Q table Q(s,a)(empty table) based on the state space S and the action list A For each attempt: 1) Observe initial sensor values 2) Discretize the sensor values and set it equal to s(initial mode) 3) Select an action a based on the exploration function and perform the action 4) Observe the agent's sensor values after the operation 5) Discretize the sensor values and set it equal to s'(Next mode) 6) Give the agent the reward r based on the reward function 7) Update the Q table 8) Set s=s'(Now state) 9) Keep last action, prev_a =a (Used in the reward function) 10) Repeat the operation from point iii)(a new step /experience ) If the agent has not reached its terminal state(collision or maximum number of steps). Start a new attempt if the agent has reached its terminal state(Restart from i). 4.2.4.2 Simulated model In the simulation, a user interface is created that shows the user the simulation of the agent, two different graphs, buttons for different functions and information panel.The information panel show the user relevant\u76f8\u5173\u7684 values from the simulation.One graph shows an average of the number of steps for every tenth attempt, while the other shows the agent's accumulated reward.One step in the program corresponds to an experienced situation,whether it has been experienced before or is new.A new attempt is initialiated each time the agent has moved the maximum number of steps for the current attempt, or has collided with an obstacle .The maximum number of steps can be changed, but is normally 400-600.The dimensions of the simulation are scaled 1\uff1a10[cm] The ultrasonic sensors on the physical agent are limited to integer values from 1 to 120cm in the code of the agents microcontrollers.Distance less than 8 cm id defined as a collision. The initial state of the physical agent is an arbitrary position in the environment.The only requirement is that the initial state is not a terminal state(collision). The commands - command 1 : request sensor values - command 2: ask the agent to drive straight ahead - command 3: ask the agent to turn right - command 4: ask the agent to turn left - command 5: ask the agent to stop, sensor values <8 cm(collision) 4.3 Q-Learning with neural network (Linear function approximation) Q-Learning with neural network is based on approximation of one or more linear Q-functions. Unlike Q-table learning.this method stores the Q functions in a neural network. By using this method ,discretization is not necessary, and the entire continuous sensor spectrum can be used. The behavior behind an artifical neuron is described in chapter 2.3.3\\ neural network \\ \\ input layer---------------------------------hidden layer-------------------------------------output layer The figure above shows the neural network as it is implemented in this system. In the network, all the neurous, including the bias from one layer, are connected to each individual neuron in the next layer. The neural network has three layers.Layer One, the input layer, has five neurons (one for each sensor),layer two has fifteen neurons, and layer three has three neurons according to the number of actions in the action list.Layer 1 is the start of the network, and the sensor values are used as input.and these must be in the interval\u95f4\u9694[-1,1] The weight matrix w(1) is used to weight the link between layer 1 and layer 2.means w(2) for layer 2 and layer 3.The matrices have a dimension of (number of neurons in layer L)x(number of neurons in Layer (l-1) + bias) w(1) has a dimension of 15x6 and w(2) dimension of 3x16. In the output layer, there are three neurons, and each individual neuron corresponds to a Q function.The number of neurons in the output layer is equal to the number of actions of the agent, and each individual neuron estimates a Q function with respect to an action in the agent's action list Equation(21)\\ These Q function estimate a Q value that is futher used to determine the optimal actions for the agent, where the optimal action for each condition is therefore defined as a = max(Q(s,a)). It is impossible to say which Q function are suitable for the system, since the neural network estimates the Q function based on the agent's experience. In order for the agent to achieve the highest total reward, it must therefore explore the environment long enough for the network to converge to an optimal function. \\ The figure shows the learning process of agent. The method needs initial values, and these are given by sensor values and the network's weight matrices.The weight matrices are initiallized with random values between 0 and 1, but not zero.If the weight matrices are initialized with a value equal to 0,all the neurons will have the same value,which means that the neural network will not be usable. This algorithm uses the agent's sensor values, as opposed to the table method which uses a discretization of values. The exploration function used is the same as for the table method, but the reward function is different. The purpose of this method is to find optimal Q functions by updating the weight machines after each experiment. 4.3.1 Reward function Since the agent's state space on the neural network is continuous, an equally elaborate reward function is not required as for the table method.The reward function is defined by the agent receiving a positive reward if it drives straight ahead, and a negative reward in the event of a turn and a collision. The reward value r must be in the interval [-1,1], and the function as defined for the system is shown in equation.\\ 4.3.2 Feedforward As described in Chapter 2 Theory, neural network in this project is a forward-connected neural network. The neurons are activated using the function of the hyperbolic forceps\u53cc\u66f2\u7ebf\u94b3 given in equation. \\ The activated values become output values in this layer, and are used as input for the next layer.The three neurons in the output layer correspond to the Q funcitons when all data is fed into the network. \\ \\ The weight matrix w has m number of rows equal to the number of neurons,and columns n equal to the number of inputs.The matrix w(2) and the vector y are multiplied by each other and the product is used in the activation function described ealier in the chapter.The elements in both the weight matrix and the input vector are real numbers,and the value of the inputs is in the interval[-1,1].Equation(26-28) shows the Q functions of the three actions. The calculations in the figures above can be abbreviated\u7f29\u5199 and shown in equation 27 in vector form.\\ \\ Activation of the hidden layer takes place in the same way as for the output layer, and the input to this layer is the sensor values.\\","title":"deeplearning"},{"location":"deeplearning/#lenet","text":"use convolution to extract spatial features. subsample using spatial average of maps. non-linearity in the form of tanh or sigmoids. multi-layer neural network(MLP) as final classifier sparse connection matrix between layers to avoid large computational cost","title":"LeNet"},{"location":"deeplearning/#alexnet","text":"use of rectified linear units(ReLU) as non-linearities. use of dropout technique to selectively ignore single neurons during training , a way to avoid overfitting of the model. overlapping max pooling, avoiding the averaging effects of average pooling. using GPUs to reduce training time","title":"AlexNet"},{"location":"deeplearning/#xception","text":"","title":"xception"},{"location":"deeplearning/#computing-the-mean-and-std-of-dataset","text":"import tensorflow as tf from PIL import ImageStat class Stats(ImageStat.Stat): def __add__(self, other): return Stats(list(map(add, self.h, other.h))) loader = DataLoader(dataset, batch_size=10, num_workers=5) statistics = None for data in loader: for b in range(data.shape[0]): if statistics is None: statistics = Stats(tf.to_pil_image(data[b])) else: statistics += Stats(tf.to_pil_image(data[b])) print(f'mean:{statistics.mean}, std:{statistics.stddev}')","title":"Computing the mean and std of dataset"},{"location":"deeplearning/#pyx-to-so","text":"python setup.py build_ext --inplace","title":"pyx to so"},{"location":"deeplearning/#cuda","text":"sudo apt-get purge nvidia* sudo apt-get autoremove sudo apt-get autoclean sudo rm -rf /usr/local/cuda*","title":"\u5f7b\u5e95\u5220\u9664cuda"},{"location":"deeplearning/#steps-to-install-pytorch-with-cuda-100","text":"conda install pytorch torchvision cudatoolkit=10.0 -c pytorch \\ pip install torchsummary","title":"Steps to Install PyTorch With CUDA 10.0"},{"location":"deeplearning/#conda-install-matplotlib","text":"conda install -c conda-forge matplotlib","title":"conda install matplotlib"},{"location":"deeplearning/#conda-install-tensorflow-object_detection-api","text":"conda install -c conda-forge tf_object_detection","title":"conda install tensorflow object_detection api"},{"location":"deeplearning/#modulenotfounderror-no-module-named-deployment","text":"from slim.deployment import model_deploy","title":"ModuleNotFoundError: No module named 'deployment'"},{"location":"deeplearning/#modulenotfounderror-no-module-named-slim","text":"conda install -c conda-forge tf-slim","title":"ModuleNotFoundError: No module named 'slim'"},{"location":"deeplearning/#modulenotfounderror-no-module-named-nets","text":"change frome nets to from slim.nets","title":"ModuleNotFoundError: No module named 'nets'"},{"location":"deeplearning/#creating-trainvaltxt","text":"import os import random import sys if len(sys.argv) < 2: print(\"no directory specified, please input target directory\") exit() root_path = sys.argv[1] xmlfilepath = root_path + '/Annotations' txtsavepath = root_path + '/ImageSets/Main' if not os.path.exists(root_path): print(\"cannot find such directory: \" + root_path) exit() if not os.path.exists(txtsavepath): os.makedirs(txtsavepath) trainval_percent = 0.9 train_percent = 0.8 total_xml = os.listdir(xmlfilepath) num = len(total_xml) list = range(num) tv = int(num * trainval_percent) tr = int(tv * train_percent) trainval = random.sample(list, tv) train = random.sample(trainval, tr) print(\"train and val size:\", tv) print(\"train size:\", tr) ftrainval = open(txtsavepath + '/trainval.txt', 'w') ftest = open(txtsavepath + '/test.txt', 'w') ftrain = open(txtsavepath + '/train.txt', 'w') fval = open(txtsavepath + '/val.txt', 'w') for i in list: name = total_xml[i][:-4] + '\\n' if i in trainval: ftrainval.write(name) if i in train: ftrain.write(name) else: fval.write(name) else: ftest.write(name) ftrainval.close() ftrain.close() fval.close() ftest.close()","title":"creating trainval.txt"},{"location":"deeplearning/#creating-tfr-datasets","text":"import hashlib import io import logging import os import random import re from lxml import etree import PIL.Image import tensorflow as tf from object_detection.utils import dataset_util from object_detection.utils import label_map_util flags = tf.app.flags flags.DEFINE_string('data_dir', '', 'Root directory to raw pet dataset.') flags.DEFINE_string('output_dir', '', 'Path to directory to output TFRecords.') flags.DEFINE_string('label_map_path', 'data/pet_label_map.pbtxt', 'Path to label map proto') FLAGS = flags.FLAGS def get_class_name_from_filename(file_name): \"\"\"Gets the class name from a file. Args: file_name: The file name to get the class name from. ie. \"american_pit_bull_terrier_105.jpg\" Returns: A string of the class name. \"\"\" print(file_name) match = re.match(r'([A-Za-z_]+)(_[0-9]+\\.jpg)', file_name, re.I) return match.groups()[0] def dict_to_tf_example(data, label_map_dict, image_subdirectory, ignore_difficult_instances=False): \"\"\"Convert XML derived dict to tf.Example proto. Notice that this function normalizes the bounding box coordinates provided by the raw data. Args: data: dict holding PASCAL XML fields for a single image (obtained by running dataset_util.recursive_parse_xml_to_dict) label_map_dict: A map from string label names to integers ids. image_subdirectory: String specifying subdirectory within the Pascal dataset directory holding the actual image data. ignore_difficult_instances: Whether to skip difficult instances in the dataset (default: False). Returns: example: The converted tf.Example. Raises: ValueError: if the image pointed to by data['filename'] is not a valid JPEG \"\"\" img_path = os.path.join(image_subdirectory, data['filename']) with tf.gfile.GFile(img_path, 'rb') as fid: encoded_jpg = fid.read() encoded_jpg_io = io.BytesIO(encoded_jpg) image = PIL.Image.open(encoded_jpg_io) if image.format != 'JPEG': raise ValueError('Image format not JPEG') key = hashlib.sha256(encoded_jpg).hexdigest() width = int(data['size']['width']) height = int(data['size']['height']) xmin = [] ymin = [] xmax = [] ymax = [] classes = [] classes_text = [] truncated = [] poses = [] difficult_obj = [] for obj in data['object']: difficult = bool(int(obj['difficult'])) if ignore_difficult_instances and difficult: continue difficult_obj.append(int(difficult)) xmin.append(float(obj['bndbox']['xmin']) / width) ymin.append(float(obj['bndbox']['ymin']) / height) xmax.append(float(obj['bndbox']['xmax']) / width) ymax.append(float(obj['bndbox']['ymax']) / height) class_name = get_class_name_from_filename(data['filename']) classes_text.append(class_name.encode('utf8')) classes.append(label_map_dict[class_name]) truncated.append(int(obj['truncated'])) poses.append(obj['pose'].encode('utf8')) example = tf.train.Example(features=tf.train.Features(feature={ 'image/height': dataset_util.int64_feature(height), 'image/width': dataset_util.int64_feature(width), 'image/filename': dataset_util.bytes_feature( data['filename'].encode('utf8')), 'image/source_id': dataset_util.bytes_feature( data['filename'].encode('utf8')), 'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')), 'image/encoded': dataset_util.bytes_feature(encoded_jpg), 'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')), 'image/object/bbox/xmin': dataset_util.float_list_feature(xmin), 'image/object/bbox/xmax': dataset_util.float_list_feature(xmax), 'image/object/bbox/ymin': dataset_util.float_list_feature(ymin), 'image/object/bbox/ymax': dataset_util.float_list_feature(ymax), 'image/object/class/text': dataset_util.bytes_list_feature(classes_text), 'image/object/class/label': dataset_util.int64_list_feature(classes), 'image/object/difficult': dataset_util.int64_list_feature(difficult_obj), 'image/object/truncated': dataset_util.int64_list_feature(truncated), 'image/object/view': dataset_util.bytes_list_feature(poses), })) return example def create_tf_record(output_filename, label_map_dict, annotations_dir, image_dir, examples): \"\"\"Creates a TFRecord file from examples. Args: output_filename: Path to where output file is saved. label_map_dict: The label map dictionary. annotations_dir: Directory where annotation files are stored. image_dir: Directory where image files are stored. examples: Examples to parse and save to tf record. \"\"\" writer = tf.python_io.TFRecordWriter(output_filename) for idx, example in enumerate(examples): if idx % 100 == 0: logging.info('On image %d of %d', idx, len(examples)) # path = os.path.join(annotations_dir, 'xmls', example + '.xml') path = os.path.join(annotations_dir, example + '.xml') if not os.path.exists(path): logging.warning('Could not find %s, ignoring example.', path) continue with tf.gfile.GFile(path, 'r') as fid: xml_str = fid.read() xml = etree.fromstring(xml_str) data = dataset_util.recursive_parse_xml_to_dict(xml)['annotation'] tf_example = dict_to_tf_example(data, label_map_dict, image_dir) writer.write(tf_example.SerializeToString()) writer.close() # TODO: Add test for pet/PASCAL main files. def main(_): data_dir = FLAGS.data_dir label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path) logging.info('Reading from Pet dataset.') image_dir = os.path.join(data_dir, 'images') annotations_dir = os.path.join(data_dir, 'annotations') examples_path = os.path.join(annotations_dir, 'trainval.txt') examples_list = dataset_util.read_examples_list(examples_path) # Test images are not included in the downloaded data set, so we shall perform # our own split. random.seed(42) random.shuffle(examples_list) num_examples = len(examples_list) num_train = int(0.7 * num_examples) train_examples = examples_list[:num_train] val_examples = examples_list[num_train:] logging.info('%d training and %d validation examples.', len(train_examples), len(val_examples)) train_output_path = os.path.join(FLAGS.output_dir, 'pet_train.record') val_output_path = os.path.join(FLAGS.output_dir, 'pet_val.record') create_tf_record(train_output_path, label_map_dict, annotations_dir, image_dir, train_examples) create_tf_record(val_output_path, label_map_dict, annotations_dir, image_dir, val_examples) if __name__ == '__main__': tf.app.run() python object_detection/dataset_tools/create_pet_tf_record.py \\ --label_map_path=object_detection/data/pet_label_map.pbtxt \\ --data_dir=`pwd` \\ --output_dir=`pwd`","title":"creating TFR datasets"},{"location":"deeplearning/#create-model-config-file","text":"# Faster R-CNN with Resnet-101 (v1), configuration for MSCOCO Dataset. # Users should configure the fine_tune_checkpoint field in the train config as # well as the label_map_path and input_path fields in the train_input_reader and # eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that # should be configured. model { faster_rcnn { num_classes: 90 image_resizer { keep_aspect_ratio_resizer { min_dimension: 600 max_dimension: 1024 } } feature_extractor { type: 'faster_rcnn_resnet101' first_stage_features_stride: 16 } first_stage_anchor_generator { grid_anchor_generator { scales: [0.25, 0.5, 1.0, 2.0] aspect_ratios: [0.5, 1.0, 2.0] height_stride: 16 width_stride: 16 } } first_stage_box_predictor_conv_hyperparams { op: CONV regularizer { l2_regularizer { weight: 0.0 } } initializer { truncated_normal_initializer { stddev: 0.01 } } } first_stage_nms_score_threshold: 0.0 first_stage_nms_iou_threshold: 0.7 first_stage_max_proposals: 300 first_stage_localization_loss_weight: 2.0 first_stage_objectness_loss_weight: 1.0 initial_crop_size: 14 maxpool_kernel_size: 2 maxpool_stride: 2 second_stage_box_predictor { mask_rcnn_box_predictor { use_dropout: false dropout_keep_probability: 1.0 fc_hyperparams { op: FC regularizer { l2_regularizer { weight: 0.0 } } initializer { variance_scaling_initializer { factor: 1.0 uniform: true mode: FAN_AVG } } } } } second_stage_post_processing { batch_non_max_suppression { score_threshold: 0.0 iou_threshold: 0.6 max_detections_per_class: 100 max_total_detections: 300 } score_converter: SOFTMAX } second_stage_localization_loss_weight: 2.0 second_stage_classification_loss_weight: 1.0 } } train_config: { batch_size: 1 optimizer { momentum_optimizer: { learning_rate: { manual_step_learning_rate { initial_learning_rate: 0.0003 schedule { step: 900000 learning_rate: .00003 } schedule { step: 1200000 learning_rate: .000003 } } } momentum_optimizer_value: 0.9 } use_moving_average: false } gradient_clipping_by_norm: 10.0 fine_tune_checkpoint: \"PATH_TO_BE_CONFIGURED/model.ckpt\" from_detection_checkpoint: true data_augmentation_options { random_horizontal_flip { } } } train_input_reader: { tf_record_input_reader { input_path: \"PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100\" } label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\" } eval_config: { num_examples: 8000 # Note: The below line limits the evaluation process to 10 evaluations. # Remove the below line to evaluate indefinitely. max_evals: 10 } eval_input_reader: { tf_record_input_reader { input_path: \"PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010\" } label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\" shuffle: false num_readers: 1 }","title":"create model config file"},{"location":"deeplearning/#train","text":"python train.py --logtostderr --train_dir=/home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/train_dir --pipeline_config_path=/home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/net.config tensorboard --logdir /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/train_dir/","title":"train"},{"location":"deeplearning/#evaluator","text":"python new_eval.py --logtostderr --checkpoint_dir /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/train_dir --eval_dir /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/eval_dir --pipeline_config_path /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/net.config tensorboard --logdir /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/eval_dir/ # new_eval.py import functools import os import tensorflow.compat.v1 as tf from tensorflow.python.util.deprecation import deprecated from object_detection.builders import dataset_builder from object_detection.builders import graph_rewriter_builder from object_detection.builders import model_builder from object_detection.legacy import evaluator from object_detection.utils import config_util from object_detection.utils import label_map_util from tensorflow.compat.v1 import ConfigProto from tensorflow.compat.v1 import InteractiveSession import keras.backend as K config = ConfigProto() config.gpu_options.allow_growth = True session = InteractiveSession(config=config) K.set_session(session) tf.logging.set_verbosity(tf.logging.INFO) flags = tf.app.flags flags.DEFINE_boolean('eval_training_data', False, 'If training data should be evaluated for this job.') flags.DEFINE_string( 'checkpoint_dir', '', 'Directory containing checkpoints to evaluate, typically ' 'set to `train_dir` used in the training job.') flags.DEFINE_string('eval_dir', '', 'Directory to write eval summaries to.') flags.DEFINE_string( 'pipeline_config_path', '', 'Path to a pipeline_pb2.TrainEvalPipelineConfig config ' 'file. If provided, other configs are ignored') flags.DEFINE_string('eval_config_path', '', 'Path to an eval_pb2.EvalConfig config file.') flags.DEFINE_string('input_config_path', '', 'Path to an input_reader_pb2.InputReader config file.') flags.DEFINE_string('model_config_path', '', 'Path to a model_pb2.DetectionModel config file.') flags.DEFINE_boolean( 'run_once', False, 'Option to only run a single pass of ' 'evaluation. Overrides the `max_evals` parameter in the ' 'provided config.') FLAGS = flags.FLAGS @deprecated(None, 'Use object_detection/model_main.py.') def main(unused_argv): assert FLAGS.checkpoint_dir, '`checkpoint_dir` is missing.' assert FLAGS.eval_dir, '`eval_dir` is missing.' tf.gfile.MakeDirs(FLAGS.eval_dir) if FLAGS.pipeline_config_path: configs = config_util.get_configs_from_pipeline_file( FLAGS.pipeline_config_path) tf.gfile.Copy( FLAGS.pipeline_config_path, os.path.join(FLAGS.eval_dir, 'pipeline.config'), overwrite=True) else: configs = config_util.get_configs_from_multiple_files( model_config_path=FLAGS.model_config_path, eval_config_path=FLAGS.eval_config_path, eval_input_config_path=FLAGS.input_config_path) for name, config in [('model.config', FLAGS.model_config_path), ('eval.config', FLAGS.eval_config_path), ('input.config', FLAGS.input_config_path)]: tf.gfile.Copy(config, os.path.join(FLAGS.eval_dir, name), overwrite=True) model_config = configs['model'] eval_config = configs['eval_config'] input_config = configs['eval_input_config'] if FLAGS.eval_training_data: input_config = configs['train_input_config'] model_fn = functools.partial( model_builder.build, model_config=model_config, is_training=False) def get_next(config): return dataset_builder.make_initializable_iterator( dataset_builder.build(config)).get_next() create_input_dict_fn = functools.partial(get_next, input_config) categories = label_map_util.create_categories_from_labelmap( input_config.label_map_path) if FLAGS.run_once: eval_config.max_evals = 1 graph_rewriter_fn = None if 'graph_rewriter_config' in configs: graph_rewriter_fn = graph_rewriter_builder.build( configs['graph_rewriter_config'], is_training=False) evaluator.evaluate( create_input_dict_fn, model_fn, eval_config, categories, FLAGS.checkpoint_dir, FLAGS.eval_dir, graph_hook_fn=graph_rewriter_fn) if __name__ == '__main__': tf.app.run()","title":"evaluator"},{"location":"deeplearning/#from-object_detection-import-evaluator","text":"ImportError: cannot import name 'evaluator' \\ from object_detection.legacy import evaluator","title":"from object_detection import evaluator"},{"location":"deeplearning/#control-trainning-steps","text":"num_steps:600","title":"control trainning steps"},{"location":"deeplearning/#tensorflow-freeze","text":"# From tensorflow/models/research/ INPUT_TYPE=image_tensor PIPELINE_CONFIG_PATH={path to pipeline config file} TRAINED_CKPT_PREFIX={path to model.ckpt} EXPORT_DIR={path to folder that will be used for export} python object_detection/export_inference_graph.py \\ --input_type=${INPUT_TYPE} \\ --pipeline_config_path=${PIPELINE_CONFIG_PATH} \\ --trained_checkpoint_prefix=${TRAINED_CKPT_PREFIX} \\ --output_directory=${EXPORT_DIR}","title":"tensorflow freeze"},{"location":"deeplearning/#modelsave-and-load","text":"model.fit(x_train, y_train, epochs = 150, batch_size = 32,callbacks=[tensorboard_callback]) model.save('./models/model.h5') model.save_weights('./models/weights.h5') model_path = './models/model.h5' model_weights_path = './models/weights.h5' model = load_model(model_path) model.load_weights(model_weights_path) array = model.predict(point_set) or tf.keras.models.save_model( model, 'models/mymode', overwrite=True, include_optimizer=True ) model = tf.keras.models.load_model('./models/mymode')","title":"model:save and load"},{"location":"deeplearning/#cudnn","text":"from tensorflow.compat.v1 import ConfigProto from tensorflow.compat.v1 import InteractiveSession import keras.backend as K K.set_image_data_format('channels_last') K.set_learning_phase(1) config = ConfigProto() config.gpu_options.allow_growth = True session = InteractiveSession(config=config) K.set_session(session)","title":"cudnn\u5931\u8d25"},{"location":"deeplearning/#pbsave-and-load","text":"def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True): graph = session.graph with graph.as_default(): freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or [])) output_names = output_names or [] output_names += [v.op.name for v in tf.global_variables()] input_graph_def = graph.as_graph_def() if clear_devices: for node in input_graph_def.node: node.device = '' frozen_graph = tf.graph_util.convert_variables_to_constants( session, input_graph_def, output_names, freeze_var_names) return frozen_graph frozen_graph = freeze_session(K.get_session(), output_names=[out.op.name for out in model.outputs]) tf.io.write_graph(frozen_graph, './models', 'xor.pbtxt', as_text=True) tf.io.write_graph(frozen_graph, './models', 'xor.pb', as_text=False) detection_graph = tf.Graph() with detection_graph.as_default(): od_graph_def = tf.GraphDef() with tf.gfile.GFile('xor.pb', 'rb') as fid: serialized_graph = fid.read() od_graph_def.ParseFromString(serialized_graph) tf.import_graph_def(od_graph_def, name='') input = tf.get_default_graph().get_tensor_by_name('input_1:0') output = tf.get_default_graph().get_tensor_by_name('fc2/Softmax:0') with detection_graph.as_default(): with tf.Session() as sess: values =sess.run(output, feed_dict={input: point_set}) print(values)","title":"pb:save and load"},{"location":"deeplearning/#_1","text":"print('model.inputs :',model.inputs) print('model.outputs : ',model.outputs) output model.inputs : [<tf.Tensor 'input_1:0' shape=(?, 2600, 3) dtype=float32>] model.outputs : [<tf.Tensor 'fc2/Softmax:0' shape=(?, 2) dtype=float32>] \u6240\u4ee5\u8f93\u5165\u5c42\u662f input_1:0 \u8f93\u51fa\u5c42\u662f fc2/Softmax:0","title":"\u67e5\u770b\u6a21\u578b\u7684\u8f93\u5165\u5c42\u548c\u8f93\u51fa\u5c42"},{"location":"deeplearning/#tensorrt","text":"\u4e0b\u8f7d\u4e0e\u60a8\u4f7f\u7528\u7684Ubuntu\u7248\u672c\u548cCPU\u67b6\u6784\u5339\u914d\u7684TensorRT\u672c\u5730repo\u6587\u4ef6\u3002 \u4eceDebian\u672c\u5730repo\u8f6f\u4ef6\u5305\u5b89\u88c5TensorRT\u3002 os=\"ubuntu1x04\" tag=\"cudax.x-trt7.x.x.x-ga-yyyymmdd\" sudo dpkg -i nv-tensorrt-repo-${os}-${tag}_1-1_amd64.deb sudo apt-key add /var/nv-tensorrt-repo-${tag}/7fa2af80.pub sudo apt-get update sudo apt-get install tensorrt \u5982\u679c\u4f7f\u7528Python 2.7\uff1a\\ sudo apt-get install python-libnvinfer-dev \\ \u5c06\u5b89\u88c5\u4ee5\u4e0b\u9644\u52a0\u8f6f\u4ef6\u5305\uff1a\\ python-libnvinfer \\ \u5982\u679c\u4f7f\u7528Python 3.x\uff1a\\ sudo apt-get install python3-libnvinfer-dev \\ \u5c06\u5b89\u88c5\u4ee5\u4e0b\u9644\u52a0\u8f6f\u4ef6\u5305\uff1a\\ python3-libnvinfer \\ \u5982\u679c\u60a8\u6253\u7b97\u5c06TensorRT\u4e0eTensorFlow\u7ed3\u5408\u4f7f\u7528\uff1a\\ sudo apt-get install uff-converter-tf \\ \u5982\u679c\u60a8\u8981\u8fd0\u884c\u9700\u8981ONNX\u7684\u793a\u4f8b \u56fe\u5f62\u5916\u79d1\u533b\u751f \u6216\u5c06Python\u6a21\u5757\u7528\u4e8e\u60a8\u81ea\u5df1\u7684\u9879\u76ee\uff0c\u8fd0\u884c\uff1a\\ sudo apt-get install onnx-graphsurgeon \\","title":"\u5b89\u88c5\uff54\uff45\uff4e\uff53\uff4f\uff52RT"},{"location":"deeplearning/#anaconda-tensorrt","text":"\u4e0b\u8f7dtar\u6587\u4ef6 TensorRT-7.0.0.11.Ubuntu-18.04.x86_64-gnu.cuda-10.0.cudnn7.6.tar.gz (wind1) star@xmatrix:~$ (wind1) star@xmatrix:~$ (wind1) star@xmatrix:~$ cd TensorRT (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ ls TensorRT TensorRT_1 TensorRT-7.0.0.11 TensorRT-7.0.0.11.Ubuntu-16.04.x86_64-gnu.cuda-10.0.cudnn7.6.tar.gz TensorRT-7.0.0(1).tar.gz (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ (wind1) star@xmatrix:~/TensorRT$ cd TensorRT-7.0.0.11 (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ ls bin data doc graphsurgeon include lib python samples targets TensorRT-Release-Notes.pdf uff (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ cd python (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ ls tensorrt-7.0.0.11-cp27-none-linux_x86_64.whl tensorrt-7.0.0.11-cp35-none-linux_x86_64.whl tensorrt-7.0.0.11-cp37-none-linux_x86_64.whl tensorrt-7.0.0.11-cp34-none-linux_x86_64.whl tensorrt-7.0.0.11-cp36-none-linux_x86_64.whl (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ pip --version pip 19.3.1 from /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages/pip (python 3.6) (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ pip3 --version pip 19.3.1 from /usr/local/lib/python3.5/dist-packages/pip (python 3.5) (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ python -m pip install tensorrt-7.0.0.11-cp36-none-linux_x86_64.whl Processing ./tensorrt-7.0.0.11-cp36-none-linux_x86_64.whl Installing collected packages: tensorrt Successfully installed tensorrt-7.0.0.11 (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ python Python 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) [GCC 7.3.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> >>> >>> import tensorrt >>> >>> exit(); (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ cd ../ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ ls bin data doc graphsurgeon include lib python samples targets TensorRT-Release-Notes.pdf uff (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ cd uff (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ ls uff-0.6.5-py2.py3-none-any.whl (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ python -m pip install uff-0.6.5-py2.py3-none-any.whl Processing ./uff-0.6.5-py2.py3-none-any.whl Requirement already satisfied: numpy>=1.11.0 in /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages (from uff==0.6.5) (1.16.4) Requirement already satisfied: protobuf>=3.3.0 in /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages (from uff==0.6.5) (3.11.2) Requirement already satisfied: six>=1.9 in /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages (from protobuf>=3.3.0->uff==0.6.5) (1.13.0) Requirement already satisfied: setuptools in /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages (from protobuf>=3.3.0->uff==0.6.5) (42.0.2.post20191203) Installing collected packages: uff Successfully installed uff-0.6.5 (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ python Python 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) [GCC 7.3.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> >>> >>> import tensorrt >>> import uff WARNING:tensorflow:From /home/star/anaconda3/envs/wind1/lib/python3.6 7.0.0.11 (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ (wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$","title":"anaconda tensorRT"},{"location":"deeplearning/#using-uff-converter-to-convert-the-frozen-tensorflow-model-to-a-uff-file","text":"conda activate wjj \\ pip install nvidia-pyindex \\ pip install uff \\ you need to find your uff installed path. \\ import uff print(uff.__path__) And after locating it , in it\u2019s bin folder there should be a script named as convert_to_uff.py . And now you need to open the terminal and simply type python3 convert_to_uff.py < path to the saved model >\\ In my case-->\\ python3 convert_to_uff.py /home/models/catsAndDogs.pb \\ And it will simply save the converted .uff in your .pb model location. And then this is how the next script should be done.","title":"Using UFF converter to convert the frozen tensorflow model to a UFF file"},{"location":"deeplearning/#nvidia","text":"NVIDIA\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6","title":"NVIDIA\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6"},{"location":"deeplearning/#could-not-create-cudnn-handle-cudnn_status_internal_error","text":"Use allow_growth memory option in TensorFlow and Keras, before your code. For Keras import tensorflow as tf config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True config.log_device_placement = True sess = tf.compat.v1.Session(config=config) tf.compat.v1.keras.backend.set_session(sess) For TensorFlow import tensorflow as tf config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True config.log_device_placement = True sess = tf.compat.v1.Session(config=config) \u8fd8\u6709\u4e00\u79cd\u60c5\u51b5,\u51cf\u5c11batch size","title":"Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR"},{"location":"deeplearning/#anchor-boxes","text":"One of the hardest concepts to grasp\u628a\u63e1 when learning about Convolutional Neural Networks for object detection is the idea of anchor boxes.It is also one of the most important parameters you can tune for improved performance on your dataset.In fact,if anchor oxes are not tuned correctly,your neural network will never even know that certain\u67d0\u4e9b small,large or irregular\u4e0d\u89c4\u5219 objects exist and will never have a chance to detect them.Luckily, there are some simple steps you can take to make sure you do not fall into this trap\u9677\u9631. what are anchor boxes? \\ when you use a neural network like yolo or ssd to predict multiple objects in a picture, the network is actually making thousands of predictions and only showing the ones that it decided were an object.The multiple predictions are output the following format:\\ Prediction 1: (X,Y,Height,Width),Class\\ ...\\ Prediction ~8000: (X,Y,Height,Width),Class Where the (X,Y,Height,Width) is called the \"bounding box\", or box surrounding the objects.This box and the object class are labelled manually by human annotators. In an extremely simplified example, imagine that we have a model that has two predictions and receives the following image:\\ We need to tell our network if each of its predictions is correct or not in order for it to be able to learn.But what do we tell the neural network it prediction should be? Should the predicted class be:\\ Prediction 1:Pear\\ Prediction 2:Apple Or should it be:\\ Prediction 1:Apple\\ Prediction 2:Pear What if the network predicts:\\ Prediction 1:Apple\\ Prediction 2:Apple We need our network's two predictors to be able to tell whether it is their job to predict the pear or the apple.To do this there are a several tools.Predictors can specialize in certain size objects, objects with a certain aspect ratio(tall vs. wide),or objects in different parts of the image.Most networks use all three criteria\u6807\u51c6.In our example of the pear/apple image,we could have Prediction 1 be for objects on the left and Prediction 2 for objects on the right side of the image.Then we would have our answer for what the network should be predict:\\ Prediction 1:Pear\\ Prediction 2:Apple Anchor Boxes in Practice\\ State of the art\u6700\u5148\u8fdb\u7684 object detection systems currently do the following:\\ 1. Create thousands of \"anchor boxes\" or \"prior boxes\" for each predictor that represent the ideal location, shape and size of the object it specializes\u4e13 in predicting. 2. For each anchor box,calculate which object's bounding box has the highest overlap divided by non-overlap.This is called Intersection Over Union or IOU. 3. If the highest IOU is greater than 50%, tell the anchor box that it should detect the object that gave the highest IOU. 4. Otherwise if the IOU is greater than 40%, tell the neural network that the true detection is ambiguous and not to learn from that example. 5. If the highest IOU is less than 40%,then the anchor box should predict that there is no object. This works well in practice and the thousands of predictors do a very good job of deciding whether their type of object apperars in an image. Using the default anchor box configuration can create predictors that are too specialized and ojects that appear in the image may not achieve an IOU of 50% with any of the anchor boxes.In this case, the neural network will never know these objects existed and will never learn to predict them. We can tweak\u8c03\u6574 our anchor boxes to be much smaller In xx net configuration, the smallest anchor box size is 32x32.This means that many objects smaller than this will go undetected. We can fix this by changing our default anchor box configurations. Reducing the smallest anchor box size, all of the things line up with at least one of our anchor boxes and our neural network can learn to detect them! Improving Anchor Box Configuration \\ As a general rule,you should ask yourself the following questions about your dataset before diving into training your model: 1. What is the smallest size box I want to be able to detect? 2. What is the largest size box I want to be able to detect? 3. What are the shapes the box can take? For example, a car detector might have short and wide anchor boxes as long as there is no chance of the car or the camera being turned on its side. You can get a rough estimate of these by actually calculating the most extreme\u6781\u7aef sizes\u3000and aspect ratios in the dataset.Yolo V3 uses K-means to estimate the ideal bounding boxes.Another option is to learn the anchor box configuration. Once you have thought through these questions you can start designing your anchor boxes. Be sure to test them by encoding your ground truth bounding boxes\u5730\u9762\u771f\u503c\u8fb9\u754c\u6846 and then decoding them as though\u5c31\u50cf they were predictions from your model.You should be able to recover the ground truth bounding boxes.","title":"Anchor boxes"},{"location":"deeplearning/#autonomous-self-learning-systems","text":"","title":"Autonomous self-learning systems"},{"location":"deeplearning/#preface","text":"This report describes the various processes that are part of the work on the main project at Oslo and Akershus University College(HIOA), department of engineering education,spring 2015. The report deals with the development of a self-learning algorithm and a demonstrator in the form of a robot that will avoid static and dynamic objects in an environment that is constantly changing.The thesis\u8bba\u6587 is given by HIOA.The report addresses the theory behind the most well-known and used self-learning algorithms,and discusses the advantages,disadvantages and uses of theese.It also contains a description of the technical solution for the demonstrator, and the method used in this project. The report is written within a topic that is considered new technical and is therefore assumed to be able to be used for futher research and/or learning within autonomous self-learning system. The reader is expected to have basic knowledge in electronics and information technology. We would like to thank our employer.Oslo and Akershus University College,for the opportunity to carry out the project and for financial support.We would also like to thank supervisor EZ for a good collaboration, as well as important and constructive guidance throughout the project period.","title":"Preface"},{"location":"deeplearning/#summary","text":"In today's society, self-learning systems are an increasingly relevant topic.Systems that are not explicity programmed to perform a specific task, but are even able to adapt,can be very useful. The system described in this thesis is realized with Q-learning by both the table method and the neural network.Q-learning is a learning algorithm based on experience.The algorithm involves an agent exploring his environment, where the environment is represented by a number of states. The agent experiments withs the environment by performing an action, and then observes the consequence of that action.The consequence is given in the form of a positive or negative reward. The goal of the method is to maximize the accumulated reward over time. Autonomous self-learning systems are becoming increasingly relevant because the system is able to adapt to partially or completely unknown situations.It learns from experience and needs less information at start-up as it acquires information along the way.In autonomous self-learning systems and self-propelled robots, avoiding obstacles is a key task.This report addresses a demonstrator of such a system, realized with Q-learning presented later in the reportk, and provides a description of the algorithm and results.","title":"Summary"},{"location":"deeplearning/#theory","text":"Before looking at the structure of a self-learning system, one can advantageouly look at what the concept of learning is.Learning is often defined as a lasting change in behavior as a result of experience(St. Olavs Hospital,undated).The property of organisms\u751f\u7269\u4f53 that is defined as learning is one of the cornerstones of what is called intelligence which, among other things, is defined as an ability to acquire and apply knowledge and skills.Humans and animals are considered intelligent, among other things,based on their ability to learn from experience.","title":"Theory"},{"location":"deeplearning/#machine-learning","text":"This subchapter is based on the theory of S.M,2009.Machine learning is a form of artifical intelligence that focuses on the development of self-learning algorithms. In most cases,self-learning systems deal with parts of natural intellignece, including memory,adaptation and generalization\u6982\u62ec.Unlike traditional non-learning systems, the method makes it possible to construct a system that is able to expand , adapt to new information and learn a given task without being specifically programmed for this.For machine learning,this system is called an agent.By using menmory,an agent can recognize the last time it was in a similiar situation and what action it took.Based on the outcome from the previous time, it can, if it was correct,choose to repeat the action, or try something new.By generallizing, the agent can recognize similarities in different situations, and thus use experienct from one situation and apply this experience in another. In order to realize\u5b9e\u73b0 this concept, machine learning uses principles\u539f\u7406 from statistics, mathematics, physics, neurology and biology. \uff37hen talking about machine learning and self-learning systems,algorithms are mainly the main product.The actual process in these algorithms can be compared to data mining\u6316\u6398.Data mining is a process that analyzes data from different perspectives and summarizes if into useful information.Both methods go through data to find patterns\u6a21\u5f0f,but instead of extractiong data for human interpretation,the information is used to improve the agent's understanding.For the agent to be able to learn, it must know how to improve , and whether it will get better or not.There are 15 more methods to solve this, which in turn provide servel main categories within machine learning:Supervised learning,unsupervised learning and reinforcement learning.","title":"machine learning"},{"location":"deeplearning/#supervised-learning","text":"An agent is given a training set with ,for example,pictures of a face and pictures without a face.The agent then prepared through a training process where it gives a forecast of what the picture is of. Whenever the forecast is incorrect, the agent is corrected.This process continues untill the model achieves a desired level of accuracy. Since the algorithm does not have a specific definition of what is a face and what is not, it must therefore learn this using examples.A good algorithm will eventually be able to estimate whether an image is of a face or not.The learning methods are best explained by examples:","title":"Supervised Learning"},{"location":"deeplearning/#unsupervised-learning","text":"When learning without supervision, information is not marked.This is ,the system is not told what is the image of a face and what is not.As a result, there is no correction or reward to indicate a potential solution, but the algorithm tries to identify the similarities between the images, swfor then categorize them and divide them into groups.","title":"Unsupervised Learning"},{"location":"deeplearning/#reinforcement-learning","text":"In reinforcement learning,the algorithm is told when the answer it gives is incorrect, but receives no suggestions on how to correct this.It must explore and try out different solutions untill it finds out how it gets the right answers.This is a kind of middle ground of supervised learning and unsupervised learning.Examples could be learning to play a board game or a robot that is going to learn to walk.Each time an agent performs an action,he or she recieves a reward or a penalty, based on how desirable\u53ef\u53d6\u7684 the outcome of the action is.For example.when an agent is trained to play a board game, he gets a positive reward for winning, and a negative reward for losing.All other cases give no reward.\\ \\ The activity mentioned above can be represent as a sequence of state-action reward:\\ \\ This means that the agent was in state s0, performed action a0,which resulted in it receiving reward r1 and ending up in state s1.Furthermore, it performed action a1, received reward r2, and ended up in state s2, and so on.\\ \uff34his sequence is made up of experiences where experience is given as:\\ \\ Experience shows that the agent was in state s,performed action a,received reward r, and ended up in state s', and represented by (s,a,s',r) In order for the agent to be able to learn from the sequences mentioned and thus call it an experience.It has a table called Q-table which acts as its memory.All data points stored in this table are called Q-values and represent how desirable\u53ef\u53d6\u7684 it is to perform a specific action in a specific state. One experience adds one data point Q(s,a) in the table that represents the agents current estimate\u4f30\u8ba1 of the optimal\u6700\u4f73 Q value. It is this information that the agent uses to learn an optimal pattern of action.The size of the table depends on how many conditions and actions are included in the problem you are trying to solve, where the number of conditions gives the number of rows ,while the number of actions gives the number of colums.For example if you have 20 states and 3 actions, you will get a table of 20x3. Before the agent begins to experiment, it knows nothing else what actions it is capable of performing, and the Q table is consequently empty.This is, all Q values are equal to 0. The key to the method described above is to update the Q value that is applied to the agent when it performs an action in a given state in the current data point when it gains an experience. This value is given by the Q function\\ \\ or more clearly:\\ \u03b1\uff0d\uff0dlearning rate\\ \u0393\uff0d\uff0ddiscount factor\\ r\uff0d\uff0dreward\\ The learning rate dicates\u6307\u793a how much of previouly\u5148\u524d acquired learning should be replaced with new information. By \u03b1=1,previous values are replaced with new information.while \u03b1=0 ,corresponds to no update.In other words,the agent will konw by \u03b1=1,assume that the last reward and resulting state are representative of future values. The discount factor indicates the weight of all futher step rewards.If \u0393=0,the agent will only consider current rewards, while it will think more long-term\u957f\u671f and strive\u52aa\u529b for higher future rewards as \u0393 approaches 1. The reward is defined when you create the program in the form of reward functions\u5956\u52b1\u662f\u5728\u521b\u5efa\u7a0b\u5e8f\u65f6\u4ee5\u5956\u52b1\u51fd\u6570\u7684\u5f62\u5f0f\u5b9a\u4e49\u7684,and can be positive or negative. What this reward value is defined as is not critical.On the other hand, It is important that there is a clear distinction\u533a\u522b between the reward for good deeds\u884c\u4e3a and the reward for bad deeds. When a new experience is added to the algorithm, a new Q value is estimated, and the old value of the Q table for the last experience is updated with the new one. Assume that the agent is a mouse that is placed in a room divded into six states, see below Figure.In state s6 there is a piece of cheese,while in state s5 there is a cat.If the agent finds the piece of cheese, it receives a reward of 10, while it receives a reward of -10 if it moves to the state where the cat is.These two states are called terminal states.The agent explores the environment untill one of the terminal states is reached,before starting a new iteration(attempt)\\ \uff34he agent can perform four actions: up,down,left,right.If it moves in a direction where there is a wall, it gets a reward of -1, and it remains in the same state. All other conditions have a value equal to 0.This is implemented in the algorithm using reward functions.\\ \\ The agent has no information about his surroundings other than that there are six states and four acitons, what state he is in at any given time and any rewards it receives as actions are performed.Nor does it know what a reward or punishment is.The Q table is illustrated in Table 1. By assuming the following sequences of experiences,(s,a,r,s') one can illustrate how a sequence updates the Q-table. For the sake of illustration, the learning rate \u03b1 and the discount factor \u0393 set equal to 1 and all Q-values have an initial value equal to 0.\\ After this sequence of experiences, the Q table with updated values will look like this:\\ \\ opp=up ned=down venstre=left h\u00d8yre=right Each experience will update the value of the state table for the performed state and action combination, and the Q values will converge\u6c47\u805a to optimal values over time.The more experiments the agent conducts\u884c\u4e3a, the better the estimates in the Qtable.In theory, the agent will eventually achieve an optimized Q-table, and will be able to choose the shortest path to the cheese each time, regardless of the starting state.","title":"Reinforcement Learning"},{"location":"deeplearning/#exploration-or-utilize","text":"One of the challenges with the Q-learning algorithm is that it does not directly tell what the agent should do, but rather functions as a tool that shows the agent what the optimal action is in a given condition.This is a challenge because the agent must explore the environment enough times to be able to build a solid foundation\u575a\u5b9e\u7684\u57fa\u7840 that provides a good estimate of the Q values.To address this, an exploration feature is implemented that determines the strategy\u6218\u7565 the agent will use to select actions, either explore or exploit. Exploration:The agent is bold\u80c6\u5927 and does not necessary choose the best course of action, with the intension of establishing a better estimate of the Q values. Utilize: The agent utilizes the experience it has already built up, and selects the optimal action for the condition it is in. That is, the action that gives the highest Q value. It is said that the agent is greedy\u8d2a\u5a6a The purpose of this feature is to establish a relationship between the two strategies. It is important to explore enough so that the agent builds a solid foundation of the Q-values, but it is also important to utilize the already acquired knowledge to ensure the highest possible reward. There are a number of ways to accomplish this, but two of the most popular methods are the greedy feature and the Boltzmann distribution, popularly called \"softmax\":","title":"Exploration or Utilize \u52d8\u63a2\u6216\u5f00\u53d1"},{"location":"deeplearning/#-greedy","text":"\u03b5-greedy is a strategy based on always choosing the optimal action except for \u03b5 of the aisles and choosing a random action of the aisles.\\ \\ there\\ a* is optimal action with probability 1-\u03b5\\ ar is random action with probability \u03b5\\ where\\ 0< \u03b5 < 1 It is possible to change over time. This allows the agent to choose a more random strategy at the beginning of the learning process, and then become more greedy as it acquires more knowledge. One of the challenges with greed is that it considers all actions except the optimal one as equal.\u5b83\u5c06\u6700\u4f73\u884c\u52a8\u4e4b\u5916\u7684\u6240\u6709\u884c\u52a8\u90fd\u89c6\u4e3a\u5e73\u7b49 If, in addition to an optimal action, there are two good actions as well as additional actions that look less promising,\u8fd8\u6709\u4e24\u4e2a\u597d\u7684\u52a8\u4f5c\u4ee5\u53ca\u770b\u8d77\u6765\u4e0d\u592a\u6709\u524d\u9014\u7684\u5176\u4ed6\u52a8\u4f5c\uff0c it will make sense\u6709\u610f\u4e49 for the agent to choose one of the two better ones instead of spending time exploring the bad actions. Using greed is as likely to explore a bad action as to explore one of the two better ones. One way to solve this is to use the Boltzmann distribution 2.3.2 Boltzmann distribution This strategy is known as \"softmax\" action selection and involves selecting an action with a probability that depends on the Q value. The probability of selecting action a in state s is proportional to\u3000 ,which means that the agent in state s chooses action a with probability \\ The parameter T specifies how random values are to be selected.\u53c2\u6570T\u6307\u5b9a\u5982\u4f55\u9009\u62e9\u968f\u673a\u503c\u3002 When T has a high value, actions are chosen to approximately the same extent.\u9009\u62e9\u7684\u52a8\u4f5c\u7684\u7a0b\u5ea6\u5927\u81f4\u76f8\u540c As T decreases, the actions with the highest Q value are more likely to be selected, while the best action is always selected when T-> 0 One of the advantages of the Q-learning algorithm is that it is exploration insensitive\u5bf9\u63a2\u7d22\u4e0d\u654f\u611f. That is, the Q values will converge to optimal values regardless of the agent's behavior as long as information is collected. This is one of the reasons why Q-learning is one of the most widely used and effective methods in reinforcement learning. On the other hand, it is a prerequisite that all combinations of condition / action are tried out many enough times, which in most cases means that the exploration and exploitation aspect must be addressed as it is not always possible for an agent to experiment infinitely. There are several methods for setting up an artificial neural network.One of the most common,which is also used in this project, is called a feedforward neural network, which means that the information is always fed from the input page and supplied further through the network.In other words,output from one layer + bias is used as input in the next.","title":"\u03b5-greedy"},{"location":"deeplearning/#fuzzy-logic","text":"The theory of fuzzy logic is based on S.K.J(2007)\\ Fuzzy logic is a decision-making or management model based on diffuse\u3000\u6269\u6563\uff0c\u6563\u53d1,and input data classified as degrees of membership in a diffuse quantity. A diffuse set has no sharp boundaries, and is a set of values that belong to the same definition, where the definition is a diffuse term often described by an adjective\u5f62\u5bb9\u8bcd. For example: big potatoes or tall people.A diffuse set is also called a semantically\u8bed\u4e49\u4e0a variable or set. Fuzzy logic classifies input data according to how large a membership the input data has to a set.The degree of membership is graded\u5206\u7ea7 between 0 and 1,and determined on the basis of a membership function\u6839\u636e\u96b6\u5c5e\u51fd\u6570\u786e\u5b9a.\uff34he member ship function is determined on the basis of knowledge of the field, and can be non-linear.Input data can have membership in servel different sets, but can only have full membership in one set.","title":"Fuzzy Logic\u6a21\u7cca\u7406\u8bba"},{"location":"deeplearning/#fuzzy-logic-behavior","text":"Diffusion\u6269\u6563: input data is graded according to the diffused quantities,and the diffused quantities are sent to subprocess 2 Logical rules:the diffused quantites are tested against rules.The degree of fulfillment \u5c65\u884c from each rule is sent to sub-process 3.The rules can be implented with {and, if, or},or other methods such as an impact\u5f71\u54cd matrix.The rules describe the relationship between the diffused input and output. Implication: the received data from sub-process 2 is tested againist rules that grade the impact on the overall consequence of each of the received data.The ratings are then sent to sub-process 4. Aggregation\u805a\u5408:all the consequence\u540e\u679c quantities are compiled into an aggregate quantity\u603b\u91cf,which is the union of the quantities from subprocess 3. A vdiffusion: The most representative element is extracted from the aggregate amount.This is usually the center of gravity\u91cd\u529b.That value will then be the control signal. The Fuzzy logic method has a number of advantages in that it is able to process imprecise\u4e0d\u7cbe\u786e data and model non-linear relationships between input and output.The principle enables an accurate and realistic\u73b0\u5b9e\u7684 classification of data, as data is rarely\u5f88\u5c11 only true or false.","title":"Fuzzy logic behavior"},{"location":"deeplearning/#technical-solution","text":"","title":"Technical solution"},{"location":"deeplearning/#agents","text":"Two versions of the agent were developed.The first version,which was later replaced, was built on the basis of drivetrain\u52a8\u529b\u603b\u6210 and mechanics\u673a\u68b0\u5e08 taken from 'WLtoys' model 'A969'. The radio-controlled car in scale 1:18 was delivered\u5df2\u4ea4\u4ed8 to drive with ratio and battery.The car measured 285x140x105mm(LxWxH) and could run for about 10 minutes on a fully charged battery. Since the car was to be equiped with an Xbee module and communicate via bluetooth.the car's original reciever was removed.The steering servo\u8f6c\u5411\u4f3a\u670d was adapted to the car's receiver and was therefore replaced in favour of a PWM-controllable servo.To improve the agent's running time, the original battery(7.4V/1100mAh) was replaced with a 7.4V/5200mAh \"Fire Bull\" type battery, which would thoretically provide approximately five times as long. To make room for all the electronics, a plate 1.5mm aluminum\u94dd was cut out and mounted on top of the car.The motor driver, microcontroller board and Xbee module were placed together with the motor and servo on the classis itself, while the battery, sensors and voltage regulator were mounted\u5b89\u88c5 on the new plate for easier access. After the mentioned changes, the agent had improved battery life, adjustable speed and the correct physical size, but it turned out not to have a good enough turning radius. It had particular problems in situations where it had to react in one direction, and then change direction. The car's turning radius was tested before the conversion was started, and it was measured that the car need 70cm to turn 90 degrees to the left or right(figure 3-4).It thus needed an area of 140cm in diameter to turn 180 degrees.It was therefore assumed that a round arena\u7ade\u6280\u573a with a diameter of about four meters was needed.The area was built,and it was found that four meters was enough to be able to drive around without obstacles,but that it became too samll when car had to avoid objects. The ultrasonic sensors are limited, through discretion\u901a\u8fc7\u659f\u914c,to integer values between 1cm and 120cm, If one is to create a look-up table for the entire spectrum\u8303\u56f4, the table will consist of 120^5 different combinations, and would with three possible actions end up with a look-up table of 120^5x3 number of places. This is a less good solution as the table would be very large(74 billion seats).A large lookup table means that the agent spends much more time learning all the combinations of conditions, and actions for these.In addition, such a solution is far too large to be implemented on a normal computer.This chanllenge is solved by discretizing\u79bb\u6563\u5316 or dividing, the continuous sensor area into smaller and more managerable state spaces.","title":"Agents"},{"location":"deeplearning/#421-condition-space-model","text":"All sensors can measure distances up to 400 cm. To avoid a large state table and to streamline the learning process, the sensor values are discretized down to 144 states. For the project described in this report, this also simplifies the reward function. Each snsor is divided into four zones based on distance in centimeters. The division is shown in Figure 4.4\\ \\ A condition consists of four parameters. Two for the left side and two for the right side of the agent's field of view.A state is a combination of distance to the nearest obstacle and in which sector the obstacle is located .a sector describes the angle at which an object is located, relative to the agent. The four parameter are defined by:\\ k1:zone\u533a left side \u533a\u57df\u5de6\u4fa7\\ k2:zone right side \u533a\u57df\u53f3\u4fa7\\ k3:sector\u90e8\u95e8 left side \u6247\u533a\u5de6\u4fa7\\ k4:sector right side \u6247\u533a\u53f3\u4fa7\\ k5:observation of dynamic or static obstacle left side \u89c2\u5bdf\u52a8\u6001\u6216\u9759\u6001\u969c\u788d\u7269\u5de6\u4fa7\\ k6:observation of dynamic or static obstacle right side \u89c2\u5bdf\u52a8\u6001\u6216\u9759\u6001\u969c\u788d\u7269\u53f3\u4fa7\\ S(state space model) = k1 x k2 x k3 x k4 x (k5 x k6)\\ The agent can perceive\u611f\u77e5 two obstacles simultaneously, but not on the same page.With two obstacles on the same side,the algorithm will prioritize\u4f18\u5148 the obstacle that is in the zone with the lowest numbering. The division of the sectors is illustrated in Figure below\\ \\ sector\u6247\u533a\\ The sectors have four different values, 0,1,2,3.The sensors perceive\u611f\u77e5 an object when the zone value is less than 2.The sectors are symmetrically\u5bf9\u79f0\u5730 distributed\u5206\u5e03.so that the agent's right field of view is discretized in the same way as the left.Sensors 1,2 and 3 on the left side correspond to sensors 3,4 and 5 for the right side.","title":"4.2.1 Condition space model"},{"location":"deeplearning/#4211-discretion-for-system-with-dynamic-obstacles","text":"In the simulation for a system with dynamic obstacles, the state space model is expanded with two additional parameters,k5 and k6. k5 indicates with the value 1 or 0 whether the obstacle detected on the left side is dynamic, while k6 does the same for the right side.The number 1 indicates a dynamic obstacle,while 0 indicates static.The state table has thus benn expanded to 576 states.,An object os classified as a dynamic obstacle in that the agent first, while standing still,performs two measurements in a row. Then the two measurements are compared, if the obstacle moves away from the agent, the first measurement will be positive number, and classified as a static obstacle.If on the other hand, the same calculation operation gives a negative number, the obstacle is classified as a dynamic obstacle and the agent must react accordingly.","title":"4.2.11 Discretion for system with dynamic obstacles"},{"location":"deeplearning/#4212-the-state-table","text":"The state table is structured as a table where all combinations of states and actions are represented as a table point.The state table for the physical model has 144 rows, and three columns that present each of the possible actions(left,right, straightfoward). A section of the condition table is shown in Table 7.","title":"4.2.1.2 the state table"},{"location":"deeplearning/#422-reward-function","text":"The reward feature determines the agent's immmediate reward after each experience.When implementing the reward function, a distinction is made between good actions that give a positive reward value,and bad actions that give a negative reward value.The reward function used for Q-learning with the table method is inspired by and shown under equation.\\ The reward function, r1 is defined to give a positive reward value when the agent drives straight ahead, while for turns a small negative reward value is given. The negative value is given to prioritize \u4f18\u5148 driving straight ahead. The parameter r2 gives a positve reward when the total change in the sensor values is positive. That is , the agent moves away from an obstacle. If the agent moves towards an obstacle , it becomes total the change in sensor values is negatie, and the agent recieve a negative reward. The function r3 gives the agent a negative reward value if it first turns to right and then to the left, or first to the left and then to right one after the other. This feature allows the agent to avoid an indecisive\u4f18\u67d4\u5be1\u65ad\u7684 sequence, where it only swings\u6447\u6446 back and forth. \uff34he function r gives the agent a reward value equal to the sum of r1,r2 and r3 if it does not collide, while in the event of a collision it gets a negative reward value of -100. 4.2.3 Exploration function The exploration function determines the agent's strategy for choosing actions. It can either explore the state space or take advantage of the experiences it already has. The function ensures an appropriate balance between these strategies (see chapter 2.3). The simulations for the exploration methods show that softmax gives the best results in the simulated model, and is thus used in the simulation. The parameter T is set equal to 24, and is reduced by 5% of its total value for each individual experiment.In the physical model, epsilon is used because it provides a better overview of how much coincidence there is for the agent to take an arbitrary action, and epsilon is set at a 5% chance of doing a random action.","title":"4.2.2 Reward function"},{"location":"deeplearning/#424-implementation","text":"The implementation of Q-learning with the table method is based on the same algorithm and reward function in the simulation and for the physical model.The different between the simulation and the physical agent is the exploration function and an additional extension of the state space in the simulation(see the discretization\u79bb\u6563\u5316 chapter).In the simulation, you have the choice between static and dynamic obstacles, and to adapt the agent to dynamic obstacle, the Q table is expanded.","title":"4.2.4 Implementation"},{"location":"deeplearning/#4241-the-q-learning-algorithm-with-the-table-method","text":"The Q-Learning algorithm with the table method At startup: - initialize state space S - initialize the action list A - initialize the Q table Q(s,a)(empty table) based on the state space S and the action list A For each attempt: 1) Observe initial sensor values 2) Discretize the sensor values and set it equal to s(initial mode) 3) Select an action a based on the exploration function and perform the action 4) Observe the agent's sensor values after the operation 5) Discretize the sensor values and set it equal to s'(Next mode) 6) Give the agent the reward r based on the reward function 7) Update the Q table 8) Set s=s'(Now state) 9) Keep last action, prev_a =a (Used in the reward function) 10) Repeat the operation from point iii)(a new step /experience ) If the agent has not reached its terminal state(collision or maximum number of steps). Start a new attempt if the agent has reached its terminal state(Restart from i).","title":"4.2.4.1 The Q-Learning algorithm with the table method"},{"location":"deeplearning/#4242-simulated-model","text":"In the simulation, a user interface is created that shows the user the simulation of the agent, two different graphs, buttons for different functions and information panel.The information panel show the user relevant\u76f8\u5173\u7684 values from the simulation.One graph shows an average of the number of steps for every tenth attempt, while the other shows the agent's accumulated reward.One step in the program corresponds to an experienced situation,whether it has been experienced before or is new.A new attempt is initialiated each time the agent has moved the maximum number of steps for the current attempt, or has collided with an obstacle .The maximum number of steps can be changed, but is normally 400-600.The dimensions of the simulation are scaled 1\uff1a10[cm] The ultrasonic sensors on the physical agent are limited to integer values from 1 to 120cm in the code of the agents microcontrollers.Distance less than 8 cm id defined as a collision. The initial state of the physical agent is an arbitrary position in the environment.The only requirement is that the initial state is not a terminal state(collision). The commands - command 1 : request sensor values - command 2: ask the agent to drive straight ahead - command 3: ask the agent to turn right - command 4: ask the agent to turn left - command 5: ask the agent to stop, sensor values <8 cm(collision)","title":"4.2.4.2 Simulated model"},{"location":"deeplearning/#43-q-learning-with-neural-network-linear-function-approximation","text":"Q-Learning with neural network is based on approximation of one or more linear Q-functions. Unlike Q-table learning.this method stores the Q functions in a neural network. By using this method ,discretization is not necessary, and the entire continuous sensor spectrum can be used. The behavior behind an artifical neuron is described in chapter 2.3.3\\ neural network \\ \\ input layer---------------------------------hidden layer-------------------------------------output layer The figure above shows the neural network as it is implemented in this system. In the network, all the neurous, including the bias from one layer, are connected to each individual neuron in the next layer. The neural network has three layers.Layer One, the input layer, has five neurons (one for each sensor),layer two has fifteen neurons, and layer three has three neurons according to the number of actions in the action list.Layer 1 is the start of the network, and the sensor values are used as input.and these must be in the interval\u95f4\u9694[-1,1] The weight matrix w(1) is used to weight the link between layer 1 and layer 2.means w(2) for layer 2 and layer 3.The matrices have a dimension of (number of neurons in layer L)x(number of neurons in Layer (l-1) + bias) w(1) has a dimension of 15x6 and w(2) dimension of 3x16. In the output layer, there are three neurons, and each individual neuron corresponds to a Q function.The number of neurons in the output layer is equal to the number of actions of the agent, and each individual neuron estimates a Q function with respect to an action in the agent's action list Equation(21)\\ These Q function estimate a Q value that is futher used to determine the optimal actions for the agent, where the optimal action for each condition is therefore defined as a = max(Q(s,a)). It is impossible to say which Q function are suitable for the system, since the neural network estimates the Q function based on the agent's experience. In order for the agent to achieve the highest total reward, it must therefore explore the environment long enough for the network to converge to an optimal function. \\ The figure shows the learning process of agent. The method needs initial values, and these are given by sensor values and the network's weight matrices.The weight matrices are initiallized with random values between 0 and 1, but not zero.If the weight matrices are initialized with a value equal to 0,all the neurons will have the same value,which means that the neural network will not be usable. This algorithm uses the agent's sensor values, as opposed to the table method which uses a discretization of values. The exploration function used is the same as for the table method, but the reward function is different. The purpose of this method is to find optimal Q functions by updating the weight machines after each experiment.","title":"4.3 Q-Learning with neural network (Linear function approximation)"},{"location":"deeplearning/#431-reward-function","text":"Since the agent's state space on the neural network is continuous, an equally elaborate reward function is not required as for the table method.The reward function is defined by the agent receiving a positive reward if it drives straight ahead, and a negative reward in the event of a turn and a collision. The reward value r must be in the interval [-1,1], and the function as defined for the system is shown in equation.\\","title":"4.3.1 Reward function"},{"location":"deeplearning/#432-feedforward","text":"As described in Chapter 2 Theory, neural network in this project is a forward-connected neural network. The neurons are activated using the function of the hyperbolic forceps\u53cc\u66f2\u7ebf\u94b3 given in equation. \\ The activated values become output values in this layer, and are used as input for the next layer.The three neurons in the output layer correspond to the Q funcitons when all data is fed into the network. \\ \\ The weight matrix w has m number of rows equal to the number of neurons,and columns n equal to the number of inputs.The matrix w(2) and the vector y are multiplied by each other and the product is used in the activation function described ealier in the chapter.The elements in both the weight matrix and the input vector are real numbers,and the value of the inputs is in the interval[-1,1].Equation(26-28) shows the Q functions of the three actions. The calculations in the figures above can be abbreviated\u7f29\u5199 and shown in equation 27 in vector form.\\ \\ Activation of the hidden layer takes place in the same way as for the output layer, and the input to this layer is the sensor values.\\","title":"4.3.2 Feedforward"},{"location":"deeplearning_work_station/","text":"\u4e24\u4e2a\u9f13\u98ce\u5f0fGPU\\ SUS GeForce RTX 2080 Ti 11G Turbo Edition GDA\\ \u6dd8\u5b9d\u94fe\u63a5 \\ \u4e00\u4e2a20\u7ebf\u7a0bcpu\\ \u82f1\u7279\u5c14\u9177\u777fi9-9820X Skylake X 10\u68383.3Ghz\\ \u6dd8\u5b9d\u94fe\u63a5 x299\u4e3b\u677f\\ \u534e\u7855WS X299 SAGE LGA 2066 Intel X299\\ \u6dd8\u5b9d\u94fe\u63a5 \u4e3b\u673a\u58f3\\ \u6dd8\u5b9d\u94fe\u63a5 \u56fa\u6001\u786c\u76d8\\ HP EX920 M.2 1TB PCIe NVMe NAND SSD\\ \u4eac\u4e1c\u94fe\u63a5 \u673a\u68b0\u786c\u76d8\uff13\uff34\uff22\\ 128GB\u5185\u5b58\\ 128GB RAM\uff08\u968f\u673a\u5b58\u53d6\u5b58\u50a8\u5668\uff09\\ \u6dd8\u5b9d\u94fe\u63a5 \u7535\u6e90\\ 1600w PSU\\ \u6dd8\u5b9d\u94fe\u63a5 \u51b7\u5374\u5668\\ CPU\u98ce\u51b7\\ \u6dd8\u5b9d\u94fe\u63a5","title":"Deeplearning work station"},{"location":"github%E7%BD%91%E9%A1%B5%E4%B8%8D%E6%98%BE%E7%A4%BA%E5%9B%BE%E7%89%87/","text":"github\u4e0d\u663e\u793a\u56fe\u7247 \u4fee\u6539/etc/hosts\uff0c\u653e\u5165\u4e0b\u9762\u5185\u5bb9\uff1a # GitHub Start 140.82.113.3 github.com 140.82.113.3 gist.github.com 185.199.108.153 assets-cdn.github.com 199.232.68.133 raw.githubusercontent.com 199.232.68.133 gist.githubusercontent.com 199.232.68.133 cloud.githubusercontent.com 199.232.68.133 camo.githubusercontent.com 199.232.68.133 avatars0.githubusercontent.com 199.232.68.133 avatars1.githubusercontent.com 199.232.68.133 avatars2.githubusercontent.com 199.232.68.133 avatars3.githubusercontent.com 151.101.184.133 avatars4.githubusercontent.com 151.101.184.133 avatars5.githubusercontent.com 151.101.184.133 avatars6.githubusercontent.com 151.101.184.133 avatars7.githubusercontent.com 151.101.184.133 avatars8.githubusercontent.com # GitHub End \u5176\u4e2d\u5404\u4e2a\uff49\uff50\u9700\u8981\u4ece \u7f51\u7ad9\u67e5\u627e","title":"github\u4e0d\u663e\u793a\u56fe\u7247"},{"location":"github%E7%BD%91%E9%A1%B5%E4%B8%8D%E6%98%BE%E7%A4%BA%E5%9B%BE%E7%89%87/#github","text":"\u4fee\u6539/etc/hosts\uff0c\u653e\u5165\u4e0b\u9762\u5185\u5bb9\uff1a # GitHub Start 140.82.113.3 github.com 140.82.113.3 gist.github.com 185.199.108.153 assets-cdn.github.com 199.232.68.133 raw.githubusercontent.com 199.232.68.133 gist.githubusercontent.com 199.232.68.133 cloud.githubusercontent.com 199.232.68.133 camo.githubusercontent.com 199.232.68.133 avatars0.githubusercontent.com 199.232.68.133 avatars1.githubusercontent.com 199.232.68.133 avatars2.githubusercontent.com 199.232.68.133 avatars3.githubusercontent.com 151.101.184.133 avatars4.githubusercontent.com 151.101.184.133 avatars5.githubusercontent.com 151.101.184.133 avatars6.githubusercontent.com 151.101.184.133 avatars7.githubusercontent.com 151.101.184.133 avatars8.githubusercontent.com # GitHub End \u5176\u4e2d\u5404\u4e2a\uff49\uff50\u9700\u8981\u4ece \u7f51\u7ad9\u67e5\u627e","title":"github\u4e0d\u663e\u793a\u56fe\u7247"},{"location":"lidar_demo%E6%95%99%E7%A8%8B/","text":"lidar_demo\u6559\u7a0b \u7cfb\u7edf\u73af\u5883 ros melodic User Guide \u7f16\u8bd1 \u521b\u5efa\uff52\uff4f\uff53\u73af\u5883\u3000\uff5e/catkin_ws/src\\ \u628alidar_demo.zip\u3000\u89e3\u538b\u5230\uff5e/catkin_ws/src\u76ee\u5f55\u4e0b\\ \u9700\u8981\u628a\uff5e/catkin_ws/src/lidar_demo/src/lidar_demo.cpp\u91cc ros::Subscriber sub = nh.subscribe<sensor_msgs::PointCloud2>(\"/points_raw\", 1, callback); \u4e2d\u7684\"/points_raw\" \u66f4\u6539\u4e3a\u96f7\u8fbe\u6570\u636e\u6e90\uff0c\u5373\u60f3\u8981\u8fdb\u884c\u805a\u7c7b\u8bc6\u522b\u7684\u70b9\u4e91\u7684\uff54\uff4f\uff50\uff49\uff43 \u5f00\u59cb\u7f16\u8bd1\\ cd ~/catkin_ws && catkin_make \u8fd0\u884c source devel/setup.bash \\ \u542f\u52a8\\ roslaunch lidar_demo lidar_demo.launch \\ \u5728\uff4c\uff41\uff55\uff4e\uff43\uff48\u6587\u4ef6\u91cc\u6709\uff15\u4e2a\u53c2\u6570\u53ef\u4ee5\u8c03\u8282 <!-- \u96f7\u8fbe\u6570\u636e\u6e90--> <arg name=\"topic_name\" default=\"/points_raw\"/> <!-- rviz \u663e\u793a\u65f6 Fixed Frame \u5904\u7684\u540d\u79f0--> <arg name=\"frame_id\" default=\"velodyne\"/> <!-- \uff10.01 \u662f1\u5206\u7c73--> <arg name=\"Cluster_D\" default=\"0.75\"/> <!-- \u8bc6\u522b\u7269\u4f53\u5305\u542b\u7684\u6700\u5c11\u70b9\u6570\uff0c\u5c11\u4e8e\u6b64\u70b9\u6570\u4e0d\u5212\u5206--> <arg name=\"Cluster_Min\" default=\"20\"/> <!-- \u8bc6\u522b\u7269\u4f53\u5305\u542b\u7684\u6700\u591a\u70b9\u6570\uff0c\u591a\u4e8e\u6b64\u70b9\u6570\u4e0d\u5212\u5206 (100000)--> <arg name=\"Cluster_Max\" default=\"1000\"/> frame_id \u5fc5\u987b\u548c\uff52\uff56\uff49\uff5a\u91cc\u7684\uff46ixed frame \u4e00\u81f4\\ Cluster_D \u662f\u805a\u884c\u534a\u5f84\uff0c0.01\u662f\uff11\u5206\u7c73\\ Cluster_Min\u3000\u8bc6\u522b\u7269\u4f53\u5305\u542b\u7684\u6700\u5c11\u70b9\u6570\uff0c\u5c11\u4e8e\u6b64\u70b9\u6570\u4e0d\u5212\u5206\\ Cluster_Max\u3000\u8bc6\u522b\u7269\u4f53\u5305\u542b\u7684\u6700\u591a\u70b9\u6570\uff0c\u591a\u4e8e\u6b64\u70b9\u6570\u4e0d\u5212\u5206\\ \uff23luster\u53c2\u6570\u6839\u636e\u6211\u4eec\u8bc6\u522b\u9700\u6c42\u53ef\u8c03","title":"lidar_demo\u6559\u7a0b"},{"location":"lidar_demo%E6%95%99%E7%A8%8B/#lidar_demo","text":"","title":"lidar_demo\u6559\u7a0b"},{"location":"lidar_demo%E6%95%99%E7%A8%8B/#_1","text":"ros melodic","title":"\u7cfb\u7edf\u73af\u5883"},{"location":"lidar_demo%E6%95%99%E7%A8%8B/#user-guide","text":"","title":"User Guide"},{"location":"lidar_demo%E6%95%99%E7%A8%8B/#_2","text":"\u521b\u5efa\uff52\uff4f\uff53\u73af\u5883\u3000\uff5e/catkin_ws/src\\ \u628alidar_demo.zip\u3000\u89e3\u538b\u5230\uff5e/catkin_ws/src\u76ee\u5f55\u4e0b\\ \u9700\u8981\u628a\uff5e/catkin_ws/src/lidar_demo/src/lidar_demo.cpp\u91cc ros::Subscriber sub = nh.subscribe<sensor_msgs::PointCloud2>(\"/points_raw\", 1, callback); \u4e2d\u7684\"/points_raw\" \u66f4\u6539\u4e3a\u96f7\u8fbe\u6570\u636e\u6e90\uff0c\u5373\u60f3\u8981\u8fdb\u884c\u805a\u7c7b\u8bc6\u522b\u7684\u70b9\u4e91\u7684\uff54\uff4f\uff50\uff49\uff43 \u5f00\u59cb\u7f16\u8bd1\\ cd ~/catkin_ws && catkin_make","title":"\u7f16\u8bd1"},{"location":"lidar_demo%E6%95%99%E7%A8%8B/#_3","text":"source devel/setup.bash \\ \u542f\u52a8\\ roslaunch lidar_demo lidar_demo.launch \\ \u5728\uff4c\uff41\uff55\uff4e\uff43\uff48\u6587\u4ef6\u91cc\u6709\uff15\u4e2a\u53c2\u6570\u53ef\u4ee5\u8c03\u8282 <!-- \u96f7\u8fbe\u6570\u636e\u6e90--> <arg name=\"topic_name\" default=\"/points_raw\"/> <!-- rviz \u663e\u793a\u65f6 Fixed Frame \u5904\u7684\u540d\u79f0--> <arg name=\"frame_id\" default=\"velodyne\"/> <!-- \uff10.01 \u662f1\u5206\u7c73--> <arg name=\"Cluster_D\" default=\"0.75\"/> <!-- \u8bc6\u522b\u7269\u4f53\u5305\u542b\u7684\u6700\u5c11\u70b9\u6570\uff0c\u5c11\u4e8e\u6b64\u70b9\u6570\u4e0d\u5212\u5206--> <arg name=\"Cluster_Min\" default=\"20\"/> <!-- \u8bc6\u522b\u7269\u4f53\u5305\u542b\u7684\u6700\u591a\u70b9\u6570\uff0c\u591a\u4e8e\u6b64\u70b9\u6570\u4e0d\u5212\u5206 (100000)--> <arg name=\"Cluster_Max\" default=\"1000\"/> frame_id \u5fc5\u987b\u548c\uff52\uff56\uff49\uff5a\u91cc\u7684\uff46ixed frame \u4e00\u81f4\\ Cluster_D \u662f\u805a\u884c\u534a\u5f84\uff0c0.01\u662f\uff11\u5206\u7c73\\ Cluster_Min\u3000\u8bc6\u522b\u7269\u4f53\u5305\u542b\u7684\u6700\u5c11\u70b9\u6570\uff0c\u5c11\u4e8e\u6b64\u70b9\u6570\u4e0d\u5212\u5206\\ Cluster_Max\u3000\u8bc6\u522b\u7269\u4f53\u5305\u542b\u7684\u6700\u591a\u70b9\u6570\uff0c\u591a\u4e8e\u6b64\u70b9\u6570\u4e0d\u5212\u5206\\ \uff23luster\u53c2\u6570\u6839\u636e\u6211\u4eec\u8bc6\u522b\u9700\u6c42\u53ef\u8c03","title":"\u8fd0\u884c"},{"location":"python/","text":"conda The following packages are not available from current channels: conda config --append channels conda-forge It tells conda to also look on the conda-forge channel when you search for packages. conda \u521b\u5efa\u73af\u5883 conda create -n xception_net python==3.6.5 numpy==1.17.4 scipy==1.3.3 h5py==2.10.0 Keras==2.3.1 tensorflow-gpu==1.15.0 As the comment at the top indicates, the output of conda list -e > requirements.txt can be used to create a conda virtual environment with conda create --name <env> --file requirements.txt cudnn_status_internal_error tensorflow You can try Allowing GPU memory growth with: import tensorflow as tf gpu = tf.config.experimental.list_physical_devices('GPU') tf.config.experimental.set_memory_growth(gpu[0], True) vscode activate conda env { \"ros.distro\": \"melodic\", \"python.autoComplete.extraPaths\": [ \"/home/pmjd/Disk/anaconda3/envs/wjj/lib/python3.6/site-packages\" ], \"python.terminal.activateEnvInCurrentTerminal\": true, \"python.condaPath\": \"/home/pmjd/Disk/anaconda3/bin/conda\", \"python.defaultInterpreterPath\": \"/home/pmjd/Disk/anaconda3/envs/wjj/bin/python\" } NameError: name 'xrange' is not defined try: # Python 2 xrange except NameError: # Python 3, xrange is now named range xrange = range TensorFlow ValueError: Cannot feed value of shape (64, 64, 3) for Tensor u'Placeholder:0', which has shape '(?, 64, 64, 3)' image has a shape of (64,64,3). Your input placeholder _x have a shape of (?, 64,64,3). The problem is that you're feeding the placeholder with a value of a different shape. You have to feed it with a value of (1, 64, 64, 3) = a batch of 1 image. Just reshape your image value to a batch with size one. np.expand_dims(img, axis=0) opencv conda opencv is not compatible with python 3. I had to install opencv3 for python 3. The marked answer in how could we install opencv on anaconda? explains how to install opencv(3) for anaconda: Run the following command: conda install -c https://conda.binstar.org/menpo opencv I realized that opencv3 is also available now, run the following command: conda install -c https://conda.binstar.org/menpo opencv3 Edit on Aug 18, 2016: You may like to add the \"menpo\" channel permanently by: conda config --add channels menpo And then opencv can be installed by: conda install opencv (or opencv3) Edit on Aug 14, 2017: \"clinicalgraphics\" channel provides relatively newer vtk version for very recent python3 conda install -c clinicalgraphics vtk Edit on April 16, 2020 (based on @AMC's comment): OpenCV can be installed through conda-forge (details see here) conda install -c conda-forge opencv pointcloud2 to array def pointcloud2_to_array(cloud_msg, squeeze=True): dtype_list = fields_to_dtype(cloud_msg.fields, cloud_msg.point_step) cloud_arr = np.fromstring(cloud_msg.data, dtype_list) cloud_arr = cloud_arr[ [fname for fname, _type in dtype_list if not (fname[:len(DUMMY_FIELD_PREFIX)] == DUMMY_FIELD_PREFIX)]] if squeeze and cloud_msg.height == 1: return np.reshape(cloud_arr, (cloud_msg.width,)) else: return np.reshape(cloud_arr, (cloud_msg.height, cloud_msg.width)) read pcd to array import numpy as np import open3d as o3d pcd = o3d.io.read_point_cloud(\"pointcloud_path.pcd\") out_arr = np.asarray(pcd.points) print (\"output array from input list : \", out_arr) kitti \u6570\u636e\u96c6\u4e0b\u8f7d \u56fe\u7247\u4e0b\u8f7d https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_image_2.zip \u70b9\u4e91\u4e0b\u8f7d https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_velodyne.zip \u6807\u7b7e\u4e0b\u8f7d https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_label_2.zip \u77eb\u6b63\u6587\u4ef6\u4e0b\u8f7d https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_calib.zip python \"//\" operator In python 2.x >>> 10/3 3 >>> # To get a floating point number from integer division: >>> 10.0/3 3.3333333333333335 >>> float(10)/3 3.3333333333333335 In python 3.x >>> 10/3 3.3333333333333335 >>> 10//3 3 python super() \u5185\u7f6e\u7684super()\u8fd4\u56de\u4e00\u4e2a\u4ee3\u7406\u5bf9\u8c61\uff08\u8d85\u7c7b\u7684\u4e34\u65f6\u5bf9\u8c61),\u8be5\u4ee3\u7406\u5bf9\u8c61\u5141\u8bb8\u6211\u4eec\u8bbf\u95ee\u57fa\u7c7b\u7684\u65b9\u6cd5\u3002\u5728python\u4e2d\uff0csuper()\u6709\u4e24\u4e2a\u4e3b\u8981\u7528\u4f8b\uff1a \u8ba9\u6211\u4eec\u907f\u514d\u663e\u793a\u4f7f\u7528\u57fa\u7c7b\u540d\u79f0 \u5904\u7406\u591a\u91cd\u7ee7\u627f\\ \u793a\u4f8b\uff11\uff1a\u5177\u6709\u5355\u7ee7\u627f\u7684super() \u5728\u5355\u7ee7\u627f\u7684\u60c5\u51b5\u4e0b\uff0c\u5b83\u5141\u8bb8\u6211\u4eec\u901a\u8fc7\u5f15\u7528\u57fa\u7c7bsuper() class Mammal(object): def __init__(self, mammalName): print(mammalName, 'is a warm-blooded animal.') class Dog(Mammal): def __init__(self): print('Dog has four legs.') super().__init__('Dog') d1 = Dog() \u8f93\u51fa Dog has four legs. Dog is a warm-blooded animal. \u8be5super()\u5185\u5efa\u8fd4\u56de\u4e00\u4e2a\u4ee3\u7406\u5bf9\u8c61\uff0c\u66ff\u4ee3\u5bf9\u8c61\uff0c\u53ef\u4ee5\u901a\u8fc7\u59d4\u6258\u8c03\u7528\u57fa\u7c7b\u7684\u65b9\u6cd5\uff0c\u8fd9\u79f0\u4e3a\u201c\u95f4\u63a5\"(\u4f7f\u7528\u5f15\u7528\u57fa\u7840\u5bf9\u8c61\u7684\u80fd\u529bsuper()) \u7531\u4e8e\u95f4\u63a5\u662f\u5728\u8fd0\u884c\u65f6\u8ba1\u7b97\u7684\uff0c\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u5728\u4e0d\u540c\u7684\u65f6\u95f4\u4f7f\u7528\u4e0d\u540c\u7684\u57fa\u7c7b\uff08\u5982\u679c\u9700\u8981\uff09 \u793a\u4f8b\uff12\uff1a\u5177\u6709\u591a\u91cd\u7ee7\u627f\u7684super() class Animal: def __init__(self, Animal): print(Animal, 'is an animal.'); class Mammal(Animal): def __init__(self, mammalName): print(mammalName, 'is a warm-blooded animal.') super().__init__(mammalName) class NonWingedMammal(Mammal): def __init__(self, NonWingedMammal): print(NonWingedMammal, \"can't fly.\") super().__init__(NonWingedMammal) class NonMarineMammal(Mammal): def __init__(self, NonMarineMammal): print(NonMarineMammal, \"can't swim.\") super().__init__(NonMarineMammal) class Dog(NonMarineMammal, NonWingedMammal): def __init__(self): print('Dog has 4 legs.'); super().__init__('Dog') d = Dog() print('') bat = NonMarineMammal('Bat') \u8f93\u51fa Dog has 4 legs. Dog can't swim. Dog can't flay. Dog is a warm-blooded animal. Dog is an animal. Bat can't swim. Bat is a warm-blooded animal. Bat is an animal. Method Resolution Order\u65b9\u6cd5\u89e3\u6790\u987a\u5e8f (MRO) >>> Dog.__mro__ (<class 'Dog'>, <class 'NonMarineMammal'>, <class 'NonWingedMammal'>, <class 'Mammal'>, <class 'Animal'>, <class 'object'>) Split method in python is outputing an index error one of your lines must be empty deque in python \uff50\uff59\uff54\uff48\uff4f\uff4e\u4e2d\u7684\u53cc\u7aef\u961f\u5217 # Python code to demonstrate deque from collections import deque # Declaring deque queue = deque(['name','age','DOB']) print(queue) ========================================================= Output: deque(['name', 'age', 'DOB']) append() :- This function is used to insert the value in its argument to the right end of deque. appendleft() :- This function is used to insert the value in its argument to the left end of deque. pop() :- This function is used to delete an argument from the right end of deque. popleft() :- This function is used to delete an argument from the left end of deque. index(ele, beg, end) :- This function returns the first index of the value mentioned in arguments, starting searching from beg till end index. insert(i, a) :- This function inserts the value mentioned in arguments(a) at index(i) specified in arguments. remove() :- This function removes the first occurrence of value mentioned in arguments. extend(iterable) :- This function is used to add multiple values at the right end of deque. The argument passed is an iterable. extendleft(iterable) :- This function is used to add multiple values at the left end of deque. The argument passed is an iterable. Order is reversed as a result of left appends. reverse() :- This function is used to reverse order of deque elements. rotate() :- This function rotates the deque by the number specified in arguments. If the number specified is negative, rotation occurs to left. Else rotation is to right. numpy.argmax numpy.argmax(a, axis=None, out=None)[source]Returns the indices of the maximum values along an axis. parameters \\ a: array_like ,Input array. axis:int, optional , By default, the index is into the flattened array, otherwise along the specified axis. outarray, optional , If provided, the result will be inserted into this array. It should be of the appropriate shape and dtype. Returns index_array:ndarray of ints Array of indices into the array. It has the same shape as a.shape with the dimension along axis removed. python random.sample() numpy.amax() python pip \u4e0d\u80fd\u7528 PIP_NO_CACHE_DIR=off pip install gym num.linspace() in Python import numpy as np print(\"B\\n\", np.linspace(2.0, 3.0, num=5, retstep=True),\"\\n\") x = np.linspace(0, 2, 10) print(\"A\\n\", np.sin(x)) Output B (array([ 2. , 2.25, 2.5 , 2.75, 3. ]), 0.25) A [ 0. 0.22039774 0.42995636 0.6183698 0.77637192 0.8961922 0.9719379 0.99988386 0.9786557 0.90929743] pip \u4e0b\u8f7d\u63d0\u901f \u56fd\u5185\u6e90\uff1a \u65b0\u7248ubuntu\u8981\u6c42\u4f7f\u7528https\u6e90\uff0c\u8981\u6ce8\u610f\u3002 \u6e05\u534e\uff1ahttps://pypi.tuna.tsinghua.edu.cn/simple \u963f\u91cc\u4e91\uff1ahttp://mirrors.aliyun.com/pypi/simple \u4e2d\u56fd\u79d1\u6280\u5927\u5b66 https://pypi.mirrors.ustc.edu.cn/simple \u534e\u4e2d\u7406\u5de5\u5927\u5b66\uff1ahttp://pypi.hustunique.com \u5c71\u4e1c\u7406\u5de5\u5927\u5b66\uff1ahttp://pypi.sdutlinux.org \u8c46\u74e3\uff1ahttp://pypi.douban.com/simple \u4e34\u65f6\u4f7f\u7528\uff1a \u53ef\u4ee5\u5728\u4f7f\u7528pip\u7684\u65f6\u5019\u52a0\u53c2\u6570 pip install -i https://pypi.tuna.tsinghua.edu.cn/simple PIL.Image\u8f6c\u6362\u6210OpenCV\u683c\u5f0f import cv2 from PIL import Image import numpy image = Image.open(\"plane.jpg\") image.show() img = cv2.cvtColor(numpy.asarray(image),cv2.COLOR_RGB2BGR) cv2.imshow(\"OpenCV\",img) cv2.waitKey() OpenCV\u8f6c\u6362\u6210PIL.Image\u683c\u5f0f import cv2 from PIL import Image import numpy img = cv2.imread(\"plane.jpg\") cv2.imshow(\"OpenCV\",img) image = Image.fromarray(cv2.cvtColor(img,cv2.COLOR_BGR2RGB)) image.show() cv2.waitKey() \u6dfb\u52a0python\u8def\u5f84 xport PYTHONPATH=$PYTHONPATH:/home/dell/Deep/DeepSpeech/training","title":"Python"},{"location":"python/#conda-the-following-packages-are-not-available-from-current-channels","text":"conda config --append channels conda-forge It tells conda to also look on the conda-forge channel when you search for packages.","title":"conda The following packages are not available from current channels:"},{"location":"python/#conda","text":"conda create -n xception_net python==3.6.5 numpy==1.17.4 scipy==1.3.3 h5py==2.10.0 Keras==2.3.1 tensorflow-gpu==1.15.0 As the comment at the top indicates, the output of conda list -e > requirements.txt can be used to create a conda virtual environment with conda create --name <env> --file requirements.txt","title":"conda \u521b\u5efa\u73af\u5883"},{"location":"python/#cudnn_status_internal_error-tensorflow","text":"You can try Allowing GPU memory growth with: import tensorflow as tf gpu = tf.config.experimental.list_physical_devices('GPU') tf.config.experimental.set_memory_growth(gpu[0], True)","title":"cudnn_status_internal_error tensorflow"},{"location":"python/#vscode-activate-conda-env","text":"{ \"ros.distro\": \"melodic\", \"python.autoComplete.extraPaths\": [ \"/home/pmjd/Disk/anaconda3/envs/wjj/lib/python3.6/site-packages\" ], \"python.terminal.activateEnvInCurrentTerminal\": true, \"python.condaPath\": \"/home/pmjd/Disk/anaconda3/bin/conda\", \"python.defaultInterpreterPath\": \"/home/pmjd/Disk/anaconda3/envs/wjj/bin/python\" }","title":"vscode activate conda env"},{"location":"python/#nameerror-name-xrange-is-not-defined","text":"try: # Python 2 xrange except NameError: # Python 3, xrange is now named range xrange = range","title":"NameError: name 'xrange' is not defined"},{"location":"python/#tensorflow-valueerror-cannot-feed-value-of-shape-64-64-3-for-tensor-uplaceholder0-which-has-shape-64-64-3","text":"image has a shape of (64,64,3). Your input placeholder _x have a shape of (?, 64,64,3). The problem is that you're feeding the placeholder with a value of a different shape. You have to feed it with a value of (1, 64, 64, 3) = a batch of 1 image. Just reshape your image value to a batch with size one. np.expand_dims(img, axis=0)","title":"TensorFlow ValueError: Cannot feed value of shape (64, 64, 3) for Tensor u'Placeholder:0', which has shape '(?, 64, 64, 3)'"},{"location":"python/#opencv-conda","text":"opencv is not compatible with python 3. I had to install opencv3 for python 3. The marked answer in how could we install opencv on anaconda? explains how to install opencv(3) for anaconda: Run the following command: conda install -c https://conda.binstar.org/menpo opencv I realized that opencv3 is also available now, run the following command: conda install -c https://conda.binstar.org/menpo opencv3 Edit on Aug 18, 2016: You may like to add the \"menpo\" channel permanently by: conda config --add channels menpo And then opencv can be installed by: conda install opencv (or opencv3) Edit on Aug 14, 2017: \"clinicalgraphics\" channel provides relatively newer vtk version for very recent python3 conda install -c clinicalgraphics vtk Edit on April 16, 2020 (based on @AMC's comment): OpenCV can be installed through conda-forge (details see here) conda install -c conda-forge opencv","title":"opencv conda"},{"location":"python/#pointcloud2-to-array","text":"def pointcloud2_to_array(cloud_msg, squeeze=True): dtype_list = fields_to_dtype(cloud_msg.fields, cloud_msg.point_step) cloud_arr = np.fromstring(cloud_msg.data, dtype_list) cloud_arr = cloud_arr[ [fname for fname, _type in dtype_list if not (fname[:len(DUMMY_FIELD_PREFIX)] == DUMMY_FIELD_PREFIX)]] if squeeze and cloud_msg.height == 1: return np.reshape(cloud_arr, (cloud_msg.width,)) else: return np.reshape(cloud_arr, (cloud_msg.height, cloud_msg.width))","title":"pointcloud2 to array"},{"location":"python/#read-pcd-to-array","text":"import numpy as np import open3d as o3d pcd = o3d.io.read_point_cloud(\"pointcloud_path.pcd\") out_arr = np.asarray(pcd.points) print (\"output array from input list : \", out_arr)","title":"read pcd to array"},{"location":"python/#kitti","text":"\u56fe\u7247\u4e0b\u8f7d https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_image_2.zip \u70b9\u4e91\u4e0b\u8f7d https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_velodyne.zip \u6807\u7b7e\u4e0b\u8f7d https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_label_2.zip \u77eb\u6b63\u6587\u4ef6\u4e0b\u8f7d https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_calib.zip","title":"kitti \u6570\u636e\u96c6\u4e0b\u8f7d"},{"location":"python/#python-operator","text":"In python 2.x >>> 10/3 3 >>> # To get a floating point number from integer division: >>> 10.0/3 3.3333333333333335 >>> float(10)/3 3.3333333333333335 In python 3.x >>> 10/3 3.3333333333333335 >>> 10//3 3","title":"python \"//\" operator"},{"location":"python/#python-super","text":"\u5185\u7f6e\u7684super()\u8fd4\u56de\u4e00\u4e2a\u4ee3\u7406\u5bf9\u8c61\uff08\u8d85\u7c7b\u7684\u4e34\u65f6\u5bf9\u8c61),\u8be5\u4ee3\u7406\u5bf9\u8c61\u5141\u8bb8\u6211\u4eec\u8bbf\u95ee\u57fa\u7c7b\u7684\u65b9\u6cd5\u3002\u5728python\u4e2d\uff0csuper()\u6709\u4e24\u4e2a\u4e3b\u8981\u7528\u4f8b\uff1a \u8ba9\u6211\u4eec\u907f\u514d\u663e\u793a\u4f7f\u7528\u57fa\u7c7b\u540d\u79f0 \u5904\u7406\u591a\u91cd\u7ee7\u627f\\","title":"python super()"},{"location":"python/#1super","text":"\u5728\u5355\u7ee7\u627f\u7684\u60c5\u51b5\u4e0b\uff0c\u5b83\u5141\u8bb8\u6211\u4eec\u901a\u8fc7\u5f15\u7528\u57fa\u7c7bsuper() class Mammal(object): def __init__(self, mammalName): print(mammalName, 'is a warm-blooded animal.') class Dog(Mammal): def __init__(self): print('Dog has four legs.') super().__init__('Dog') d1 = Dog() \u8f93\u51fa Dog has four legs. Dog is a warm-blooded animal. \u8be5super()\u5185\u5efa\u8fd4\u56de\u4e00\u4e2a\u4ee3\u7406\u5bf9\u8c61\uff0c\u66ff\u4ee3\u5bf9\u8c61\uff0c\u53ef\u4ee5\u901a\u8fc7\u59d4\u6258\u8c03\u7528\u57fa\u7c7b\u7684\u65b9\u6cd5\uff0c\u8fd9\u79f0\u4e3a\u201c\u95f4\u63a5\"(\u4f7f\u7528\u5f15\u7528\u57fa\u7840\u5bf9\u8c61\u7684\u80fd\u529bsuper()) \u7531\u4e8e\u95f4\u63a5\u662f\u5728\u8fd0\u884c\u65f6\u8ba1\u7b97\u7684\uff0c\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u5728\u4e0d\u540c\u7684\u65f6\u95f4\u4f7f\u7528\u4e0d\u540c\u7684\u57fa\u7c7b\uff08\u5982\u679c\u9700\u8981\uff09","title":"\u793a\u4f8b\uff11\uff1a\u5177\u6709\u5355\u7ee7\u627f\u7684super()"},{"location":"python/#2super","text":"class Animal: def __init__(self, Animal): print(Animal, 'is an animal.'); class Mammal(Animal): def __init__(self, mammalName): print(mammalName, 'is a warm-blooded animal.') super().__init__(mammalName) class NonWingedMammal(Mammal): def __init__(self, NonWingedMammal): print(NonWingedMammal, \"can't fly.\") super().__init__(NonWingedMammal) class NonMarineMammal(Mammal): def __init__(self, NonMarineMammal): print(NonMarineMammal, \"can't swim.\") super().__init__(NonMarineMammal) class Dog(NonMarineMammal, NonWingedMammal): def __init__(self): print('Dog has 4 legs.'); super().__init__('Dog') d = Dog() print('') bat = NonMarineMammal('Bat') \u8f93\u51fa Dog has 4 legs. Dog can't swim. Dog can't flay. Dog is a warm-blooded animal. Dog is an animal. Bat can't swim. Bat is a warm-blooded animal. Bat is an animal.","title":"\u793a\u4f8b\uff12\uff1a\u5177\u6709\u591a\u91cd\u7ee7\u627f\u7684super()"},{"location":"python/#method-resolution-order-mro","text":">>> Dog.__mro__ (<class 'Dog'>, <class 'NonMarineMammal'>, <class 'NonWingedMammal'>, <class 'Mammal'>, <class 'Animal'>, <class 'object'>)","title":"Method Resolution Order\u65b9\u6cd5\u89e3\u6790\u987a\u5e8f (MRO)"},{"location":"python/#split-method-in-python-is-outputing-an-index-error","text":"one of your lines must be empty","title":"Split method in python is outputing an index error"},{"location":"python/#deque-in-python-python","text":"# Python code to demonstrate deque from collections import deque # Declaring deque queue = deque(['name','age','DOB']) print(queue) ========================================================= Output: deque(['name', 'age', 'DOB']) append() :- This function is used to insert the value in its argument to the right end of deque. appendleft() :- This function is used to insert the value in its argument to the left end of deque. pop() :- This function is used to delete an argument from the right end of deque. popleft() :- This function is used to delete an argument from the left end of deque. index(ele, beg, end) :- This function returns the first index of the value mentioned in arguments, starting searching from beg till end index. insert(i, a) :- This function inserts the value mentioned in arguments(a) at index(i) specified in arguments. remove() :- This function removes the first occurrence of value mentioned in arguments. extend(iterable) :- This function is used to add multiple values at the right end of deque. The argument passed is an iterable. extendleft(iterable) :- This function is used to add multiple values at the left end of deque. The argument passed is an iterable. Order is reversed as a result of left appends. reverse() :- This function is used to reverse order of deque elements. rotate() :- This function rotates the deque by the number specified in arguments. If the number specified is negative, rotation occurs to left. Else rotation is to right.","title":"deque in python \uff50\uff59\uff54\uff48\uff4f\uff4e\u4e2d\u7684\u53cc\u7aef\u961f\u5217"},{"location":"python/#numpyargmax","text":"numpy.argmax(a, axis=None, out=None)[source]Returns the indices of the maximum values along an axis. parameters \\ a: array_like ,Input array. axis:int, optional , By default, the index is into the flattened array, otherwise along the specified axis. outarray, optional , If provided, the result will be inserted into this array. It should be of the appropriate shape and dtype. Returns index_array:ndarray of ints Array of indices into the array. It has the same shape as a.shape with the dimension along axis removed.","title":"numpy.argmax"},{"location":"python/#python-randomsample","text":"","title":"python random.sample()"},{"location":"python/#numpyamax","text":"","title":"numpy.amax()"},{"location":"python/#python-pip","text":"PIP_NO_CACHE_DIR=off pip install gym","title":"python pip \u4e0d\u80fd\u7528"},{"location":"python/#numlinspace-in-python","text":"import numpy as np print(\"B\\n\", np.linspace(2.0, 3.0, num=5, retstep=True),\"\\n\") x = np.linspace(0, 2, 10) print(\"A\\n\", np.sin(x)) Output B (array([ 2. , 2.25, 2.5 , 2.75, 3. ]), 0.25) A [ 0. 0.22039774 0.42995636 0.6183698 0.77637192 0.8961922 0.9719379 0.99988386 0.9786557 0.90929743]","title":"num.linspace() in Python"},{"location":"python/#pip","text":"\u56fd\u5185\u6e90\uff1a \u65b0\u7248ubuntu\u8981\u6c42\u4f7f\u7528https\u6e90\uff0c\u8981\u6ce8\u610f\u3002 \u6e05\u534e\uff1ahttps://pypi.tuna.tsinghua.edu.cn/simple \u963f\u91cc\u4e91\uff1ahttp://mirrors.aliyun.com/pypi/simple \u4e2d\u56fd\u79d1\u6280\u5927\u5b66 https://pypi.mirrors.ustc.edu.cn/simple \u534e\u4e2d\u7406\u5de5\u5927\u5b66\uff1ahttp://pypi.hustunique.com \u5c71\u4e1c\u7406\u5de5\u5927\u5b66\uff1ahttp://pypi.sdutlinux.org \u8c46\u74e3\uff1ahttp://pypi.douban.com/simple \u4e34\u65f6\u4f7f\u7528\uff1a \u53ef\u4ee5\u5728\u4f7f\u7528pip\u7684\u65f6\u5019\u52a0\u53c2\u6570 pip install -i https://pypi.tuna.tsinghua.edu.cn/simple","title":"pip \u4e0b\u8f7d\u63d0\u901f"},{"location":"python/#pilimageopencv","text":"import cv2 from PIL import Image import numpy image = Image.open(\"plane.jpg\") image.show() img = cv2.cvtColor(numpy.asarray(image),cv2.COLOR_RGB2BGR) cv2.imshow(\"OpenCV\",img) cv2.waitKey()","title":"PIL.Image\u8f6c\u6362\u6210OpenCV\u683c\u5f0f"},{"location":"python/#opencvpilimage","text":"import cv2 from PIL import Image import numpy img = cv2.imread(\"plane.jpg\") cv2.imshow(\"OpenCV\",img) image = Image.fromarray(cv2.cvtColor(img,cv2.COLOR_BGR2RGB)) image.show() cv2.waitKey()","title":"OpenCV\u8f6c\u6362\u6210PIL.Image\u683c\u5f0f"},{"location":"python/#python","text":"xport PYTHONPATH=$PYTHONPATH:/home/dell/Deep/DeepSpeech/training","title":"\u6dfb\u52a0python\u8def\u5f84"},{"location":"ros%20openai/","text":"ubuntu18 melodic sudo apt-get install\\ python-pip python3-vcstool python3-pyqt4\\ pyqt5-dev-tools\\ libbluetooth-dev libspnav-dev\\ pyqt4-dev-tools libcwiid-dev\\ cmake gcc g++ qt4-qmake libqt4-dev\\ libusb-dev libftdi-dev\\ python3-defusedxml python3-vcstool\\ ros-melodic-octomap-msgs \\ ros-melodic-joy \\ ros-melodic-geodesy \\ ros-melodic-octomap-ros \\ ros-melodic-control-toolbox \\ ros-melodic-pluginlib \\ ros-melodic-trajectory-msgs \\ ros-melodic-control-msgs \\ ros-melodic-std-srvs \\ ros-melodic-nodelet \\ ros-melodic-urdf \\ ros-melodic-rviz \\ ros-melodic-kdl-conversions \\ ros-melodic-eigen-conversions \\ ros-melodic-tf2-sensor-msgs \\ ros-melodic-pcl-ros\\ ros-melodic-navigation\\ ros-melodic-sophus sudo pip install gym\\ sudo apt-get install python-skimage\\ sudo pip install h5py\\ pip install tensorflow-gpu (if you have a gpu if not then just pip install tensorflow)\\ sudo pip install keras cd ~\\ git clone https://github.com/erlerobot/gym-gazebo\\ cd gym-gazebo\\ sudo pip install -e . cd gym-gazebo/gym_gazebo/envs/installation\\ bash setup_melodic.bash","title":"Ros openai"},{"location":"ros%20openai/#ubuntu18-melodic","text":"sudo apt-get install\\ python-pip python3-vcstool python3-pyqt4\\ pyqt5-dev-tools\\ libbluetooth-dev libspnav-dev\\ pyqt4-dev-tools libcwiid-dev\\ cmake gcc g++ qt4-qmake libqt4-dev\\ libusb-dev libftdi-dev\\ python3-defusedxml python3-vcstool\\ ros-melodic-octomap-msgs \\ ros-melodic-joy \\ ros-melodic-geodesy \\ ros-melodic-octomap-ros \\ ros-melodic-control-toolbox \\ ros-melodic-pluginlib \\ ros-melodic-trajectory-msgs \\ ros-melodic-control-msgs \\ ros-melodic-std-srvs \\ ros-melodic-nodelet \\ ros-melodic-urdf \\ ros-melodic-rviz \\ ros-melodic-kdl-conversions \\ ros-melodic-eigen-conversions \\ ros-melodic-tf2-sensor-msgs \\ ros-melodic-pcl-ros\\ ros-melodic-navigation\\ ros-melodic-sophus sudo pip install gym\\ sudo apt-get install python-skimage\\ sudo pip install h5py\\ pip install tensorflow-gpu (if you have a gpu if not then just pip install tensorflow)\\ sudo pip install keras cd ~\\ git clone https://github.com/erlerobot/gym-gazebo\\ cd gym-gazebo\\ sudo pip install -e . cd gym-gazebo/gym_gazebo/envs/installation\\ bash setup_melodic.bash","title":"ubuntu18 melodic"},{"location":"tf_document/","text":"tf.compat.v1.train.Saver tf.compat.v1.train.Saver \\ Saves and restores\u8fd8\u539f variables tf.compat.v1.train.Saver( var_list=None, reshape=False, sharded=False, max_to_keep=5, keep_checkpoint_every_n_hours=10000.0, name=None, restore_sequentially=False, saver_def=None, builder=None, defer_build=False, allow_empty=False, write_version=tf.train.SaverDef.V2, pad_step_number=False, save_relative_paths=False, filename=None ) The Saver class adds ops\u884c\u52a8 to save and restore variables to and form checkpoints.It also provides convenience method to run these ops. Checkpoints are binary files in a proprietary\u6240\u6709\u6743 format which map variable names to tensor values.Checkpoints \u662f\u4e13\u6709\u683c\u5f0f\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u8be5\u6587\u4ef6\u5c06\u53d8\u91cf\u540d\u6620\u5c04\u5230\u5f20\u91cf\u503c.The best way to examine\u5ba1\u67e5 the contents of a checkpoints is to load it using a Saver var_list A list of Variable/SaveableObject, or a dictionary mapping names to SaveableObjects.If None,defaults to the list of all saveable objects. reshape If True,allows restoring\u6062\u590d parameters from a checkpoint where the variables have a different shape. shareded If True,shard the checkpoints,one per device\u6bcf\u4e2a\u8bbe\u5907\u4e00\u4e2a max_to_keep Maximum number of recent checkpoints to keep,Default to 5 keep_checkpoint_every_n_hours How often to keep checkpoints,Defaults to 10,000 hours. name String, Optional name to use as a prefix when adding operations. restore_sequentially A Bool,which if true,causes restore of different variables to happen sequentially within each device.This can lower memory usage when restoring large models. saver_def Optional SaverDef proto\u539f\u578b to use instead of running the builder.This is only useful for specialty code that wants to recreate a Saver object for a previously built Graph that had a Saver.The saver_def proto should be the one returned by the as_saver_def() call of the Saver that was created for that Graph. builder Optional SaverBuilder to use if a saver_def was not provided. Default to BuikSaverBuilder(). defer_build If True,defer adding the save and restore ops to the build() call. In that case build() should be called before finalizing\u5b9a\u6848 the graph or using the saver. allow_empty If False(defalut) rasie an error if there are no variables in the graph.Otherwise, construct the saver anyway and make it a no-op. write_version controls what format to use when saving checkpoints. It also affects certain filepath matching logic. The V2 format is the recommended choice: it is much more optimized than V1 in terms of memory required and latency incurred during restore. Regardless of this flag, the Saver is able to restore from both V2 and V1 checkpoints. pad_step_number if True, pads the global step number in the checkpoint filepaths to some fixed width (8 by default). This is turned off by default. save_relative_paths If True, will write relative paths to the checkpoint state file. This is needed if the user wants to copy the checkpoint directory and reload from the copied directory. filename If known at graph construction time, filename used for variable loading/saving. tf.compat.v1.variable_scope tf.compat.v1.variable_scope \\ A context manager for define ops that create variables(layers)\u7528\u4e8e\u5b9a\u4e49\u521b\u5efa\u53d8\u91cf\uff08\u5c42\uff09\u7684\u64cd\u4f5c\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668 tf.compat.v1.variable_scope( name_or_scope, default_name=None, values=None, initializer=None, regularizer=None, caching_device=None, partitioner=None, custom_getter=None, reuse=None, dtype=None, use_resource=None, constraint=None, auxiliary_name_scope=True ) This context manager validates\u9a8c\u8bc1 that the values are from the same graph,ensures that graph is the default graph,and pushes a name scope\u8303\u56f4 and a variable scope. Variable scope allows you to create new variables and to share already created ones while providing checks to not create or share by accident.\u53d8\u91cf\u4f5c\u7528\u57df\u5141\u8bb8\u60a8\u521b\u5efa\u65b0\u53d8\u91cf\u5e76\u5171\u4eab\u5df2\u521b\u5efa\u7684\u53d8\u91cf\uff0c\u540c\u65f6\u63d0\u4f9b\u68c0\u67e5\u4ee5\u9632\u6b62\u610f\u5916\u521b\u5efa\u6216\u5171\u4eab. Keep in mind that the counters for default_name are discarded\u4e22\u5f03 once the parent scope is exited. Therefore when the code re-enters the scope (for instance by saving it), all nested\u5d4c\u5957\u7684 default_name counters will be restarted. Note that reuse flag is inherited: if we open a resuing scope,then all its sub-scope become reusing as well. A note about name scoping:Setting reuse does not impact the naming of other ops such as mult. reuse True,None,or tf.compat.v1.AUTO_REUSE;if True, we go into reuse mode for this scope as well as all sub-scopes; if tf.compat.v1.AUTO_REUSE, we create variables if they do not exist, and return them otherwise; if None, we inherit the parent scope's reuse flag. When eager execution is enabled, new variables are always created unless an EagerVariableStore or template is currently active. tf.compat.v1.layers.dense tf.compat.v1.layers.dense \\ Functional interface for the densely-connected layer. tf.compat.v1.layers.dense( inputs, units, activation=None, use_bias=True, kernel_initializer=None, bias_initializer=tf.zeros_initializer(), kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, trainable=True, name=None, reuse=None ) The layer implement the operation: output=activation(inputs * kernel + bias) where activation is the activation function passed as the activation argument(if not \uff2eone ),kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer(only if use_bias is True). inputs Tensor input. units Integer or Long, dimensionality of the output space. activation Activation function(callable), Set it to None to maintain a linear activation. use_bias Boolean, whether the layer uses a bias kernel_initializer Initializer function for the weight matrix. If None (default), weights are initialized using the default initializer used by tf.compat.v1.get_variable. bias_initializer Initializer function for the bias kernel_regularizer Regularizer\u6b63\u5219\u5316\u5668 function for the weight matrix bias_regularizer Regularizer function for the bias. activity_regularizer Regularizer function for the output. kernel_constraint An optional projection function to be applied to the kernel after being updated by an Optimizer (e.g. used to implement norm constraints or value constraints for layer weights). The function must take as input the unprojected variable and must return the projected variable (which must have the same shape). Constraints\u7ea6\u675f\u6761\u4ef6 are not safe to use when doing asynchronous distributed training. bias_constraint An optional projection function to be applied to the bias after being updated by an Optimizer. trainable Boolean, if True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable). name String, the name of the layer. reuse Boolean, whether to reuse the weights of a previous layer by the same name. tf.compat.v1.layers.BatchNormalization tf.compat.v1.layers.BatchNormalization \\ Batch Normalization layer tf.compat.v1.layers.BatchNormalization( axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer=tf.zeros_initializer(), gamma_initializer=tf.ones_initializer(), moving_mean_initializer=tf.zeros_initializer(), moving_variance_initializer=tf.ones_initializer(), beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None, renorm=False, renorm_clipping=None, renorm_momentum=0.99, fused=None, trainable=True, virtual_batch_size=None, adjustment=None, name=None, **kwargs ) fused if None or True, use a faster, fused implementation if possible. If False, use the system recommended implementation. tf.math.reduce_max tf.math.reduce_max \\ Computes the maximum of elements across dimensions of a tensor tf.math.reduce_max( input_tensor, axis=None, keepdims=False, name=None ) Reduces input_tensor along the dimensions given in axis.Unless keepdims is true, the rank of the tensor is reduced by 1 for each of the entires in axis, which must be unique. If keepdims is true, the reduced dimensions are retained with length 1. If axis is None,all dimensions are reduced, and a tensor with a single element is returned. input_tensor The tensor to reduce.Should have real numeric type axis The dimensions to reduce.If None (the default),reduces all dimensions.Must be in range [-rank(input_tensor),rank(input_tensor)] keepdims If true,retains reduced dimensions with length 1. name A name for the operation(optional). tf.tile tf.tile \\ Constructs\u6784\u9020 a tensor by tiling\u5e73\u94fa a given tensor tf.tile( input, multiples, name=None ) This operation creates a new tensor by replicating\u590d\u5236 input multiples times.The output tensor's i'th dimension has input.dims(i)*multiples[i] elements, and the values of input are replicated multiples[i] times along the i'th dimension. input A tensor,1-D or higher multiples A tensor. Must be one of the following types: int32, int64, 1-D Length must be the same as the number of dimensions in input. name A name for the operation(optional). tf.concat tf.concat \\ Concatenates tensors along one dimension. tf.concat( values, axis, name='concat' ) tf.cast tf.cast \\ Cast a tensor to a new type. tf.cast( x, dtype, name=None ) The operation supports data types (for x and dtype) of uint8, uint16, uint32, uint64, int8, int16, int32, int64, float16, float32, float64, complex64, complex128, bfloat16. In case of casting from complex types (complex64, complex128) to real types, only the real part of x is returned. In case of casting from real types to complex types (complex64, complex128), the imaginary part of the returned value is set to 0. The handling of complex types here matches the behavior of numpy. tf.where Return the elements where condition is True (multiplexing x and y). tf.where( condition, x=None, y=None, name=None ) \\","title":"tf_document"},{"location":"tf_document/#tfcompatv1trainsaver","text":"tf.compat.v1.train.Saver \\ Saves and restores\u8fd8\u539f variables tf.compat.v1.train.Saver( var_list=None, reshape=False, sharded=False, max_to_keep=5, keep_checkpoint_every_n_hours=10000.0, name=None, restore_sequentially=False, saver_def=None, builder=None, defer_build=False, allow_empty=False, write_version=tf.train.SaverDef.V2, pad_step_number=False, save_relative_paths=False, filename=None ) The Saver class adds ops\u884c\u52a8 to save and restore variables to and form checkpoints.It also provides convenience method to run these ops. Checkpoints are binary files in a proprietary\u6240\u6709\u6743 format which map variable names to tensor values.Checkpoints \u662f\u4e13\u6709\u683c\u5f0f\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u8be5\u6587\u4ef6\u5c06\u53d8\u91cf\u540d\u6620\u5c04\u5230\u5f20\u91cf\u503c.The best way to examine\u5ba1\u67e5 the contents of a checkpoints is to load it using a Saver var_list A list of Variable/SaveableObject, or a dictionary mapping names to SaveableObjects.If None,defaults to the list of all saveable objects. reshape If True,allows restoring\u6062\u590d parameters from a checkpoint where the variables have a different shape. shareded If True,shard the checkpoints,one per device\u6bcf\u4e2a\u8bbe\u5907\u4e00\u4e2a max_to_keep Maximum number of recent checkpoints to keep,Default to 5 keep_checkpoint_every_n_hours How often to keep checkpoints,Defaults to 10,000 hours. name String, Optional name to use as a prefix when adding operations. restore_sequentially A Bool,which if true,causes restore of different variables to happen sequentially within each device.This can lower memory usage when restoring large models. saver_def Optional SaverDef proto\u539f\u578b to use instead of running the builder.This is only useful for specialty code that wants to recreate a Saver object for a previously built Graph that had a Saver.The saver_def proto should be the one returned by the as_saver_def() call of the Saver that was created for that Graph. builder Optional SaverBuilder to use if a saver_def was not provided. Default to BuikSaverBuilder(). defer_build If True,defer adding the save and restore ops to the build() call. In that case build() should be called before finalizing\u5b9a\u6848 the graph or using the saver. allow_empty If False(defalut) rasie an error if there are no variables in the graph.Otherwise, construct the saver anyway and make it a no-op. write_version controls what format to use when saving checkpoints. It also affects certain filepath matching logic. The V2 format is the recommended choice: it is much more optimized than V1 in terms of memory required and latency incurred during restore. Regardless of this flag, the Saver is able to restore from both V2 and V1 checkpoints. pad_step_number if True, pads the global step number in the checkpoint filepaths to some fixed width (8 by default). This is turned off by default. save_relative_paths If True, will write relative paths to the checkpoint state file. This is needed if the user wants to copy the checkpoint directory and reload from the copied directory. filename If known at graph construction time, filename used for variable loading/saving.","title":"tf.compat.v1.train.Saver"},{"location":"tf_document/#tfcompatv1variable_scope","text":"tf.compat.v1.variable_scope \\ A context manager for define ops that create variables(layers)\u7528\u4e8e\u5b9a\u4e49\u521b\u5efa\u53d8\u91cf\uff08\u5c42\uff09\u7684\u64cd\u4f5c\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668 tf.compat.v1.variable_scope( name_or_scope, default_name=None, values=None, initializer=None, regularizer=None, caching_device=None, partitioner=None, custom_getter=None, reuse=None, dtype=None, use_resource=None, constraint=None, auxiliary_name_scope=True ) This context manager validates\u9a8c\u8bc1 that the values are from the same graph,ensures that graph is the default graph,and pushes a name scope\u8303\u56f4 and a variable scope. Variable scope allows you to create new variables and to share already created ones while providing checks to not create or share by accident.\u53d8\u91cf\u4f5c\u7528\u57df\u5141\u8bb8\u60a8\u521b\u5efa\u65b0\u53d8\u91cf\u5e76\u5171\u4eab\u5df2\u521b\u5efa\u7684\u53d8\u91cf\uff0c\u540c\u65f6\u63d0\u4f9b\u68c0\u67e5\u4ee5\u9632\u6b62\u610f\u5916\u521b\u5efa\u6216\u5171\u4eab. Keep in mind that the counters for default_name are discarded\u4e22\u5f03 once the parent scope is exited. Therefore when the code re-enters the scope (for instance by saving it), all nested\u5d4c\u5957\u7684 default_name counters will be restarted. Note that reuse flag is inherited: if we open a resuing scope,then all its sub-scope become reusing as well. A note about name scoping:Setting reuse does not impact the naming of other ops such as mult. reuse True,None,or tf.compat.v1.AUTO_REUSE;if True, we go into reuse mode for this scope as well as all sub-scopes; if tf.compat.v1.AUTO_REUSE, we create variables if they do not exist, and return them otherwise; if None, we inherit the parent scope's reuse flag. When eager execution is enabled, new variables are always created unless an EagerVariableStore or template is currently active.","title":"tf.compat.v1.variable_scope"},{"location":"tf_document/#tfcompatv1layersdense","text":"tf.compat.v1.layers.dense \\ Functional interface for the densely-connected layer. tf.compat.v1.layers.dense( inputs, units, activation=None, use_bias=True, kernel_initializer=None, bias_initializer=tf.zeros_initializer(), kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, trainable=True, name=None, reuse=None ) The layer implement the operation: output=activation(inputs * kernel + bias) where activation is the activation function passed as the activation argument(if not \uff2eone ),kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer(only if use_bias is True). inputs Tensor input. units Integer or Long, dimensionality of the output space. activation Activation function(callable), Set it to None to maintain a linear activation. use_bias Boolean, whether the layer uses a bias kernel_initializer Initializer function for the weight matrix. If None (default), weights are initialized using the default initializer used by tf.compat.v1.get_variable. bias_initializer Initializer function for the bias kernel_regularizer Regularizer\u6b63\u5219\u5316\u5668 function for the weight matrix bias_regularizer Regularizer function for the bias. activity_regularizer Regularizer function for the output. kernel_constraint An optional projection function to be applied to the kernel after being updated by an Optimizer (e.g. used to implement norm constraints or value constraints for layer weights). The function must take as input the unprojected variable and must return the projected variable (which must have the same shape). Constraints\u7ea6\u675f\u6761\u4ef6 are not safe to use when doing asynchronous distributed training. bias_constraint An optional projection function to be applied to the bias after being updated by an Optimizer. trainable Boolean, if True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable). name String, the name of the layer. reuse Boolean, whether to reuse the weights of a previous layer by the same name.","title":"tf.compat.v1.layers.dense"},{"location":"tf_document/#tfcompatv1layersbatchnormalization","text":"tf.compat.v1.layers.BatchNormalization \\ Batch Normalization layer tf.compat.v1.layers.BatchNormalization( axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer=tf.zeros_initializer(), gamma_initializer=tf.ones_initializer(), moving_mean_initializer=tf.zeros_initializer(), moving_variance_initializer=tf.ones_initializer(), beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None, renorm=False, renorm_clipping=None, renorm_momentum=0.99, fused=None, trainable=True, virtual_batch_size=None, adjustment=None, name=None, **kwargs ) fused if None or True, use a faster, fused implementation if possible. If False, use the system recommended implementation.","title":"tf.compat.v1.layers.BatchNormalization"},{"location":"tf_document/#tfmathreduce_max","text":"tf.math.reduce_max \\ Computes the maximum of elements across dimensions of a tensor tf.math.reduce_max( input_tensor, axis=None, keepdims=False, name=None ) Reduces input_tensor along the dimensions given in axis.Unless keepdims is true, the rank of the tensor is reduced by 1 for each of the entires in axis, which must be unique. If keepdims is true, the reduced dimensions are retained with length 1. If axis is None,all dimensions are reduced, and a tensor with a single element is returned. input_tensor The tensor to reduce.Should have real numeric type axis The dimensions to reduce.If None (the default),reduces all dimensions.Must be in range [-rank(input_tensor),rank(input_tensor)] keepdims If true,retains reduced dimensions with length 1. name A name for the operation(optional).","title":"tf.math.reduce_max"},{"location":"tf_document/#tftile","text":"tf.tile \\ Constructs\u6784\u9020 a tensor by tiling\u5e73\u94fa a given tensor tf.tile( input, multiples, name=None ) This operation creates a new tensor by replicating\u590d\u5236 input multiples times.The output tensor's i'th dimension has input.dims(i)*multiples[i] elements, and the values of input are replicated multiples[i] times along the i'th dimension. input A tensor,1-D or higher multiples A tensor. Must be one of the following types: int32, int64, 1-D Length must be the same as the number of dimensions in input. name A name for the operation(optional).","title":"tf.tile"},{"location":"tf_document/#tfconcat","text":"tf.concat \\ Concatenates tensors along one dimension. tf.concat( values, axis, name='concat' )","title":"tf.concat"},{"location":"tf_document/#tfcast","text":"tf.cast \\ Cast a tensor to a new type. tf.cast( x, dtype, name=None ) The operation supports data types (for x and dtype) of uint8, uint16, uint32, uint64, int8, int16, int32, int64, float16, float32, float64, complex64, complex128, bfloat16. In case of casting from complex types (complex64, complex128) to real types, only the real part of x is returned. In case of casting from real types to complex types (complex64, complex128), the imaginary part of the returned value is set to 0. The handling of complex types here matches the behavior of numpy.","title":"tf.cast"},{"location":"tf_document/#tfwhere","text":"Return the elements where condition is True (multiplexing x and y). tf.where( condition, x=None, y=None, name=None ) \\","title":"tf.where"},{"location":"ubuntu/","text":"****\u66f4\u6539mac\u5730\u5740 \u4f7f\u7528ifconfig\u3000\u67e5\u770b\u7f51\u53e3 \u5b89\u88c5macmanager sudo apt install macchanger \u968f\u673a\u751f\u6210\u4e00\u4e2amac\u5730\u5740 sudo macchanger -r enp60s0 enp60s0 \u662f\u7f51\u53e3\uff0c -r \u4ee3\u8868\u7684\u662f\u968f\u673a random \u7684\u610f\u601d\uff0c macchanger \u4f1a\u5e2e\u6211\u4eec\u4fee\u6539\u6210\u4e00\u4e2a\u968f\u673a\u4ea7\u751f\u7684 MAC \u53f7 \u4fee\u6539\u4e3a\u6307\u5b9a\u7684mac\u5730\u5740 sudo macchanger -m AA:BB:CC:DD:EE:FF enp60s0 \\ ubuntu18.04 \u7f51\u53e3\u521b\u5efa\u7f51\u7edc\u5171\u4eab \u7ec8\u7aef\u8f93\u5165nm-connection-editor\u6253\u5f00\u7f51\u7edc\u8fde\u63a5 \u521b\u5efa\u4ee5\u592a\u7f51\u94fe\u63a5 \u914d\u7f6e\u7f51\u7edc\u94fe\u63a5 \u5c06\u5176\u4ed6\u9700\u8981\u4e0a\u7f51\u7684\u8bbe\u5907\u901a\u8fc7\u7f51\u7ebf\u94fe\u63a5\u5230\u5171\u4eab\u7f51\u7edc\u5373\u53ef xvfb\u3000ssh xvfb-run -s \"-screen 0 1400x900x24\" python <your_script.py> x forward ssh -X username@ip vscode \u63d2\u4ef6\u3000Remote-ssh conda install jupyter github\u4e0b\u8f7d\u63d0\u901f git clone https://github.com/Amritpal-001/Reinforcement-learning-projects.git \u6539\u4e3a git clone http://hub.fastgit.org/Amritpal-001/Reinforcement-learning-projects.git github \u4e0b\u8f7d\u5de5\u5177 https://d.serctl.com/ https://www.python.org/ftp \u4e0b\u8f7d\u6162 wget https://www.python.org/ftp/python/3.7.5/Python-3.7.5.tgz \u6539\u4e3a wget https://npm.taobao.org/mirrors/python/3.7.5/Python-3.7.5.tgz pip \u4e0b\u8f7d\u4e1c\u897f\u6162 pip3.7.5 install -i http://mirrors.aliyun.com/pypi/simple/ psutil decorator numpy protobuf==3.11^C scipy sympy cffi grpcio grpcio-tools requests --user --trusted-host mirrors.aliyun.com mkdocs mkdocs build mkdocs serve push sites folders vscode python ros debug \u9996\u5148 catkin_make -DCMAKE_BUILD_TYPE=DEBUG \u5176\u6b21\u70b9\u51fbdebug\u6309\u94ae\uff0c\u9009\u62e9\u751f\u6210\u65b0\u7684launch\u6587\u4ef6\u3002 \uff44ebug\u65f6\u9047\u5230\u8def\u5f84\u95ee\u9898,\u4f8b\u5982rosmsg\u8def\u5f84\uff0c\u6700\u597d\u5728\u6587\u4ef6\u5c5e\u6027\u91cc\u590d\u5236\u8def\u5f84\uff0c\u4e0d\u7136\u5bb9\u6613\u51fa\u9519 import sys sys.path.append(\"/home/pmjd/Downloads/catkin_ws/devel/lib/python2.7/dist-packages\") Tilix doesnt open in the folder from where it is split Update ~.bashrc (or ~.zshrc if you are using zsh) to execute vte.sh directly, this involves adding the following line at the end of the file. if [[ $TILIX_ID ]]; then source /etc/profile.d/vte.sh fi On Ubuntu (18.04), a symlink is probably missing. You can create it with: ln -s /etc/profile.d/vte-2.91.sh /etc/profile.d/vte.sh sudo dist-upgrade \u662f\u6bc1\u706d\u6027\u7684\uff0c\u4f1a\u5347\u7ea7cuda github init git config --global user.email \"you@example.com\" git config --global user.name \"Your Name\" git add . git commit -m \"1\" git push \u547d\u4ee4\u884c\u6253\u5f00\u6587\u4ef6\u7ba1\u7406\u5668 nautilus --browser ~/\u6587\u6863 \u5b89\u88c5\u6c49\u8bed\u8f93\u5165\u6cd5 \u5148\u5728\u8bbe\u7f6e\u91cc\u8bbe\u7f6elanguage\u91cc\u6dfb\u52a0Chinese sudo apt-get install ibus-libpinyin ibus-clutter Docker\u521b\u5efa\u5bb9\u5668 \u6784\u5efaubuntu18.04\u6620\u50cf \u5b89\u88c5\u4f9d\u8d56 sudo apt-get install debootstrap sudo apt install docker.io sudo chmod 666 /var/run/docker.sock \u521b\u5efaubuntu 18.04 \u955c\u50cf sudo debootstrap bionic bionic > /dev/null sudo tar -C bionic -c . | docker import - bionic/smart_eye \u6d4b\u8bd5 docker run bionic cat /etc/lsb-release \u67e5\u770b\u955c\u50cf docker images \u5220\u9664\u955c\u50cf docker rmi [IMAGE ID] \u67e5\u770b\u5bb9\u5668\u8fd0\u884c\u60c5\u51b5 sudo docker ps -a \u9000\u51fa\u5bb9\u5668 sudo docker stop \u5bb9\u5668id \u5220\u9664\u5bb9\u5668 sudo docker rm \u5bb9\u5668id \u542f\u52a8\u955c\u50cf docker run -it bionic/smart_eye /bin/bash \u9000\u51fa\u955c\u50cf exit \u6587\u4ef6\u4f20\u9012 \u4ece\u672c\u5730\u81f3docker docker cp FILE_PATH \u5bb9\u5668ID:/root \u4ecedocker \u81f3\u672c\u5730 docker cp \u5bb9\u5668ID:/root/data.tar /home/user \u66f4\u6539docker image\u5b58\u653e\u8def\u5f84 sudo service docker stop sudo touch /etc/docker/daemon.json daemon.json { \"data-root\":\"/home/pmjd/docker\" } sudo service docker start Docker\u66f4\u65b0apt source.list sudo nano /etc/apt/sources.list \u6dfb\u52a0 deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse Docker \u542f\u52a8\u5bb9\u5668 docker start \u5bb9\u5668ID docker attach \u5bb9\u5668ID \u5bfc\u51fa\u5bb9\u5668\u5feb\u7167 docker export c91c33f28594 > smart_eye_docker.tar \u5bb9\u5668\u5feb\u7167\u5bfc\u5165\u4e3a\u955c\u50cf cat smart_eye_docker.tar | docker import - test/smart_eye:v1 \u518d\u4fdd\u5b58\u6b64\u955c\u50cf docker save -o smart_eye_image_docker.tar test/smart_eye \u518d\u52a0\u8f7d\u955c\u50cf docker load --input samrt_eye_image_docker.tar \u542f\u52a8 docker run -it test/smart_eye:v1 /bin/bash docker \u542f\u52a8bash docker start c91c33f28594 docker exec -it c91c33f28594 /home/run.sh docker exec mycontainer /bin/sh -c \"cmd1;cmd2;...;cmdn\" docker bash file example #!/bin/sh docker run -it --net host --add-host in_release_docker:127.0.0.1 --add-host localhost:127.0.0.1 --hostname in_release_docker --rm promote/smart_eye:v1 /bin/bash -c \"/home/run.sh\" \u542f\u52a8\u5feb\u6377\u65b9\u5f0f 1.desktop [Desktop Entry] Name=Smart_eye GenericName=3D modeler Keywords=3d;cg;modeling;animation;painting;sculpting;texturing;video editing;video tracking;rendering;render engine;cycles;game engine;python; Exec=/bin/bash -c '/home/promote/run.sh' #Icon=/home/pmjd/Disk/blender-2.90.1-linux64/blender.svg Terminal=true Type=Application Categories=Graphics;3DGraphics; MimeType=application/x-blender; \u754c\u9762\u663e\u793a xhost + -e DISPLAY=${DISPLAY} -v /tmp/.X11-unix:/tmp/.X11-unix \u5b89\u88c5NVIDIA Container Toolkit distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\ && curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \\ && curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list curl -s -L https://nvidia.github.io/nvidia-container-runtime/experimental/$distribution/nvidia-container-runtime.list | sudo tee /etc/apt/sources.list.d/nvidia-container-runtime.list sudo apt-get update sudo apt-get install -y nvidia-docker2 \u8fd9\u4e2a\u65f6\u5019 /etc/docker/daemon.json \u5185\u5bb9\u4fee\u6539\u4e3a { \"runtimes\": { \"nvidia\": { \"path\": \"nvidia-container-runtime\", \"runtimeArgs\": [] } }, \"data-root\":\"/home/pmjd/docker\" } \u91cd\u542fdocker sudo systemctl restart docker \u4e0b\u8f7d\u8fd0\u884c\uff1a sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi sudo docker run --rm --gpus all nvidia/cuda:10.1-base nvidia-smi \u4e0b\u8f7d\u8fd0\u884ctensorflow: docker run --gpus all --runtime=nvidia -it tensorflow/tensorflow:2.3.0-gpu bash docker\u914d\u7f6e\u7f51\u7edc --net=host docker \u652f\u6301\u9ea6\u514b,\u6c49\u8bed docker run -it --volume=/run/user/1000/pulse:/run/user/1000/pulse --user promote --gpus all --runtime=nvidia --name=xiaomeng -e LANG=C.UTF-8 --device /dev/snd promote/xiaomeng:v1.6 /bin/bash Nano \u5220\u9664\u884c ctrl+k Nano\u663e\u793a\u884c\u53f7 alt+shift+3 \u4fee\u6539\u9ed8\u8ba4python\u7248\u672c \u5220\u9664/usr/bin \u4e0b\u7684Python\u94fe\u63a5 sudo rm /usr/bin/python \u7528\u4e0b\u9762\u547d\u4ee4\u5efa\u7acb\u65b0\u7684\u94fe\u63a5 sudo ln -s /usr/bin/python3.6 /usr/bin/python \u7528\u4e0b\u9762\u7684\u547d\u4ee4\u8fd8\u539f2\u7248\u672c sudo ln -s /usr/bin/python2.7 /usr/bin/python \u66f4\u79d1\u5b66\u7684\u505a\u6cd5\u662f\uff1a sudo update-alternatives --install /usr/bin/python python /usr/bin/python2.7 1 sudo update-alternatives --install /usr/bin/python python /usr/bin/python3.6 2 sudo update-alternatives --config python \u9009\u62e9\u8981\u4f7f\u7528\u7684\u7248\u672c\uff0c\u56de\u8f66\uff0c\u641e\u5b9a \u8bbe\u7f6e\u9ed8\u8ba4pip\u7248\u672c sudo update-alternatives --install /usr/bin/pip pip /usr/bin/pip2 1 sudo update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 2 sudo update-alternatives --config pip pip\u5347\u7ea7 sudo pip install --upgrade pip \u684c\u9762\u5feb\u6377\u65b9\u5f0f [Desktop Entry] Name=smart_view GenericName=3D modeler Keywords=python; Exec=/bin/bash -c 'source /opt/ros/melodic/setup.bash;rosrun image_view image_view image:=/smart_eye_view' #Icon=/home/pmjd/Disk/blender-2.90.1-linux64/blender.svg Terminal=false Type=Application Categories=Graphics;3DGraphics; MimeType=application/x-blender; Name[en_US]=smart_view","title":"Ubuntu"},{"location":"ubuntu/#mac","text":"\u4f7f\u7528ifconfig\u3000\u67e5\u770b\u7f51\u53e3 \u5b89\u88c5macmanager sudo apt install macchanger \u968f\u673a\u751f\u6210\u4e00\u4e2amac\u5730\u5740 sudo macchanger -r enp60s0 enp60s0 \u662f\u7f51\u53e3\uff0c -r \u4ee3\u8868\u7684\u662f\u968f\u673a random \u7684\u610f\u601d\uff0c macchanger \u4f1a\u5e2e\u6211\u4eec\u4fee\u6539\u6210\u4e00\u4e2a\u968f\u673a\u4ea7\u751f\u7684 MAC \u53f7 \u4fee\u6539\u4e3a\u6307\u5b9a\u7684mac\u5730\u5740 sudo macchanger -m AA:BB:CC:DD:EE:FF enp60s0 \\","title":"****\u66f4\u6539mac\u5730\u5740"},{"location":"ubuntu/#ubuntu1804","text":"\u7ec8\u7aef\u8f93\u5165nm-connection-editor\u6253\u5f00\u7f51\u7edc\u8fde\u63a5 \u521b\u5efa\u4ee5\u592a\u7f51\u94fe\u63a5 \u914d\u7f6e\u7f51\u7edc\u94fe\u63a5 \u5c06\u5176\u4ed6\u9700\u8981\u4e0a\u7f51\u7684\u8bbe\u5907\u901a\u8fc7\u7f51\u7ebf\u94fe\u63a5\u5230\u5171\u4eab\u7f51\u7edc\u5373\u53ef","title":"ubuntu18.04 \u7f51\u53e3\u521b\u5efa\u7f51\u7edc\u5171\u4eab"},{"location":"ubuntu/#xvfb-ssh","text":"xvfb-run -s \"-screen 0 1400x900x24\" python <your_script.py>","title":"xvfb\u3000ssh"},{"location":"ubuntu/#x-forward","text":"ssh -X username@ip","title":"x forward"},{"location":"ubuntu/#vscode-remote-ssh","text":"conda install jupyter","title":"vscode \u63d2\u4ef6\u3000Remote-ssh"},{"location":"ubuntu/#github","text":"git clone https://github.com/Amritpal-001/Reinforcement-learning-projects.git \u6539\u4e3a git clone http://hub.fastgit.org/Amritpal-001/Reinforcement-learning-projects.git","title":"github\u4e0b\u8f7d\u63d0\u901f"},{"location":"ubuntu/#github_1","text":"https://d.serctl.com/","title":"github \u4e0b\u8f7d\u5de5\u5177"},{"location":"ubuntu/#httpswwwpythonorgftp","text":"wget https://www.python.org/ftp/python/3.7.5/Python-3.7.5.tgz \u6539\u4e3a wget https://npm.taobao.org/mirrors/python/3.7.5/Python-3.7.5.tgz","title":"https://www.python.org/ftp \u4e0b\u8f7d\u6162"},{"location":"ubuntu/#pip","text":"pip3.7.5 install -i http://mirrors.aliyun.com/pypi/simple/ psutil decorator numpy protobuf==3.11^C scipy sympy cffi grpcio grpcio-tools requests --user --trusted-host mirrors.aliyun.com","title":"pip \u4e0b\u8f7d\u4e1c\u897f\u6162"},{"location":"ubuntu/#mkdocs","text":"mkdocs build mkdocs serve push sites folders","title":"mkdocs"},{"location":"ubuntu/#vscode-python-ros-debug","text":"\u9996\u5148 catkin_make -DCMAKE_BUILD_TYPE=DEBUG \u5176\u6b21\u70b9\u51fbdebug\u6309\u94ae\uff0c\u9009\u62e9\u751f\u6210\u65b0\u7684launch\u6587\u4ef6\u3002 \uff44ebug\u65f6\u9047\u5230\u8def\u5f84\u95ee\u9898,\u4f8b\u5982rosmsg\u8def\u5f84\uff0c\u6700\u597d\u5728\u6587\u4ef6\u5c5e\u6027\u91cc\u590d\u5236\u8def\u5f84\uff0c\u4e0d\u7136\u5bb9\u6613\u51fa\u9519 import sys sys.path.append(\"/home/pmjd/Downloads/catkin_ws/devel/lib/python2.7/dist-packages\")","title":"vscode python ros debug"},{"location":"ubuntu/#tilix-doesnt-open-in-the-folder-from-where-it-is-split","text":"Update ~.bashrc (or ~.zshrc if you are using zsh) to execute vte.sh directly, this involves adding the following line at the end of the file. if [[ $TILIX_ID ]]; then source /etc/profile.d/vte.sh fi On Ubuntu (18.04), a symlink is probably missing. You can create it with: ln -s /etc/profile.d/vte-2.91.sh /etc/profile.d/vte.sh","title":"Tilix doesnt open in the folder from where it is split"},{"location":"ubuntu/#sudo-dist-upgrade-cuda","text":"","title":"sudo dist-upgrade \u662f\u6bc1\u706d\u6027\u7684\uff0c\u4f1a\u5347\u7ea7cuda"},{"location":"ubuntu/#github-init","text":"git config --global user.email \"you@example.com\" git config --global user.name \"Your Name\" git add . git commit -m \"1\" git push","title":"github init"},{"location":"ubuntu/#_1","text":"nautilus --browser ~/\u6587\u6863","title":"\u547d\u4ee4\u884c\u6253\u5f00\u6587\u4ef6\u7ba1\u7406\u5668"},{"location":"ubuntu/#_2","text":"\u5148\u5728\u8bbe\u7f6e\u91cc\u8bbe\u7f6elanguage\u91cc\u6dfb\u52a0Chinese sudo apt-get install ibus-libpinyin ibus-clutter","title":"\u5b89\u88c5\u6c49\u8bed\u8f93\u5165\u6cd5"},{"location":"ubuntu/#docker","text":"","title":"Docker\u521b\u5efa\u5bb9\u5668"},{"location":"ubuntu/#ubuntu1804_1","text":"","title":"\u6784\u5efaubuntu18.04\u6620\u50cf"},{"location":"ubuntu/#_3","text":"sudo apt-get install debootstrap sudo apt install docker.io sudo chmod 666 /var/run/docker.sock","title":"\u5b89\u88c5\u4f9d\u8d56"},{"location":"ubuntu/#ubuntu-1804","text":"sudo debootstrap bionic bionic > /dev/null sudo tar -C bionic -c . | docker import - bionic/smart_eye","title":"\u521b\u5efaubuntu 18.04 \u955c\u50cf"},{"location":"ubuntu/#_4","text":"docker run bionic cat /etc/lsb-release \u67e5\u770b\u955c\u50cf docker images \u5220\u9664\u955c\u50cf docker rmi [IMAGE ID] \u67e5\u770b\u5bb9\u5668\u8fd0\u884c\u60c5\u51b5 sudo docker ps -a \u9000\u51fa\u5bb9\u5668 sudo docker stop \u5bb9\u5668id \u5220\u9664\u5bb9\u5668 sudo docker rm \u5bb9\u5668id \u542f\u52a8\u955c\u50cf docker run -it bionic/smart_eye /bin/bash \u9000\u51fa\u955c\u50cf exit \u6587\u4ef6\u4f20\u9012 \u4ece\u672c\u5730\u81f3docker docker cp FILE_PATH \u5bb9\u5668ID:/root \u4ecedocker \u81f3\u672c\u5730 docker cp \u5bb9\u5668ID:/root/data.tar /home/user","title":"\u6d4b\u8bd5"},{"location":"ubuntu/#docker-image","text":"sudo service docker stop sudo touch /etc/docker/daemon.json daemon.json { \"data-root\":\"/home/pmjd/docker\" } sudo service docker start","title":"\u66f4\u6539docker image\u5b58\u653e\u8def\u5f84"},{"location":"ubuntu/#dockerapt-sourcelist","text":"sudo nano /etc/apt/sources.list \u6dfb\u52a0 deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse","title":"Docker\u66f4\u65b0apt source.list"},{"location":"ubuntu/#docker_1","text":"docker start \u5bb9\u5668ID docker attach \u5bb9\u5668ID \u5bfc\u51fa\u5bb9\u5668\u5feb\u7167 docker export c91c33f28594 > smart_eye_docker.tar \u5bb9\u5668\u5feb\u7167\u5bfc\u5165\u4e3a\u955c\u50cf cat smart_eye_docker.tar | docker import - test/smart_eye:v1 \u518d\u4fdd\u5b58\u6b64\u955c\u50cf docker save -o smart_eye_image_docker.tar test/smart_eye \u518d\u52a0\u8f7d\u955c\u50cf docker load --input samrt_eye_image_docker.tar \u542f\u52a8 docker run -it test/smart_eye:v1 /bin/bash docker \u542f\u52a8bash docker start c91c33f28594 docker exec -it c91c33f28594 /home/run.sh docker exec mycontainer /bin/sh -c \"cmd1;cmd2;...;cmdn\" docker bash file example #!/bin/sh docker run -it --net host --add-host in_release_docker:127.0.0.1 --add-host localhost:127.0.0.1 --hostname in_release_docker --rm promote/smart_eye:v1 /bin/bash -c \"/home/run.sh\" \u542f\u52a8\u5feb\u6377\u65b9\u5f0f 1.desktop [Desktop Entry] Name=Smart_eye GenericName=3D modeler Keywords=3d;cg;modeling;animation;painting;sculpting;texturing;video editing;video tracking;rendering;render engine;cycles;game engine;python; Exec=/bin/bash -c '/home/promote/run.sh' #Icon=/home/pmjd/Disk/blender-2.90.1-linux64/blender.svg Terminal=true Type=Application Categories=Graphics;3DGraphics; MimeType=application/x-blender; \u754c\u9762\u663e\u793a xhost + -e DISPLAY=${DISPLAY} -v /tmp/.X11-unix:/tmp/.X11-unix","title":"Docker \u542f\u52a8\u5bb9\u5668"},{"location":"ubuntu/#nvidia-container-toolkit","text":"distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\ && curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \\ && curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list curl -s -L https://nvidia.github.io/nvidia-container-runtime/experimental/$distribution/nvidia-container-runtime.list | sudo tee /etc/apt/sources.list.d/nvidia-container-runtime.list sudo apt-get update sudo apt-get install -y nvidia-docker2 \u8fd9\u4e2a\u65f6\u5019 /etc/docker/daemon.json \u5185\u5bb9\u4fee\u6539\u4e3a { \"runtimes\": { \"nvidia\": { \"path\": \"nvidia-container-runtime\", \"runtimeArgs\": [] } }, \"data-root\":\"/home/pmjd/docker\" } \u91cd\u542fdocker sudo systemctl restart docker \u4e0b\u8f7d\u8fd0\u884c\uff1a sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi sudo docker run --rm --gpus all nvidia/cuda:10.1-base nvidia-smi \u4e0b\u8f7d\u8fd0\u884ctensorflow: docker run --gpus all --runtime=nvidia -it tensorflow/tensorflow:2.3.0-gpu bash docker\u914d\u7f6e\u7f51\u7edc --net=host docker \u652f\u6301\u9ea6\u514b,\u6c49\u8bed docker run -it --volume=/run/user/1000/pulse:/run/user/1000/pulse --user promote --gpus all --runtime=nvidia --name=xiaomeng -e LANG=C.UTF-8 --device /dev/snd promote/xiaomeng:v1.6 /bin/bash","title":"\u5b89\u88c5NVIDIA Container Toolkit"},{"location":"ubuntu/#nano","text":"ctrl+k Nano\u663e\u793a\u884c\u53f7 alt+shift+3","title":"Nano \u5220\u9664\u884c"},{"location":"ubuntu/#python","text":"\u5220\u9664/usr/bin \u4e0b\u7684Python\u94fe\u63a5 sudo rm /usr/bin/python \u7528\u4e0b\u9762\u547d\u4ee4\u5efa\u7acb\u65b0\u7684\u94fe\u63a5 sudo ln -s /usr/bin/python3.6 /usr/bin/python \u7528\u4e0b\u9762\u7684\u547d\u4ee4\u8fd8\u539f2\u7248\u672c sudo ln -s /usr/bin/python2.7 /usr/bin/python \u66f4\u79d1\u5b66\u7684\u505a\u6cd5\u662f\uff1a sudo update-alternatives --install /usr/bin/python python /usr/bin/python2.7 1 sudo update-alternatives --install /usr/bin/python python /usr/bin/python3.6 2 sudo update-alternatives --config python \u9009\u62e9\u8981\u4f7f\u7528\u7684\u7248\u672c\uff0c\u56de\u8f66\uff0c\u641e\u5b9a","title":"\u4fee\u6539\u9ed8\u8ba4python\u7248\u672c"},{"location":"ubuntu/#pip_1","text":"sudo update-alternatives --install /usr/bin/pip pip /usr/bin/pip2 1 sudo update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 2 sudo update-alternatives --config pip","title":"\u8bbe\u7f6e\u9ed8\u8ba4pip\u7248\u672c"},{"location":"ubuntu/#pip_2","text":"sudo pip install --upgrade pip","title":"pip\u5347\u7ea7"},{"location":"ubuntu/#_5","text":"[Desktop Entry] Name=smart_view GenericName=3D modeler Keywords=python; Exec=/bin/bash -c 'source /opt/ros/melodic/setup.bash;rosrun image_view image_view image:=/smart_eye_view' #Icon=/home/pmjd/Disk/blender-2.90.1-linux64/blender.svg Terminal=false Type=Application Categories=Graphics;3DGraphics; MimeType=application/x-blender; Name[en_US]=smart_view","title":"\u684c\u9762\u5feb\u6377\u65b9\u5f0f"},{"location":"util_wiki/","text":"rqt_image_view \u63d0\u793a ImageView.callback_image() could not convert image from '8UC3' to 'rgb8' ([8UC3] is not a color format. but [rgb8] is. The conversion does not make sense) \u89e3\u51b3\u529e\u6cd5:\\ - \u9996\u5148\u5728\uff4f\uff50\uff45\uff4e\uff43\uff56\u5904\u628a\uff18\uff35\uff23\uff13\u683c\u5f0f\u7684\uff4d\uff41\uff54\u8f6c\u5316\u4e3a\uff52\uff47\uff42\u683c\u5f0f cv::cvtColor(compoundFrame, compoundFrame, cv::COLOR_RGB2BGR); - \u7136\u540ecv_bridge\u7f16\u7801\u683c\u5f0f\u6539\u4e3asensor_msgs::image_encodings::RGB8 img_bridge = cv_bridge::CvImage(headers, sensor_msgs::image_encodings::RGB8, compoundFrame); Ros\u81ea\u5b9a\u4e49\uff4d\uff53\uff47\u7f16\u8bd1\u65f6\u62a5\u9519\uff1a\u7f3a\u5c11\u5934\u6587\u4ef6 \u53ea\u9700\u5728CMakeLists.txt\u91cc\u6dfb\u52a0\\ add_dependencies(GetLaneExtDemo smart_eye_gencpp) \\ smart_eye \u53ef\u6362\u6210\u4efb\u610fpkg \u540d\u79f0\\ GetLaneExtDemo\u3000\u4e3a\u8282\u70b9\u540d\u79f0 ModuleNotFoundError: No module named \u2018rospkg\u2019 pip install rospkg //\u66f4\u65b0\u65b9\u5f0f1 sudo apt-get install python-rospkg //\u66f4\u65b0\u65b9\u5f0f2 //\u7f51\u4e0a\u8bf4\u6709\u7684\u65b9\u5f0f1\u80fd\u89e3\u51b3\uff0c\u6709\u7684\u65b9\u5f0f2\u53ef\u4ee5\u89e3\u51b3\uff0c\u7528pip\u66f4\u65b0\u7684\u524d\u63d0\u662f\u5b89\u88c5\u4e86pip ImportError: No module named genmsg \u547d\u4ee4\uff1a\u628asudo make \u6539\u4e3a\u3000make,\u5c31\u53ef\u4ee5\u627e\u5230\u5e93\uff0c\u76ee\u524d\u539f\u56e0\u4e0d\u660e ui_mainwindow.h: No such file or directory set(CMAKE_AUTOUIC ON) set(CMAKE_INCLUDE_CURRENT_DIR ON) git\u7684\u547d\u4ee4\u884c\u5e94\u7528 git status git add . git commit -m \"q\" git push \u867d\u7136\u6709\u4e0d\u540c\u7684\uff52\uff4f\uff53\u5de5\u4f5c\u7a7a\u95f4 \u4f46\u662f\u6700\u597d\u4e0d\u8981\u6709\u76f8\u540c\u7684\u8282\u70b9\u540d\u5b57\u3002\u56e0\u4e3a\u5bb9\u6613\u5f15\u8d77\u6df7\u4e71\uff0c\u8fd0\u884c\u6df7\u4e71\u7b49\u3002\u3002\u3002","title":"rqt_image_view \u63d0\u793a ImageView.callback_image() could not convert image from '8UC3' to 'rgb8' ([8UC3] is not a color format. but [rgb8] is. The conversion does not make sense)"},{"location":"util_wiki/#rqt_image_view-imageviewcallback_image-could-not-convert-image-from-8uc3-to-rgb8-8uc3-is-not-a-color-format-but-rgb8-is-the-conversion-does-not-make-sense","text":"\u89e3\u51b3\u529e\u6cd5:\\ - \u9996\u5148\u5728\uff4f\uff50\uff45\uff4e\uff43\uff56\u5904\u628a\uff18\uff35\uff23\uff13\u683c\u5f0f\u7684\uff4d\uff41\uff54\u8f6c\u5316\u4e3a\uff52\uff47\uff42\u683c\u5f0f cv::cvtColor(compoundFrame, compoundFrame, cv::COLOR_RGB2BGR); - \u7136\u540ecv_bridge\u7f16\u7801\u683c\u5f0f\u6539\u4e3asensor_msgs::image_encodings::RGB8 img_bridge = cv_bridge::CvImage(headers, sensor_msgs::image_encodings::RGB8, compoundFrame);","title":"rqt_image_view \u63d0\u793a ImageView.callback_image() could not convert image from '8UC3' to 'rgb8' ([8UC3] is not a color format. but [rgb8] is. The conversion does not make sense)"},{"location":"util_wiki/#rosmsg","text":"\u53ea\u9700\u5728CMakeLists.txt\u91cc\u6dfb\u52a0\\ add_dependencies(GetLaneExtDemo smart_eye_gencpp) \\ smart_eye \u53ef\u6362\u6210\u4efb\u610fpkg \u540d\u79f0\\ GetLaneExtDemo\u3000\u4e3a\u8282\u70b9\u540d\u79f0","title":"Ros\u81ea\u5b9a\u4e49\uff4d\uff53\uff47\u7f16\u8bd1\u65f6\u62a5\u9519\uff1a\u7f3a\u5c11\u5934\u6587\u4ef6"},{"location":"util_wiki/#modulenotfounderror-no-module-named-rospkg","text":"pip install rospkg //\u66f4\u65b0\u65b9\u5f0f1 sudo apt-get install python-rospkg //\u66f4\u65b0\u65b9\u5f0f2 //\u7f51\u4e0a\u8bf4\u6709\u7684\u65b9\u5f0f1\u80fd\u89e3\u51b3\uff0c\u6709\u7684\u65b9\u5f0f2\u53ef\u4ee5\u89e3\u51b3\uff0c\u7528pip\u66f4\u65b0\u7684\u524d\u63d0\u662f\u5b89\u88c5\u4e86pip","title":"ModuleNotFoundError: No module named \u2018rospkg\u2019"},{"location":"util_wiki/#importerror-no-module-named-genmsg","text":"\u547d\u4ee4\uff1a\u628asudo make \u6539\u4e3a\u3000make,\u5c31\u53ef\u4ee5\u627e\u5230\u5e93\uff0c\u76ee\u524d\u539f\u56e0\u4e0d\u660e","title":"ImportError: No module named genmsg"},{"location":"util_wiki/#ui_mainwindowh-no-such-file-or-directory","text":"set(CMAKE_AUTOUIC ON) set(CMAKE_INCLUDE_CURRENT_DIR ON)","title":"ui_mainwindow.h: No such file or directory"},{"location":"util_wiki/#git","text":"git status git add . git commit -m \"q\" git push","title":"git\u7684\u547d\u4ee4\u884c\u5e94\u7528"},{"location":"util_wiki/#ros","text":"\u4f46\u662f\u6700\u597d\u4e0d\u8981\u6709\u76f8\u540c\u7684\u8282\u70b9\u540d\u5b57\u3002\u56e0\u4e3a\u5bb9\u6613\u5f15\u8d77\u6df7\u4e71\uff0c\u8fd0\u884c\u6df7\u4e71\u7b49\u3002\u3002\u3002","title":"\u867d\u7136\u6709\u4e0d\u540c\u7684\uff52\uff4f\uff53\u5de5\u4f5c\u7a7a\u95f4"},{"location":"voxelnet/","text":"VoxelNet To interface a highly sparse\u7a00\u758f LIDAR point cloud with a region proposal network(RPN\u533a\u57df\u5efa\u8bae\u7f51\u7edc),most existing efforts have focused on hand-crafted\u624b\u5de5\u5236\u4f5c feature representations\u7279\u5f81\u8868\u793a\uff0cfor example, a bird's eye view projectiong\u9e1f\u77b0\u56fe. In this work,we remove the need of manual feature engineering for 3D point clouds and purpose VoxelNet, a generic 3D detection network that unifies feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network. Specially,VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a unified feature representation through the newly introduced voxel feature encoding(VFE) layer.In this way,the point cloud is encoded as a descriptive volumetric representation, which is then connected to a RPN to generate detections. Our network learns an effective discriminative representation\u533a\u5206\u6027\u8868\u793a of objects with various geometrics, leading to encouraging results in 3D detection of pedestrains and cyclists, based on only Lidar. \\ VoxelNet architecture. The feature learning network takes a raw point cloud as input, partitions the space into voxels, and transforms points within each voxel to a vector representation characterizing the shape information.The space is represented as a sparse 4D tensor.The convolutional middle layers processes the 4D tensor to aggregate spatial context \u805a\u5408\u7a7a\u95f4\u8bed\u5883.Finally,a RPN generates the 3D detection. Scalling up 3D feature learning networks to orders of magnitude more points and to 3D detection tasks are the main challenges that we address in this paper. We present VoxelNet, a generic 3D detection framework that simultaneously\u540c\u65f6 learns a discriminative\u5224\u522b\u6027 feature representation from point clouds and predicts accurate 3D bounding boxes, in an end-to-end fashion\u65b9\u5f0f. We design a novel\u65b0\u9896\u7684 voxel feature encoding(VFE) layer,which enables inter-point interaction\u70b9\u95f4\u4ea4\u4e92\u3000within a voxel,by combining point-wise features\u9010\u70b9\u7279\u5f81 with a locally aggregated feature\u805a\u5408\u7279\u5f81. Stacking \u5806\u53e0 multiple VFE layers allows learning complex features for characterizing\u8868\u5f81 local 3D shape information. Specially, VoxelNet divides the point cloud into equally spaced 3D voxel\u7b49\u8ddd\u7684\uff13\uff24\u4f53\u7d20 ,encodes each voxel via stacked VFE layers, and then 3D convolution furture\u8fdb\u4e00\u6b65 aggregate\u805a\u5408 local voxel features, transforming the pointcloud into a high-dimensional volumetric\u4f53\u79ef representation. Finally,a RPN consumes the volumetric represetation and yields\u4ea7\u751f the detection result. This efficient algorithm benefits both from the sparse\u7a00\u758f\u7684 point structure and efficient parallel processing on the voxel grid\u4f53\u7d20\u7f51\u683c\u7684\u9ad8\u6548\u5e76\u884c\u5904\u7406. VoxelNet Architecture The proposed VoxelNet consists of three functional blocks:(1) Feature learning network,(2) Convolutional middle layers, and (3) Region proposal network Feature Learning Network Voxel Partition Given a point cloud, we subdivide the 3D space into equally spaced voxels as shown in Figure 2.Suppose the point cloud encompasses\u5305\u542b 3D space with range D,H,W\uff08\u70b9\u4e91\u5c3a\u5bf8\uff09 along the Z,Y,X axes respectively.We define each voxel of size vD,vH,and vW\uff08\u4f53\u7d20\u5c3a\u5bf8\uff09 accordingly.The resulting\u6240\u751f\u6210\u7684 3D voxel grid is of size D'=D/vD, H'=H/vH, W'=W/vW\uff08\u70b9\u4e91\u5305\u542b\u4f53\u7d20\u4e2a\u6570\uff09.Here, for simplicity,we assume D,H,W are a multiple of vD,vH,vW.\u6211\u4eec\u5047\u8bbeD,H,W\u662fvD, vH, vW\u7684\u500d\u6570\u3002 Grouping We group the points according to the voxel they reside in\u6211\u4eec\u6839\u636e\u5b83\u4eec\u6240\u5728\u7684\u4f53\u7d20\u5bf9\u70b9\u8fdb\u884c\u5206\u7ec4.Due to factors such as distance,occlusion\u906e\u6321,object's relative pose\u7269\u4f53\u7684\u76f8\u5bf9\u59ff\u52bf,and non-uniform\u4e0d\u5747\u5300 sampling,the LiDAR point cloud is sparse and highly variable\u9ad8\u5ea6\u53ef\u53d8\u7684 point density\u70b9\u5bc6\u5ea6 throughout the space.Therefore, after gouping, a voxel will contain a variable number of points. Random Sampling Typically a high-definition LiDAR point cloud is composed of ~100k points. Directly processing all the points not only imposes\u52a0\u5f3a increased memory/efficiency burdens\u8d1f\u62c5\u3000on the computing platform, but also highly variable point density throughout the space might bias the detection.To this end\u56e0\u6b64,we randomly sample a fixed number, T,of points from those voxels containing more than T points.This sampling strategy\u6218\u7565 has two purpose,(1)computational saving;and (2)decrease the imbalance of points between the voxels which reduces the sampling bias, and adds more variation\u53d8\u5316 to training. Stacked Voxel Feature Encoding \\ \\ The key innovation\u9769\u65b0 is the chain\u94fe of VFE layers.For simplicity, Figure 2 illustrates the hierachical\u9636\u7ea7\u5f0f feature encoding process for one voxel.Without loss of generality\u6982\u8981,(\u5728\u4e0d\u5931\u4e00\u822c\u6027\u7684\u524d\u63d0\u4e0b)we use VFE Layer-1 to describe the details in the following paragraph\u6bb5\u843d.Figure 3 shows the architecture for VFE Layer-1.\\ Denote\u8868\u793a as a non-empty voxel containing t\u2264\uff34 LiDAR points,where contains XYZ coordinates for the i-th point and is the received reflectance\u53cd\u5c04\u7387.We first compute the local mean\u5c40\u90e8\u5747\u503c as the centroid\u8d28\u5fc3 of all the points in V, denoted as .Then we augment\u589e\u52a0 each point with the relative offset w.r.t. the centriod and obtain the input feature set , Next, each is transformed through the fully connected network(FCN) into a feature space,where we can aggregate\u6c47\u603b information from the point features to ecode the shape of the surface contained within the voxel.The FCN is composed of a linear layer, a batch normalization(BN\u6279\u91cf\u6807\u51c6\u5316) layer and a rectified\u7ea0\u6b63\u7684 linear unit(ReLU) layer.After obtaining point-wise feature representations, we use element-wise MaxPooling across all associated to V to get the locally aggregated feature Finally, we augment each with to form the point-wise concatenated\u7ea7\u8054\u7684 feature as Thus we obtain the output feature set .All non-empty voxels are ecoded in the same way and they share the same set of parameters in FCN. We use to represent the i-th VFE layer that transforms input features of dimension into output features of dimension . The linear layer learns a matrix of size , and the point-wise concatenation yields the output of dimension . Because the output feature combines both point-wise features and locally aggregated feature, stacking VFE layers encodes point interactions\u4e92\u52a8\u3000within a voxel and enables the final feature representation to learn descriptive shape information. The voxel-wise feature is obtained by transforming the output of VFE-n into via FCN and applying element-wise Maxpool where C is the dimension of the voxel-wise feature, as shown in Figure2. Saparse Tensor Representation \u7a00\u758f\u5f20\u91cf\u8868\u793a\u3000By processing only the non-empty voxels, we obtain a list of voxel features,each uniquely\u72ec\u7279\u7684 associated to the spatial\u7a7a\u95f4 coordinates of a pictular non-empty voxel.The obtained list of voxel-wise features can be represented as a sparse 4D tensor, of size C x D' x H' x W' as shown in Figure2.Although the point cloud contains ~100k points, more than 90% of voxels typically are empty.Representing non-empty voxel features as a sparse tensor greatly reduce the memory usage and computation cost during backpropagation\u53cd\u5411\u4f20\u64ad, and it is a critical step in our efficient implementation. Convolutional Middle Layers We use ConvMD to represent an M-dimensional convolution operator where and are the number of input and output channels, k,s, and p are the M-dimensional vectors correspoinding to kernel size, stride size and padding\u586b\u5145 size respectively.When the size across the M-dimensions are the same, we use a scalar to represent the size e.g. k for k=(k,k,k). Each convolutional middle layer applies 3D convolution,BN layer, and ReLU layer sequentially\u4f9d\u6b21.The convolutional middle layers aggregate voxel-wise features within a progressively\u9010\u6b65 expanding\u6269\u5927\u7684 receptive\u63a5\u6536 field\uff0cadding more context to the shape description.The detialed sizes of the filters in the convolutional middle layers are explained in Section 3. Region Proposal Network \\ Recently, region proposal\u63d0\u6848 networks have become an important building block of top-performing object detection frameworks. In this work,we make servel key modifications to the RPN architecture proposed in [34], and combine it with the feature learning network and convolutional middle layers to form an end-to-end trainable pipeline\u7ba1\u9053. The input to our RPN is the feature map provided by the convolutional middle layers.The architecture of this network is illustrate in Figure 4.The network has three blocks of fully convolutional layers.The first layer of each block downsamples the feature map by half via a convolution with a stride size of 2,followed by a sequence of convolutions of stride 1(xq means q applications of the filter).After each convolution layer,BN and ReLU operations are applied.We then upsample the output of every block to a fixed size and concatanate to construct the high resolution feature map. Finally, this feature map is mapped to the desired learning targets:(1) a probability score map and (2) a regression map.\uff32\uff30\uff2e\u7684\u8f93\u5165\u662f\u5377\u79ef\u4e2d\u95f4\u5c42\u63d0\u4f9b\u7684\u7279\u5f81\u56fe\u3002\u8be5\u7f51\u7edc\u7684\u4f53\u7cfb\u7ed3\u6784\u5982\u56fe\uff14\u6240\u793a\u3002\u8be5\u7f51\u7edc\u5177\u6709\u4e09\u4e2a\u5b8c\u5168\u5377\u57fa\u5c42\u7684\u5757\u3002\u6bcf\u4e2a\u5757\u7684\u7b2c\u4e00\u5c42\u901a\u8fc7\u6b65\u5e45\u4e3a\uff12\u7684\u5377\u79ef\u5bf9\u7279\u5f81\u56fe\u8fdb\u884c\u4e00\u534a\u4e0b\u91c7\u6837\uff0c\u7136\u540e\u662f\u6b65\u5e45\uff11\u7684\u5377\u79ef\u5e8f\u5217\uff08xq \u8868\u793a\u6ee4\u6ce2\u5668\u7684\uff51\u4e2a\u5e94\u7528\uff09\u3002\u5728\u6bcf\u4e2a\u5377\u57fa\u5c42\u4e4b\u540e\uff0cBN \u548cReLU \u64cd\u4f5c\u88ab\u5e94\u7528\u3002\u7136\u540e\uff0c\u6211\u4eec\u5c06\u6bcf\u4e2a\u5757\u7684\u8f93\u51fa\u4e0a\u91c7\u6837\u5230\u56fa\u5b9a\u5927\u5c0f\uff0c\u5e76\u6c47\u603b\u4ee5\u6784\u5efa\u9ad8\u5206\u8fa8\u7387\u7279\u5f81\u56fe\u3002\u6700\u540e\uff0c\u5c06\u6b64\u7279\u5f81\u56fe\u6620\u5c04\u5230\u6240\u9700\u7684\u5b66\u4e60\u76ee\u6807\uff1a\uff08\uff11\uff09\u6982\u7387\u5206\u6570\u56fe\u548c\uff08\uff12\uff09\u56de\u5f52\u56fe\u3002 Loss Function Let be the set of positive anchors\u951a\u70b9 and be the set of negative anchors. We parameterize a 3D ground truth box as ,where represent the center location, are length ,width,height of the box, and is the yaw rotation around Z-axis. To retrieve\u627e\u56de the ground truth box from a matching positive anchorparameterized as ,we define the residual vector\u6b8b\u5dee\u5411\u91cf containing the 7 regression targets corresponding to center location \u25b3x,\u25b3y,\u25b3z,three dimensions \u25b3l, \u25b3w, \u25b3h, and the rotation \u25b3\u0398, which are computed as :\\ \\ where is the diagonal\u5bf9\u89d2\u7ebf of the base of the anchor box.Here, we aim to directly estimate the oriented\u5b9a\u5411\u7684 3D box and normallize \u0394x and \u0394y homogeneously\u5747\u5300\u7684 with the diagonal\u5bf9\u89d2\u7ebf ,We define the loss function as follows:\\ \\ where and represent the softmax output for positive anchor and negative anchor respectively,while and are the regression\u56de\u5f52 output and ground truth for positive anchor .The first two terms are the normalized classification loss for and , where the stands for binary cross entropy loss\u4e8c\u5143\u4ea4\u53c9\u71b5\u635f\u5931 and \u03b1\uff0c\u03b2 are postive constants balancing the relative importance. The last term is the regression loss, where we use the SmoothL1 function. Efficient Implementation Gpus are optimized\u4f18\u5316 for processing dense\u7a20\u5bc6 tensor structures\uff0eThe problem with working directly with the point cloud is that the points are sparsely distributed across sapce and each voxel has a variable number of points.We devised\u8bbe\u8ba1\u7684 a method that converts the point cloud into a dense tensor structure where stacked VFE operations can be processed in parallel across points and voxels.\\ \\ The method is summarized in Figure 5. We initialize a KxTx7 dimensional tensor structure to store the voxel input feature where K is the maximum number of non-empty voxels, T is the maximum number of points per voxel, and 7 is the input encoding dimension for each point.The points are randomized\u968f\u673a\u7684 before processing.For each point in pointcloud, we check if the corresponding voxel already exists. This lookup operation is done efficiently in O(1) using a hash table where the voxel coordinate is used as the hash key.If the voxel is already initialized we insert the point to voxel location if there are less than T points, otherwise the point is ignored.If the voxel is not initialized, we initialize a new voxel, store its coordinate in the voxel coordinate buffer, and insert the point to this voxel location.The voxel input feature and coordinate buffers can be constructed\u5efa via a signle pass over the point list, therefore its complexity is O(n).\u4f53\u7d20\u8f93\u5165\u7279\u5f81\u548c\u5750\u6807\u7f13\u51b2\u533a\u53ef\u4ee5\u901a\u8fc7\u5bf9\u70b9\u5217\u8868\u7684\u4e00\u6b21\u904d\u5386\u6765\u6784\u9020\uff0c\u56e0\u6b64\u5176\u590d\u6742\u5ea6\u4e3aO\uff08n\uff09\u3002To further improve the memory/compute efficiency it is possible to only store a limited number of voxels(K) and ignore points coming from voxels with few points. After the voxel input buffer is constructed,the stacked VFE only involves\u6d89\u53ca point level and voxel level dense operations which can be computed on a GPU in parallel.Note that, after concatenation\u7ea7\u8054 operations in VFE, we reset the features corresponding to empty points to zero such that they do not affect the computed voxel features.Finally, using the stored coordinate buffer we reorganize\u6539\u7ec4 the computed sparse voxel-wise structures to the dense voxel grid.\u6700\u540e\uff0c\u4f7f\u7528\u5b58\u50a8\u7684\u5750\u6807\u7f13\u51b2\u533a\uff0c\u6211\u4eec\u5c06\u8ba1\u7b97\u7684\u7a00\u758f\u4f53\u7d20\u7ed3\u6784\u91cd\u7ec4\u4e3a\u5bc6\u96c6\u4f53\u7d20\u7f51\u683c.The following convolutional middle layers and RPN operations work on a dense voxel grid which can be efficiently implemented on a GPU. Training Details Network Details Our experimental setup\u5b9e\u9a8c\u8bbe\u7f6e is based on the LiDAR specifications of the KITTI dataset.\\ Car Detection For this task,we consider point clouds within the range of [-3,1]x[-40,40]x[0,70.4]meters along Z,Y,X axis respectively.Points taht are Points that are projected outside of image boundaries are removed.We choose a voxel size of meters,which leads to D'=10,H'=400,W'=352. We set T = 35 as the maximum number of randomly sampled points in each non-empty voxel.We use two VFE layers VFE-1(7,32) and VFE-2(32,128).The final FCN maps VFE-2 output to .Thus our feature learning net generate a sparse tensor of shape 128x10x400x352.To aggregate voxel-wise features, we employ three convolution middle layers sequentially as Conv3D(128,64,3,(2,1,1),(1,1,1)),Conv3D(64,64,3,(1,1,1),(0,1,1)),and Conv3D(64,64,3,(2,1,1),(1,1,1)), which yields a 4D tensor of size 64x2x400x352.After reshaping, the input to RPN is a feature map of size 128x400x352,where the dimensions correspond to channel,height, and width of the 3D tensor.Figure 4 illustrates the detailed network architecture for this task.Unlike, we use only one anchor size, meters, centered at\u96c6\u4e2d\u4e8e meters with two rotations, 0 and 90 degrees.Our anchor matching criteria\u5339\u914d\u6807\u51c6 as follows: An anchor is considered as positive if it has the highest Intersection over Union(IoU) with a ground truth or its IoU with ground truth is above 0.6(in bird's eye view).An anchor is considered as negative if the IoU between it and all ground true boxes is less than 0.45. We treat anchors as don't care if they have 0.45\u2264IoU\u22640.6 with any ground truth.We set \u03b1\uff1d1.5 and \u03b2=1 in Eqn.2. Pedestrain and Cyclist Detection The input range is [-3,1]x[-20,20]x[0,48] meters along Z,Y,X axis respectively.We use the same voxel size as for car detection,which yields D=10, H=200, W=240.We set T=45 in order to obtain more LiDAR points for better capturing shape information.The feature learning network and convolutional middle layers ate identical\u76f8\u540c to the networks used in car detection task.For the RPN, we make one modification to block 1 in Figure 4 by changing the stride size in the first 2D convolution from 2 to 1. This allows finer resolution in anchor matching, which is necessary for detecting pedestrains and cyclists.We use anchor size meters centered at with 0 and 90 degrees rotation for pedestrain detection and use anchor size meters centered at with 0 and 90 degrees rotation for cyclist detecction.The specific anchor matching criteria is as follows:We assign an anchor as positive if it has the highest IoU with a ground truth, or its IoU with ground truth is above 0.5. An anchor is considered as negative if its IoU with every ground truth is less than 0.35.For anchors having 0.35\u2264IoU\u22640.5 with any ground truth,we treat them as don't care. During training, we use stochastic\u968f\u673a gradient descent(SGD) with learning rate 0.01 for the first 150 epochs and decrease the learning rate to 0.001 for the last 10 epochs.We use a batchsize of 16 point clouds. Data Augmentation \u6570\u636e\u6269\u5c55 \uff37ith less than 4000 training point clouds, training our network from scratch will inevitably \u4e0d\u53ef\u907f\u514d suffer from overfitting. To reduce this issue, we introduce three different forms of data augmentation. The augmented training data are generated on-the-fly\u5373\u65f6\u3000without the need to be stored on disk. Define set as the whole point cloud, consisting of N points.We parameterize a 3D bounding box ,where are center locations, l,w,h are length, width, height, and \u03b8 is the yaw rotation around Z-axis.We define as the set containing all LiDAR points within , where p=[x,y,z,r] denotes a particular LiDAR point in the whole set M. The first form of data augmentation applies perturbation\u6444\u52a8 independently to each ground truth 3D bounding box together with those LiDAR points within the box.Specifically,around Z-axis we rotate and the associated with respect to ( ) by a uniformally\u7edf\u4e00\u7684 distributed random variable . Then we add a translation (\u0394x,\u0394y,\u0394z) to the XYZ components of and to each point in ,where \u0394x,\u0394y,\u0394z are drawn independently from a Gaussian distribution with mean zero and standard deviation 1.0. To avoid physically impossible outcomes, we perform a collision\u78b0\u649e test between any two boxes after the perturbation and revert\u8fd8\u539f to orignal if a collision is detected.Since the perturbation is applied to each ground truth box and the associated LiDAR points independently, the network is able to learn from substantially\u5b9e\u8d28\u4e0a more variations than from the orignal training data. Secondly,we apply global scaling to all ground truth boxes and to whole point cloud M. Specifically, we multiply the XYZ coordinates and the three dimensions of each , and the XYZ coordinates of all points in M with a random variable drawn from uniform distribution [0.95,1.05]. Introducing global scale augementation improves robustness of the network for detecting objects with various sizes and distances as shown in image-based classification and detection tasks. Finally, we apply global rotation to all ground truth boxes and to the whole point cloud M. The rotation is applied along Z-axis and around(0,0,0). The global rotation offset is determined by sampling from uniform distribution [-\u03c0/4,\u03c0/4].By rotating the entire point cloud, we simulate the vehicle making a turn.","title":"Voxelnet"},{"location":"voxelnet/#voxelnet","text":"To interface a highly sparse\u7a00\u758f LIDAR point cloud with a region proposal network(RPN\u533a\u57df\u5efa\u8bae\u7f51\u7edc),most existing efforts have focused on hand-crafted\u624b\u5de5\u5236\u4f5c feature representations\u7279\u5f81\u8868\u793a\uff0cfor example, a bird's eye view projectiong\u9e1f\u77b0\u56fe. In this work,we remove the need of manual feature engineering for 3D point clouds and purpose VoxelNet, a generic 3D detection network that unifies feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network. Specially,VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a unified feature representation through the newly introduced voxel feature encoding(VFE) layer.In this way,the point cloud is encoded as a descriptive volumetric representation, which is then connected to a RPN to generate detections. Our network learns an effective discriminative representation\u533a\u5206\u6027\u8868\u793a of objects with various geometrics, leading to encouraging results in 3D detection of pedestrains and cyclists, based on only Lidar. \\ VoxelNet architecture. The feature learning network takes a raw point cloud as input, partitions the space into voxels, and transforms points within each voxel to a vector representation characterizing the shape information.The space is represented as a sparse 4D tensor.The convolutional middle layers processes the 4D tensor to aggregate spatial context \u805a\u5408\u7a7a\u95f4\u8bed\u5883.Finally,a RPN generates the 3D detection. Scalling up 3D feature learning networks to orders of magnitude more points and to 3D detection tasks are the main challenges that we address in this paper. We present VoxelNet, a generic 3D detection framework that simultaneously\u540c\u65f6 learns a discriminative\u5224\u522b\u6027 feature representation from point clouds and predicts accurate 3D bounding boxes, in an end-to-end fashion\u65b9\u5f0f. We design a novel\u65b0\u9896\u7684 voxel feature encoding(VFE) layer,which enables inter-point interaction\u70b9\u95f4\u4ea4\u4e92\u3000within a voxel,by combining point-wise features\u9010\u70b9\u7279\u5f81 with a locally aggregated feature\u805a\u5408\u7279\u5f81. Stacking \u5806\u53e0 multiple VFE layers allows learning complex features for characterizing\u8868\u5f81 local 3D shape information. Specially, VoxelNet divides the point cloud into equally spaced 3D voxel\u7b49\u8ddd\u7684\uff13\uff24\u4f53\u7d20 ,encodes each voxel via stacked VFE layers, and then 3D convolution furture\u8fdb\u4e00\u6b65 aggregate\u805a\u5408 local voxel features, transforming the pointcloud into a high-dimensional volumetric\u4f53\u79ef representation. Finally,a RPN consumes the volumetric represetation and yields\u4ea7\u751f the detection result. This efficient algorithm benefits both from the sparse\u7a00\u758f\u7684 point structure and efficient parallel processing on the voxel grid\u4f53\u7d20\u7f51\u683c\u7684\u9ad8\u6548\u5e76\u884c\u5904\u7406.","title":"VoxelNet"},{"location":"voxelnet/#voxelnet-architecture","text":"The proposed VoxelNet consists of three functional blocks:(1) Feature learning network,(2) Convolutional middle layers, and (3) Region proposal network","title":"VoxelNet Architecture"},{"location":"voxelnet/#feature-learning-network","text":"Voxel Partition Given a point cloud, we subdivide the 3D space into equally spaced voxels as shown in Figure 2.Suppose the point cloud encompasses\u5305\u542b 3D space with range D,H,W\uff08\u70b9\u4e91\u5c3a\u5bf8\uff09 along the Z,Y,X axes respectively.We define each voxel of size vD,vH,and vW\uff08\u4f53\u7d20\u5c3a\u5bf8\uff09 accordingly.The resulting\u6240\u751f\u6210\u7684 3D voxel grid is of size D'=D/vD, H'=H/vH, W'=W/vW\uff08\u70b9\u4e91\u5305\u542b\u4f53\u7d20\u4e2a\u6570\uff09.Here, for simplicity,we assume D,H,W are a multiple of vD,vH,vW.\u6211\u4eec\u5047\u8bbeD,H,W\u662fvD, vH, vW\u7684\u500d\u6570\u3002 Grouping We group the points according to the voxel they reside in\u6211\u4eec\u6839\u636e\u5b83\u4eec\u6240\u5728\u7684\u4f53\u7d20\u5bf9\u70b9\u8fdb\u884c\u5206\u7ec4.Due to factors such as distance,occlusion\u906e\u6321,object's relative pose\u7269\u4f53\u7684\u76f8\u5bf9\u59ff\u52bf,and non-uniform\u4e0d\u5747\u5300 sampling,the LiDAR point cloud is sparse and highly variable\u9ad8\u5ea6\u53ef\u53d8\u7684 point density\u70b9\u5bc6\u5ea6 throughout the space.Therefore, after gouping, a voxel will contain a variable number of points. Random Sampling Typically a high-definition LiDAR point cloud is composed of ~100k points. Directly processing all the points not only imposes\u52a0\u5f3a increased memory/efficiency burdens\u8d1f\u62c5\u3000on the computing platform, but also highly variable point density throughout the space might bias the detection.To this end\u56e0\u6b64,we randomly sample a fixed number, T,of points from those voxels containing more than T points.This sampling strategy\u6218\u7565 has two purpose,(1)computational saving;and (2)decrease the imbalance of points between the voxels which reduces the sampling bias, and adds more variation\u53d8\u5316 to training. Stacked Voxel Feature Encoding \\ \\ The key innovation\u9769\u65b0 is the chain\u94fe of VFE layers.For simplicity, Figure 2 illustrates the hierachical\u9636\u7ea7\u5f0f feature encoding process for one voxel.Without loss of generality\u6982\u8981,(\u5728\u4e0d\u5931\u4e00\u822c\u6027\u7684\u524d\u63d0\u4e0b)we use VFE Layer-1 to describe the details in the following paragraph\u6bb5\u843d.Figure 3 shows the architecture for VFE Layer-1.\\ Denote\u8868\u793a as a non-empty voxel containing t\u2264\uff34 LiDAR points,where contains XYZ coordinates for the i-th point and is the received reflectance\u53cd\u5c04\u7387.We first compute the local mean\u5c40\u90e8\u5747\u503c as the centroid\u8d28\u5fc3 of all the points in V, denoted as .Then we augment\u589e\u52a0 each point with the relative offset w.r.t. the centriod and obtain the input feature set , Next, each is transformed through the fully connected network(FCN) into a feature space,where we can aggregate\u6c47\u603b information from the point features to ecode the shape of the surface contained within the voxel.The FCN is composed of a linear layer, a batch normalization(BN\u6279\u91cf\u6807\u51c6\u5316) layer and a rectified\u7ea0\u6b63\u7684 linear unit(ReLU) layer.After obtaining point-wise feature representations, we use element-wise MaxPooling across all associated to V to get the locally aggregated feature Finally, we augment each with to form the point-wise concatenated\u7ea7\u8054\u7684 feature as Thus we obtain the output feature set .All non-empty voxels are ecoded in the same way and they share the same set of parameters in FCN. We use to represent the i-th VFE layer that transforms input features of dimension into output features of dimension . The linear layer learns a matrix of size , and the point-wise concatenation yields the output of dimension . Because the output feature combines both point-wise features and locally aggregated feature, stacking VFE layers encodes point interactions\u4e92\u52a8\u3000within a voxel and enables the final feature representation to learn descriptive shape information. The voxel-wise feature is obtained by transforming the output of VFE-n into via FCN and applying element-wise Maxpool where C is the dimension of the voxel-wise feature, as shown in Figure2. Saparse Tensor Representation \u7a00\u758f\u5f20\u91cf\u8868\u793a\u3000By processing only the non-empty voxels, we obtain a list of voxel features,each uniquely\u72ec\u7279\u7684 associated to the spatial\u7a7a\u95f4 coordinates of a pictular non-empty voxel.The obtained list of voxel-wise features can be represented as a sparse 4D tensor, of size C x D' x H' x W' as shown in Figure2.Although the point cloud contains ~100k points, more than 90% of voxels typically are empty.Representing non-empty voxel features as a sparse tensor greatly reduce the memory usage and computation cost during backpropagation\u53cd\u5411\u4f20\u64ad, and it is a critical step in our efficient implementation.","title":"Feature Learning Network"},{"location":"voxelnet/#convolutional-middle-layers","text":"We use ConvMD to represent an M-dimensional convolution operator where and are the number of input and output channels, k,s, and p are the M-dimensional vectors correspoinding to kernel size, stride size and padding\u586b\u5145 size respectively.When the size across the M-dimensions are the same, we use a scalar to represent the size e.g. k for k=(k,k,k). Each convolutional middle layer applies 3D convolution,BN layer, and ReLU layer sequentially\u4f9d\u6b21.The convolutional middle layers aggregate voxel-wise features within a progressively\u9010\u6b65 expanding\u6269\u5927\u7684 receptive\u63a5\u6536 field\uff0cadding more context to the shape description.The detialed sizes of the filters in the convolutional middle layers are explained in Section 3.","title":"Convolutional Middle Layers"},{"location":"voxelnet/#region-proposal-network","text":"\\ Recently, region proposal\u63d0\u6848 networks have become an important building block of top-performing object detection frameworks. In this work,we make servel key modifications to the RPN architecture proposed in [34], and combine it with the feature learning network and convolutional middle layers to form an end-to-end trainable pipeline\u7ba1\u9053. The input to our RPN is the feature map provided by the convolutional middle layers.The architecture of this network is illustrate in Figure 4.The network has three blocks of fully convolutional layers.The first layer of each block downsamples the feature map by half via a convolution with a stride size of 2,followed by a sequence of convolutions of stride 1(xq means q applications of the filter).After each convolution layer,BN and ReLU operations are applied.We then upsample the output of every block to a fixed size and concatanate to construct the high resolution feature map. Finally, this feature map is mapped to the desired learning targets:(1) a probability score map and (2) a regression map.\uff32\uff30\uff2e\u7684\u8f93\u5165\u662f\u5377\u79ef\u4e2d\u95f4\u5c42\u63d0\u4f9b\u7684\u7279\u5f81\u56fe\u3002\u8be5\u7f51\u7edc\u7684\u4f53\u7cfb\u7ed3\u6784\u5982\u56fe\uff14\u6240\u793a\u3002\u8be5\u7f51\u7edc\u5177\u6709\u4e09\u4e2a\u5b8c\u5168\u5377\u57fa\u5c42\u7684\u5757\u3002\u6bcf\u4e2a\u5757\u7684\u7b2c\u4e00\u5c42\u901a\u8fc7\u6b65\u5e45\u4e3a\uff12\u7684\u5377\u79ef\u5bf9\u7279\u5f81\u56fe\u8fdb\u884c\u4e00\u534a\u4e0b\u91c7\u6837\uff0c\u7136\u540e\u662f\u6b65\u5e45\uff11\u7684\u5377\u79ef\u5e8f\u5217\uff08xq \u8868\u793a\u6ee4\u6ce2\u5668\u7684\uff51\u4e2a\u5e94\u7528\uff09\u3002\u5728\u6bcf\u4e2a\u5377\u57fa\u5c42\u4e4b\u540e\uff0cBN \u548cReLU \u64cd\u4f5c\u88ab\u5e94\u7528\u3002\u7136\u540e\uff0c\u6211\u4eec\u5c06\u6bcf\u4e2a\u5757\u7684\u8f93\u51fa\u4e0a\u91c7\u6837\u5230\u56fa\u5b9a\u5927\u5c0f\uff0c\u5e76\u6c47\u603b\u4ee5\u6784\u5efa\u9ad8\u5206\u8fa8\u7387\u7279\u5f81\u56fe\u3002\u6700\u540e\uff0c\u5c06\u6b64\u7279\u5f81\u56fe\u6620\u5c04\u5230\u6240\u9700\u7684\u5b66\u4e60\u76ee\u6807\uff1a\uff08\uff11\uff09\u6982\u7387\u5206\u6570\u56fe\u548c\uff08\uff12\uff09\u56de\u5f52\u56fe\u3002","title":"Region Proposal Network"},{"location":"voxelnet/#loss-function","text":"Let be the set of positive anchors\u951a\u70b9 and be the set of negative anchors. We parameterize a 3D ground truth box as ,where represent the center location, are length ,width,height of the box, and is the yaw rotation around Z-axis. To retrieve\u627e\u56de the ground truth box from a matching positive anchorparameterized as ,we define the residual vector\u6b8b\u5dee\u5411\u91cf containing the 7 regression targets corresponding to center location \u25b3x,\u25b3y,\u25b3z,three dimensions \u25b3l, \u25b3w, \u25b3h, and the rotation \u25b3\u0398, which are computed as :\\ \\ where is the diagonal\u5bf9\u89d2\u7ebf of the base of the anchor box.Here, we aim to directly estimate the oriented\u5b9a\u5411\u7684 3D box and normallize \u0394x and \u0394y homogeneously\u5747\u5300\u7684 with the diagonal\u5bf9\u89d2\u7ebf ,We define the loss function as follows:\\ \\ where and represent the softmax output for positive anchor and negative anchor respectively,while and are the regression\u56de\u5f52 output and ground truth for positive anchor .The first two terms are the normalized classification loss for and , where the stands for binary cross entropy loss\u4e8c\u5143\u4ea4\u53c9\u71b5\u635f\u5931 and \u03b1\uff0c\u03b2 are postive constants balancing the relative importance. The last term is the regression loss, where we use the SmoothL1 function.","title":"Loss Function"},{"location":"voxelnet/#efficient-implementation","text":"Gpus are optimized\u4f18\u5316 for processing dense\u7a20\u5bc6 tensor structures\uff0eThe problem with working directly with the point cloud is that the points are sparsely distributed across sapce and each voxel has a variable number of points.We devised\u8bbe\u8ba1\u7684 a method that converts the point cloud into a dense tensor structure where stacked VFE operations can be processed in parallel across points and voxels.\\ \\ The method is summarized in Figure 5. We initialize a KxTx7 dimensional tensor structure to store the voxel input feature where K is the maximum number of non-empty voxels, T is the maximum number of points per voxel, and 7 is the input encoding dimension for each point.The points are randomized\u968f\u673a\u7684 before processing.For each point in pointcloud, we check if the corresponding voxel already exists. This lookup operation is done efficiently in O(1) using a hash table where the voxel coordinate is used as the hash key.If the voxel is already initialized we insert the point to voxel location if there are less than T points, otherwise the point is ignored.If the voxel is not initialized, we initialize a new voxel, store its coordinate in the voxel coordinate buffer, and insert the point to this voxel location.The voxel input feature and coordinate buffers can be constructed\u5efa via a signle pass over the point list, therefore its complexity is O(n).\u4f53\u7d20\u8f93\u5165\u7279\u5f81\u548c\u5750\u6807\u7f13\u51b2\u533a\u53ef\u4ee5\u901a\u8fc7\u5bf9\u70b9\u5217\u8868\u7684\u4e00\u6b21\u904d\u5386\u6765\u6784\u9020\uff0c\u56e0\u6b64\u5176\u590d\u6742\u5ea6\u4e3aO\uff08n\uff09\u3002To further improve the memory/compute efficiency it is possible to only store a limited number of voxels(K) and ignore points coming from voxels with few points. After the voxel input buffer is constructed,the stacked VFE only involves\u6d89\u53ca point level and voxel level dense operations which can be computed on a GPU in parallel.Note that, after concatenation\u7ea7\u8054 operations in VFE, we reset the features corresponding to empty points to zero such that they do not affect the computed voxel features.Finally, using the stored coordinate buffer we reorganize\u6539\u7ec4 the computed sparse voxel-wise structures to the dense voxel grid.\u6700\u540e\uff0c\u4f7f\u7528\u5b58\u50a8\u7684\u5750\u6807\u7f13\u51b2\u533a\uff0c\u6211\u4eec\u5c06\u8ba1\u7b97\u7684\u7a00\u758f\u4f53\u7d20\u7ed3\u6784\u91cd\u7ec4\u4e3a\u5bc6\u96c6\u4f53\u7d20\u7f51\u683c.The following convolutional middle layers and RPN operations work on a dense voxel grid which can be efficiently implemented on a GPU.","title":"Efficient Implementation"},{"location":"voxelnet/#training-details","text":"","title":"Training Details"},{"location":"voxelnet/#network-details","text":"Our experimental setup\u5b9e\u9a8c\u8bbe\u7f6e is based on the LiDAR specifications of the KITTI dataset.\\ Car Detection For this task,we consider point clouds within the range of [-3,1]x[-40,40]x[0,70.4]meters along Z,Y,X axis respectively.Points taht are Points that are projected outside of image boundaries are removed.We choose a voxel size of meters,which leads to D'=10,H'=400,W'=352. We set T = 35 as the maximum number of randomly sampled points in each non-empty voxel.We use two VFE layers VFE-1(7,32) and VFE-2(32,128).The final FCN maps VFE-2 output to .Thus our feature learning net generate a sparse tensor of shape 128x10x400x352.To aggregate voxel-wise features, we employ three convolution middle layers sequentially as Conv3D(128,64,3,(2,1,1),(1,1,1)),Conv3D(64,64,3,(1,1,1),(0,1,1)),and Conv3D(64,64,3,(2,1,1),(1,1,1)), which yields a 4D tensor of size 64x2x400x352.After reshaping, the input to RPN is a feature map of size 128x400x352,where the dimensions correspond to channel,height, and width of the 3D tensor.Figure 4 illustrates the detailed network architecture for this task.Unlike, we use only one anchor size, meters, centered at\u96c6\u4e2d\u4e8e meters with two rotations, 0 and 90 degrees.Our anchor matching criteria\u5339\u914d\u6807\u51c6 as follows: An anchor is considered as positive if it has the highest Intersection over Union(IoU) with a ground truth or its IoU with ground truth is above 0.6(in bird's eye view).An anchor is considered as negative if the IoU between it and all ground true boxes is less than 0.45. We treat anchors as don't care if they have 0.45\u2264IoU\u22640.6 with any ground truth.We set \u03b1\uff1d1.5 and \u03b2=1 in Eqn.2. Pedestrain and Cyclist Detection The input range is [-3,1]x[-20,20]x[0,48] meters along Z,Y,X axis respectively.We use the same voxel size as for car detection,which yields D=10, H=200, W=240.We set T=45 in order to obtain more LiDAR points for better capturing shape information.The feature learning network and convolutional middle layers ate identical\u76f8\u540c to the networks used in car detection task.For the RPN, we make one modification to block 1 in Figure 4 by changing the stride size in the first 2D convolution from 2 to 1. This allows finer resolution in anchor matching, which is necessary for detecting pedestrains and cyclists.We use anchor size meters centered at with 0 and 90 degrees rotation for pedestrain detection and use anchor size meters centered at with 0 and 90 degrees rotation for cyclist detecction.The specific anchor matching criteria is as follows:We assign an anchor as positive if it has the highest IoU with a ground truth, or its IoU with ground truth is above 0.5. An anchor is considered as negative if its IoU with every ground truth is less than 0.35.For anchors having 0.35\u2264IoU\u22640.5 with any ground truth,we treat them as don't care. During training, we use stochastic\u968f\u673a gradient descent(SGD) with learning rate 0.01 for the first 150 epochs and decrease the learning rate to 0.001 for the last 10 epochs.We use a batchsize of 16 point clouds.","title":"Network Details"},{"location":"voxelnet/#data-augmentation","text":"\uff37ith less than 4000 training point clouds, training our network from scratch will inevitably \u4e0d\u53ef\u907f\u514d suffer from overfitting. To reduce this issue, we introduce three different forms of data augmentation. The augmented training data are generated on-the-fly\u5373\u65f6\u3000without the need to be stored on disk. Define set as the whole point cloud, consisting of N points.We parameterize a 3D bounding box ,where are center locations, l,w,h are length, width, height, and \u03b8 is the yaw rotation around Z-axis.We define as the set containing all LiDAR points within , where p=[x,y,z,r] denotes a particular LiDAR point in the whole set M. The first form of data augmentation applies perturbation\u6444\u52a8 independently to each ground truth 3D bounding box together with those LiDAR points within the box.Specifically,around Z-axis we rotate and the associated with respect to ( ) by a uniformally\u7edf\u4e00\u7684 distributed random variable . Then we add a translation (\u0394x,\u0394y,\u0394z) to the XYZ components of and to each point in ,where \u0394x,\u0394y,\u0394z are drawn independently from a Gaussian distribution with mean zero and standard deviation 1.0. To avoid physically impossible outcomes, we perform a collision\u78b0\u649e test between any two boxes after the perturbation and revert\u8fd8\u539f to orignal if a collision is detected.Since the perturbation is applied to each ground truth box and the associated LiDAR points independently, the network is able to learn from substantially\u5b9e\u8d28\u4e0a more variations than from the orignal training data. Secondly,we apply global scaling to all ground truth boxes and to whole point cloud M. Specifically, we multiply the XYZ coordinates and the three dimensions of each , and the XYZ coordinates of all points in M with a random variable drawn from uniform distribution [0.95,1.05]. Introducing global scale augementation improves robustness of the network for detecting objects with various sizes and distances as shown in image-based classification and detection tasks. Finally, we apply global rotation to all ground truth boxes and to the whole point cloud M. The rotation is applied along Z-axis and around(0,0,0). The global rotation offset is determined by sampling from uniform distribution [-\u03c0/4,\u03c0/4].By rotating the entire point cloud, we simulate the vehicle making a turn.","title":"Data Augmentation \u6570\u636e\u6269\u5c55"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/","text":"\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50\u5c01\u88c5\uff561.0 \u786c\u4ef6\u73af\u5883\u7ec4\u6210 \u9002\u7528\u4e8eAGX\u63a7\u5236\u5668 velodyne\u6fc0\u5149\u96f7\u8fbe\uff0c \u6ce8\uff1a\u5176\u4ed6\u54c1\u724c\u96f7\u8fbe\u53ea\u9700\u66f4\u6362\u9a71\u52a8\u548c\u5bf9\u5e94\u7684\u70b9\u4e91topic \u8f6f\u4ef6\u73af\u5883\u90e8\u7f72 ubuntu18.04\u64cd\u4f5c\u7cfb\u7edf \u6839\u636eAGX\u63d0\u4f9b\u7684\u7cfb\u7edf\u5305Jetpack\u5b89\u88c5 Cuda\u548ccudnn\u5b89\u88c5 \u901a\u8fc7Jetpack\u5b89\u88c5 ROS\u5b89\u88c5 \u4e0b\u8f7d git clone https://github.com/jetsonhacks/installROSXavier.git \u5207\u6362\u76ee\u5f55 cd installROSXavier \u5b89\u88c5 ./installROS.sh -p ros-melodic-desktop -p ros-melodic-rgbd-launch \u63d2\u4ef6\u5b89\u88c5 sudo apt-get install ros-melodic-jsk-rviz-plugins caffe \u90e8\u7f72 \u4e0b\u8f7d git clone https://github.com/BVLC/caffe.git \u5b89\u88c5\u4f9d\u8d56 sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler sudo apt-get install --no-install-recommends libboost-all-dev sudo apt-get install libopenblas-dev sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev \u5207\u6362\u76ee\u5f55 cd caffe \u65b0\u5efaMakefile.config\u6587\u4ef6 ,\u5e76\u7c98\u8d34\u4e0b\u9762\u7684\u5185\u5bb9 ``` \uff41## Refer to http://caffe.berkeleyvision.org/installation.html # Contributions simplifying and improving our build system are welcome! cuDNN acceleration switch (uncomment to build with cuDNN). USE_CUDNN := 1 CPU-only switch (uncomment to build without GPU support). CPU_ONLY := 1 uncomment to disable IO dependencies and corresponding data layers USE_OPENCV := 0 USE_LEVELDB := 0 USE_LMDB := 0 uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary) You should not set this flag if you will be reading LMDBs with any possibility of simultaneous read and write ALLOW_LMDB_NOLOCK := 1 Uncomment if you're using OpenCV 3 OPENCV_VERSION := 3 To customize your choice of compiler, uncomment and set the following. N.B. the default for Linux is g++ and the default for OSX is clang++ CUSTOM_CXX := g++ CUDA directory contains bin/ and lib/ directories that we need. CUDA_DIR := /usr/local/cuda On Ubuntu 14.04, if cuda tools are installed via \"sudo apt-get install nvidia-cuda-toolkit\" then use this instead: CUDA_DIR := /usr CUDA architecture setting: going with all of them. For CUDA < 6.0, comment the lines after *_35 for compatibility. CUDA_ARCH := -gencode arch=compute_30,code=sm_30 \\ -gencode arch=compute_35,code=sm_35 \\ -gencode arch=compute_50,code=sm_50 \\ -gencode arch=compute_52,code=sm_52 \\ -gencode arch=compute_61,code=sm_61 \\ -gencode arch=compute_61,code=compute_61 # -gencode arch=compute_20,code=sm_20 \\ # -gencode arch=compute_20,code=sm_21 \\ BLAS choice: atlas for ATLAS (default) mkl for MKL open for OpenBlas BLAS := atlas BLAS := open Custom (MKL/ATLAS/OpenBLAS) include and lib directories. Leave commented to accept the defaults for your choice of BLAS (which should work)! BLAS_INCLUDE := /path/to/your/blas BLAS_LIB := /path/to/your/blas Homebrew puts openblas in a directory that is not on the standard search path BLAS_INCLUDE := $(shell brew --prefix openblas)/include BLAS_LIB := $(shell brew --prefix openblas)/lib This is required only if you will compile the matlab interface. MATLAB directory should contain the mex binary in /bin. MATLAB_DIR := /usr/local MATLAB_DIR := /Applications/MATLAB_R2012b.app NOTE: this is required only if you will compile the python interface. We need to be able to find Python.h and numpy/arrayobject.h. PYTHON_INCLUDE := /usr/include/python2.7 \\ /usr/lib/python2.7/dist-packages/numpy/core/include Anaconda Python distribution is quite popular. Include path: Verify anaconda location, sometimes it's in root. ANACONDA_HOME := $(HOME)/anaconda2 # PYTHON_INCLUDE := $(ANACONDA_HOME)/include \\ $(ANACONDA_HOME)/include/python2.7 \\ $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include \\ Uncomment to use Python 3 (default is Python 2) PYTHON_LIBRARIES := boost_python3 python3.5m # PYTHON_INCLUDE := /usr/include/python3.5m \\ /usr/lib/python3.5/dist-packages/numpy/core/include We need to be able to find libpythonX.X.so or .dylib. PYTHON_LIB := /usr/lib PYTHON_LIB := $(ANACONDA_HOME)/lib Homebrew installs numpy in a non standard path (keg only) PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core. file )'))/include PYTHON_LIB += $(shell brew --prefix numpy)/lib Uncomment to support layers written in Python (will link against Python libs) WITH_PYTHON_LAYER := 1 Whatever else you find you need goes here. INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial/ LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/aarch64-linux-gnu/hdf5/serial If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies INCLUDE_DIRS += $(shell brew --prefix)/include LIBRARY_DIRS += $(shell brew --prefix)/lib Uncomment to use pkg-config to specify OpenCV library paths. (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.) USE_PKG_CONFIG := 1 N.B. both build and distribute dirs are cleared on make clean BUILD_DIR := build DISTRIBUTE_DIR := distribute Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171 DEBUG := 1 The ID of the GPU that 'make runtest' will use to run unit tests. TEST_GPUID := 0 enable pretty build (comment to see full commands) Q ?= @ ``` * \u7f16\u8bd1 * make -j 8 * sudo make distribute qt\u5b89\u88c5 sudo apt-get install qttools5-dev-tools libqt5svg5-dev qtmultimedia5-dev \u7b97\u6cd5\u76d2\u5b50\u7ec4\u6210\u90e8\u5206 \u70b9\u4e91\u6807\u5b9a\u529f\u80fd \u70b9\u4e91\u5efa\u56fe\u529f\u80fd \u70b9\u4e91\u805a\u7c7b\u529f\u80fd \u70b9\u4e91\u667a\u80fd\u8bc6\u522b\u529f\u80fd \u7a0b\u5e8f\u6587\u4ef6\u5939\u76ee\u5f55\u7ed3\u6784 \u7a0b\u5e8f catkin_ws src common detected_objects_visualizer lidar_cnn_seg_detect lidar_demo ndt_mapping rockauto_msgs ros2qt velodyne CMakeLists.txt \u6211\u4eec\u9700\u8981\u628acatkin_ws\u6587\u4ef6\u5939\u62f7\u8d1d\u5230\u5de5\u63a7\u673a\u7684home\u76ee\u5f55\u4e0b\u3002 \u7f16\u8bd1\u7a0b\u5e8f \u5207\u6362\u76ee\u5f55 cd catkin_ws \u7f16\u8bd1 catkin_make source \u73af\u5883 source devel/setup.bash \u7a0b\u5e8f\u542f\u52a8 rosrun ros2qt ros2qt \u5e94\u7528\u8bb2\u89e3 \u7a0b\u5e8f\u542f\u52a8\u540e\u5f39\u51fa\u4e0b\u56fe\u754c\u9762 \u5982\u4e0b\u56fe\u914d\u7f6e Fixed Frame \u4e3a velodyne ,\u52fe\u9009 pointCloud2 \u590d\u9009\u6846\uff0c\u8bbe\u7f6e\u5176 Topic \u4e3a point_raw \u53ef\u89c2\u770b\u96f7\u8fbe\u6570\u636e \u542f\u7528\uff33\uff2c\uff21\uff2d\u529f\u80fd\uff0c\u5219\u52fe\u9009 SLAM \u590d\u9009\u6846,\u5c06\u5176 Topic \u8bbe\u7f6e\u4e3a point_raw \uff0c\u540c\u65f6\u5c06 pointCloud2 \u7684 Topic \u8bbe\u7f6e\u4e3a cloud \u3002 Slam\u63d0\u4f9b\u4e86\u53ef\u8c03\u53c2\u6570\uff1a Topic:\u8bbe\u7f6e\u6784\u56fe\u4e3b\u9898 Size:\u8bbe\u7f6e\u70b9\u4e91\u5730\u56fe\u4e2d\u70b9\u7684\u663e\u793a\u5927\u5c0f Color:\u70b9\u4e91\u5730\u56fe\u7684\u989c\u8272 Res:\u5206\u8fa8\u7387 Step_size:\u6b65\u5e45 Trans_epsilon:\u6536\u655b\u5747\u65b9\u5dee Max_iter:\u6700\u5927\u8fed\u4ee3\u6b21\u6570 Voxel_leaf_size:Voxel\u5c3a\u5bf8 Min_scan_range:\u6700\u5c0f\u626b\u63cf\u8303\u56f4 Max_scan_range:\u6700\u5927\u626b\u63cf\u8303\u56f4 Scan_rate:\u626b\u63cf\u5468\u671f \u663e\u793a\u969c\u788d\u7269\uff0c\u5219\u53d6\u6d88 SLAM \u52fe\u9009\uff0c\u5c06 pointCloud2 \u7684 Topic \u66f4\u6539\u56de point_raw .\u52fe\u9009 \uff22oundingBox \u590d\u9009\u6846\u3002 \u663e\u793a\u4f4d\u7f6e\uff0c\u901f\u5ea6\uff0c\u52a0\u901f\u5ea6\u548c\u8def\u5f84 \u663e\u793a\u805a\u7c7b\u969c\u788d\u7269\uff0c\u5219\u52fe\u9009'BoundingBox2'\u590d\u9009\u6846\u3002 \u8fd9\u91cc\u67093\u4e2a\u53c2\u6570: Cluster_D:\u805a\u7c7b\u76f4\u5f84 Cluster_Min:\u6700\u5c0f\u805a\u7c7b\u70b9\u6570 Cluster_Max:\u6700\u5927\u805a\u7c7b\u70b9\u6570 \u70b9\u4e91\u6807\u5b9a \u9996\u5148\u8981\u52fe\u9009 PointCloud2 \u663e\u793a\u70b9\u4e91\uff0c\u518d\u52fe\u9009 \u6807\u5b9a \u53ef\u79fb\u52a8xyz\u4e09\u8f74\uff0c\u4e5f\u53ef\u7ed5\u4e09\u8f74\u65cb\u8f6c\u3002 \u7a0b\u5e8f\u8bb2\u89e3 \u70b9\u4e91\u6807\u5b9a\u529f\u80fd \u6807\u5b9a\u7a0b\u5e8f\u529f\u80fd\u4ee3\u7801\u6bb5\u4f4d\u4e8e\"ros2qt\\src\\qnode.cpp\"\u6587\u4ef6 void QNode::biaodingCallback(const PointCloud::ConstPtr& msg) { //\u70b9\u4e91\u65cb\u8f6c Eigen::Affine3f transform_2 = Eigen::Affine3f::Identity(); transform_2.translation() << x_pianyi, y_pianyi, z_pianyi; float theta_2 = 0*M_PI/180; transform_2.rotate (Eigen::AngleAxisf (theta_2, Eigen::Vector3f::UnitY())); pcl::PointCloud<pcl::PointXYZ>::Ptr transformed_cloud_2 (new pcl::PointCloud<pcl::PointXYZ> ()); pcl::transformPointCloud (*msg, *transformed_cloud_2, transform_2); Eigen::Affine3f transform = Eigen::Affine3f::Identity(); transform.translation() << 0.0, 0.0, 0.0; float theta = x_xuanzhuan*M_PI/180; transform.rotate (Eigen::AngleAxisf (theta, Eigen::Vector3f::UnitX())); pcl::PointCloud<pcl::PointXYZ>::Ptr transformed_cloud (new pcl::PointCloud<pcl::PointXYZ> ()); pcl::transformPointCloud (*transformed_cloud_2, *transformed_cloud, transform); Eigen::Affine3f transform_3 = Eigen::Affine3f::Identity(); transform_3.translation() << 0.0, 0.0, 0.0; float theta_3 = z_xuanzhuan*M_PI/180; transform_3.rotate (Eigen::AngleAxisf (theta_3, Eigen::Vector3f::UnitZ())); pcl::PointCloud<pcl::PointXYZ>::Ptr transformed_cloud_3 (new pcl::PointCloud<pcl::PointXYZ> ()); pcl::transformPointCloud (*transformed_cloud, *transformed_cloud_3, transform_3); Eigen::Affine3f transform_4 = Eigen::Affine3f::Identity(); transform_4.translation() << 0.0, 0.0, 0.0; float theta_4 = y_xuanzhuan*M_PI/180; transform_4.rotate (Eigen::AngleAxisf (theta_4, Eigen::Vector3f::UnitY())); pcl::PointCloud<pcl::PointXYZ>::Ptr transformed_cloud_4 (new pcl::PointCloud<pcl::PointXYZ> ()); pcl::transformPointCloud (*transformed_cloud_3, *transformed_cloud_4, transform_4); transformed_cloud_4->header.frame_id = frame_id; pub.publish(transformed_cloud_4); } \u70b9\u4e91\u7684\u6807\u5b9a\uff0c\u4e3b\u8981\u662f\u70b9\u4e91\u7684\u65cb\u8f6c\u548c\u5e73\u79fb\u529f\u80fd\uff0c\u53ef\u4ee5\u53c2\u8003pcl\uff0c\u8fd9\u91cc\u7b80\u5355\u8bb2\u89e3\u4e00\u4e0b\u65cb\u8f6c\u5e73\u79fb\u529f\u80fd \u9996\u5148\u5b9a\u4e49\u4e00\u4e2aAffine3f\u7ed3\u6784\u4f53\uff0c\u7528\u6765\u5b58\u50a8\u70b9\u4e91\u65cb\u8f6c\u548c\u5e73\u79fb\u4fe1\u606f Eigen::Affine3f transform_2 = Eigen::Affine3f::Identity(); \u518d\u628a\uff58\uff0c\uff59\uff0c\uff5a\u8f74\u7684\u5e73\u79fb\u91cfx_pianyi, y_pianyi, z_pianyi\u4f20\u9012\u8fdbAffine3f\u7ed3\u6784\u4f53 transform_2.translation() << x_pianyi, y_pianyi, z_pianyi; \u5b9a\u4e49\u4e00\u4e2a\u65cb\u8f6c\u89d2\u5ea6 float theta_2 = 0*M_PI/180; \u628a\u7ed5\uff39\u8f74\u65cb\u8f6c\u89d2\u5ea6\uff0c\u4f20\u5165Affin3f\u7ed3\u6784\u4f53\uff0c\u65cb\u8f6c\u8f74Eigen::Vector3f::UnitY() transform_2.rotate (Eigen::AngleAxisf (theta_2, Eigen::Vector3f::UnitY())); \u5b9a\u4e49\u65cb\u8f6c\u540e\u7684\u70b9\u4e91 pcl::PointCloudpcl::pointxyz::Ptr transformed_cloud_2 (new pcl::PointCloudpcl::pointxyz ()); \u6267\u884c\u70b9\u4e91\u7684\u5e73\u79fb\u548c\u65cb\u8f6c\uff0c\u5e76\u628a\u7ed3\u679c\u5b58\u5165 transformed_cloud_2 pcl::transformPointCloud (*msg, *transformed_cloud_2, transform_2); \u70b9\u4e91\u5efa\u56fe\u529f\u80fd \u5efa\u56fe\u7a0b\u5e8f\u5305\u5373ndt_mapping\u6587\u4ef6\u5939,\u91cd\u70b9\u8fd0\u7b97\u903b\u8f91\u5728ndt_mapping.cpp\u6587\u4ef6\u5185\uff0c\u8fd9\u91cc\u89e3\u6790\u4e00\u4e0b ndt_mapping::ndt_mapping() { transform_pub = nh_.advertise<sensor_msgs::PointCloud2> (\"/cloud\", 1, false); points_sub_ = nh_.subscribe(slam_topic, 100000, &ndt_mapping::points_callback,this); ndt_map_pub_ = nh_.advertise<sensor_msgs::PointCloud2>(\"/ndt_map\", 1000); current_pose_pub_ = nh_.advertise<geometry_msgs::PoseStamped>(\"/current_pose\", 1000); max_iter_ = max_iter_1; ndt_res_ = ndt_res_1; step_size_ = step_size_1; trans_eps_ = trans_eps_1; voxel_leaf_size_ = voxel_leaf_size_1; scan_rate_ = scan_rate_1; min_scan_range_ = min_scan_range_1; max_scan_range_ = max_scan_range_1; min_add_scan_shift_ = min_add_scan_shift_1; initial_scan_loaded = 0; min_add_scan_shift_ = 1.0; _tf_x=0.0, _tf_y=0.0, _tf_z=0.0, _tf_roll=0.0, _tf_pitch=0.0, _tf_yaw=0.0; Eigen::Translation3f tl_btol(_tf_x, _tf_y, _tf_z); Eigen::AngleAxisf rot_x_btol(_tf_roll, Eigen::Vector3f::UnitX()); Eigen::AngleAxisf rot_y_btol(_tf_pitch, Eigen::Vector3f::UnitY()); Eigen::AngleAxisf rot_z_btol(_tf_yaw, Eigen::Vector3f::UnitZ()); tf_btol_ = (tl_btol * rot_z_btol * rot_y_btol * rot_x_btol).matrix(); tf_ltob_ = tf_btol_.inverse(); map_.header.frame_id = \"velodyne\"; current_pose_.x = current_pose_.y = current_pose_.z = 0.0;current_pose_.roll = current_pose_.pitch = current_pose_.yaw = 0.0; previous_pose_.x = previous_pose_.y = previous_pose_.z = 0.0;previous_pose_.roll = previous_pose_.pitch = previous_pose_.yaw = 0.0; voxel_grid_filter_.setLeafSize(voxel_leaf_size_, voxel_leaf_size_, voxel_leaf_size_); ndt.setTransformationEpsilon(trans_eps_); ndt.setStepSize(step_size_); ndt.setResolution(ndt_res_); ndt.setMaximumIterations(max_iter_); is_first_map_ = true; }; ndt_mapping::ndt_mapping()\u51fd\u6570\uff0c\u4e3b\u8981\u8fdb\u884c\u4e86\u4e00\u4e9b\u53c2\u6570\u8bbe\u5b9a\uff0c\u548ctopic\u8bbe\u5b9a\u3002 void ndt_mapping::points_callback(const sensor_msgs::PointCloud2::ConstPtr& input) { pcl::PointCloud<pcl::PointXYZI> tmp, scan; pcl::PointCloud<pcl::PointXYZI>::Ptr filtered_scan_ptr(new pcl::PointCloud<pcl::PointXYZI>()); pcl::PointCloud<pcl::PointXYZI>::Ptr transformed_scan_ptr(new pcl::PointCloud<pcl::PointXYZI>()); tf::Quaternion q; Eigen::Matrix4f t_localizer(Eigen::Matrix4f::Identity()); Eigen::Matrix4f t_base_link(Eigen::Matrix4f::Identity()); static tf::TransformBroadcaster br_; tf::Transform transform; pcl::fromROSMsg(*input, tmp); double r; Eigen::Vector3d point_pos; pcl::PointXYZI p; for (pcl::PointCloud<pcl::PointXYZI>::const_iterator item = tmp.begin(); item != tmp.end(); item++) { use_imu_ = false; if(use_imu_){ // deskew(TODO:inplement of predicting pose by imu) point_pos.x() = (double)item->x; point_pos.y() = (double)item->y; point_pos.z() = (double)item->z; double s = scan_rate_ * (double(item->intensity) - int(item->intensity)); point_pos.x() -= s * current_pose_msg_.pose.position.x;//current_pose_imu_ point_pos.y() -= s * current_pose_msg_.pose.position.y; point_pos.z() -= s * current_pose_msg_.pose.position.z; Eigen::Quaterniond start_quat, end_quat, mid_quat; mid_quat.setIdentity(); end_quat = Eigen::Quaterniond( current_pose_msg_.pose.orientation.w, current_pose_msg_.pose.orientation.x, current_pose_msg_.pose.orientation.y, current_pose_msg_.pose.orientation.z); start_quat = mid_quat.slerp(s, end_quat); point_pos = start_quat.conjugate() * start_quat * point_pos; point_pos.x() += current_pose_msg_.pose.position.x; point_pos.y() += current_pose_msg_.pose.position.y; point_pos.z() += current_pose_msg_.pose.position.z; p.x = point_pos.x(); p.y = point_pos.y(); p.z = point_pos.z(); } else{ p.x = (double)item->x; p.y = (double)item->y; p.z = (double)item->z; } p.intensity = (double)item->intensity; r = sqrt(pow(p.x, 2.0) + pow(p.y, 2.0)); if (min_scan_range_ < r && r < max_scan_range_) { scan.push_back(p); } } pcl::PointCloud<pcl::PointXYZI>::Ptr scan_ptr(new pcl::PointCloud<pcl::PointXYZI>(scan)); if (initial_scan_loaded == 0) { pcl::transformPointCloud(*scan_ptr, *transformed_scan_ptr, tf_btol_); map_ += *transformed_scan_ptr; initial_scan_loaded = 1; } voxel_grid_filter_.setInputCloud(scan_ptr); voxel_grid_filter_.filter(*filtered_scan_ptr); ndt.setInputSource(filtered_scan_ptr); pcl::PointCloud<pcl::PointXYZI>::Ptr map_ptr(new pcl::PointCloud<pcl::PointXYZI>(map_)); if (is_first_map_ == true){ ndt.setInputTarget(map_ptr); is_first_map_ = false; } Eigen::Translation3f init_translation(current_pose_.x, current_pose_.y, current_pose_.z); Eigen::AngleAxisf init_rotation_x(current_pose_.roll, Eigen::Vector3f::UnitX()); Eigen::AngleAxisf init_rotation_y(current_pose_.pitch, Eigen::Vector3f::UnitY()); Eigen::AngleAxisf init_rotation_z(current_pose_.yaw, Eigen::Vector3f::UnitZ()); Eigen::Matrix4f init_guess = (init_translation * init_rotation_z * init_rotation_y * init_rotation_x).matrix() * tf_btol_; pcl::PointCloud<pcl::PointXYZI>::Ptr output_cloud(new pcl::PointCloud<pcl::PointXYZI>); ndt.align(*output_cloud, init_guess); t_localizer = ndt.getFinalTransformation(); t_base_link = t_localizer * tf_ltob_; pcl::transformPointCloud(*scan_ptr, *transformed_scan_ptr, t_localizer); sensor_msgs::PointCloud2::Ptr tt(new sensor_msgs::PointCloud2); pcl::toROSMsg(*transformed_scan_ptr, *tt); tt->header.frame_id = \"velodyne\"; transform_pub.publish(tt); tf::Matrix3x3 mat_b; mat_b.setValue(static_cast<double>(t_base_link(0, 0)), static_cast<double>(t_base_link(0, 1)), static_cast<double>(t_base_link(0, 2)), static_cast<double>(t_base_link(1, 0)), static_cast<double>(t_base_link(1, 1)), static_cast<double>(t_base_link(1, 2)), static_cast<double>(t_base_link(2, 0)), static_cast<double>(t_base_link(2, 1)), static_cast<double>(t_base_link(2, 2))); current_pose_.x = t_base_link(0, 3);current_pose_.y = t_base_link(1, 3);current_pose_.z = t_base_link(2, 3); mat_b.getRPY(current_pose_.roll, current_pose_.pitch, current_pose_.yaw, 1); transform.setOrigin(tf::Vector3(current_pose_.x, current_pose_.y, current_pose_.z)); q.setRPY(current_pose_.roll, current_pose_.pitch, current_pose_.yaw); transform.setRotation(q); br_.sendTransform(tf::StampedTransform(transform, input->header.stamp, \"velodyne\", \"base_link\")); double shift = sqrt(pow(current_pose_.x - previous_pose_.x, 2.0) + pow(current_pose_.y - previous_pose_.y, 2.0)); if (shift >= min_add_scan_shift_) { map_ += *transformed_scan_ptr; previous_pose_.x = current_pose_.x;previous_pose_.y = current_pose_.y;previous_pose_.z = current_pose_.z; previous_pose_.roll = current_pose_.roll;previous_pose_.pitch = current_pose_.pitch;previous_pose_.yaw = current_pose_.yaw; ndt.setInputTarget(map_ptr); sensor_msgs::PointCloud2::Ptr map_msg_ptr(new sensor_msgs::PointCloud2); pcl::toROSMsg(*map_ptr, *map_msg_ptr); ndt_map_pub_.publish(*map_msg_ptr); } current_pose_msg_.header.frame_id = \"velodyne\"; current_pose_msg_.header.stamp = input->header.stamp; current_pose_msg_.pose.position.x = current_pose_.x;current_pose_msg_.pose.position.y = current_pose_.y;current_pose_msg_.pose.position.z = current_pose_.z; current_pose_msg_.pose.orientation.x = q.x();current_pose_msg_.pose.orientation.y = q.y();current_pose_msg_.pose.orientation.z = q.z();current_pose_msg_.pose.orientation.w = q.w(); current_pose_pub_.publish(current_pose_msg_); std::cout << \"-----------------------------------------------------------------\" << std::endl; std::cout << \"\u6784\u5efa\u5730\u56fe\" << std::endl; std::cout << \"-----------------------------------------------------------------\" << std::endl; } ndt_mapping::points_callback\u51fd\u6570\u5373\u8fdb\u884c\u6784\u56fe\uff0c\u5c06\u521d\u59cb\u5316\u70b9\u4e91\u52a0\u5165\u81f3\u5730\u56fe\uff0c\u82e5\u70b9\u4e91\u5730\u56fe\u6ca1\u6709\u521d\u59cb\u5316\u8f7d\u5165\uff0c\u5219\u5c06\u7b2c\u4e00\u5e27\u56fe\u50cf\u4f5c\u4e3a\u521d\u59cb\u56fe\u50cf\uff0c\u7136\u540e\u5c06\u914d\u51c6\u4e4b\u540e\u7684\u56fe\u50cf\u9010\u5e27\u52a0\u5165map\u3002\u901a\u8fc7tf_btol\u53d8\u6362\u77e9\u9635\u5c06\u539f\u59cb\u70b9\u4e91\u8fdb\u884c\u8f6c\u5316\u3002tf_btol\u662f\u8f66\u8f86\u5728\u8d77\u59cb\u4f4d\u7f6e\u662f\u4e0d\u5728\u5168\u5c40\u5730\u56fe\u539f\u70b9\u65f6\u7684\u53d8\u6362\u77e9\u9635\u3002\u7136\u540e\u5bf9\u539f\u59cb\u8f93\u5165\u70b9\u4e91\u8fdb\u884c\u4f53\u7d20\u8fc7\u6ee4\uff0c\u9009\u62e9\u4e0d\u540c\u7684\u65b9\u6cd5\u8fdb\u884c\u53c2\u6570\u8bbe\u7f6e init_guess\u662fndt\u914d\u51c6\u65f6\u5019\u7684\u521d\u59cb\u4f4d\u7f6e\uff0c\u8be5\u4f4d\u7f6e\u4e00\u822c\u7531\u524d\u4e00\u5e27\u4f4d\u7f6e\u52a0\u4e0a\u5fae\u5c0f\u65f6\u95f4\u6bb5\u5185\u7684\u53d8\u5316\uff0c\u5f53\u91c7\u7528imu\u6216odom\u65f6\u53ef\u4ee5\u5229\u7528\u5176\u8fdb\u884c\u8f85\u52a9\u7cbe\u786e\u5b9a\u4f4d\u521d\u59cb\u4f4d\u7f6e\u3002\u5982\u679c\u672a\u4f7f\u7528imu\u4ee5\u53caodom\u5219\u4f7f\u7528\u539f\u6765\u7684init_guess \u70b9\u4e91\u805a\u7c7b\u529f\u80fd \u70b9\u4e91\u805a\u7c7b\u529f\u80fd\u5728\"lidar_demo/src/lidar_demo.cpp\"\u6587\u4ef6 void callback(const boost::shared_ptr<const sensor_msgs::PointCloud2>& msg) { ros::NodeHandle n; n.getParam(\"Cluster_D\", Cluster_D); n.getParam(\"Cluster_Max\", Cluster_Max); n.getParam(\"Cluster_Min\", Cluster_Min); pcl::PCLPointCloud2 pcl_pc2; pcl_conversions::toPCL(*msg,pcl_pc2); pcl::PointCloud<pcl::PointXYZ>::Ptr temp_cloud(new pcl::PointCloud<pcl::PointXYZ>); pcl::fromPCLPointCloud2(pcl_pc2,*temp_cloud); std::vector<int> mapping; pcl::removeNaNFromPointCloud(*temp_cloud, *temp_cloud, mapping); pub2.publish(temp_cloud); pcl::PointIndices::Ptr inliers (new pcl::PointIndices); pcl::ModelCoefficients::Ptr coefficients (new pcl::ModelCoefficients); pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_plane (new pcl::PointCloud<pcl::PointXYZ> ()); pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_f (new pcl::PointCloud<pcl::PointXYZ>); if (temp_cloud->points.size() == 0) { std::cout << \"cloud in ROI is empty\" << std::endl; return; } pcl::search::KdTree<pcl::PointXYZ>::Ptr tree (new pcl::search::KdTree<pcl::PointXYZ>); tree->setInputCloud (temp_cloud); std::vector<pcl::PointIndices> cluster_indices; pcl::EuclideanClusterExtraction<pcl::PointXYZ> ec; cout << Cluster_D << \":\" << Cluster_Min << \":\" << Cluster_Max<<endl; ec.setClusterTolerance (Cluster_D); ec.setMinClusterSize (Cluster_Min); ec.setMaxClusterSize (Cluster_Max); ec.setSearchMethod (tree); ec.setInputCloud (temp_cloud); ec.extract (cluster_indices); jsk_recognition_msgs::BoundingBoxArray BOXS; int j = 0; vector<Eigen::Vector3f> center; for (std::vector<pcl::PointIndices>::const_iterator it = cluster_indices.begin (); it != cluster_indices.end (); ++it) { pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_cluster (new pcl::PointCloud<pcl::PointXYZ>); for (std::vector<int>::const_iterator pit = it->indices.begin (); pit != it->indices.end (); ++pit) cloud_cluster->push_back ((*temp_cloud)[*pit]); cloud_cluster->width = cloud_cluster->size (); cloud_cluster->height = 1; cloud_cluster->is_dense = true; std::cout << \"Cluster has : \" << cloud_cluster->size () << \" data points.\" << std::endl; j++; jsk_recognition_msgs::BoundingBox box2; Eigen::Vector3f mass_center; pcl::MomentOfInertiaEstimation<pcl::PointXYZ> feature_extractor; feature_extractor.setInputCloud(cloud_cluster); feature_extractor.compute(); pcl::PointXYZ min_point_OBB; pcl::PointXYZ max_point_OBB; pcl::PointXYZ position_OBB; Eigen::Matrix3f rotational_matrix_OBB; feature_extractor.getOBB(min_point_OBB, max_point_OBB, position_OBB, rotational_matrix_OBB); Eigen::Quaternionf quat (rotational_matrix_OBB); feature_extractor.getMassCenter (mass_center); center.push_back(mass_center); pcl::PointXYZ min; pcl::PointXYZ max; pcl::getMinMax3D(*cloud_cluster,min,max); max.x = max.x; box2.label = j+1; box2.pose.position.x = (max.x + min.x) / 2; box2.pose.position.y = (max.y + min.y) / 2; box2.pose.position.z = (max.z + min.z) / 2; box2.dimensions.x = (max.x - min.x); box2.dimensions.y = (max.y - min.y); box2.dimensions.z = (max.z - min.z); box2.header.frame_id = frame_id; BOXS.boxes.push_back(box2); } BOXS.header.frame_id = frame_id; pub.publish(BOXS); } \u70b9\u4e91\u805a\u7c7b\u4e3b\u8981\u4f7f\u7528pcl\u5e93\u805a\u7c7b\u7b97\u6cd5 pcl::EuclideanClusterExtractionpcl::pointxyz ec; \u53ea\u9700\u8bbe\u5b9a\u53c2\u6570\uff1a Cluster_D:\u805a\u7c7b\u76f4\u5f84 Cluster_Min:\u6700\u5c0f\u805a\u7c7b\u70b9\u6570 Cluster_Max:\u6700\u5927\u805a\u7c7b\u70b9\u6570 \u70b9\u4e91\u667a\u80fd\u8bc6\u522b\u529f\u80fd \u4e3b\u8981\u4ee3\u7801\u5206\u5e03\u4f4d\u4e8elidar_cnn_seg_detect\uff0cdetected_objects_visualizer\uff0ccommon\uff0crockauto_msgs\u6587\u4ef6\u5939\u5185 \u70b9\u4e91\u667a\u80fd\u8bc6\u522b\u529f\u80fd\uff0c\u91c7\u7528\u7684\u662fapolo\u7684\u795e\u7ecf\u7f51\u7edc\u8bc6\u522b\u7b97\u6cd5\uff0c\u53ef\u5230\u8be5\u7b97\u6cd5\u7684 \u7ef4\u62a4\u5e73\u53f0 \u8fdb\u884c\u6df1\u5165\u7814\u7a76\uff0c\u8be5\u7b97\u6cd5\u4e0evoxelnet\u7f51\u7edc\u76f8\u4f3c\uff0c\u4e5f\u53ef\u53c2\u8003 github \u6211\u4eec\u5728\u667a\u80fd\u8bc6\u522b\u7684\u57fa\u7840\u4e0a\u6dfb\u52a0\u4e86\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\u663e\u793a VisualizeDetectedObjects::ObjectsToLabels(const rockauto_msgs::DetectedObjectArray &in_objects) { visualization_msgs::MarkerArray label_markers; for (auto const &object: in_objects.objects) { if (IsObjectValid(object)) { visualization_msgs::Marker label_marker; label_marker.lifetime = ros::Duration(marker_display_duration_); label_marker.header = in_objects.header; label_marker.ns = ros_namespace_ + \"/label_markers\"; label_marker.action = visualization_msgs::Marker::ADD; label_marker.type = visualization_msgs::Marker::TEXT_VIEW_FACING; label_marker.scale.x = 1.5; label_marker.scale.y = 1.5; label_marker.scale.z = 1.5; label_marker.color = label_color_; label_marker.id = marker_id_++; if(!object.label.empty() && object.label != \"unknown\") label_marker.text = object.label + \" \"; //Object Class if available float velocity_x = 0.0, velocity_y = 0.0, a_x =0.0, a_y = 0.0; if(past_x.size() ==0) { velocity_x = 0.0; velocity_y = 0.0; } else{ int before = 0; float past_d =100.0; for(int i=0;i<past_x.size();i++) { float d = sqrt(pow((past_x[i] - object.pose.position.x),2)+pow((past_y[i] - object.pose.position.y),2)); if(d < past_d) { past_d = d; before = i; } } velocity_x = (-past_x[before] + object.pose.position.x)/(0.1); velocity_y = (-past_y[before] + object.pose.position.y)/(0.1); a_x = (-past_vx[before] + velocity_x)/(0.1); a_y = (-past_vy[before] + velocity_y)/(0.1); } std::stringstream distance_stream; distance_stream << std::fixed << std::setprecision(1) << sqrt((object.pose.position.x * object.pose.position.x) + (object.pose.position.y * object.pose.position.y)); std::stringstream velocity_stream; velocity_stream <<std::fixed<<std::setprecision(2)<<\"v_x:\"<<velocity_x <<\"m/s a_x:\"<<a_x<<\"m/s2\\n\" <<\"v_y:\"<<velocity_y<<\"m/s a_y:\"<<a_y<<\"m/s2\"; std::string distance_str = distance_stream.str() + \" m\\n\" + velocity_stream.str(); label_marker.text += distance_str; if (object.velocity_reliable) { double velocity = object.velocity.linear.x; if (velocity < -0.1) { velocity *= -1; } if (abs(velocity) < object_speed_threshold_) { velocity = 0.0; } tf::Quaternion q(object.pose.orientation.x, object.pose.orientation.y, object.pose.orientation.z, object.pose.orientation.w); double roll, pitch, yaw; tf::Matrix3x3(q).getRPY(roll, pitch, yaw); // convert m/s to km/h std::stringstream kmh_velocity_stream; kmh_velocity_stream << std::fixed << std::setprecision(1) << (velocity * 3.6); std::string text = \"\\n<\" + std::to_string(object.id) + \"> \" + kmh_velocity_stream.str() + \" km/h\"; label_marker.text += text; } label_marker.pose.position.x = object.pose.position.x; label_marker.pose.position.y = object.pose.position.y; label_marker.pose.position.z = label_height_; label_marker.scale.z = 1.0; if (!label_marker.text.empty()) label_markers.markers.push_back(label_marker); past_x.push_back(object.pose.position.x); past_y.push_back(object.pose.position.y); past_vx.push_back(velocity_x); past_vy.push_back(velocity_y); } } // end in_objects.objects loop return label_markers; }//ObjectsToLabels \u6ce8\u610f\u4e8b\u9879 \u7a0b\u5e8f\u8981\u653e\u5728\u82f1\u6587\u8def\u5f84\u4e0b\uff0c\u6700\u597d\u4e0d\u8981\u653e\u5728\u4e2d\u6587\u76ee\u5f55\u4e0b\uff0c\u56e0\u4e3a\u5bb9\u6613\u51fa\u73b0\u4e2d\u6587\u4e71\u7801\u95ee\u9898\u800c\u5f71\u54cd\u7a0b\u5e8f\u8fd0\u884c","title":"\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50\u5c01\u88c5\uff561.0"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#v10","text":"","title":"\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50\u5c01\u88c5\uff561.0"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#_1","text":"\u9002\u7528\u4e8eAGX\u63a7\u5236\u5668 velodyne\u6fc0\u5149\u96f7\u8fbe\uff0c \u6ce8\uff1a\u5176\u4ed6\u54c1\u724c\u96f7\u8fbe\u53ea\u9700\u66f4\u6362\u9a71\u52a8\u548c\u5bf9\u5e94\u7684\u70b9\u4e91topic","title":"\u786c\u4ef6\u73af\u5883\u7ec4\u6210"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#_2","text":"","title":"\u8f6f\u4ef6\u73af\u5883\u90e8\u7f72"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#ubuntu1804","text":"\u6839\u636eAGX\u63d0\u4f9b\u7684\u7cfb\u7edf\u5305Jetpack\u5b89\u88c5","title":"ubuntu18.04\u64cd\u4f5c\u7cfb\u7edf"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#cudacudnn","text":"\u901a\u8fc7Jetpack\u5b89\u88c5","title":"Cuda\u548ccudnn\u5b89\u88c5"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#ros","text":"\u4e0b\u8f7d git clone https://github.com/jetsonhacks/installROSXavier.git \u5207\u6362\u76ee\u5f55 cd installROSXavier \u5b89\u88c5 ./installROS.sh -p ros-melodic-desktop -p ros-melodic-rgbd-launch \u63d2\u4ef6\u5b89\u88c5 sudo apt-get install ros-melodic-jsk-rviz-plugins","title":"ROS\u5b89\u88c5"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#caffe","text":"\u4e0b\u8f7d git clone https://github.com/BVLC/caffe.git \u5b89\u88c5\u4f9d\u8d56 sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler sudo apt-get install --no-install-recommends libboost-all-dev sudo apt-get install libopenblas-dev sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev \u5207\u6362\u76ee\u5f55 cd caffe \u65b0\u5efaMakefile.config\u6587\u4ef6 ,\u5e76\u7c98\u8d34\u4e0b\u9762\u7684\u5185\u5bb9 ``` \uff41## Refer to http://caffe.berkeleyvision.org/installation.html # Contributions simplifying and improving our build system are welcome!","title":"caffe \u90e8\u7f72"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#cudnn-acceleration-switch-uncomment-to-build-with-cudnn","text":"","title":"cuDNN acceleration switch (uncomment to build with cuDNN)."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#use_cudnn-1","text":"","title":"USE_CUDNN := 1"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#cpu-only-switch-uncomment-to-build-without-gpu-support","text":"","title":"CPU-only switch (uncomment to build without GPU support)."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#cpu_only-1","text":"","title":"CPU_ONLY := 1"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#uncomment-to-disable-io-dependencies-and-corresponding-data-layers","text":"","title":"uncomment to disable IO dependencies and corresponding data layers"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#use_opencv-0","text":"","title":"USE_OPENCV := 0"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#use_leveldb-0","text":"","title":"USE_LEVELDB := 0"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#use_lmdb-0","text":"","title":"USE_LMDB := 0"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#uncomment-to-allow-mdb_nolock-when-reading-lmdb-files-only-if-necessary","text":"","title":"uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#you-should-not-set-this-flag-if-you-will-be-reading-lmdbs-with-any","text":"","title":"You should not set this flag if you will be reading LMDBs with any"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#possibility-of-simultaneous-read-and-write","text":"","title":"possibility of simultaneous read and write"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#allow_lmdb_nolock-1","text":"","title":"ALLOW_LMDB_NOLOCK := 1"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#uncomment-if-youre-using-opencv-3","text":"OPENCV_VERSION := 3","title":"Uncomment if you're using OpenCV 3"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#to-customize-your-choice-of-compiler-uncomment-and-set-the-following","text":"","title":"To customize your choice of compiler, uncomment and set the following."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#nb-the-default-for-linux-is-g-and-the-default-for-osx-is-clang","text":"","title":"N.B. the default for Linux is g++ and the default for OSX is clang++"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#custom_cxx-g","text":"","title":"CUSTOM_CXX := g++"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#cuda-directory-contains-bin-and-lib-directories-that-we-need","text":"CUDA_DIR := /usr/local/cuda","title":"CUDA directory contains bin/ and lib/ directories that we need."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#on-ubuntu-1404-if-cuda-tools-are-installed-via","text":"","title":"On Ubuntu 14.04, if cuda tools are installed via"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#sudo-apt-get-install-nvidia-cuda-toolkit-then-use-this-instead","text":"","title":"\"sudo apt-get install nvidia-cuda-toolkit\" then use this instead:"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#cuda_dir-usr","text":"","title":"CUDA_DIR := /usr"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#cuda-architecture-setting-going-with-all-of-them","text":"","title":"CUDA architecture setting: going with all of them."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#for-cuda-60-comment-the-lines-after-_35-for-compatibility","text":"CUDA_ARCH := -gencode arch=compute_30,code=sm_30 \\ -gencode arch=compute_35,code=sm_35 \\ -gencode arch=compute_50,code=sm_50 \\ -gencode arch=compute_52,code=sm_52 \\ -gencode arch=compute_61,code=sm_61 \\ -gencode arch=compute_61,code=compute_61 # -gencode arch=compute_20,code=sm_20 \\ # -gencode arch=compute_20,code=sm_21 \\","title":"For CUDA &lt; 6.0, comment the lines after *_35 for compatibility."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#blas-choice","text":"","title":"BLAS choice:"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#atlas-for-atlas-default","text":"","title":"atlas for ATLAS (default)"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#mkl-for-mkl","text":"","title":"mkl for MKL"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#open-for-openblas","text":"","title":"open for OpenBlas"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#blas-atlas","text":"BLAS := open","title":"BLAS := atlas"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#custom-mklatlasopenblas-include-and-lib-directories","text":"","title":"Custom (MKL/ATLAS/OpenBLAS) include and lib directories."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#leave-commented-to-accept-the-defaults-for-your-choice-of-blas","text":"","title":"Leave commented to accept the defaults for your choice of BLAS"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#which-should-work","text":"","title":"(which should work)!"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#blas_include-pathtoyourblas","text":"","title":"BLAS_INCLUDE := /path/to/your/blas"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#blas_lib-pathtoyourblas","text":"","title":"BLAS_LIB := /path/to/your/blas"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#homebrew-puts-openblas-in-a-directory-that-is-not-on-the-standard-search-path","text":"","title":"Homebrew puts openblas in a directory that is not on the standard search path"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#blas_include-shell-brew-prefix-openblasinclude","text":"","title":"BLAS_INCLUDE := $(shell brew --prefix openblas)/include"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#blas_lib-shell-brew-prefix-openblaslib","text":"","title":"BLAS_LIB := $(shell brew --prefix openblas)/lib"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#this-is-required-only-if-you-will-compile-the-matlab-interface","text":"","title":"This is required only if you will compile the matlab interface."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#matlab-directory-should-contain-the-mex-binary-in-bin","text":"","title":"MATLAB directory should contain the mex binary in /bin."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#matlab_dir-usrlocal","text":"","title":"MATLAB_DIR := /usr/local"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#matlab_dir-applicationsmatlab_r2012bapp","text":"","title":"MATLAB_DIR := /Applications/MATLAB_R2012b.app"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#note-this-is-required-only-if-you-will-compile-the-python-interface","text":"","title":"NOTE: this is required only if you will compile the python interface."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#we-need-to-be-able-to-find-pythonh-and-numpyarrayobjecth","text":"PYTHON_INCLUDE := /usr/include/python2.7 \\ /usr/lib/python2.7/dist-packages/numpy/core/include","title":"We need to be able to find Python.h and numpy/arrayobject.h."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#anaconda-python-distribution-is-quite-popular-include-path","text":"","title":"Anaconda Python distribution is quite popular. Include path:"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#verify-anaconda-location-sometimes-its-in-root","text":"","title":"Verify anaconda location, sometimes it's in root."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#anaconda_home-homeanaconda2","text":"# PYTHON_INCLUDE := $(ANACONDA_HOME)/include \\ $(ANACONDA_HOME)/include/python2.7 \\ $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include \\","title":"ANACONDA_HOME := $(HOME)/anaconda2"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#uncomment-to-use-python-3-default-is-python-2","text":"","title":"Uncomment to use Python 3 (default is Python 2)"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#python_libraries-boost_python3-python35m","text":"# PYTHON_INCLUDE := /usr/include/python3.5m \\","title":"PYTHON_LIBRARIES := boost_python3 python3.5m"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#usrlibpython35dist-packagesnumpycoreinclude","text":"","title":"/usr/lib/python3.5/dist-packages/numpy/core/include"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#we-need-to-be-able-to-find-libpythonxxso-or-dylib","text":"PYTHON_LIB := /usr/lib","title":"We need to be able to find libpythonX.X.so or .dylib."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#python_lib-anaconda_homelib","text":"","title":"PYTHON_LIB := $(ANACONDA_HOME)/lib"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#homebrew-installs-numpy-in-a-non-standard-path-keg-only","text":"","title":"Homebrew installs numpy in a non standard path (keg only)"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#python_include-dir-shell-python-c-import-numpycore-printnumpycorefileinclude","text":"","title":"PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.file)'))/include"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#python_lib-shell-brew-prefix-numpylib","text":"","title":"PYTHON_LIB += $(shell brew --prefix numpy)/lib"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#uncomment-to-support-layers-written-in-python-will-link-against-python-libs","text":"","title":"Uncomment to support layers written in Python (will link against Python libs)"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#with_python_layer-1","text":"","title":"WITH_PYTHON_LAYER := 1"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#whatever-else-you-find-you-need-goes-here","text":"INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial/ LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/aarch64-linux-gnu/hdf5/serial","title":"Whatever else you find you need goes here."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#if-homebrew-is-installed-at-a-non-standard-location-for-example-your-home-directory-and-you-use-it-for-general-dependencies","text":"","title":"If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#include_dirs-shell-brew-prefixinclude","text":"","title":"INCLUDE_DIRS += $(shell brew --prefix)/include"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#library_dirs-shell-brew-prefixlib","text":"","title":"LIBRARY_DIRS += $(shell brew --prefix)/lib"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#uncomment-to-use-pkg-config-to-specify-opencv-library-paths","text":"","title":"Uncomment to use pkg-config to specify OpenCV library paths."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#usually-not-necessary-opencv-libraries-are-normally-installed-in-one-of-the-above-library_dirs","text":"","title":"(Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#use_pkg_config-1","text":"","title":"USE_PKG_CONFIG := 1"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#nb-both-build-and-distribute-dirs-are-cleared-on-make-clean","text":"BUILD_DIR := build DISTRIBUTE_DIR := distribute","title":"N.B. both build and distribute dirs are cleared on make clean"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#uncomment-for-debugging-does-not-work-on-osx-due-to-httpsgithubcombvlccaffeissues171","text":"","title":"Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#debug-1","text":"","title":"DEBUG := 1"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#the-id-of-the-gpu-that-make-runtest-will-use-to-run-unit-tests","text":"TEST_GPUID := 0","title":"The ID of the GPU that 'make runtest' will use to run unit tests."},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#enable-pretty-build-comment-to-see-full-commands","text":"Q ?= @ ``` * \u7f16\u8bd1 * make -j 8 * sudo make distribute","title":"enable pretty build (comment to see full commands)"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#qt","text":"sudo apt-get install qttools5-dev-tools libqt5svg5-dev qtmultimedia5-dev","title":"qt\u5b89\u88c5"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#_3","text":"\u70b9\u4e91\u6807\u5b9a\u529f\u80fd \u70b9\u4e91\u5efa\u56fe\u529f\u80fd \u70b9\u4e91\u805a\u7c7b\u529f\u80fd \u70b9\u4e91\u667a\u80fd\u8bc6\u522b\u529f\u80fd \u7a0b\u5e8f\u6587\u4ef6\u5939\u76ee\u5f55\u7ed3\u6784 \u7a0b\u5e8f catkin_ws src common detected_objects_visualizer lidar_cnn_seg_detect lidar_demo ndt_mapping rockauto_msgs ros2qt velodyne CMakeLists.txt \u6211\u4eec\u9700\u8981\u628acatkin_ws\u6587\u4ef6\u5939\u62f7\u8d1d\u5230\u5de5\u63a7\u673a\u7684home\u76ee\u5f55\u4e0b\u3002","title":"\u7b97\u6cd5\u76d2\u5b50\u7ec4\u6210\u90e8\u5206"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#_4","text":"\u5207\u6362\u76ee\u5f55 cd catkin_ws \u7f16\u8bd1 catkin_make source \u73af\u5883 source devel/setup.bash","title":"\u7f16\u8bd1\u7a0b\u5e8f"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#_5","text":"rosrun ros2qt ros2qt","title":"\u7a0b\u5e8f\u542f\u52a8"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#_6","text":"\u7a0b\u5e8f\u542f\u52a8\u540e\u5f39\u51fa\u4e0b\u56fe\u754c\u9762 \u5982\u4e0b\u56fe\u914d\u7f6e Fixed Frame \u4e3a velodyne ,\u52fe\u9009 pointCloud2 \u590d\u9009\u6846\uff0c\u8bbe\u7f6e\u5176 Topic \u4e3a point_raw \u53ef\u89c2\u770b\u96f7\u8fbe\u6570\u636e \u542f\u7528\uff33\uff2c\uff21\uff2d\u529f\u80fd\uff0c\u5219\u52fe\u9009 SLAM \u590d\u9009\u6846,\u5c06\u5176 Topic \u8bbe\u7f6e\u4e3a point_raw \uff0c\u540c\u65f6\u5c06 pointCloud2 \u7684 Topic \u8bbe\u7f6e\u4e3a cloud \u3002 Slam\u63d0\u4f9b\u4e86\u53ef\u8c03\u53c2\u6570\uff1a Topic:\u8bbe\u7f6e\u6784\u56fe\u4e3b\u9898 Size:\u8bbe\u7f6e\u70b9\u4e91\u5730\u56fe\u4e2d\u70b9\u7684\u663e\u793a\u5927\u5c0f Color:\u70b9\u4e91\u5730\u56fe\u7684\u989c\u8272 Res:\u5206\u8fa8\u7387 Step_size:\u6b65\u5e45 Trans_epsilon:\u6536\u655b\u5747\u65b9\u5dee Max_iter:\u6700\u5927\u8fed\u4ee3\u6b21\u6570 Voxel_leaf_size:Voxel\u5c3a\u5bf8 Min_scan_range:\u6700\u5c0f\u626b\u63cf\u8303\u56f4 Max_scan_range:\u6700\u5927\u626b\u63cf\u8303\u56f4 Scan_rate:\u626b\u63cf\u5468\u671f \u663e\u793a\u969c\u788d\u7269\uff0c\u5219\u53d6\u6d88 SLAM \u52fe\u9009\uff0c\u5c06 pointCloud2 \u7684 Topic \u66f4\u6539\u56de point_raw .\u52fe\u9009 \uff22oundingBox \u590d\u9009\u6846\u3002 \u663e\u793a\u4f4d\u7f6e\uff0c\u901f\u5ea6\uff0c\u52a0\u901f\u5ea6\u548c\u8def\u5f84 \u663e\u793a\u805a\u7c7b\u969c\u788d\u7269\uff0c\u5219\u52fe\u9009'BoundingBox2'\u590d\u9009\u6846\u3002 \u8fd9\u91cc\u67093\u4e2a\u53c2\u6570: Cluster_D:\u805a\u7c7b\u76f4\u5f84 Cluster_Min:\u6700\u5c0f\u805a\u7c7b\u70b9\u6570 Cluster_Max:\u6700\u5927\u805a\u7c7b\u70b9\u6570 \u70b9\u4e91\u6807\u5b9a \u9996\u5148\u8981\u52fe\u9009 PointCloud2 \u663e\u793a\u70b9\u4e91\uff0c\u518d\u52fe\u9009 \u6807\u5b9a \u53ef\u79fb\u52a8xyz\u4e09\u8f74\uff0c\u4e5f\u53ef\u7ed5\u4e09\u8f74\u65cb\u8f6c\u3002","title":"\u5e94\u7528\u8bb2\u89e3"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#_7","text":"","title":"\u7a0b\u5e8f\u8bb2\u89e3"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#_8","text":"\u6807\u5b9a\u7a0b\u5e8f\u529f\u80fd\u4ee3\u7801\u6bb5\u4f4d\u4e8e\"ros2qt\\src\\qnode.cpp\"\u6587\u4ef6 void QNode::biaodingCallback(const PointCloud::ConstPtr& msg) { //\u70b9\u4e91\u65cb\u8f6c Eigen::Affine3f transform_2 = Eigen::Affine3f::Identity(); transform_2.translation() << x_pianyi, y_pianyi, z_pianyi; float theta_2 = 0*M_PI/180; transform_2.rotate (Eigen::AngleAxisf (theta_2, Eigen::Vector3f::UnitY())); pcl::PointCloud<pcl::PointXYZ>::Ptr transformed_cloud_2 (new pcl::PointCloud<pcl::PointXYZ> ()); pcl::transformPointCloud (*msg, *transformed_cloud_2, transform_2); Eigen::Affine3f transform = Eigen::Affine3f::Identity(); transform.translation() << 0.0, 0.0, 0.0; float theta = x_xuanzhuan*M_PI/180; transform.rotate (Eigen::AngleAxisf (theta, Eigen::Vector3f::UnitX())); pcl::PointCloud<pcl::PointXYZ>::Ptr transformed_cloud (new pcl::PointCloud<pcl::PointXYZ> ()); pcl::transformPointCloud (*transformed_cloud_2, *transformed_cloud, transform); Eigen::Affine3f transform_3 = Eigen::Affine3f::Identity(); transform_3.translation() << 0.0, 0.0, 0.0; float theta_3 = z_xuanzhuan*M_PI/180; transform_3.rotate (Eigen::AngleAxisf (theta_3, Eigen::Vector3f::UnitZ())); pcl::PointCloud<pcl::PointXYZ>::Ptr transformed_cloud_3 (new pcl::PointCloud<pcl::PointXYZ> ()); pcl::transformPointCloud (*transformed_cloud, *transformed_cloud_3, transform_3); Eigen::Affine3f transform_4 = Eigen::Affine3f::Identity(); transform_4.translation() << 0.0, 0.0, 0.0; float theta_4 = y_xuanzhuan*M_PI/180; transform_4.rotate (Eigen::AngleAxisf (theta_4, Eigen::Vector3f::UnitY())); pcl::PointCloud<pcl::PointXYZ>::Ptr transformed_cloud_4 (new pcl::PointCloud<pcl::PointXYZ> ()); pcl::transformPointCloud (*transformed_cloud_3, *transformed_cloud_4, transform_4); transformed_cloud_4->header.frame_id = frame_id; pub.publish(transformed_cloud_4); } \u70b9\u4e91\u7684\u6807\u5b9a\uff0c\u4e3b\u8981\u662f\u70b9\u4e91\u7684\u65cb\u8f6c\u548c\u5e73\u79fb\u529f\u80fd\uff0c\u53ef\u4ee5\u53c2\u8003pcl\uff0c\u8fd9\u91cc\u7b80\u5355\u8bb2\u89e3\u4e00\u4e0b\u65cb\u8f6c\u5e73\u79fb\u529f\u80fd \u9996\u5148\u5b9a\u4e49\u4e00\u4e2aAffine3f\u7ed3\u6784\u4f53\uff0c\u7528\u6765\u5b58\u50a8\u70b9\u4e91\u65cb\u8f6c\u548c\u5e73\u79fb\u4fe1\u606f Eigen::Affine3f transform_2 = Eigen::Affine3f::Identity(); \u518d\u628a\uff58\uff0c\uff59\uff0c\uff5a\u8f74\u7684\u5e73\u79fb\u91cfx_pianyi, y_pianyi, z_pianyi\u4f20\u9012\u8fdbAffine3f\u7ed3\u6784\u4f53 transform_2.translation() << x_pianyi, y_pianyi, z_pianyi; \u5b9a\u4e49\u4e00\u4e2a\u65cb\u8f6c\u89d2\u5ea6 float theta_2 = 0*M_PI/180; \u628a\u7ed5\uff39\u8f74\u65cb\u8f6c\u89d2\u5ea6\uff0c\u4f20\u5165Affin3f\u7ed3\u6784\u4f53\uff0c\u65cb\u8f6c\u8f74Eigen::Vector3f::UnitY() transform_2.rotate (Eigen::AngleAxisf (theta_2, Eigen::Vector3f::UnitY())); \u5b9a\u4e49\u65cb\u8f6c\u540e\u7684\u70b9\u4e91 pcl::PointCloudpcl::pointxyz::Ptr transformed_cloud_2 (new pcl::PointCloudpcl::pointxyz ()); \u6267\u884c\u70b9\u4e91\u7684\u5e73\u79fb\u548c\u65cb\u8f6c\uff0c\u5e76\u628a\u7ed3\u679c\u5b58\u5165 transformed_cloud_2 pcl::transformPointCloud (*msg, *transformed_cloud_2, transform_2);","title":"\u70b9\u4e91\u6807\u5b9a\u529f\u80fd"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#_9","text":"\u5efa\u56fe\u7a0b\u5e8f\u5305\u5373ndt_mapping\u6587\u4ef6\u5939,\u91cd\u70b9\u8fd0\u7b97\u903b\u8f91\u5728ndt_mapping.cpp\u6587\u4ef6\u5185\uff0c\u8fd9\u91cc\u89e3\u6790\u4e00\u4e0b ndt_mapping::ndt_mapping() { transform_pub = nh_.advertise<sensor_msgs::PointCloud2> (\"/cloud\", 1, false); points_sub_ = nh_.subscribe(slam_topic, 100000, &ndt_mapping::points_callback,this); ndt_map_pub_ = nh_.advertise<sensor_msgs::PointCloud2>(\"/ndt_map\", 1000); current_pose_pub_ = nh_.advertise<geometry_msgs::PoseStamped>(\"/current_pose\", 1000); max_iter_ = max_iter_1; ndt_res_ = ndt_res_1; step_size_ = step_size_1; trans_eps_ = trans_eps_1; voxel_leaf_size_ = voxel_leaf_size_1; scan_rate_ = scan_rate_1; min_scan_range_ = min_scan_range_1; max_scan_range_ = max_scan_range_1; min_add_scan_shift_ = min_add_scan_shift_1; initial_scan_loaded = 0; min_add_scan_shift_ = 1.0; _tf_x=0.0, _tf_y=0.0, _tf_z=0.0, _tf_roll=0.0, _tf_pitch=0.0, _tf_yaw=0.0; Eigen::Translation3f tl_btol(_tf_x, _tf_y, _tf_z); Eigen::AngleAxisf rot_x_btol(_tf_roll, Eigen::Vector3f::UnitX()); Eigen::AngleAxisf rot_y_btol(_tf_pitch, Eigen::Vector3f::UnitY()); Eigen::AngleAxisf rot_z_btol(_tf_yaw, Eigen::Vector3f::UnitZ()); tf_btol_ = (tl_btol * rot_z_btol * rot_y_btol * rot_x_btol).matrix(); tf_ltob_ = tf_btol_.inverse(); map_.header.frame_id = \"velodyne\"; current_pose_.x = current_pose_.y = current_pose_.z = 0.0;current_pose_.roll = current_pose_.pitch = current_pose_.yaw = 0.0; previous_pose_.x = previous_pose_.y = previous_pose_.z = 0.0;previous_pose_.roll = previous_pose_.pitch = previous_pose_.yaw = 0.0; voxel_grid_filter_.setLeafSize(voxel_leaf_size_, voxel_leaf_size_, voxel_leaf_size_); ndt.setTransformationEpsilon(trans_eps_); ndt.setStepSize(step_size_); ndt.setResolution(ndt_res_); ndt.setMaximumIterations(max_iter_); is_first_map_ = true; }; ndt_mapping::ndt_mapping()\u51fd\u6570\uff0c\u4e3b\u8981\u8fdb\u884c\u4e86\u4e00\u4e9b\u53c2\u6570\u8bbe\u5b9a\uff0c\u548ctopic\u8bbe\u5b9a\u3002 void ndt_mapping::points_callback(const sensor_msgs::PointCloud2::ConstPtr& input) { pcl::PointCloud<pcl::PointXYZI> tmp, scan; pcl::PointCloud<pcl::PointXYZI>::Ptr filtered_scan_ptr(new pcl::PointCloud<pcl::PointXYZI>()); pcl::PointCloud<pcl::PointXYZI>::Ptr transformed_scan_ptr(new pcl::PointCloud<pcl::PointXYZI>()); tf::Quaternion q; Eigen::Matrix4f t_localizer(Eigen::Matrix4f::Identity()); Eigen::Matrix4f t_base_link(Eigen::Matrix4f::Identity()); static tf::TransformBroadcaster br_; tf::Transform transform; pcl::fromROSMsg(*input, tmp); double r; Eigen::Vector3d point_pos; pcl::PointXYZI p; for (pcl::PointCloud<pcl::PointXYZI>::const_iterator item = tmp.begin(); item != tmp.end(); item++) { use_imu_ = false; if(use_imu_){ // deskew(TODO:inplement of predicting pose by imu) point_pos.x() = (double)item->x; point_pos.y() = (double)item->y; point_pos.z() = (double)item->z; double s = scan_rate_ * (double(item->intensity) - int(item->intensity)); point_pos.x() -= s * current_pose_msg_.pose.position.x;//current_pose_imu_ point_pos.y() -= s * current_pose_msg_.pose.position.y; point_pos.z() -= s * current_pose_msg_.pose.position.z; Eigen::Quaterniond start_quat, end_quat, mid_quat; mid_quat.setIdentity(); end_quat = Eigen::Quaterniond( current_pose_msg_.pose.orientation.w, current_pose_msg_.pose.orientation.x, current_pose_msg_.pose.orientation.y, current_pose_msg_.pose.orientation.z); start_quat = mid_quat.slerp(s, end_quat); point_pos = start_quat.conjugate() * start_quat * point_pos; point_pos.x() += current_pose_msg_.pose.position.x; point_pos.y() += current_pose_msg_.pose.position.y; point_pos.z() += current_pose_msg_.pose.position.z; p.x = point_pos.x(); p.y = point_pos.y(); p.z = point_pos.z(); } else{ p.x = (double)item->x; p.y = (double)item->y; p.z = (double)item->z; } p.intensity = (double)item->intensity; r = sqrt(pow(p.x, 2.0) + pow(p.y, 2.0)); if (min_scan_range_ < r && r < max_scan_range_) { scan.push_back(p); } } pcl::PointCloud<pcl::PointXYZI>::Ptr scan_ptr(new pcl::PointCloud<pcl::PointXYZI>(scan)); if (initial_scan_loaded == 0) { pcl::transformPointCloud(*scan_ptr, *transformed_scan_ptr, tf_btol_); map_ += *transformed_scan_ptr; initial_scan_loaded = 1; } voxel_grid_filter_.setInputCloud(scan_ptr); voxel_grid_filter_.filter(*filtered_scan_ptr); ndt.setInputSource(filtered_scan_ptr); pcl::PointCloud<pcl::PointXYZI>::Ptr map_ptr(new pcl::PointCloud<pcl::PointXYZI>(map_)); if (is_first_map_ == true){ ndt.setInputTarget(map_ptr); is_first_map_ = false; } Eigen::Translation3f init_translation(current_pose_.x, current_pose_.y, current_pose_.z); Eigen::AngleAxisf init_rotation_x(current_pose_.roll, Eigen::Vector3f::UnitX()); Eigen::AngleAxisf init_rotation_y(current_pose_.pitch, Eigen::Vector3f::UnitY()); Eigen::AngleAxisf init_rotation_z(current_pose_.yaw, Eigen::Vector3f::UnitZ()); Eigen::Matrix4f init_guess = (init_translation * init_rotation_z * init_rotation_y * init_rotation_x).matrix() * tf_btol_; pcl::PointCloud<pcl::PointXYZI>::Ptr output_cloud(new pcl::PointCloud<pcl::PointXYZI>); ndt.align(*output_cloud, init_guess); t_localizer = ndt.getFinalTransformation(); t_base_link = t_localizer * tf_ltob_; pcl::transformPointCloud(*scan_ptr, *transformed_scan_ptr, t_localizer); sensor_msgs::PointCloud2::Ptr tt(new sensor_msgs::PointCloud2); pcl::toROSMsg(*transformed_scan_ptr, *tt); tt->header.frame_id = \"velodyne\"; transform_pub.publish(tt); tf::Matrix3x3 mat_b; mat_b.setValue(static_cast<double>(t_base_link(0, 0)), static_cast<double>(t_base_link(0, 1)), static_cast<double>(t_base_link(0, 2)), static_cast<double>(t_base_link(1, 0)), static_cast<double>(t_base_link(1, 1)), static_cast<double>(t_base_link(1, 2)), static_cast<double>(t_base_link(2, 0)), static_cast<double>(t_base_link(2, 1)), static_cast<double>(t_base_link(2, 2))); current_pose_.x = t_base_link(0, 3);current_pose_.y = t_base_link(1, 3);current_pose_.z = t_base_link(2, 3); mat_b.getRPY(current_pose_.roll, current_pose_.pitch, current_pose_.yaw, 1); transform.setOrigin(tf::Vector3(current_pose_.x, current_pose_.y, current_pose_.z)); q.setRPY(current_pose_.roll, current_pose_.pitch, current_pose_.yaw); transform.setRotation(q); br_.sendTransform(tf::StampedTransform(transform, input->header.stamp, \"velodyne\", \"base_link\")); double shift = sqrt(pow(current_pose_.x - previous_pose_.x, 2.0) + pow(current_pose_.y - previous_pose_.y, 2.0)); if (shift >= min_add_scan_shift_) { map_ += *transformed_scan_ptr; previous_pose_.x = current_pose_.x;previous_pose_.y = current_pose_.y;previous_pose_.z = current_pose_.z; previous_pose_.roll = current_pose_.roll;previous_pose_.pitch = current_pose_.pitch;previous_pose_.yaw = current_pose_.yaw; ndt.setInputTarget(map_ptr); sensor_msgs::PointCloud2::Ptr map_msg_ptr(new sensor_msgs::PointCloud2); pcl::toROSMsg(*map_ptr, *map_msg_ptr); ndt_map_pub_.publish(*map_msg_ptr); } current_pose_msg_.header.frame_id = \"velodyne\"; current_pose_msg_.header.stamp = input->header.stamp; current_pose_msg_.pose.position.x = current_pose_.x;current_pose_msg_.pose.position.y = current_pose_.y;current_pose_msg_.pose.position.z = current_pose_.z; current_pose_msg_.pose.orientation.x = q.x();current_pose_msg_.pose.orientation.y = q.y();current_pose_msg_.pose.orientation.z = q.z();current_pose_msg_.pose.orientation.w = q.w(); current_pose_pub_.publish(current_pose_msg_); std::cout << \"-----------------------------------------------------------------\" << std::endl; std::cout << \"\u6784\u5efa\u5730\u56fe\" << std::endl; std::cout << \"-----------------------------------------------------------------\" << std::endl; } ndt_mapping::points_callback\u51fd\u6570\u5373\u8fdb\u884c\u6784\u56fe\uff0c\u5c06\u521d\u59cb\u5316\u70b9\u4e91\u52a0\u5165\u81f3\u5730\u56fe\uff0c\u82e5\u70b9\u4e91\u5730\u56fe\u6ca1\u6709\u521d\u59cb\u5316\u8f7d\u5165\uff0c\u5219\u5c06\u7b2c\u4e00\u5e27\u56fe\u50cf\u4f5c\u4e3a\u521d\u59cb\u56fe\u50cf\uff0c\u7136\u540e\u5c06\u914d\u51c6\u4e4b\u540e\u7684\u56fe\u50cf\u9010\u5e27\u52a0\u5165map\u3002\u901a\u8fc7tf_btol\u53d8\u6362\u77e9\u9635\u5c06\u539f\u59cb\u70b9\u4e91\u8fdb\u884c\u8f6c\u5316\u3002tf_btol\u662f\u8f66\u8f86\u5728\u8d77\u59cb\u4f4d\u7f6e\u662f\u4e0d\u5728\u5168\u5c40\u5730\u56fe\u539f\u70b9\u65f6\u7684\u53d8\u6362\u77e9\u9635\u3002\u7136\u540e\u5bf9\u539f\u59cb\u8f93\u5165\u70b9\u4e91\u8fdb\u884c\u4f53\u7d20\u8fc7\u6ee4\uff0c\u9009\u62e9\u4e0d\u540c\u7684\u65b9\u6cd5\u8fdb\u884c\u53c2\u6570\u8bbe\u7f6e init_guess\u662fndt\u914d\u51c6\u65f6\u5019\u7684\u521d\u59cb\u4f4d\u7f6e\uff0c\u8be5\u4f4d\u7f6e\u4e00\u822c\u7531\u524d\u4e00\u5e27\u4f4d\u7f6e\u52a0\u4e0a\u5fae\u5c0f\u65f6\u95f4\u6bb5\u5185\u7684\u53d8\u5316\uff0c\u5f53\u91c7\u7528imu\u6216odom\u65f6\u53ef\u4ee5\u5229\u7528\u5176\u8fdb\u884c\u8f85\u52a9\u7cbe\u786e\u5b9a\u4f4d\u521d\u59cb\u4f4d\u7f6e\u3002\u5982\u679c\u672a\u4f7f\u7528imu\u4ee5\u53caodom\u5219\u4f7f\u7528\u539f\u6765\u7684init_guess","title":"\u70b9\u4e91\u5efa\u56fe\u529f\u80fd"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#_10","text":"\u70b9\u4e91\u805a\u7c7b\u529f\u80fd\u5728\"lidar_demo/src/lidar_demo.cpp\"\u6587\u4ef6 void callback(const boost::shared_ptr<const sensor_msgs::PointCloud2>& msg) { ros::NodeHandle n; n.getParam(\"Cluster_D\", Cluster_D); n.getParam(\"Cluster_Max\", Cluster_Max); n.getParam(\"Cluster_Min\", Cluster_Min); pcl::PCLPointCloud2 pcl_pc2; pcl_conversions::toPCL(*msg,pcl_pc2); pcl::PointCloud<pcl::PointXYZ>::Ptr temp_cloud(new pcl::PointCloud<pcl::PointXYZ>); pcl::fromPCLPointCloud2(pcl_pc2,*temp_cloud); std::vector<int> mapping; pcl::removeNaNFromPointCloud(*temp_cloud, *temp_cloud, mapping); pub2.publish(temp_cloud); pcl::PointIndices::Ptr inliers (new pcl::PointIndices); pcl::ModelCoefficients::Ptr coefficients (new pcl::ModelCoefficients); pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_plane (new pcl::PointCloud<pcl::PointXYZ> ()); pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_f (new pcl::PointCloud<pcl::PointXYZ>); if (temp_cloud->points.size() == 0) { std::cout << \"cloud in ROI is empty\" << std::endl; return; } pcl::search::KdTree<pcl::PointXYZ>::Ptr tree (new pcl::search::KdTree<pcl::PointXYZ>); tree->setInputCloud (temp_cloud); std::vector<pcl::PointIndices> cluster_indices; pcl::EuclideanClusterExtraction<pcl::PointXYZ> ec; cout << Cluster_D << \":\" << Cluster_Min << \":\" << Cluster_Max<<endl; ec.setClusterTolerance (Cluster_D); ec.setMinClusterSize (Cluster_Min); ec.setMaxClusterSize (Cluster_Max); ec.setSearchMethod (tree); ec.setInputCloud (temp_cloud); ec.extract (cluster_indices); jsk_recognition_msgs::BoundingBoxArray BOXS; int j = 0; vector<Eigen::Vector3f> center; for (std::vector<pcl::PointIndices>::const_iterator it = cluster_indices.begin (); it != cluster_indices.end (); ++it) { pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_cluster (new pcl::PointCloud<pcl::PointXYZ>); for (std::vector<int>::const_iterator pit = it->indices.begin (); pit != it->indices.end (); ++pit) cloud_cluster->push_back ((*temp_cloud)[*pit]); cloud_cluster->width = cloud_cluster->size (); cloud_cluster->height = 1; cloud_cluster->is_dense = true; std::cout << \"Cluster has : \" << cloud_cluster->size () << \" data points.\" << std::endl; j++; jsk_recognition_msgs::BoundingBox box2; Eigen::Vector3f mass_center; pcl::MomentOfInertiaEstimation<pcl::PointXYZ> feature_extractor; feature_extractor.setInputCloud(cloud_cluster); feature_extractor.compute(); pcl::PointXYZ min_point_OBB; pcl::PointXYZ max_point_OBB; pcl::PointXYZ position_OBB; Eigen::Matrix3f rotational_matrix_OBB; feature_extractor.getOBB(min_point_OBB, max_point_OBB, position_OBB, rotational_matrix_OBB); Eigen::Quaternionf quat (rotational_matrix_OBB); feature_extractor.getMassCenter (mass_center); center.push_back(mass_center); pcl::PointXYZ min; pcl::PointXYZ max; pcl::getMinMax3D(*cloud_cluster,min,max); max.x = max.x; box2.label = j+1; box2.pose.position.x = (max.x + min.x) / 2; box2.pose.position.y = (max.y + min.y) / 2; box2.pose.position.z = (max.z + min.z) / 2; box2.dimensions.x = (max.x - min.x); box2.dimensions.y = (max.y - min.y); box2.dimensions.z = (max.z - min.z); box2.header.frame_id = frame_id; BOXS.boxes.push_back(box2); } BOXS.header.frame_id = frame_id; pub.publish(BOXS); } \u70b9\u4e91\u805a\u7c7b\u4e3b\u8981\u4f7f\u7528pcl\u5e93\u805a\u7c7b\u7b97\u6cd5 pcl::EuclideanClusterExtractionpcl::pointxyz ec; \u53ea\u9700\u8bbe\u5b9a\u53c2\u6570\uff1a Cluster_D:\u805a\u7c7b\u76f4\u5f84 Cluster_Min:\u6700\u5c0f\u805a\u7c7b\u70b9\u6570 Cluster_Max:\u6700\u5927\u805a\u7c7b\u70b9\u6570","title":"\u70b9\u4e91\u805a\u7c7b\u529f\u80fd"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#_11","text":"\u4e3b\u8981\u4ee3\u7801\u5206\u5e03\u4f4d\u4e8elidar_cnn_seg_detect\uff0cdetected_objects_visualizer\uff0ccommon\uff0crockauto_msgs\u6587\u4ef6\u5939\u5185 \u70b9\u4e91\u667a\u80fd\u8bc6\u522b\u529f\u80fd\uff0c\u91c7\u7528\u7684\u662fapolo\u7684\u795e\u7ecf\u7f51\u7edc\u8bc6\u522b\u7b97\u6cd5\uff0c\u53ef\u5230\u8be5\u7b97\u6cd5\u7684 \u7ef4\u62a4\u5e73\u53f0 \u8fdb\u884c\u6df1\u5165\u7814\u7a76\uff0c\u8be5\u7b97\u6cd5\u4e0evoxelnet\u7f51\u7edc\u76f8\u4f3c\uff0c\u4e5f\u53ef\u53c2\u8003 github \u6211\u4eec\u5728\u667a\u80fd\u8bc6\u522b\u7684\u57fa\u7840\u4e0a\u6dfb\u52a0\u4e86\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\u663e\u793a VisualizeDetectedObjects::ObjectsToLabels(const rockauto_msgs::DetectedObjectArray &in_objects) { visualization_msgs::MarkerArray label_markers; for (auto const &object: in_objects.objects) { if (IsObjectValid(object)) { visualization_msgs::Marker label_marker; label_marker.lifetime = ros::Duration(marker_display_duration_); label_marker.header = in_objects.header; label_marker.ns = ros_namespace_ + \"/label_markers\"; label_marker.action = visualization_msgs::Marker::ADD; label_marker.type = visualization_msgs::Marker::TEXT_VIEW_FACING; label_marker.scale.x = 1.5; label_marker.scale.y = 1.5; label_marker.scale.z = 1.5; label_marker.color = label_color_; label_marker.id = marker_id_++; if(!object.label.empty() && object.label != \"unknown\") label_marker.text = object.label + \" \"; //Object Class if available float velocity_x = 0.0, velocity_y = 0.0, a_x =0.0, a_y = 0.0; if(past_x.size() ==0) { velocity_x = 0.0; velocity_y = 0.0; } else{ int before = 0; float past_d =100.0; for(int i=0;i<past_x.size();i++) { float d = sqrt(pow((past_x[i] - object.pose.position.x),2)+pow((past_y[i] - object.pose.position.y),2)); if(d < past_d) { past_d = d; before = i; } } velocity_x = (-past_x[before] + object.pose.position.x)/(0.1); velocity_y = (-past_y[before] + object.pose.position.y)/(0.1); a_x = (-past_vx[before] + velocity_x)/(0.1); a_y = (-past_vy[before] + velocity_y)/(0.1); } std::stringstream distance_stream; distance_stream << std::fixed << std::setprecision(1) << sqrt((object.pose.position.x * object.pose.position.x) + (object.pose.position.y * object.pose.position.y)); std::stringstream velocity_stream; velocity_stream <<std::fixed<<std::setprecision(2)<<\"v_x:\"<<velocity_x <<\"m/s a_x:\"<<a_x<<\"m/s2\\n\" <<\"v_y:\"<<velocity_y<<\"m/s a_y:\"<<a_y<<\"m/s2\"; std::string distance_str = distance_stream.str() + \" m\\n\" + velocity_stream.str(); label_marker.text += distance_str; if (object.velocity_reliable) { double velocity = object.velocity.linear.x; if (velocity < -0.1) { velocity *= -1; } if (abs(velocity) < object_speed_threshold_) { velocity = 0.0; } tf::Quaternion q(object.pose.orientation.x, object.pose.orientation.y, object.pose.orientation.z, object.pose.orientation.w); double roll, pitch, yaw; tf::Matrix3x3(q).getRPY(roll, pitch, yaw); // convert m/s to km/h std::stringstream kmh_velocity_stream; kmh_velocity_stream << std::fixed << std::setprecision(1) << (velocity * 3.6); std::string text = \"\\n<\" + std::to_string(object.id) + \"> \" + kmh_velocity_stream.str() + \" km/h\"; label_marker.text += text; } label_marker.pose.position.x = object.pose.position.x; label_marker.pose.position.y = object.pose.position.y; label_marker.pose.position.z = label_height_; label_marker.scale.z = 1.0; if (!label_marker.text.empty()) label_markers.markers.push_back(label_marker); past_x.push_back(object.pose.position.x); past_y.push_back(object.pose.position.y); past_vx.push_back(velocity_x); past_vy.push_back(velocity_y); } } // end in_objects.objects loop return label_markers; }//ObjectsToLabels","title":"\u70b9\u4e91\u667a\u80fd\u8bc6\u522b\u529f\u80fd"},{"location":"%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%AE%97%E6%B3%95%E7%9B%92%E5%AD%90/#_12","text":"\u7a0b\u5e8f\u8981\u653e\u5728\u82f1\u6587\u8def\u5f84\u4e0b\uff0c\u6700\u597d\u4e0d\u8981\u653e\u5728\u4e2d\u6587\u76ee\u5f55\u4e0b\uff0c\u56e0\u4e3a\u5bb9\u6613\u51fa\u73b0\u4e2d\u6587\u4e71\u7801\u95ee\u9898\u800c\u5f71\u54cd\u7a0b\u5e8f\u8fd0\u884c","title":"\u6ce8\u610f\u4e8b\u9879"}]}