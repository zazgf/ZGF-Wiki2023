<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>RL - ZGF Wiki</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../css/theme.css" />
  <link rel="stylesheet" href="../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "RL";
    var mkdocs_page_input_path = "RL.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> ZGF Wiki</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../c%2B%2B/">C++</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../python/">Python</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../ubuntu18/">Ubuntu18</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../ubuntu20/">Ubuntu20</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../Ros/">Ros</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../Ros%E5%BB%BA%E6%A8%A1/">Ros建模</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../tf_document/">tf_document</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../caffe/">Caffe</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../deeplearning/">deeplearning</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../voxelnet/">voxelnet</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../radar_camera_fusion/">radar_camera</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../python%E6%9C%89%E7%94%A8%E7%9A%84%E5%87%BD%E6%95%B0/">python有用的函数(一)</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">ZGF Wiki</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>RL</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h2 id="what-to-learn-in-model-free-rl">what to learn in model-free RL</h2>
<p>There are two main approaches to representing and training agents with model-free RL:</p>
<h3 id="policy-optimization">Policy Optimization</h3>
<p>Methods in this family represent a policy explicitly as $\pi_\theta(\alpha,s)$.They optimize the parameters $\theta$ either directly by gradient ascent on the performance objective $J(\pi_\theta)$ ,or indirectly, by maximizing local approximation of $J(\pi_\theta)$ ,This optimiation is almost always performed on-policy,which means that each update only uses data collected while acting according to the most recent version of the policy.Policy optimization also usually invovles learning an approximator $V_\phi(s)$ for the on-policy value function $V^\pi(s)$ ,which gets used in figuring out how to update the policy.</p>
<p>A couple of examples of policy optimization methods are:</p>
<ul>
<li>A2C/A3C, which performs gradient ascent to directly maximize performance.</li>
</ul>
<h4 id="a2ca3c">A2C/A3C</h4>
<p>We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.</p>
<p>Deep RL algorithms based on experience replay have achieved unprecedented success in challenging domains such as Atari 2600.However experience replay has serval drawbacks : it uses more memory and computation per real interaction; and it requires off-policy learning algorithms that can update from data generated by an older policy.</p>
<p>The best of the proposed methods, asynchronous advantage actor-critic(A3C), also mastered a variety of continuous motor control tasks as well as learned general strategies for exploring 3D mazes purely from visual inputs.</p>
<h2 id="related-work">Related Work</h2>
<p>The General Reinforcement Learning Architecture (Gorila) of performs asynchronous training of reinforcement learning agents in a distributed setting.In Gorila, each process contains an actor that acts in its own copy of the environment, a seperate replay memory, and a learner that samples data from the replay memory, and a learner that samples data from the replay memory and computes gradients of the DQN loss with respect to the policy parameters. The gradients are asynchronously sent to a central parameter server which updates a central copy of the model. The updated policy parameters are sent to the actor-learners at fixed intervals.By using 100 separate actor-learner processes and 30 parameter server instances, a total of 130 machines, Gorila was able to significantly outperform DQN over 49 Atari games. On many games Gorila reached the score achieved by DQN over 20 times faster than DQN. We also note that a similar way of parallelizing DQN was proposed by Chavez.</p>
<p>In earlier work Li applied the Map Reduce framework to parallelizing batch reinforcement learning methods with linear function approximation.Parallelism was used to speed up large matrix operations but not to paralleize the collection of experience or stabilize learning. Grounds proposed a parallel vision of the Sarsa algorithm that uses multiple separate actor-learners to accelerate training .Each actor-learner learns separately and periodically sends updates to weights that have changed significantly to the other learners using peer-to-peer communication.</p>
<p>T studied convergence properties of Q-learning in the asynchronous optimization setting. These results show that Q-learning is still guaranteed to converage when some of the information is outdated as long as outdated information is always eventually discarded and several other technical assumptions are satisfied. Even earlier, studied the related problem of distributed dynamic programming.</p>
<p>Another related area of work is in evolutionary methods, which are often straightforward to parallelize by distributing fitness evaluations over multiple machines or threads.Such parallel evolutionary approaches have recently been applied to some visual reinforcement learning tasks.In one example, Koutnik evolved convolutional neural network controllers for the TORCS driving simulator by performing fitness evaluations on 8 CPU cores in parallel.</p>
<h3 id="reinforcement-learning-background">Reinforcement Learning Background</h3>
<p>We consider the standard reinforcement learning setting where an agent interacts with an environment $\varepsilon$ over a number of discrete time steps.At each time step $t$ ,the agent receives a state $s_t$ and selects an action $a_t$ from some set of possible actions $A$ according to its policy $\pi$ ,where $\pi$ is a mapping from states $s_t$ to actions $a_t$ . In return the agent receives the next state $s_{t+1}$ and receives a scalar reward $\tau_t$ . The process continues until the agent reaches a terminal state after  which the process restarts. The return $R_t=\sum^{\infty}<em>{k=0}\gamma^k\tau</em>{k+1}$ is the total accumulated return from time step $t$ with discount factor $\gamma\in(0,1]$ .The goal of the agent is to maximize the expected return from each state $s_t$.</p>
<p>The action value $Q^\pi(s,a)=\mathbb{E}[R_t|s_t=s,a]$ is the expected return for selecting action $a$ in the state $s$ and following policy $\pi$ . The optimal value function $Q^*(s,a)=max_\pi Q^\pi(s,a)$ gives the maximum action value for state $s$ and action $a$ achievable by any policy. Similarly, the value of the state $s$ and action $a$ achievable by any policy. Similarly, the value of state $s$ under policy $\pi$ is defined as $V^\pi(s)=\mathbb{E}[R_t|s_t=s]$ and is simply the expected return for following policy $\pi$ from state $s$.</p>
<p>In value-based model-free reinforcement learning methods, the action value function is represented using a function approximator, such as a neural network. Let $Q(s,a;\theta) $ be an approximate action-value function with parameters $\theta$ . The updates to $\theta$ can be derived from a variety of reinforcement learning algorithms.One example of such an algorithm is Q-learning, which aims to directly approximate the optimal action value function: $Q^*(s,a)\approx Q(s,a,\theta)$ . In one-step Q-learning, the parameters $\theta$ of the action value function $Q(s,a,\theta)$ are learned by iteratively minimizing a sequence of loss functions, where the $i$th loss function defined as</p>
<p>$$
L_i(\theta_i)=\mathbb{E}\left(\tau+\gamma\underset{a'}{max}Q(s',a';\theta_{i-1})-Q(s,a;\theta_i\right)^2</p>
<p>$$</p>
<p>where $s'$ is the state encountered after state $s$.</p>
<p>We refer to the above method as one-step Q-learning because it updates the action value Q(s,a) toward the onestep return $\tau+\gamma\underset{a'}{max}Q(s',a';\theta)$.One drawback of using one-step methods is that obtaining a reward $\tau$ only directly affects the value of the state action pair $s,a$ that led to the reward. The values of other state action pairs are affected only indirectly through the updated value $Q(s,a)$ . This can make the learning process slow since many updates are required the propagate a reward to the relevant preceding states and actions.</p>
<p>One way of propagating rewards faster is by using n-step returns.In $n$-step Q-learning, $Q(s,a)$ is updated toward the $n$-step return defined as $\tau_t+\gamma\tau_{t+1}+...+\gamma^{n-1}\tau_{t+n-1}+\underset{a}{max}\gamma^nQ(s_{t+n},a)$ . This results in a single reward $\tau$ directly affecting the values of $n$ preceding state action pairs.This makes the process of propagating rewards to relevant state-action pairs potentially much more efficient.</p>
<p>In construct to value-based methods, policy-based model-free methods directly parameterize the policy $\pi(a|s;\theta)$ and update the parameters $\theta$ by performing, typically approximate, gradient ascent on $\mathbb{E}[R_t]$ . One example of such a method is the REINFORCE family of algorithms due to Williams. Standard REINFORCE updates the policy parameters $\theta$ in the direction $\triangledown_\theta log\pi(a_t|s_t;\theta)R_t$,which is an unbiased estimate of $\triangledown_\theta\mathbb{E}[R_t]$. It is possible to reduce the variance of this estimate while keeping it unbiased by substracting a learned  function of the state   $b_t(s_t)$, known as a baseline, from the return. The resulting gradient is $\triangledown_\theta log\pi(a_t|s_t;\theta)(R_t-b_t(s_t)).$</p>
<p>A learned estimate of the value function is commonly used as the baseline $b_t(s_t)\approx V^\pi(s_t)$ leading to a much lower variance estimate of the policy gradient. When an aprroximate value function is used as the baseline, the quantity $R_t-b_t$ used to scale the policy gradient can be seen as an estimate of the advantage of action $a_t$ in state $s_t$, or $A(a_t,s_t)=Q(a_t,s_t)-V(s_t)$ ,because $R_t$ is an estimate of $Q^\pi(a_t,s_t)$ and $b_t$ is an estimate of $V^\pi(s_t)$. This approach can be viewed as an actor-critic architecture where the policy $\pi$ is the actor and the baseline $b_t$ is the critic.</p>
<h3 id="asynchronous-rl-framework">Asynchronous RL Framework</h3>
<p>We now present multi-threaded asynchronous variants of one-step Sarsa, one-step Q-learning, n-step Q-learning, and advantage actor-critic. The aim in designing these methods was to find RL algorithms that can train deep neural network policies reliably and without large resource requirements. While the underlying RL methods are quite different , with actor-critic being an on-policy policy search method and Q-learning being an off-policy value-based method ,we use two main ideas to make all four algorithms practical given our design goal.</p>
<p>First, we use asynchronous actor-learners , similarly to the Gorila framework , but instead of using sparate machines and a parameter server , we use multiple CPU threads  on a single machine. Keeping the learners on a single machine removes the communication costs of sending gradients and parameters and enables us to use Hogwild! style updates for training.</p>
<p>Second ,we make the observation that multiple actors-learners running in parallel are likely to be exploring different parts of the environment.</p>
<p>asynchronous one-step Q-learning pseudocode for each actor-learner thread</p>
<p>//Assume global shared θ,$\theta^-$, and counter $T=0$.</p>
<p>Initialize thread step counter $t\leftarrow 0$</p>
<p>Initialize target network weights $\theta^-\leftarrow\theta$</p>
<p>Initialize network gradients $d\theta\leftarrow 0$</p>
<p>Get initial state $s$</p>
<p><strong>repeat</strong></p>
<p>$\$Take action</p>
              
            </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../mathjaxhelper.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
