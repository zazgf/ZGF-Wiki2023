{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to ZGF Wiki\n\n\n\n\n\n\n\u867d\u53e4\u72b9\u4e0d\u53ca\n\n\n\n\n\n\n\n\n\n\n\u5b50\u66f0\uff1a\u201c\u5b66\u800c\u65f6\u4e60\u4e4b\uff0c\u4e0d\u4ea6\u8bf4\u4e4e\uff1f\u6709\u670b\u81ea\u8fdc\u65b9\u6765\uff0c\u4e0d\u4ea6\u4e50\u4e4e\uff1f\u4eba\u4e0d\u77e5\u800c\u4e0d\u6120\uff0c\u4e0d\u4ea6\u541b\u5b50\u4e4e\uff1f\u201d\n\n\n\n\n\n\n\u66fe\u5b50\u66f0\uff1a\u201c\u543e\u65e5\u4e09\u7701\u543e\u8eab\uff1a\u4e3a\u4eba\u8c0b\u800c\u4e0d\u5fe0\u4e4e\uff1f\u4e0e\u670b\u53cb\u4ea4\u800c\u4e0d\u4fe1\u4e4e\uff1f\u4f20\u4e0d\u4e60\u4e4e\uff1f\u201d\n\n\n\n\n\n\n\u5b50\u66f0\uff1a\u201c\u6e29\u6545\u800c\u77e5\u65b0\uff0c\u53ef\u4ee5\u4e3a\u5e08\u77e3\u3002\u201d\n\n\n\n\n\n\n\u5b50\u66f0\uff1a\u201c\u5b66\u800c\u4e0d\u601d\u5219\u7f54\uff0c\u601d\u800c\u4e0d\u5b66\u5219\u6b86\u3002\u201d\n\n\n\n\n\n\n\u5b50\u66f0\uff1a\u201c\u7531\uff0c\u8bf2\u5973\u77e5\u4e4b\u4e4e\uff01\u77e5\u4e4b\u4e3a\u77e5\u4e4b\uff0c\u4e0d\u77e5\u4e3a\u4e0d\u77e5\uff0c\u662f\u77e5\u4e5f\u3002\u201d\n\n\n\n\n\n\n\u5b50\u66f0\uff1a\u201c\u89c1\u8d24\u601d\u9f50\u7109\uff0c\u89c1\u4e0d\u8d24\u800c\u5185\u81ea\u7701\u4e5f\u3002\u201d\n\n\n\n\n\n\n\u5b50\u66f0\uff1a\u201c\u4e09\u4eba\u884c\uff0c\u5fc5\u6709\u6211\u5e08\u7109\u3002\u62e9\u5176\u5584\u8005\u800c\u4ece\u4e4b\uff0c\u5176\u4e0d\u5584\u8005\u800c\u6539\u4e4b\u3002\u201d\n\n\n\n\n\n\n\u66fe\u5b50\u66f0\uff1a\u201c\u58eb\u4e0d\u53ef\u4ee5\u4e0d\u5f18\u6bc5\uff0c\u4efb\u91cd\u800c\u9053\u8fdc\u3002\u4ec1\u4ee5\u4e3a\u5df1\u4efb\uff0c\u4e0d\u4ea6\u91cd\u4e4e\uff1f\u6b7b\u800c\u540e\u5df2\uff0c\u4e0d\u4ea6\u8fdc\u4e4e\uff1f\u201d\n\n\n\n\n\n\n\u5b50\u66f0\uff1a\u201c\u5c81\u5bd2\uff0c\u7136\u540e\u77e5\u677e\u67cf\u4e4b\u540e\u51cb\u4e5f\u3002\u201d\n\n\n\n\n\n\n\u5b50\u8d21\u95ee\u66f0\uff1a\u201c\u6709\u4e00\u8a00\u800c\u53ef\u4ee5\u7ec8\u8eab\u884c\u4e4b\u8005\u4e4e\uff1f\u201d        \u5b50\u66f0\uff1a\u201c\u5176\u6055\u4e4e\uff01\u5df1\u6240\u4e0d\u6b32\uff0c\u52ff\u65bd\u4e8e\u4eba\u3002\u201d\n\n\n\n\n\n\n\u5b50\u66f0\uff1a\u201c\u541b\u5b50\u6210\u4eba\u4e4b\u7f8e\uff0c\u4e0d\u6210\u4eba\u4e4b\u6076\u3002\u5c0f\u4eba\u53cd\u662f\u3002\u201d",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-zgf-wiki",
            "text": "\u867d\u53e4\u72b9\u4e0d\u53ca      \u5b50\u66f0\uff1a\u201c\u5b66\u800c\u65f6\u4e60\u4e4b\uff0c\u4e0d\u4ea6\u8bf4\u4e4e\uff1f\u6709\u670b\u81ea\u8fdc\u65b9\u6765\uff0c\u4e0d\u4ea6\u4e50\u4e4e\uff1f\u4eba\u4e0d\u77e5\u800c\u4e0d\u6120\uff0c\u4e0d\u4ea6\u541b\u5b50\u4e4e\uff1f\u201d    \u66fe\u5b50\u66f0\uff1a\u201c\u543e\u65e5\u4e09\u7701\u543e\u8eab\uff1a\u4e3a\u4eba\u8c0b\u800c\u4e0d\u5fe0\u4e4e\uff1f\u4e0e\u670b\u53cb\u4ea4\u800c\u4e0d\u4fe1\u4e4e\uff1f\u4f20\u4e0d\u4e60\u4e4e\uff1f\u201d    \u5b50\u66f0\uff1a\u201c\u6e29\u6545\u800c\u77e5\u65b0\uff0c\u53ef\u4ee5\u4e3a\u5e08\u77e3\u3002\u201d    \u5b50\u66f0\uff1a\u201c\u5b66\u800c\u4e0d\u601d\u5219\u7f54\uff0c\u601d\u800c\u4e0d\u5b66\u5219\u6b86\u3002\u201d    \u5b50\u66f0\uff1a\u201c\u7531\uff0c\u8bf2\u5973\u77e5\u4e4b\u4e4e\uff01\u77e5\u4e4b\u4e3a\u77e5\u4e4b\uff0c\u4e0d\u77e5\u4e3a\u4e0d\u77e5\uff0c\u662f\u77e5\u4e5f\u3002\u201d    \u5b50\u66f0\uff1a\u201c\u89c1\u8d24\u601d\u9f50\u7109\uff0c\u89c1\u4e0d\u8d24\u800c\u5185\u81ea\u7701\u4e5f\u3002\u201d    \u5b50\u66f0\uff1a\u201c\u4e09\u4eba\u884c\uff0c\u5fc5\u6709\u6211\u5e08\u7109\u3002\u62e9\u5176\u5584\u8005\u800c\u4ece\u4e4b\uff0c\u5176\u4e0d\u5584\u8005\u800c\u6539\u4e4b\u3002\u201d    \u66fe\u5b50\u66f0\uff1a\u201c\u58eb\u4e0d\u53ef\u4ee5\u4e0d\u5f18\u6bc5\uff0c\u4efb\u91cd\u800c\u9053\u8fdc\u3002\u4ec1\u4ee5\u4e3a\u5df1\u4efb\uff0c\u4e0d\u4ea6\u91cd\u4e4e\uff1f\u6b7b\u800c\u540e\u5df2\uff0c\u4e0d\u4ea6\u8fdc\u4e4e\uff1f\u201d    \u5b50\u66f0\uff1a\u201c\u5c81\u5bd2\uff0c\u7136\u540e\u77e5\u677e\u67cf\u4e4b\u540e\u51cb\u4e5f\u3002\u201d    \u5b50\u8d21\u95ee\u66f0\uff1a\u201c\u6709\u4e00\u8a00\u800c\u53ef\u4ee5\u7ec8\u8eab\u884c\u4e4b\u8005\u4e4e\uff1f\u201d        \u5b50\u66f0\uff1a\u201c\u5176\u6055\u4e4e\uff01\u5df1\u6240\u4e0d\u6b32\uff0c\u52ff\u65bd\u4e8e\u4eba\u3002\u201d    \u5b50\u66f0\uff1a\u201c\u541b\u5b50\u6210\u4eba\u4e4b\u7f8e\uff0c\u4e0d\u6210\u4eba\u4e4b\u6076\u3002\u5c0f\u4eba\u53cd\u662f\u3002\u201d",
            "title": "Welcome to ZGF Wiki"
        },
        {
            "location": "/Atlas200\u9a8c\u6536\u62a5\u544a/",
            "text": "Atlas200\u9a8c\u6536\u62a5\u544a\n\n\n\n\n\u5728\u7ebf\u652f\u6301\n\n- \n\u665f\u817e\u5b66\u9662\n\n- \nAtlas 200 DK\n\n\n\u6027\u80fd\u7279\u70b9\n\n- \u53ef\u63d0\u4f9b16TOPS (INT8) \u7684\u5cf0\u503c\u8ba1\u7b97\u80fd\u529b\u3002\n- \u652f\u6301\u4e24\u8defCamera\u8f93\u5165\uff0c\u4e24\u8defISP\u56fe\u50cf\u5904\u7406\uff0c\u652f\u6301HDR10\u9ad8\u52a8\u6001\u8303\u56f4\u6280\u672f\u6807\u51c6\n- \u652f\u63011000M\u4ee5\u592a\u7f51\u5bf9\u5916\u63d0\u4f9b\u9ad8\u901f\u7f51\u7edc\u8fde\u63a5\uff0c\u5339\u914d\u5f3a\u52b2\u8ba1\u7b97\u80fd\u529b\n- \u901a\u7528\u768440-pin\u6269\u5c55\u63a5\u53e3\uff0c\u65b9\u4fbf\u4ea7\u54c1\u539f\u578b\u8bbe\u8ba1\n- \u652f\u63015v~28v\u5bbd\u8303\u56f4\u76f4\u6d41\u7535\u6e90\u8f93\u5165\n\n\n\u9700\u8981\u914d\u4ef6\n\n- micro sd\u5361 32G\n- typeC usb \u7ebf\n- \u7f51\u7ebf\n- \u8bfb\u5361\u5668\n- Ubuntu\u670d\u52a1\u5668\n\n\n\u5236\u4f5cUsb\u7cfb\u7edf\u542f\u52a8\u76d8\n\n- \u5c06SD\u5361\u653e\u5165\u8bfb\u5361\u5668\uff0c\u5c06\u8bfb\u5361\u5668\u4e0eUbuntu\u670d\u52a1\u5668\u7684USB\u8fde\u63a5\uff0c\u7136\u540e\u901a\u8fc7\u5236\u5361\u811a\u672c\u8fdb\u884cSD\u5361\u7684\u5236\u4f5c\n- \u8f6f\u4ef6\u5305\u51c6\u5907\n  - \u5236\u5361\u5165\u53e3\u811a\u672c  \nmake_sd_card.py\n\n  - \u5236\u4f5cSD\u5361\u64cd\u4f5c\u7cfb\u7edf\u811a\u672c \nmake_ubuntu_sd.sh\n\n    - \u8bf7\u9009\u62e9\u914d\u5957\u7684CANN\u7248\u672c\uff1a20.1\u3002\n    - \u201c\u4ea7\u54c1\u7cfb\u5217\u201d\u8bf7\u9009\u62e9\u201cAtlas 200 DK\u201d\u3002\n    - \u201c\u4ea7\u54c1\u578b\u53f7\u201d\u8bf7\u9009\u62e9\u201cAtlas 200 DK \u5f00\u53d1\u8005\u5957\u4ef6\u201d\u3002\n  - \nUbuntu\u64cd\u4f5c\u7cfb\u7edf\u955c\u50cf\u5305\n\n    - ubuntu-18.04.xx-server-arm64.iso\n  - \n\u5f00\u53d1\u8005\u677f\u9a71\u52a8\u5305\n\n    - A200dk-npu-driver-{software version}-ubuntu18.04-aarch64-minirc.tar.gz\n    - \u8bf7\u9009\u62e9\u914d\u5957\u7684CANN\u7248\u672c\uff1a20.1\u3002\n    - \u201c\u4ea7\u54c1\u7cfb\u5217\u201d\u8bf7\u9009\u62e9\u201cAtlas 200 DK\u201d\u3002\n    - \u201c\u4ea7\u54c1\u578b\u53f7\u201d\u8bf7\u9009\u62e9\u201cAtlas 200 DK \u5f00\u53d1\u8005\u5957\u4ef6\u201d\u3002\n  - \n\u5f00\u53d1\u8005\u677f\u8fd0\u884c\u5305\n\n    - Ascend-cann-minirc_{software version}_ubuntu18.04-aarch64.zip\n    - \u8bf7\u4ece\u201cCANN\u8f6f\u4ef6\u5305\u201d\u4e2d\u9009\u62e9\u5bf9\u5e94\u7248\u672c\u7684\u8f6f\u4ef6\u5305\u4e0b\u8f7d\u3002\n\n\n\u64cd\u4f5c\u6b65\u9aa4\n\n1. \u8bf7\u5c06SD\u5361\u653e\u5165\u8bfb\u5361\u5668\uff0c\u5e76\u5c06\u8bfb\u5361\u5668\u4e0eUbuntu\u670d\u52a1\u5668\u7684USB\u63a5\u53e3\u8fde\u63a5\u3002\n2. \nsudo apt-get install qemu-user-static binfmt-support python3-yaml gcc-aarch64-linux-gnu g++-aarch64-linux-gnu\n\n3. \u628a\u51c6\u5907\u597d\u7684\u6587\u4ef6\u5305\u653e\u5728/home/username/mksd\u76ee\u5f55\u4e0b\uff0c\ncd /home/username/mksd\n\n4. \u6267\u884c\nfdisk -l\n\u67e5\u770bsd\u5361\u6240\u5728usb\u8bbe\u5907\u540d\u79f0\u4e3a\u201c/dev/sda\u201d\uff0c\u53ef\u901a\u8fc7\u63d2\u62d4SD\u5361\u7684\u65b9\u5f0f\u786e\u5b9a\u8bbe\u5907\u540d\u79f0\u3002\n5. \u8fd0\u884cSD\u5236\u5361\u811a\u672c\u201cmake_sd_card.py\u201d\u3002\n   1. \npython3 make_sd_card.py local /dev/sda\n\n   2. \u201clocal\u201d\u8868\u793a\u4f7f\u7528\u672c\u5730\u65b9\u5f0f\u5236\u4f5cSD\u5361\u3002\n   3. \u201c/dev/sda\u201d\u4e3aSD\u5361\u6240\u5728\u7684USB\u8bbe\u5907\u540d\u79f0\u3002\n\n\n\u9047\u5230\u95ee\u9898\n\uff1a\u5236\u4f5csd\u5361\u4e0d\u6210\u529f\uff0c\u7ecf\u8fc7\u53cd\u590d\u534f\u52a9\u534e\u4e3a\u65b9\u6280\u672f\u4eba\u5458\uff0c\u53d1\u73b0\u534e\u4e3a\u5b98\u65b9\u63d0\u4f9b\u7684\u811a\u672c\u6587\u4ef6\nmake_ubuntu_sd.sh\n\u6709bug\u3002\u4f7f\u7528\u4f53\u9a8c\u5dee\u3002\u534e\u4e3a\u65b9\u89e3\u51b3\u95ee\u9898\u5468\u671f\u957f\uff0c\u8fd9\u4e2a\u95ee\u9898\u7528\u4e86\u4e24\u5929\u624d\u89e3\u51b3\u3002\u540e\u671f\u5e94\u7528\u8fc7\u7a0b\u4e2d\u534e\u4e3aatlas\u5e73\u53f0\u4e5f\u4f1a\u5b58\u5728\u4e00\u4e9bbug\uff0c\u5f00\u53d1\u901f\u5ea6\u53d7\u9650\u4e8e\u534e\u4e3a\u65b9\u89e3\u51b3atlas\u4ea7\u54c1bug\u7684\u901f\u5ea6\u3002\u9047\u5230Atlas\u5e73\u53f0\u95ee\u9898\uff0c\u53ea\u80fd\u5728\u534e\u4e3a\u4e91\u8bba\u575b\u4e0a\u8fdb\u884c\u63d0\u95ee\uff0c\u89e3\u51b3\u95ee\u9898\u5468\u671f\u957f\u3002\n\n\n\n\n\u83b7\u5f97\u4fee\u6b63\u540e\u7684\nmake_ubuntu_sd.sh\n\u811a\u672c\u6587\u4ef6\u5b89\u88c5\u6210\u529f\u3002\n\n\n\n\nAtlas200 \u5f00\u673a\u542f\u52a8\u6210\u529f\uff0c\u8fdb\u5165\u7cfb\u7edf\u3002\\\n\n\u8fdb\u884c\u670d\u52a1\u5668ip\u8bbe\u7f6e\n\uff1a\\\nip:192.168.0.3\\\nnetmask:255.255.0.0\\\ngateway:192.168.0.1\\\n\nssh\u767b\u5165Atlas200\n:\\\n\nssh HwHiAiUser@192.168.0.2\n\\\n\u5bc6\u7801:\\\n\nMind@123\n\\",
            "title": "Atlas200\u9a8c\u6536\u62a5\u544a"
        },
        {
            "location": "/Atlas200\u9a8c\u6536\u62a5\u544a/#atlas200",
            "text": "\u5728\u7ebf\u652f\u6301 \n-  \u665f\u817e\u5b66\u9662 \n-  Atlas 200 DK  \u6027\u80fd\u7279\u70b9 \n- \u53ef\u63d0\u4f9b16TOPS (INT8) \u7684\u5cf0\u503c\u8ba1\u7b97\u80fd\u529b\u3002\n- \u652f\u6301\u4e24\u8defCamera\u8f93\u5165\uff0c\u4e24\u8defISP\u56fe\u50cf\u5904\u7406\uff0c\u652f\u6301HDR10\u9ad8\u52a8\u6001\u8303\u56f4\u6280\u672f\u6807\u51c6\n- \u652f\u63011000M\u4ee5\u592a\u7f51\u5bf9\u5916\u63d0\u4f9b\u9ad8\u901f\u7f51\u7edc\u8fde\u63a5\uff0c\u5339\u914d\u5f3a\u52b2\u8ba1\u7b97\u80fd\u529b\n- \u901a\u7528\u768440-pin\u6269\u5c55\u63a5\u53e3\uff0c\u65b9\u4fbf\u4ea7\u54c1\u539f\u578b\u8bbe\u8ba1\n- \u652f\u63015v~28v\u5bbd\u8303\u56f4\u76f4\u6d41\u7535\u6e90\u8f93\u5165  \u9700\u8981\u914d\u4ef6 \n- micro sd\u5361 32G\n- typeC usb \u7ebf\n- \u7f51\u7ebf\n- \u8bfb\u5361\u5668\n- Ubuntu\u670d\u52a1\u5668  \u5236\u4f5cUsb\u7cfb\u7edf\u542f\u52a8\u76d8 \n- \u5c06SD\u5361\u653e\u5165\u8bfb\u5361\u5668\uff0c\u5c06\u8bfb\u5361\u5668\u4e0eUbuntu\u670d\u52a1\u5668\u7684USB\u8fde\u63a5\uff0c\u7136\u540e\u901a\u8fc7\u5236\u5361\u811a\u672c\u8fdb\u884cSD\u5361\u7684\u5236\u4f5c\n- \u8f6f\u4ef6\u5305\u51c6\u5907\n  - \u5236\u5361\u5165\u53e3\u811a\u672c   make_sd_card.py \n  - \u5236\u4f5cSD\u5361\u64cd\u4f5c\u7cfb\u7edf\u811a\u672c  make_ubuntu_sd.sh \n    - \u8bf7\u9009\u62e9\u914d\u5957\u7684CANN\u7248\u672c\uff1a20.1\u3002\n    - \u201c\u4ea7\u54c1\u7cfb\u5217\u201d\u8bf7\u9009\u62e9\u201cAtlas 200 DK\u201d\u3002\n    - \u201c\u4ea7\u54c1\u578b\u53f7\u201d\u8bf7\u9009\u62e9\u201cAtlas 200 DK \u5f00\u53d1\u8005\u5957\u4ef6\u201d\u3002\n  -  Ubuntu\u64cd\u4f5c\u7cfb\u7edf\u955c\u50cf\u5305 \n    - ubuntu-18.04.xx-server-arm64.iso\n  -  \u5f00\u53d1\u8005\u677f\u9a71\u52a8\u5305 \n    - A200dk-npu-driver-{software version}-ubuntu18.04-aarch64-minirc.tar.gz\n    - \u8bf7\u9009\u62e9\u914d\u5957\u7684CANN\u7248\u672c\uff1a20.1\u3002\n    - \u201c\u4ea7\u54c1\u7cfb\u5217\u201d\u8bf7\u9009\u62e9\u201cAtlas 200 DK\u201d\u3002\n    - \u201c\u4ea7\u54c1\u578b\u53f7\u201d\u8bf7\u9009\u62e9\u201cAtlas 200 DK \u5f00\u53d1\u8005\u5957\u4ef6\u201d\u3002\n  -  \u5f00\u53d1\u8005\u677f\u8fd0\u884c\u5305 \n    - Ascend-cann-minirc_{software version}_ubuntu18.04-aarch64.zip\n    - \u8bf7\u4ece\u201cCANN\u8f6f\u4ef6\u5305\u201d\u4e2d\u9009\u62e9\u5bf9\u5e94\u7248\u672c\u7684\u8f6f\u4ef6\u5305\u4e0b\u8f7d\u3002  \u64cd\u4f5c\u6b65\u9aa4 \n1. \u8bf7\u5c06SD\u5361\u653e\u5165\u8bfb\u5361\u5668\uff0c\u5e76\u5c06\u8bfb\u5361\u5668\u4e0eUbuntu\u670d\u52a1\u5668\u7684USB\u63a5\u53e3\u8fde\u63a5\u3002\n2.  sudo apt-get install qemu-user-static binfmt-support python3-yaml gcc-aarch64-linux-gnu g++-aarch64-linux-gnu \n3. \u628a\u51c6\u5907\u597d\u7684\u6587\u4ef6\u5305\u653e\u5728/home/username/mksd\u76ee\u5f55\u4e0b\uff0c cd /home/username/mksd \n4. \u6267\u884c fdisk -l \u67e5\u770bsd\u5361\u6240\u5728usb\u8bbe\u5907\u540d\u79f0\u4e3a\u201c/dev/sda\u201d\uff0c\u53ef\u901a\u8fc7\u63d2\u62d4SD\u5361\u7684\u65b9\u5f0f\u786e\u5b9a\u8bbe\u5907\u540d\u79f0\u3002\n5. \u8fd0\u884cSD\u5236\u5361\u811a\u672c\u201cmake_sd_card.py\u201d\u3002\n   1.  python3 make_sd_card.py local /dev/sda \n   2. \u201clocal\u201d\u8868\u793a\u4f7f\u7528\u672c\u5730\u65b9\u5f0f\u5236\u4f5cSD\u5361\u3002\n   3. \u201c/dev/sda\u201d\u4e3aSD\u5361\u6240\u5728\u7684USB\u8bbe\u5907\u540d\u79f0\u3002  \u9047\u5230\u95ee\u9898 \uff1a\u5236\u4f5csd\u5361\u4e0d\u6210\u529f\uff0c\u7ecf\u8fc7\u53cd\u590d\u534f\u52a9\u534e\u4e3a\u65b9\u6280\u672f\u4eba\u5458\uff0c\u53d1\u73b0\u534e\u4e3a\u5b98\u65b9\u63d0\u4f9b\u7684\u811a\u672c\u6587\u4ef6 make_ubuntu_sd.sh \u6709bug\u3002\u4f7f\u7528\u4f53\u9a8c\u5dee\u3002\u534e\u4e3a\u65b9\u89e3\u51b3\u95ee\u9898\u5468\u671f\u957f\uff0c\u8fd9\u4e2a\u95ee\u9898\u7528\u4e86\u4e24\u5929\u624d\u89e3\u51b3\u3002\u540e\u671f\u5e94\u7528\u8fc7\u7a0b\u4e2d\u534e\u4e3aatlas\u5e73\u53f0\u4e5f\u4f1a\u5b58\u5728\u4e00\u4e9bbug\uff0c\u5f00\u53d1\u901f\u5ea6\u53d7\u9650\u4e8e\u534e\u4e3a\u65b9\u89e3\u51b3atlas\u4ea7\u54c1bug\u7684\u901f\u5ea6\u3002\u9047\u5230Atlas\u5e73\u53f0\u95ee\u9898\uff0c\u53ea\u80fd\u5728\u534e\u4e3a\u4e91\u8bba\u575b\u4e0a\u8fdb\u884c\u63d0\u95ee\uff0c\u89e3\u51b3\u95ee\u9898\u5468\u671f\u957f\u3002   \u83b7\u5f97\u4fee\u6b63\u540e\u7684 make_ubuntu_sd.sh \u811a\u672c\u6587\u4ef6\u5b89\u88c5\u6210\u529f\u3002   Atlas200 \u5f00\u673a\u542f\u52a8\u6210\u529f\uff0c\u8fdb\u5165\u7cfb\u7edf\u3002\\ \u8fdb\u884c\u670d\u52a1\u5668ip\u8bbe\u7f6e \uff1a\\\nip:192.168.0.3\\\nnetmask:255.255.0.0\\\ngateway:192.168.0.1\\ ssh\u767b\u5165Atlas200 :\\ ssh HwHiAiUser@192.168.0.2 \\\n\u5bc6\u7801:\\ Mind@123 \\",
            "title": "Atlas200\u9a8c\u6536\u62a5\u544a"
        },
        {
            "location": "/Autoware.auto/",
            "text": "Autoware.Auto\n\n\nInstall\n\n\n\n\nubuntu 20.04\n\n\nROS2_FOXY\n\n\n\n\nsudo apt update && sudo apt install locales\nsudo locale-gen en_US en_US.UTF-8\nsudo update-locale LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8\nexport LANG=en_US.UTF-8\n\nsudo apt install curl\nsudo apt update && sudo apt install curl gnupg2 lsb-release\n\ncurl -s https://raw.githubusercontent.com/ros/rosdistro/master/ros.asc | sudo apt-key add -\n\nsudo sh -c 'echo \"deb [arch=$(dpkg --print-architecture)] http://packages.ros.org/ros2/ubuntu $(lsb_release -cs) main\" > /etc/apt/sources.list.d/ros2-latest.list'\n\nsudo apt update && sudo apt install -y \\\n  build-essential \\\n  cmake \\\n  git \\\n  libbullet-dev \\\n  python3-colcon-common-extensions \\\n  python3-flake8 \\\n  python3-pip \\\n  python3-pytest-cov \\\n  python3-rosdep \\\n  python3-setuptools \\\n  python3-vcstool \\\n  wget\n# install some pip packages needed for testing\npython3 -m pip install -U \\\n  argcomplete \\\n  flake8-blind-except \\\n  flake8-builtins \\\n  flake8-class-newline \\\n  flake8-comprehensions \\\n  flake8-deprecated \\\n  flake8-docstrings \\\n  flake8-import-order \\\n  flake8-quotes \\\n  pytest-repeat \\\n  pytest-rerunfailures \\\n  pytest\n# install Fast-RTPS dependencies\nsudo apt install --no-install-recommends -y \\\n  libasio-dev \\\n  libtinyxml2-dev\n# install Cyclone DDS dependencies\nsudo apt install --no-install-recommends -y \\\n  libcunit1-dev\n\n\n\n\n\u83b7\u53d6ros2\u4ee3\u7801\n\n\nmkdir -p ~/ros2_foxy/src\ncd ~/ros2_foxy\nwget https://raw.githubusercontent.com/ros2/ros2/foxy/ros2.repos\nvcs import src < ros2.repos\n\n\n\n\u4f7f\u7528rosdep\u5b89\u88c5\u4f9d\u8d56\n\n\nsudo rosdep init\nrosdep update\nrosdep install --from-paths src --ignore-src --rosdistro foxy -y --skip-keys \"console_bridge fastcdr fastrtps rti-connext-dds-5.3.1 urdfdom_headers\"\n\n\n\nBuild\n\n\ncd ~/ros2_foxy\ncolcon build --symlink-install\n\n\n\nbuild\u8fc7\u7a0b\u4e2d\u4f1a\u51fa\u73b0\u4e0b\u8f7d\u9519\u8bef\uff0c\u4f7f\u7528hub.fastgit.org\u4ee3\u66ffgithub.com",
            "title": "Autoware.auto"
        },
        {
            "location": "/Autoware.auto/#autowareauto",
            "text": "",
            "title": "Autoware.Auto"
        },
        {
            "location": "/Autoware.auto/#install",
            "text": "ubuntu 20.04  ROS2_FOXY   sudo apt update && sudo apt install locales\nsudo locale-gen en_US en_US.UTF-8\nsudo update-locale LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8\nexport LANG=en_US.UTF-8\n\nsudo apt install curl\nsudo apt update && sudo apt install curl gnupg2 lsb-release\n\ncurl -s https://raw.githubusercontent.com/ros/rosdistro/master/ros.asc | sudo apt-key add -\n\nsudo sh -c 'echo \"deb [arch=$(dpkg --print-architecture)] http://packages.ros.org/ros2/ubuntu $(lsb_release -cs) main\" > /etc/apt/sources.list.d/ros2-latest.list'\n\nsudo apt update && sudo apt install -y \\\n  build-essential \\\n  cmake \\\n  git \\\n  libbullet-dev \\\n  python3-colcon-common-extensions \\\n  python3-flake8 \\\n  python3-pip \\\n  python3-pytest-cov \\\n  python3-rosdep \\\n  python3-setuptools \\\n  python3-vcstool \\\n  wget\n# install some pip packages needed for testing\npython3 -m pip install -U \\\n  argcomplete \\\n  flake8-blind-except \\\n  flake8-builtins \\\n  flake8-class-newline \\\n  flake8-comprehensions \\\n  flake8-deprecated \\\n  flake8-docstrings \\\n  flake8-import-order \\\n  flake8-quotes \\\n  pytest-repeat \\\n  pytest-rerunfailures \\\n  pytest\n# install Fast-RTPS dependencies\nsudo apt install --no-install-recommends -y \\\n  libasio-dev \\\n  libtinyxml2-dev\n# install Cyclone DDS dependencies\nsudo apt install --no-install-recommends -y \\\n  libcunit1-dev  \u83b7\u53d6ros2\u4ee3\u7801  mkdir -p ~/ros2_foxy/src\ncd ~/ros2_foxy\nwget https://raw.githubusercontent.com/ros2/ros2/foxy/ros2.repos\nvcs import src < ros2.repos  \u4f7f\u7528rosdep\u5b89\u88c5\u4f9d\u8d56  sudo rosdep init\nrosdep update\nrosdep install --from-paths src --ignore-src --rosdistro foxy -y --skip-keys \"console_bridge fastcdr fastrtps rti-connext-dds-5.3.1 urdfdom_headers\"",
            "title": "Install"
        },
        {
            "location": "/Autoware.auto/#build",
            "text": "cd ~/ros2_foxy\ncolcon build --symlink-install  build\u8fc7\u7a0b\u4e2d\u4f1a\u51fa\u73b0\u4e0b\u8f7d\u9519\u8bef\uff0c\u4f7f\u7528hub.fastgit.org\u4ee3\u66ffgithub.com",
            "title": "Build"
        },
        {
            "location": "/OpenCV/",
            "text": "install\n\n\n\u4e0b\u8f7d\n\n\ncmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/home/promote/NeDisk/third_part/opencv-3.4.2/install_gcc7 -D OPENCV_EXTRA_MODULES_PATH=/home/promote/NeDisk/third_part/opencv_contrib-3.4.2/modules -D WITH_GTK=ON -D WITH_CUDA=ON -D BUILD_opencv_cudacodec=OFF -D BUILD_TIFF=ON ..\n\n\n\n\u5982\u679c\u5b89\u88c5\u8fc7\u7a0b\u4e2d\u51fa\u73b0\uff0c\u4e0b\u8f7d\u5931\u8d25\uff0c\u67e5\u770bbuild\u6587\u4ef6\u5939\u4e0b\u7684CMakeDownloadLog.txt\uff0c\u5728\n\u6b64\u5904\n\u4e0b\u8f7d\uff0c\u4fdd\u5b58\u81f3opencv\u6587\u4ef6\u5939\u4e0b\u7684.cache\u6587\u4ef6\u5939\u91cc\u7684\u5bf9\u5e94\u4f4d\u7f6e\n\n\nCMake \u6307\u5b9a\u4f7f\u7528OpenCV\u7248\u672c\n\n\nset(OpenCV_DIR \"/home/promote/NeDisk/third_part/opencv-4.5.2/install/lib/cmake/opencv4\") #OpenCVConfig.cmake path\nfind_package(OpenCV REQUIRED)\n\ninclude_directories(${OpenCV_INCLUDE_DIRS})\nadd_executable(main main.cpp)\ntarget_link_libraries(main ${OpenCV_LIBS})\n\n\n\nfatal error: opencv2/xfeatures2d/cuda.hpp: No such file or directory\n\n\n\u5728 <\u4f60\u7684\u8def\u5f84>/opencv-3.4.2/modules/stitching/CMakeLists.txt \u91cc\u6dfb\u52a0\n\n\nINCLUDE_DIRECTORIES(\"/home/promote/NeDisk/opencv_contrib-3.4.2/modules/xfeatures2d/include\")\n\n\n\nerror while loading shared libraries: libopencv_imgproc.so.3.4: cannot open shared object file: No such file or directory\n\n\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/promote/NeDisk/third_part/opencv-3.4.2/install_gcc7/lib\n\n\n\nconvexHull\n\n\nstd::vector<cv::Point2f> points;\nfor(unsigned int i = 0; i<current_cluster->points.size(); i++)\n{\n    cv::Point2f pt;\n    pt.x = current_cluster->points[i].x;\n    pt.y = current_cluster->points[i].y;\n    points.push_back(pt);\n}\nstd::vector<cv::Point2f> hull;\ncv::convexHull(points,hull);\npolygon_.header = in_ros_header;\nfor(size_t i=0; i<hull.size()+1; i++)\n{\n    geometry_msgs::Point32 point;\n    point.x = hull[i%hull.size()].x;\n    point.y = hull[i%hull.size()].y;\n    point.z = min_point_.z;\n    polygon.points.push_back(point);\n}\n\n\n\n\n\n\n\nminAreaRect\n\n\ncv::RotatedRect box= minAreaRect(hull);\n\n\n\n\u8fd4\u56de\u503c\u5185\u5bb9\uff1a\n\n\ncenter    The rectangle mass center.\nsize      Width and height of the rectangle.\nangle     The rotation angle in a clockwise direction. When the angle is 0, 90, 180, 270 etc., the rectangle becomes an up-right rectangle.\n\n\n\nundefined reference to `TIFFLastDirectory@LIBTIFF_4.0'\n\n\ntarget_link_libraries(${PROJECT_NAME}_lib\n    ...\n    /usr/lib/x86_64-linux-gnu/libtiff.so.5  \n  )",
            "title": "OpenCV"
        },
        {
            "location": "/OpenCV/#install",
            "text": "\u4e0b\u8f7d  cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/home/promote/NeDisk/third_part/opencv-3.4.2/install_gcc7 -D OPENCV_EXTRA_MODULES_PATH=/home/promote/NeDisk/third_part/opencv_contrib-3.4.2/modules -D WITH_GTK=ON -D WITH_CUDA=ON -D BUILD_opencv_cudacodec=OFF -D BUILD_TIFF=ON ..  \u5982\u679c\u5b89\u88c5\u8fc7\u7a0b\u4e2d\u51fa\u73b0\uff0c\u4e0b\u8f7d\u5931\u8d25\uff0c\u67e5\u770bbuild\u6587\u4ef6\u5939\u4e0b\u7684CMakeDownloadLog.txt\uff0c\u5728 \u6b64\u5904 \u4e0b\u8f7d\uff0c\u4fdd\u5b58\u81f3opencv\u6587\u4ef6\u5939\u4e0b\u7684.cache\u6587\u4ef6\u5939\u91cc\u7684\u5bf9\u5e94\u4f4d\u7f6e",
            "title": "install"
        },
        {
            "location": "/OpenCV/#cmake-opencv",
            "text": "set(OpenCV_DIR \"/home/promote/NeDisk/third_part/opencv-4.5.2/install/lib/cmake/opencv4\") #OpenCVConfig.cmake path\nfind_package(OpenCV REQUIRED)\n\ninclude_directories(${OpenCV_INCLUDE_DIRS})\nadd_executable(main main.cpp)\ntarget_link_libraries(main ${OpenCV_LIBS})",
            "title": "CMake \u6307\u5b9a\u4f7f\u7528OpenCV\u7248\u672c"
        },
        {
            "location": "/OpenCV/#fatal-error-opencv2xfeatures2dcudahpp-no-such-file-or-directory",
            "text": "\u5728 <\u4f60\u7684\u8def\u5f84>/opencv-3.4.2/modules/stitching/CMakeLists.txt \u91cc\u6dfb\u52a0  INCLUDE_DIRECTORIES(\"/home/promote/NeDisk/opencv_contrib-3.4.2/modules/xfeatures2d/include\")",
            "title": "fatal error: opencv2/xfeatures2d/cuda.hpp: No such file or directory"
        },
        {
            "location": "/OpenCV/#error-while-loading-shared-libraries-libopencv_imgprocso34-cannot-open-shared-object-file-no-such-file-or-directory",
            "text": "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/promote/NeDisk/third_part/opencv-3.4.2/install_gcc7/lib",
            "title": "error while loading shared libraries: libopencv_imgproc.so.3.4: cannot open shared object file: No such file or directory"
        },
        {
            "location": "/OpenCV/#convexhull",
            "text": "std::vector<cv::Point2f> points;\nfor(unsigned int i = 0; i<current_cluster->points.size(); i++)\n{\n    cv::Point2f pt;\n    pt.x = current_cluster->points[i].x;\n    pt.y = current_cluster->points[i].y;\n    points.push_back(pt);\n}\nstd::vector<cv::Point2f> hull;\ncv::convexHull(points,hull);\npolygon_.header = in_ros_header;\nfor(size_t i=0; i<hull.size()+1; i++)\n{\n    geometry_msgs::Point32 point;\n    point.x = hull[i%hull.size()].x;\n    point.y = hull[i%hull.size()].y;\n    point.z = min_point_.z;\n    polygon.points.push_back(point);\n}",
            "title": "convexHull"
        },
        {
            "location": "/OpenCV/#minarearect",
            "text": "cv::RotatedRect box= minAreaRect(hull);  \u8fd4\u56de\u503c\u5185\u5bb9\uff1a  center    The rectangle mass center.\nsize      Width and height of the rectangle.\nangle     The rotation angle in a clockwise direction. When the angle is 0, 90, 180, 270 etc., the rectangle becomes an up-right rectangle.",
            "title": "minAreaRect"
        },
        {
            "location": "/OpenCV/#undefined-reference-to-tifflastdirectorylibtiff_40",
            "text": "target_link_libraries(${PROJECT_NAME}_lib\n    ...\n    /usr/lib/x86_64-linux-gnu/libtiff.so.5  \n  )",
            "title": "undefined reference to `TIFFLastDirectory@LIBTIFF_4.0'"
        },
        {
            "location": "/PCL/",
            "text": "PCA\n\n\n\n\nUse the PCA principal component analysis method to obtain the three main directions of the point cloud, obtain the centroid, calculate the covariance\u534f\u65b9\u5dee, obtain the coveriance matrix, and obtain the eigenvalues and feature vectors of the covariance matrix. The eigenvector is the main direction.\n\n\n\n\nEigen::vector4f pcaCentroid;\npcl::compute3DCentroid(*cloud, pcaCentroid);\nEigen::Matrix3f covariance;\npcl::computeCovarianceMatrixNormalized(*cloud, pcaCentroid, covariance);\nEigen::SelfAdjointEigenSolver<Eigen::Matrix3f> eigen_solver(covariance, Eigen::ComputeEigenVectors);\nEigen::Matrix3f eigenVectorsPCA = eigen_solver.eigenvectors();\nEigen::Vector3f eigenValuesPCA = eigen_solver.eigenvalues();\neigenVectorsPCA.col(2) = eigenVectorsPCA.col(0).cross(eigenVectorsPCA.col(1));\neigenVectorsPCA.col(0) = eigenVectorsPCA.col(1).cross(eigenVectorsPCA.col(2));\neigenVectorsPCA.col(1) = eigenVectorsPCA.col(2).cross(eigenVectorsPCA.col(0));\n\n\n\n\n\n\n\nUsing the main direction and centroid obtained in 1.convert the input point cloud to the origin.and the main direction and the coordinate system direction return to establish the bounding box of the point cloud transformed to the origin.\n\n\n\n\n\n\nSet the main direction and bounding box for the input point cloud through the inverse transformation of the input point cloud to the original point cloud.\n\n\n\n\n\n\nComplete code \n\n\n\n\n\n\n#include <vtkAutoInit.h>\nVTK_MODULE_INIT(vtkRenderingOpenGL);\nVTK_MODULE_INIT(vtkInteractionStyle);\nVTK_MODULE_INIT(vtkRenderingFreeType);\n#include <iostream>\n#include <string>\n#include <pcl/io/pcd_io.h>\n#include <pcl/point_cloud.h>\n#include <pcl/point_types.h>\n#include <Eigen/Core>\n#include <pcl/common/transforms.h>\n#include <pcl/common/common.h>\n#include <pcl/visualization/pcl_visualizer.h>\n\nusing namespace std;\ntypedef pcl::PointXYZ PointType;\nint main(int argc, char **argv)\n{\n    pcl::PointCloud<PointType>::Ptr cloud(new pcl::PointCloud<PointType>());\n    std::cout << \"Please enter the point cloud file name to be displayed.\";\n    std::string fileName(\"rabbit\");\n    getline(cin, fileName);\n    fileName += \".pcd\";\n    pcl::io::loadPCDFile(fileName, *cloud);\n    Eigen::Vector4f pcaCentroid;\n    pcl::compute3DCentroid(*cloud, pcaCentroid);\n    Eigen::Matrix3f covariance;\n    pcl::computeCovarianceMatrixNormalized(*cloud, pcaCentroid,covariance);\n    Eigen::SelfAdjointEigenSolver<Eigen::Matrix3f> eigen_solver (covariance, Eigen::ComputeEigenvectors);\n    Eigen::Matrix3f eigenVectorsPCA = eigen_solver.eigenvectors();\n    Eigen::Vector3f eigenValuesPCA = eigen_sovler.eigenvalues();\n    eigenVectorsPCA.col(2) = eigenVectorsPCA.col(0).cross(eigenVectorsPCA.col(1));\n    eigenVectorsPCA.col(0) = eigenVectorsPCA.col(1).cross(eigenVectorsPCA.col(2));\n    eigenVectorsPCA.col(1) = eigenVectorsPCA.col(2).cross(eigenVectorsPCA.col(0));\n\n    std::cout << \"Eigenvalue va(3x1) :\\n\" << eigenValuesPCA << std::endl;\n    std::cout << \"Feature vector ve(3x3) :\\n\" << eigenVectorsPCA << std::endl;\n    std::cout << \"centroid point (4x1) :\\n\" << pcaCentroid << std::endl;\n\n    /** Another way to calculate the eigenvalues and eigenvectors of the point cloud covariance matrix: through the pca interface in PCL as follows\n     * pcl::PointCloud<pcl::PointXYZ>::Ptr cloudPCAprojection (new pcl::PointCloud<pcl::PointXYZ>);\n     * pcl::PCA<pcl::PointXYZ> pca;\n     * pca.setInputCloud(cloudSegmented);\n     * pca.project(*cloudSegmented, *cloudPCAprojection);\n     * std::cerr << std::endl << \"EigenVectors :\" << pca.getEigenVectors() << std::endl;\n     * std::cerr << std::endl << \"EigenValues :\" << pca.getEigenValues() <<  std::endl;\n     **/\n\n    Eigen::Matrix4f tm = Eigen::Matrix4f::Identity();\n    Eigen::Matrix4f tm_inv = Eigen::Matrix4f::Identity();\n    tm.block<3,3>(0,0) = eigenVectorsPCA.transpose(); //R\n    tm.block<3,1>(0,3) = -0.1f * (eigenVectorsPCA.transpose())*(pcaCentroid.head<3>()); //-R*t\n    tm_inv = tm.inverse();\n    std::cout << \"Transformation matrix tm(4x4): \\n \" << tm << std::endl;\n    std::cout << \"inverter matrix tm'(4x4): \\n\" << tm_inv << std::endl;\n    pcl::PointCloud<PointType>::Ptr transformedCloud(new pcl::PointCloud<PointType>);\n    pcl::transformPointCloud(*cloud, *transformedCloud, tm);\n    PointType min_p1, max_p1;\n    Eigen::Vector3f c1, c;\n    pcl::getMinMax3D (*transformedCloud, min_p1, max_p1);\n    c1 = 0.5f *(min_p1.getVector3fMap() + max_p1.getVector3fMap());\n    std::cout << \"Centre c1(3x1):\\n \" << c1 << std::endl;\n\n    Eigen::Affine3f tm_inv_aff(tm_inv);\n    pcl::transformPoint (c1, c, tm_inv_aff);\n\n    Eigen::vector3f whd, whd1;\n    whd1 = max_p1.getVector3fMap() - min_p1.getVector3fMap();\n    whd = whd1;\n    float sc1 = (whd1(0) + whd1(1) + whd1(2))/3; //the average scale of the point cloud, used to set the size of the main direction arrow\n\n    std::cout << \"width1= \" << whd1(0) << std::endl;\n    std::cout << \"heigth1 = \" << whd1(1) << std::endl;\n    std::cout << \"depth1 = \" << whd1(2) << std::endl;\n    std::cout << \"scale1 = \" << sc1 << std::endl;\n\n    const Eigen::Quaternionf bboxQ1(Eigen::Quaternionf::Identity());\n    const Eigen::Vector3f bboxT1(c1);\n\n    const Eigen::Quaternionf bboxQ(tm_inv.block<3,3>(0,0));\n    const Eigen::Vector3f bboxT(c);\n\n    //the main direction of the point cloud transformed to the origin\n    PointType op;\n    op.x = 0.0;\n    op.y = 0.0;\n    op.z = 0.0;\n    Eigen::Vector3f px,py,pz;\n    Eigen::Affine3f tm_aff(tm);\n    pcl::transformVector(eigenVectorsPCA.col(0), px, tm_aff);\n    pcl::transformVector(eigenVectorsPCA.col(1), py, tm_aff);\n    pcl::transformVector(eigenVectorsPCA.col(2), pz, tm_aff);\n    PointType pcaX;\n    pcaX.x = sc1*px(0);\n    pcaX.y = sc1*px(1);\n    pcaX.z = sc1*px(2);\n    PointType pcaY;\n    pcaY.x = sc1*py(0);\n    pcaY.y = sc1*py(1);\n    pcaY.z = sc1*py(2);\n    PointType pcaZ;\n    pcaZ.x = sc1*pz(0);\n    pcaZ.y = sc1*pz(1);\n    pcaZ.z = sc1*pz(2);\n\n    //the main directio of the point cloud transformed to the origin\n\n    PointType cp;\n    cp.x = pcaCentroid(0);\n    cp.y = pcaCentroid(1);\n    cp.z = pcaCentroid(2);\n    PointType pcX;\n    pcX.x = sc1 * eigenVectorsPCA(0,0) + cp.x;\n    pcX.y = sc1 * eigenVectorsPCA(1,0) + cp.y;\n    pcX.z = sc1 * eigenVectorsPCA(2,0) + cp.z;\n    PointType pcY;\n    pcY.x = sc1* eigenVectorsPCA(0,1) + cp.x;\n    pcY.y = sc1* eigenVectorsPCA(1,1) + cp.y;\n    pcY.z = sc1* eigenVectorsPCA(2,1) + cp.z;\n    PointType pcZ;\n    pcZ.x = sc1* eigenVectorsPCA(0,2) + cp.x;\n    pcZ.y = sc1* eigenVectorsPCA(1,2) + cp.y;\n    pcZ.z = sc1* eigenVectorsPCA(2,2) + cp.z;\n\n    //pcl visualization\n    pcl::visualization::PCLVisualizer viewer;\n    pcl::visualization::PointCloudColorHandlerCustom<PointType> tc_handler(transformedCloud,0,255,0);\n\n    viewer.addPointCloud(transformedCloud, tc_handler, \"transformedCloud\");\n    viewer.addCube(bboxT1, bboxQ1, whd1(0), whd1(1), whd1(2), \"bbox1\");\n    viewer.setShapeRenderingProperties(pcl::visualization::PCL_VISUALIZER_REPRESENTATION, pcl::visualization::PCL_VISUALIZER_REPRESENTATION_WIREFRAME, \"bbox1\");\n    viewer.setShapeRenderingProperties(pcl::visualization::PCL_VISUALIZER_COLOR, 0.0, 1.0, 0.0, \"bbox1\");\n\n    viewer.addArrow(pcaX, op, 1.0, 0.0, 0.0, false, \"arrow_X\");\n    viewer.addArrow(pcaY, op, 0.0, 1.0, 0.0, false, \"arrow_Y\");\n    viewer.addArrow(pcaZ, op, 0.0, 0.0, 1.0, false, \"arrow_Z\");\n\n    pcl::visualization::PointCloudColorHandlerCustom<PointType> color_handler(cloud, 255,0,0);\n    viewer.addPointCloud(cloud, color_handler, \"cloud\");\n    viewer.addCube(bboxT, bboxQ, whd(0), whd(1), whd(2), \"bbox\");\n\n    viewer.setShapeRenderingProperties(pcl::visualization::PCL_VISUALIZER_REPRESENTATION,pcl::visualization::PCL_VISUALIZER_REPRESENTATION_WIREFRAME, \"bbox\");\n    viewer.setShapeRenderingProperties(pcl::visualization::PCL_VISUALIZER_COLOR, 1.0, 0.0, 0.0);\n\n    viewer.addArrow(pcX, cp ,1.0, 0.0, 0.0, false, \"arrow_x\");\n    viewer.addArrow(pcY, cp, 0.0, 1.0, 0.0, false, \"arrow_y\");\n    viewer.addArrow(pcZ, cp, 0.0, 0.0, 1.0, false, \"arrow_z\");\n    viewer.addCoordinateSystem(0.5f * sc1);\n    viewer.setBackgroundColor(1.0, 1.0, 1.0);\n    while(!viewer.wasStopped())\n    {\n        viewer.spinOnce(100);\n    }\n    return 0;\n}\n\n\n\n\n\nNormal estimation set number of threads\n\n\nfpfh setNumberOfThreads\n\n\nnormal_estimation.setNumberOfThreads(in_ompnum_threads);\n...\nfpfh.setNumberOfThreads(in_ompnum_threads);\n\n\n\nCMakeLists.txt\n\n\nfind_package(OpenMP)\nif(OPENMP_FOUND)\n  set_target_properties(lidar_euclidean_cluster_detect PROPERTIES\n    COMPILE_FLAGS ${OpenMP_CXX_FLAGS}\n    LINK_FLAGS ${OpenMP_CXX_FLAGS}\n  )\nendif()\n\n\n\n\u624d\u53ef\u542f\u52a8\u591a\u7ebf\u7a0b\n\n\n\u5b9a\u4e49Vector \u5e76\u521d\u59cb\u5316\n\n\nstd::vector<float> cluster_fpfh_histogram(33, 0.0);\n\n\n\nthrust\n\n\n\n\nthrust provides such a rich collection of data parallel primitives such as scan, sort, and reduce,which can be composed together to implent complex alogrithms with concise\u7b80\u6d01 readable source code.By describing your computation in terms of these high-level abstractions you provide.Thrust with the freedom to select the most efficient implementation automatically. As a result, thrust can be utilized in rapid prototyping of cuda applications, where programmer productivity matters most, as well as in production, where robustness and absolute performance are crucial.\n\n\nVectors\n\n\nThrust provides two vector containers, \nhost_vector\n and \ndevice_vector\n. As the names suggest, \nhost_vector\n is stored in host memory while \ndevice_vector\n lives in GPU device memory. Thrust's vector containers are just like \nstd::vector\n , \nhost_vector\n are generic containers( able to store any data type) that can be resized dynamically.The following source code illustrates the use of Thrust's vector containers.\n\n\n#include <thrust/host_vector.h>\n#include <thrust/device_vector.h>\n#include <iostream>\n\nint main(void)\n{\n    //H has storage for 4 integers\n    thrust::host_vector<int> H(4);\n\n    //initialize individual elements\n    H[0] = 14;\n    H[1] = 20;\n    H[2] = 38;\n    H[3] = 46;\n\n    std::cout << \"H has size \" << H.size() << std::endl;\n\n    for (int i=0; i< H.size();i++)\n    {\n        std::cout << \"[\" << i <<\"] = \" << H[i] << endl;\n    }\n\n    H.resize(2);\n\n    thrust::device_vector<int> D=H;\n    D[0] = 99;\n    D[1] = 88;\n\n    for(int i=0; i < D.size(); i++)\n    {\n        std::cout << \"D[\" << i << \"] = \" << D[i] << std::endl;\n    }\n    return 0;\n}\n\n\n\n\n#include <thrust/host_vector.h>\n#include <thrust/device_vector.h>\n\n#include <thrust/copy.h>\n#include <thrust/fill.h>\n#include <thrust/sequence.h>\n\n#include <iostream>\n\nint main(void)\n{\n    //initial all ten integers of a device_vector to 1\n    thrust::device_vector<int> D(10,1);\n\n    //set the first seven elements of a vector to 9\n    thrust::fill(D.begin(), D.begin()+7, 9);\n\n    //initialize a host_vector with the first five elements of D\n    thrust::host_vector<int> H(D.begin(), D.begin() + 5);\n\n    //set the elements of H to 0, 1,2,3 ...\n    thrust::sequence (H.begin(), H.end());\n\n    //copy all of H back to the begining of D \n    thrust::copy(H.begin(), H.end(), D.begin());\n\n    for (int i=0, i< D.size(); i++)\n    {\n        std::cout << \"D[\" << i << \"] = \" << D[i] << std::endl;\n    }\n    return 0;\n}\n\n\n\nAlthough vector iterators are similar to pointers they carry more information with them. Notice that we did not have to tell \nthrust::fill\n that it was operating on a \ndevice_vector\n iterator. This information is captured in the type of the iterator returned by \nD.begin()\n which is different than the type returned by \nH.begin()\n. When a Thrust function is called.it inspects the type of the iterator to determine whether to use a host or a device implementation. This process is known as static dispatching since the host/device dispatch is resolved at compile time.Note this implies that there is no runtime overhead to the dispatch process.\n\n\nYou may wonder what happens when a \"raw\" pointer is used as an argument to a Thrust function .Like the STL, Thrust permits this usage and it will dispatch the host path of the algorithm. If the pointer in question is in fact a pointer to device memory then you'll need to wrap it with \nthrust::device_ptr\n before calling the function. For example\n\n\nsize_t N = 10;\n\n//raw pointer to device memory \nint * raw_ptr;\ncudaMalloc((void **) &raw_ptr, N*sizeof(int));\n\n//wrap raw pointer with a device_ptr\nthrust::device_ptr<int> dev_ptr(raw_ptr);\n\n//use device_ptr in thrust algorithms\nthrust::fill(dev_ptr, dev_ptr+N, (int)0);\n\n\n\nTo extract a raw pointer from a \ndevice_ptr\n the \nraw_pointer_cast\n should be applied as follows:\n\n\nsize_t N = 10;\n//create a device_ptr\nthrust::device_ptr<int> dev_ptr = thrust::device_malloc<int>(N);\n\n//extract raw pointer from device_ptr\nint *raw_ptr = thrust::raw_pointer_cast(dev_ptr);\n\n\n\nAnther reason to distinguish between iterators and pointers is that iterators can be used to traverse\u904d\u5386 many kinds of data structures. For example, the STL provides a linked list container (std::list) that provides bidirectional (but not random access) iterators. Although Thrust does not provide device implementations of such containers. It is compatible with them.\n\n\n#include <thrust/device_vector.h>\n#include <thrust/copy.h>\n#include <list>\n#include <vector>\n\nint main(void)\n{\n    //create an STL list with 4 values\n    std::list<int> stl_list;\n    stl_list.push_back(10);\n    stl_list.push_back(20);\n    stl_list.push_back(30);\n    stl_list.push_back(40);\n\n    //initialize a device_vector with the list\n    thrust::device_vector<int> D(stl_list.begin(),stl_list.end());\n\n    //copy a device_vector into a STL vector\n    std::vector<int> stl_vector(D.size());\n    thrust::copy(D.begin(),D.end(), stl_vector.begin());\n    return 0;\n}\n\n\n\n\nAlgorithm\n\n\nAll algorithms in Trust have implementations for both host and device.Specially, when a Thrust algorithm is invoked with a host iterator, then the host path is dispatched. Similarly, a device implementation is called when a device iterator is used to define a range.\n\n\nWith the exception of \nthrust::copy\n , which can copy data between host and device, all iterator arguments to a Thrust algorithm should live in the same place; either all on the host or all on the device. When this requirement is violated the compiler will produce an error message.\n\n\n#include <thrust/device_vector.h>\n#include <thrust/transform.h>\n#include <thrust/sequence.h>\n#include <thrust/copy.h>\n#include <thrust/fill.h>\n#include <thrust/replace.h>\n#include <thrust/functional.h>\n#include <iostream>\n\nint main(void)\n{\n    //allocate three device_vectors with 10 elements\n    thrust::device_vector<int> X(10);\n    thrust::device_vector<int> Y(10);\n    thrust::device_vector<int> Z(10);\n\n    //initialize X to 0,1,2,3 ...\n    thrust::sequence(X.begin(), X.end());\n\n    //compute Y=-X\n    thrust::transform(X.begin(), X.end(), Y.begin(), thrust::negate<int>());\n\n    //fill z with twos\n    thrust::fill(Z.begin(), Z.end(), 2);\n\n    //compute Y = X mod 2\n    thrust::transform(X.begin(), X.end(), Z.begin(), Y.begin(), thrust::modulus<int>() );\n\n    //replace all the ones in Y with tens\n    thrust::replace(Y.begin(), Y.end(), 1, 10);\n\n    //print Y\n    thrust::copy(Y.begin(), Y.end(), std::ostream_iterator<int> (std::cout, \"\\n\"));\n    return 0;\n\n\n\nFor example , conside the vector operation \ny<-a*x+y\n where \nx\n and \ny\n are vectors and \na\n is a scalar constant. This is the well known SAXPY operation provided by BLAS library.\n\n\nIf we want to implement SAXPY with Thrust we have a few options. The first is to use two transformations (one addition and one multiplication) and a temporary vector filled with the value \na\n. A better choice is to use a single transformation with a user-define functor that does exactly what we want. We illustrate both approachs in the source code bellow.\n\n\nstruct saxpy_functor\n{\n    const float a;\n    saxpy_functor(float _a):a(_a){}\n    __host__ __device__\n        float operator()(const float& x, const float& y) const {\n            return a*x +y;\n        }\n}\n\nvoid saxpy_fast(float A, thrust::device_vector<float> & X , thrust::device_vector<float> &Y)\n{\n    // Y <- A * X + Y\n    thrust::transform(X.begin(), X.end(), Y.begin(), Y.end(),saxpy_functor(A));\n}\n\nvoid saxpy_slow(float A, thrust::device_vector<float>& X, thrust::device_vector<float>& Y)\n{\n    thrust::device_vector<float> temp(X.size());\n\n    //temp <- A\n    thrust::fill(temp.begin(), temp.end(), A);\n\n    //temp <- A* X\n    thrust::transform(X.begin(), X.end(), temp.begin(), temp.begin(), thrust::multiplies<float> ());\n\n    //Y <- A*X +Y\n    thrust::transform(temp.begin(), temp.end(), Y.begin(), Y.begin(), thrust::plus<float>());\n\n}\n\n\n\nBoth \nsaxpy_fast\n and \nsaxpy_slow\n are valid SAXPY implementations, however \nsaxpy_fast\n will be significantly faster than \nsaxpy_slow\n. Ignoring the cost of allocating the temp vector and the arithmetic operations we have the following costs:\n\n\nsaxpy_fast\n: performs 2N reads and N writes.\n\nsaxpy_slow\n: performs 4N reads and 3N writes.\n\n\nSince SAXPY is memory bound(its performance is limited by memory bandwidth, not floating point performance) the larger number of reads and writes makes \nsaxpy_slow\n much more expensive. In contrast,\nsaxpy_fast\n will perform about as fast as SAXPY in an optimized BLAS implementation. In memory bound algorithms like SAXPY it is generally worthwhile to apply kernel fusion (combining multiple operations into a single kernel) to minimize the number of memory transactions.\n\n\ntrust::transform\n only supports transformations with one or two input arguments (eg. $f(x)\\rightarrow y\\quad and\\quad f(x,x)\\rightarrow y$). When a transformation uses more than two input arguments it is necessary to use a different approach. The \narbitrary_transformation\n example demonstrates a solution that uses \nthrust::zip_iterator\n and \nthrust::for_each\n.\n\n\nreduction\n\n\nA reduction algorithm uses a binary operation to reduce an input sequence to a single value. For example, the sum of an array of numbers is obtained by reducing the array with a plus operation. Similarly , the maximum of an array is obtained by reducing with an oeprator that takes two inputs and returns the maximum. The sum of an array is implemented with \nthrust::reduce\n as follows:\n\n\nint sum = thrust::reduce(D.begin(), D.end(), (int)0, thrust::plus<int>());\n\n\n\nThe first two arguments to \nreduce\n define the range of values while the third and fourth parameters provide the initial value and reduction operator respectively.Actually, this kind of reduction is so common that it is the default choice when no initial value or operator is provided. The following three lines are therefore equivalent:\n\n\nint sum = thrust::reduce(D.begin(), D.end(), (int)0, thrust::plus<int>());\nint sum = thrust::reduce(D.begin(), D.end(), (int)0);\nint sum = thrust::reduce(D.begin(), D.end());\n\n\n\nAlthough \nthrust::reduce\n is sufficient to implement a wide variety of reduction, Thrust provides a few additional functions for convenience (like STL). for example \nthrust::count\n returns the number of instances of a specific value in a given sequence:\n\n\n#include <thrust/count.h>\n#include <thrust/device_vector.h>\n...// put three 1s in a device_vector\n\nthrust::device_vector<int> vec(5,0);\nvec[1] = 1;\nvec[3] = 1;\nvec[4] = 1;\n\n//count the 1s\nint result = thrust::count(vec.begin(), vec.end(), 1);\n//result is 3 \n\n\n\nother reduction operations include \nthrust::count_if\n,\nthrust::min_element\n,\nthrust::max_element\n, \nthrust::is_sorted\n, \nthrust::inner_product\n and several others. \n\n\nThe SAXPY example in the Transformations section showed how \nkernel fusion\n can be used to reduce the number of memory transfers used by a transformation kernel. With \nthrust::transform_reduce\n we can also apply kernel fusion to reduction kernels. Consider the following example which computes the norm of a vector.\n\n\n#include <thrust/transform_reduce.h>\n#include <thrust/functional.h>\n#include <thrust/device_vector.h>\n#include <thrust/host_vector.h>\n#include <cmath>\n\n// square<T> computes the square of a number f(x) -> x*x\ntemplate <typename T>\nstruct square\n{\n    __host__ __device__\n        T operator()(const T& x)const{\n            return x*x;\n        }\n}\nint main(void)\n{\n    //initialize host array\n    float x[4] = {1.0, 2.0, 3.0, 4.0};\n\n    //transfer to device\n    thrust::device_vector<float> d_x(x, x+4);\n\n    //setup arguments\n    square<float> unary_op;\n    thrust::plus<float> binary_op;\n    float init = 0;\n    //compute norm\n    float norm = std::sqrt(thrust::transform_reduce(d_x.begin(), d_x.end(), unary_op, init, binary_op));\n\n    std::cout << norm << std::endl;\n    return 0;\n}\n\n\n\n\nHere we have a unary operator called \nsquare\n that squares each element of the input sequence. The sum of squares is then computed using a standard \nplus\n reduction. Like the slower version of SAXPY transformation.we could implement \nnorm\n with multiple passes: first a \ntransform\n using \nsquare\n or perhaps just \nmultiples\n and then a \nplus\n reduction over a temporary array . However this would be unnecessarily wasteful and considerably slower. By fusing the square operation with the reduction kernel we again have a highly optimized implementation which offers the same performance as hand-written kernels.\n\n\nPrefix-Sums\n\n\nParrallel prefix-sums, or scan operations, are important building blocks in many parallel algorithms such as stream compaction and radix sort. Consider the following source code which illustrates an inclusive scan operation using the default \nplus\n oeprator.\n\n\n#include <thrust/scan.h>\n\nint data[6]  = {1, 0, 2, 2, 1, 3};\nthrust::inclusive_scan(data, data+6, data); //in-place scan\n//data is now { 1, 1, 3, 5, 6, 9}\n\n\n\nIn an inclusive scan each element of the output is the corresponding \npartial sum\n of the input range. For example , $data[2] = data[0] + data[1] + data[2]$ . An exclusive scan is similar, but shifted by one place to the right.\n\n\n#include <thrust/scan.h>\nint data[6] = {1, 0, 2, 2, 1, 3};\nthrust::exclusive_scan(data, data+6, data); //in-place scan\n//data is now {0, 1, 1, 3, 5, 6}\n\n\n\nso now $data[2] = data[1] + data[2]$. As these examples show \uff0c\ninclusive_scan\n and \nexclusive_scan\n are permitted to be performed in-place. Thrust also provides the functions \ntransform_inclusive_scan\n and \ntransform_exclusive_scan\n which apply a unary function to the input sequence before performing the scan. \n\n\nReodering\n\n\nThrust provides support for partitioning\u5206\u533a and stream compaction\u6d41\u538b\u7f29 through the following algorithms.\n\n\ncopy_if\n copy elements that pass a predicate test\n\n\npartition\n reorder elements according to predicate (true value precede false value)\n\n\nremove\n and \nremove_if\n: remove elements that fail a predicate test\n\n\nunique\n : remove consecutive duplicates within a sequence\n\n\nSorting\n\n\nThrust offers several functions to sort data or rearrange data according to a given criterion\u6807\u51c6. The \nthrust::sort\n and \nthrust::stable_sort\n functions are direct analogs of \nsort\n and \nstable_sort\n in STL.\n\n\n#include <thrust/sort.h>\n...\nconst int N=6;\nint A[N] = {1, 4, 2, 8, 5, 7};\n\nthrust::sort{A, A+N}\n//A is now {1, 2, 4, 5, 7, 8}\n\n\n\nIn addition, Thrust provides \nthrust::sort_by_key\n and \nthrust::stable_sort_by_key\n , which sort key-value pairs stored in separate places.\n\n\n#include <thrust/sort.h>\n...\nconst int N = 6;\nint keys[N] = {1, 4, 2, 8, 5, 7};\nchar values[N] = {'a', 'b', 'c', 'd', 'e', 'f'}\nthrust::sort_by_key(keys, keys+N, values);\n//keys is now {1, 2, 4, 5, 7, 8}\n//value is now {'a', 'c', 'b', 'e', 'f', 'd'}\n\n\n\nFancy Iterators\n\n\nFancy iterators perform a variety of valuabe purposes.\n\n\nConstant iterator\n\n\nArguably the simplest of the bunch, \nconstant_iterator\n is simply an iterator that returns the same value whenever we access it. In the following example we initialize a constant iterator with the value 10.\n\n\n#include <thrust/iterator/constant_iterator.h>\n...\n//create iterators\nthrust::constant_iterator<int> first(10);\nthrust::constant_iterator<int> last = first+3;\n\nfirst[0]; //returns 10\nfirst[1]; //returns 10\nfirst[100]; //returns 10\n\n//sum of [first, last)\nthrust::reduce(first, last); //return 30(i.e. 3*10)\n\n\n\n\nCounting iterator\n\n\nIf a sequence of increasing value is required. the \ncounting_iterator\n is the appropriate choice. Here we initialize a \ncounting_iterator\n with the value 10 and access it like an array.\n\n\n#include <thrust/iterator/counting_iterator.h>\n...\n// create iterators\nthrust::counting_iterator<int> first(10);\nthrust::counting_iterator<int> last = first + 3;\n\nfirst[0]   // returns 10\nfirst[1]   // returns 11\nfirst[100] // returns 110\n\n// sum of [first, last)\nthrust::reduce(first, last);   // returns 33 (i.e. 10 + 11 + 12)\n\n\n\nwhile \nconstant_iterator\n and \ncounting_iterator\n act as array, they don't actually require any memory storage.Whenever we dereference one of these iterators it generates the appropriate value on-the-fly and returns it to the calling function.\n\n\ntransform_iterator\n\n\n#include <thrust/iterator/transform_iterator.h>\n// initialize vector\nthrust::device_vector<int> vec(3);\nvec[0] = 10; vec[1] = 20; vec[2] = 30;\n\n// create iterator (type omitted)\n...\nfirst = thrust::make_transform_iterator(vec.begin(), negate<int>());\n...\nlast  = thrust::make_transform_iterator(vec.end(),   negate<int>());\n\nfirst[0]   // returns -10\nfirst[1]   // returns -20\nfirst[2]   // returns -30\n\n// sum of [first, last)\nthrust::reduce(first, last);   // returns -60 (i.e. -10 + -20 + -30)\n\n\n\navoid creating a variable to store \nfirst\n and \nlast\n\n\n// sum of [first, last)\nthrust::reduce(thrust::make_transform_iterator(vec.begin(), negate<int>()),\n               thrust::make_transform_iterator(vec.end(),   negate<int>()));\n\n\n\npermutation_iterator\n\n\npermutation_iterator\n is similar: it allows us to fuse gather and scatter operations with Thrust algorithms, or even other fancy iterators. The following example shows how to fuse a gather operation with a reduction.\n\n\n#include <thrust/iterator/permutation_iterator.h>\n\n...\n\n// gather locations\nthrust::device_vector<int> map(4);\nmap[0] = 3;\nmap[1] = 1;\nmap[2] = 0;\nmap[3] = 5;\n\n// array to gather from\nthrust::device_vector<int> source(6);\nsource[0] = 10;\nsource[1] = 20;\nsource[2] = 30;\nsource[3] = 40;\nsource[4] = 50;\nsource[5] = 60;\n\n// fuse gather with reduction: \n//   sum = source[map[0]] + source[map[1]] + ...\nint sum = thrust::reduce(thrust::make_permutation_iterator(source.begin(), map.begin()),\n                         thrust::make_permutation_iterator(source.begin(), map.end()));\n\n\n\nHere we have used the \nmake_permutation_iterator\n function to simplify the construction of the \npermutation_iterators\n. The first argument to \nmake_permutation_iterator\n is the source array of the gather operation and the second is the list of map indices. Note that we pass in \nsource.begin()\n for the first argument in both cases, but vary the second argument to define the beginning and end of the sequence.\n\n\nzip_iterator\n\n\nKeep reading, we\u2019ve saved the best iterator for last! The \nzip_iterator\n is an extremely useful gadget: it takes multiple input sequences and yields a sequence of tuples. In this example we \u201czip\u201d together a sequence of int and a sequence of char into a sequence of \ntuple<int,char>\n and compute the tuple with the maximum value.\n\n\n#include <thrust/iterator/zip_iterator.h>\n...\n// initialize vectors\nthrust::device_vector<int>  A(3);\nthrust::device_vector<char> B(3);\nA[0] = 10;  A[1] = 20;  A[2] = 30;\nB[0] = 'x'; B[1] = 'y'; B[2] = 'z';\n\n// create iterator (type omitted)\nfirst = thrust::make_zip_iterator(thrust::make_tuple(A.begin(), B.begin()));\nlast  = thrust::make_zip_iterator(thrust::make_tuple(A.end(),   B.end()));\n\nfirst[0]   // returns tuple(10, 'x')\nfirst[1]   // returns tuple(20, 'y')\nfirst[2]   // returns tuple(30, 'z')\n\n// maximum of [first, last)\nthrust::maximum< tuple<int,char> > binary_op;\nthrust::tuple<int,char> init = first[0];\nthrust::reduce(first, last, init, binary_op); // returns tuple(30, 'z')\n\n\n\nWhat makes \nzip_iterator\n so useful is that most algorithms accept either one, or occasionally two, input sequences. The \nzip_iterator\n allows us to combine many independent sequences into a single sequence of tuples, which can be processed by a broad set of algorithms.\n\n\nRefer to the \narbitrary_transformation\n example to see how to implement a ternary transformation with \nzip_iterator\n and \nfor_each\n. A simple extension of this example would allow you to compute transformations with multiple output sequences as well.\n\n\nIn addition to convenience, \nzip_iterator\n allows us to implement programs more efficiently. For example, storing 3d points as an array of \nfloat3\n in CUDA is generally a bad idea, since array accesses are not properly coalesced. With \nzip_iterator\n we can store the three coordinates in three separate arrays, which does permit coalesced memory access. In this case, we use \nzip_iterator\n to create a virtual array of 3d vectors which we can feed in to Thrust algorithms. Refer to the \ndot_products_with_zip\n example for additional details.\n\n\n__shared__\n\n\n__shared__ int local_offset[BLOCK_SIZE_X];\n\n\n\nThink of shared memory as an explicitly managed cache - it's only useful if you need to access data more than once.either within the same thread or from different threads within the same block. If you're only accessing data once then shared memory isn't going to help you.",
            "title": "PCL"
        },
        {
            "location": "/PCL/#pca",
            "text": "Use the PCA principal component analysis method to obtain the three main directions of the point cloud, obtain the centroid, calculate the covariance\u534f\u65b9\u5dee, obtain the coveriance matrix, and obtain the eigenvalues and feature vectors of the covariance matrix. The eigenvector is the main direction.   Eigen::vector4f pcaCentroid;\npcl::compute3DCentroid(*cloud, pcaCentroid);\nEigen::Matrix3f covariance;\npcl::computeCovarianceMatrixNormalized(*cloud, pcaCentroid, covariance);\nEigen::SelfAdjointEigenSolver<Eigen::Matrix3f> eigen_solver(covariance, Eigen::ComputeEigenVectors);\nEigen::Matrix3f eigenVectorsPCA = eigen_solver.eigenvectors();\nEigen::Vector3f eigenValuesPCA = eigen_solver.eigenvalues();\neigenVectorsPCA.col(2) = eigenVectorsPCA.col(0).cross(eigenVectorsPCA.col(1));\neigenVectorsPCA.col(0) = eigenVectorsPCA.col(1).cross(eigenVectorsPCA.col(2));\neigenVectorsPCA.col(1) = eigenVectorsPCA.col(2).cross(eigenVectorsPCA.col(0));    Using the main direction and centroid obtained in 1.convert the input point cloud to the origin.and the main direction and the coordinate system direction return to establish the bounding box of the point cloud transformed to the origin.    Set the main direction and bounding box for the input point cloud through the inverse transformation of the input point cloud to the original point cloud.    Complete code     #include <vtkAutoInit.h>\nVTK_MODULE_INIT(vtkRenderingOpenGL);\nVTK_MODULE_INIT(vtkInteractionStyle);\nVTK_MODULE_INIT(vtkRenderingFreeType);\n#include <iostream>\n#include <string>\n#include <pcl/io/pcd_io.h>\n#include <pcl/point_cloud.h>\n#include <pcl/point_types.h>\n#include <Eigen/Core>\n#include <pcl/common/transforms.h>\n#include <pcl/common/common.h>\n#include <pcl/visualization/pcl_visualizer.h>\n\nusing namespace std;\ntypedef pcl::PointXYZ PointType;\nint main(int argc, char **argv)\n{\n    pcl::PointCloud<PointType>::Ptr cloud(new pcl::PointCloud<PointType>());\n    std::cout << \"Please enter the point cloud file name to be displayed.\";\n    std::string fileName(\"rabbit\");\n    getline(cin, fileName);\n    fileName += \".pcd\";\n    pcl::io::loadPCDFile(fileName, *cloud);\n    Eigen::Vector4f pcaCentroid;\n    pcl::compute3DCentroid(*cloud, pcaCentroid);\n    Eigen::Matrix3f covariance;\n    pcl::computeCovarianceMatrixNormalized(*cloud, pcaCentroid,covariance);\n    Eigen::SelfAdjointEigenSolver<Eigen::Matrix3f> eigen_solver (covariance, Eigen::ComputeEigenvectors);\n    Eigen::Matrix3f eigenVectorsPCA = eigen_solver.eigenvectors();\n    Eigen::Vector3f eigenValuesPCA = eigen_sovler.eigenvalues();\n    eigenVectorsPCA.col(2) = eigenVectorsPCA.col(0).cross(eigenVectorsPCA.col(1));\n    eigenVectorsPCA.col(0) = eigenVectorsPCA.col(1).cross(eigenVectorsPCA.col(2));\n    eigenVectorsPCA.col(1) = eigenVectorsPCA.col(2).cross(eigenVectorsPCA.col(0));\n\n    std::cout << \"Eigenvalue va(3x1) :\\n\" << eigenValuesPCA << std::endl;\n    std::cout << \"Feature vector ve(3x3) :\\n\" << eigenVectorsPCA << std::endl;\n    std::cout << \"centroid point (4x1) :\\n\" << pcaCentroid << std::endl;\n\n    /** Another way to calculate the eigenvalues and eigenvectors of the point cloud covariance matrix: through the pca interface in PCL as follows\n     * pcl::PointCloud<pcl::PointXYZ>::Ptr cloudPCAprojection (new pcl::PointCloud<pcl::PointXYZ>);\n     * pcl::PCA<pcl::PointXYZ> pca;\n     * pca.setInputCloud(cloudSegmented);\n     * pca.project(*cloudSegmented, *cloudPCAprojection);\n     * std::cerr << std::endl << \"EigenVectors :\" << pca.getEigenVectors() << std::endl;\n     * std::cerr << std::endl << \"EigenValues :\" << pca.getEigenValues() <<  std::endl;\n     **/\n\n    Eigen::Matrix4f tm = Eigen::Matrix4f::Identity();\n    Eigen::Matrix4f tm_inv = Eigen::Matrix4f::Identity();\n    tm.block<3,3>(0,0) = eigenVectorsPCA.transpose(); //R\n    tm.block<3,1>(0,3) = -0.1f * (eigenVectorsPCA.transpose())*(pcaCentroid.head<3>()); //-R*t\n    tm_inv = tm.inverse();\n    std::cout << \"Transformation matrix tm(4x4): \\n \" << tm << std::endl;\n    std::cout << \"inverter matrix tm'(4x4): \\n\" << tm_inv << std::endl;\n    pcl::PointCloud<PointType>::Ptr transformedCloud(new pcl::PointCloud<PointType>);\n    pcl::transformPointCloud(*cloud, *transformedCloud, tm);\n    PointType min_p1, max_p1;\n    Eigen::Vector3f c1, c;\n    pcl::getMinMax3D (*transformedCloud, min_p1, max_p1);\n    c1 = 0.5f *(min_p1.getVector3fMap() + max_p1.getVector3fMap());\n    std::cout << \"Centre c1(3x1):\\n \" << c1 << std::endl;\n\n    Eigen::Affine3f tm_inv_aff(tm_inv);\n    pcl::transformPoint (c1, c, tm_inv_aff);\n\n    Eigen::vector3f whd, whd1;\n    whd1 = max_p1.getVector3fMap() - min_p1.getVector3fMap();\n    whd = whd1;\n    float sc1 = (whd1(0) + whd1(1) + whd1(2))/3; //the average scale of the point cloud, used to set the size of the main direction arrow\n\n    std::cout << \"width1= \" << whd1(0) << std::endl;\n    std::cout << \"heigth1 = \" << whd1(1) << std::endl;\n    std::cout << \"depth1 = \" << whd1(2) << std::endl;\n    std::cout << \"scale1 = \" << sc1 << std::endl;\n\n    const Eigen::Quaternionf bboxQ1(Eigen::Quaternionf::Identity());\n    const Eigen::Vector3f bboxT1(c1);\n\n    const Eigen::Quaternionf bboxQ(tm_inv.block<3,3>(0,0));\n    const Eigen::Vector3f bboxT(c);\n\n    //the main direction of the point cloud transformed to the origin\n    PointType op;\n    op.x = 0.0;\n    op.y = 0.0;\n    op.z = 0.0;\n    Eigen::Vector3f px,py,pz;\n    Eigen::Affine3f tm_aff(tm);\n    pcl::transformVector(eigenVectorsPCA.col(0), px, tm_aff);\n    pcl::transformVector(eigenVectorsPCA.col(1), py, tm_aff);\n    pcl::transformVector(eigenVectorsPCA.col(2), pz, tm_aff);\n    PointType pcaX;\n    pcaX.x = sc1*px(0);\n    pcaX.y = sc1*px(1);\n    pcaX.z = sc1*px(2);\n    PointType pcaY;\n    pcaY.x = sc1*py(0);\n    pcaY.y = sc1*py(1);\n    pcaY.z = sc1*py(2);\n    PointType pcaZ;\n    pcaZ.x = sc1*pz(0);\n    pcaZ.y = sc1*pz(1);\n    pcaZ.z = sc1*pz(2);\n\n    //the main directio of the point cloud transformed to the origin\n\n    PointType cp;\n    cp.x = pcaCentroid(0);\n    cp.y = pcaCentroid(1);\n    cp.z = pcaCentroid(2);\n    PointType pcX;\n    pcX.x = sc1 * eigenVectorsPCA(0,0) + cp.x;\n    pcX.y = sc1 * eigenVectorsPCA(1,0) + cp.y;\n    pcX.z = sc1 * eigenVectorsPCA(2,0) + cp.z;\n    PointType pcY;\n    pcY.x = sc1* eigenVectorsPCA(0,1) + cp.x;\n    pcY.y = sc1* eigenVectorsPCA(1,1) + cp.y;\n    pcY.z = sc1* eigenVectorsPCA(2,1) + cp.z;\n    PointType pcZ;\n    pcZ.x = sc1* eigenVectorsPCA(0,2) + cp.x;\n    pcZ.y = sc1* eigenVectorsPCA(1,2) + cp.y;\n    pcZ.z = sc1* eigenVectorsPCA(2,2) + cp.z;\n\n    //pcl visualization\n    pcl::visualization::PCLVisualizer viewer;\n    pcl::visualization::PointCloudColorHandlerCustom<PointType> tc_handler(transformedCloud,0,255,0);\n\n    viewer.addPointCloud(transformedCloud, tc_handler, \"transformedCloud\");\n    viewer.addCube(bboxT1, bboxQ1, whd1(0), whd1(1), whd1(2), \"bbox1\");\n    viewer.setShapeRenderingProperties(pcl::visualization::PCL_VISUALIZER_REPRESENTATION, pcl::visualization::PCL_VISUALIZER_REPRESENTATION_WIREFRAME, \"bbox1\");\n    viewer.setShapeRenderingProperties(pcl::visualization::PCL_VISUALIZER_COLOR, 0.0, 1.0, 0.0, \"bbox1\");\n\n    viewer.addArrow(pcaX, op, 1.0, 0.0, 0.0, false, \"arrow_X\");\n    viewer.addArrow(pcaY, op, 0.0, 1.0, 0.0, false, \"arrow_Y\");\n    viewer.addArrow(pcaZ, op, 0.0, 0.0, 1.0, false, \"arrow_Z\");\n\n    pcl::visualization::PointCloudColorHandlerCustom<PointType> color_handler(cloud, 255,0,0);\n    viewer.addPointCloud(cloud, color_handler, \"cloud\");\n    viewer.addCube(bboxT, bboxQ, whd(0), whd(1), whd(2), \"bbox\");\n\n    viewer.setShapeRenderingProperties(pcl::visualization::PCL_VISUALIZER_REPRESENTATION,pcl::visualization::PCL_VISUALIZER_REPRESENTATION_WIREFRAME, \"bbox\");\n    viewer.setShapeRenderingProperties(pcl::visualization::PCL_VISUALIZER_COLOR, 1.0, 0.0, 0.0);\n\n    viewer.addArrow(pcX, cp ,1.0, 0.0, 0.0, false, \"arrow_x\");\n    viewer.addArrow(pcY, cp, 0.0, 1.0, 0.0, false, \"arrow_y\");\n    viewer.addArrow(pcZ, cp, 0.0, 0.0, 1.0, false, \"arrow_z\");\n    viewer.addCoordinateSystem(0.5f * sc1);\n    viewer.setBackgroundColor(1.0, 1.0, 1.0);\n    while(!viewer.wasStopped())\n    {\n        viewer.spinOnce(100);\n    }\n    return 0;\n}",
            "title": "PCA"
        },
        {
            "location": "/PCL/#normal-estimation-set-number-of-threads",
            "text": "",
            "title": "Normal estimation set number of threads"
        },
        {
            "location": "/PCL/#fpfh-setnumberofthreads",
            "text": "normal_estimation.setNumberOfThreads(in_ompnum_threads);\n...\nfpfh.setNumberOfThreads(in_ompnum_threads);  CMakeLists.txt  find_package(OpenMP)\nif(OPENMP_FOUND)\n  set_target_properties(lidar_euclidean_cluster_detect PROPERTIES\n    COMPILE_FLAGS ${OpenMP_CXX_FLAGS}\n    LINK_FLAGS ${OpenMP_CXX_FLAGS}\n  )\nendif()  \u624d\u53ef\u542f\u52a8\u591a\u7ebf\u7a0b",
            "title": "fpfh setNumberOfThreads"
        },
        {
            "location": "/PCL/#vector",
            "text": "std::vector<float> cluster_fpfh_histogram(33, 0.0);",
            "title": "\u5b9a\u4e49Vector \u5e76\u521d\u59cb\u5316"
        },
        {
            "location": "/PCL/#thrust",
            "text": "thrust provides such a rich collection of data parallel primitives such as scan, sort, and reduce,which can be composed together to implent complex alogrithms with concise\u7b80\u6d01 readable source code.By describing your computation in terms of these high-level abstractions you provide.Thrust with the freedom to select the most efficient implementation automatically. As a result, thrust can be utilized in rapid prototyping of cuda applications, where programmer productivity matters most, as well as in production, where robustness and absolute performance are crucial.",
            "title": "thrust"
        },
        {
            "location": "/PCL/#vectors",
            "text": "Thrust provides two vector containers,  host_vector  and  device_vector . As the names suggest,  host_vector  is stored in host memory while  device_vector  lives in GPU device memory. Thrust's vector containers are just like  std::vector  ,  host_vector  are generic containers( able to store any data type) that can be resized dynamically.The following source code illustrates the use of Thrust's vector containers.  #include <thrust/host_vector.h>\n#include <thrust/device_vector.h>\n#include <iostream>\n\nint main(void)\n{\n    //H has storage for 4 integers\n    thrust::host_vector<int> H(4);\n\n    //initialize individual elements\n    H[0] = 14;\n    H[1] = 20;\n    H[2] = 38;\n    H[3] = 46;\n\n    std::cout << \"H has size \" << H.size() << std::endl;\n\n    for (int i=0; i< H.size();i++)\n    {\n        std::cout << \"[\" << i <<\"] = \" << H[i] << endl;\n    }\n\n    H.resize(2);\n\n    thrust::device_vector<int> D=H;\n    D[0] = 99;\n    D[1] = 88;\n\n    for(int i=0; i < D.size(); i++)\n    {\n        std::cout << \"D[\" << i << \"] = \" << D[i] << std::endl;\n    }\n    return 0;\n}  #include <thrust/host_vector.h>\n#include <thrust/device_vector.h>\n\n#include <thrust/copy.h>\n#include <thrust/fill.h>\n#include <thrust/sequence.h>\n\n#include <iostream>\n\nint main(void)\n{\n    //initial all ten integers of a device_vector to 1\n    thrust::device_vector<int> D(10,1);\n\n    //set the first seven elements of a vector to 9\n    thrust::fill(D.begin(), D.begin()+7, 9);\n\n    //initialize a host_vector with the first five elements of D\n    thrust::host_vector<int> H(D.begin(), D.begin() + 5);\n\n    //set the elements of H to 0, 1,2,3 ...\n    thrust::sequence (H.begin(), H.end());\n\n    //copy all of H back to the begining of D \n    thrust::copy(H.begin(), H.end(), D.begin());\n\n    for (int i=0, i< D.size(); i++)\n    {\n        std::cout << \"D[\" << i << \"] = \" << D[i] << std::endl;\n    }\n    return 0;\n}  Although vector iterators are similar to pointers they carry more information with them. Notice that we did not have to tell  thrust::fill  that it was operating on a  device_vector  iterator. This information is captured in the type of the iterator returned by  D.begin()  which is different than the type returned by  H.begin() . When a Thrust function is called.it inspects the type of the iterator to determine whether to use a host or a device implementation. This process is known as static dispatching since the host/device dispatch is resolved at compile time.Note this implies that there is no runtime overhead to the dispatch process.  You may wonder what happens when a \"raw\" pointer is used as an argument to a Thrust function .Like the STL, Thrust permits this usage and it will dispatch the host path of the algorithm. If the pointer in question is in fact a pointer to device memory then you'll need to wrap it with  thrust::device_ptr  before calling the function. For example  size_t N = 10;\n\n//raw pointer to device memory \nint * raw_ptr;\ncudaMalloc((void **) &raw_ptr, N*sizeof(int));\n\n//wrap raw pointer with a device_ptr\nthrust::device_ptr<int> dev_ptr(raw_ptr);\n\n//use device_ptr in thrust algorithms\nthrust::fill(dev_ptr, dev_ptr+N, (int)0);  To extract a raw pointer from a  device_ptr  the  raw_pointer_cast  should be applied as follows:  size_t N = 10;\n//create a device_ptr\nthrust::device_ptr<int> dev_ptr = thrust::device_malloc<int>(N);\n\n//extract raw pointer from device_ptr\nint *raw_ptr = thrust::raw_pointer_cast(dev_ptr);  Anther reason to distinguish between iterators and pointers is that iterators can be used to traverse\u904d\u5386 many kinds of data structures. For example, the STL provides a linked list container (std::list) that provides bidirectional (but not random access) iterators. Although Thrust does not provide device implementations of such containers. It is compatible with them.  #include <thrust/device_vector.h>\n#include <thrust/copy.h>\n#include <list>\n#include <vector>\n\nint main(void)\n{\n    //create an STL list with 4 values\n    std::list<int> stl_list;\n    stl_list.push_back(10);\n    stl_list.push_back(20);\n    stl_list.push_back(30);\n    stl_list.push_back(40);\n\n    //initialize a device_vector with the list\n    thrust::device_vector<int> D(stl_list.begin(),stl_list.end());\n\n    //copy a device_vector into a STL vector\n    std::vector<int> stl_vector(D.size());\n    thrust::copy(D.begin(),D.end(), stl_vector.begin());\n    return 0;\n}",
            "title": "Vectors"
        },
        {
            "location": "/PCL/#algorithm",
            "text": "All algorithms in Trust have implementations for both host and device.Specially, when a Thrust algorithm is invoked with a host iterator, then the host path is dispatched. Similarly, a device implementation is called when a device iterator is used to define a range.  With the exception of  thrust::copy  , which can copy data between host and device, all iterator arguments to a Thrust algorithm should live in the same place; either all on the host or all on the device. When this requirement is violated the compiler will produce an error message.  #include <thrust/device_vector.h>\n#include <thrust/transform.h>\n#include <thrust/sequence.h>\n#include <thrust/copy.h>\n#include <thrust/fill.h>\n#include <thrust/replace.h>\n#include <thrust/functional.h>\n#include <iostream>\n\nint main(void)\n{\n    //allocate three device_vectors with 10 elements\n    thrust::device_vector<int> X(10);\n    thrust::device_vector<int> Y(10);\n    thrust::device_vector<int> Z(10);\n\n    //initialize X to 0,1,2,3 ...\n    thrust::sequence(X.begin(), X.end());\n\n    //compute Y=-X\n    thrust::transform(X.begin(), X.end(), Y.begin(), thrust::negate<int>());\n\n    //fill z with twos\n    thrust::fill(Z.begin(), Z.end(), 2);\n\n    //compute Y = X mod 2\n    thrust::transform(X.begin(), X.end(), Z.begin(), Y.begin(), thrust::modulus<int>() );\n\n    //replace all the ones in Y with tens\n    thrust::replace(Y.begin(), Y.end(), 1, 10);\n\n    //print Y\n    thrust::copy(Y.begin(), Y.end(), std::ostream_iterator<int> (std::cout, \"\\n\"));\n    return 0;  For example , conside the vector operation  y<-a*x+y  where  x  and  y  are vectors and  a  is a scalar constant. This is the well known SAXPY operation provided by BLAS library.  If we want to implement SAXPY with Thrust we have a few options. The first is to use two transformations (one addition and one multiplication) and a temporary vector filled with the value  a . A better choice is to use a single transformation with a user-define functor that does exactly what we want. We illustrate both approachs in the source code bellow.  struct saxpy_functor\n{\n    const float a;\n    saxpy_functor(float _a):a(_a){}\n    __host__ __device__\n        float operator()(const float& x, const float& y) const {\n            return a*x +y;\n        }\n}\n\nvoid saxpy_fast(float A, thrust::device_vector<float> & X , thrust::device_vector<float> &Y)\n{\n    // Y <- A * X + Y\n    thrust::transform(X.begin(), X.end(), Y.begin(), Y.end(),saxpy_functor(A));\n}\n\nvoid saxpy_slow(float A, thrust::device_vector<float>& X, thrust::device_vector<float>& Y)\n{\n    thrust::device_vector<float> temp(X.size());\n\n    //temp <- A\n    thrust::fill(temp.begin(), temp.end(), A);\n\n    //temp <- A* X\n    thrust::transform(X.begin(), X.end(), temp.begin(), temp.begin(), thrust::multiplies<float> ());\n\n    //Y <- A*X +Y\n    thrust::transform(temp.begin(), temp.end(), Y.begin(), Y.begin(), thrust::plus<float>());\n\n}  Both  saxpy_fast  and  saxpy_slow  are valid SAXPY implementations, however  saxpy_fast  will be significantly faster than  saxpy_slow . Ignoring the cost of allocating the temp vector and the arithmetic operations we have the following costs:  saxpy_fast : performs 2N reads and N writes. saxpy_slow : performs 4N reads and 3N writes.  Since SAXPY is memory bound(its performance is limited by memory bandwidth, not floating point performance) the larger number of reads and writes makes  saxpy_slow  much more expensive. In contrast, saxpy_fast  will perform about as fast as SAXPY in an optimized BLAS implementation. In memory bound algorithms like SAXPY it is generally worthwhile to apply kernel fusion (combining multiple operations into a single kernel) to minimize the number of memory transactions.  trust::transform  only supports transformations with one or two input arguments (eg. $f(x)\\rightarrow y\\quad and\\quad f(x,x)\\rightarrow y$). When a transformation uses more than two input arguments it is necessary to use a different approach. The  arbitrary_transformation  example demonstrates a solution that uses  thrust::zip_iterator  and  thrust::for_each .",
            "title": "Algorithm"
        },
        {
            "location": "/PCL/#reduction",
            "text": "A reduction algorithm uses a binary operation to reduce an input sequence to a single value. For example, the sum of an array of numbers is obtained by reducing the array with a plus operation. Similarly , the maximum of an array is obtained by reducing with an oeprator that takes two inputs and returns the maximum. The sum of an array is implemented with  thrust::reduce  as follows:  int sum = thrust::reduce(D.begin(), D.end(), (int)0, thrust::plus<int>());  The first two arguments to  reduce  define the range of values while the third and fourth parameters provide the initial value and reduction operator respectively.Actually, this kind of reduction is so common that it is the default choice when no initial value or operator is provided. The following three lines are therefore equivalent:  int sum = thrust::reduce(D.begin(), D.end(), (int)0, thrust::plus<int>());\nint sum = thrust::reduce(D.begin(), D.end(), (int)0);\nint sum = thrust::reduce(D.begin(), D.end());  Although  thrust::reduce  is sufficient to implement a wide variety of reduction, Thrust provides a few additional functions for convenience (like STL). for example  thrust::count  returns the number of instances of a specific value in a given sequence:  #include <thrust/count.h>\n#include <thrust/device_vector.h>\n...// put three 1s in a device_vector\n\nthrust::device_vector<int> vec(5,0);\nvec[1] = 1;\nvec[3] = 1;\nvec[4] = 1;\n\n//count the 1s\nint result = thrust::count(vec.begin(), vec.end(), 1);\n//result is 3   other reduction operations include  thrust::count_if , thrust::min_element , thrust::max_element ,  thrust::is_sorted ,  thrust::inner_product  and several others.   The SAXPY example in the Transformations section showed how  kernel fusion  can be used to reduce the number of memory transfers used by a transformation kernel. With  thrust::transform_reduce  we can also apply kernel fusion to reduction kernels. Consider the following example which computes the norm of a vector.  #include <thrust/transform_reduce.h>\n#include <thrust/functional.h>\n#include <thrust/device_vector.h>\n#include <thrust/host_vector.h>\n#include <cmath>\n\n// square<T> computes the square of a number f(x) -> x*x\ntemplate <typename T>\nstruct square\n{\n    __host__ __device__\n        T operator()(const T& x)const{\n            return x*x;\n        }\n}\nint main(void)\n{\n    //initialize host array\n    float x[4] = {1.0, 2.0, 3.0, 4.0};\n\n    //transfer to device\n    thrust::device_vector<float> d_x(x, x+4);\n\n    //setup arguments\n    square<float> unary_op;\n    thrust::plus<float> binary_op;\n    float init = 0;\n    //compute norm\n    float norm = std::sqrt(thrust::transform_reduce(d_x.begin(), d_x.end(), unary_op, init, binary_op));\n\n    std::cout << norm << std::endl;\n    return 0;\n}  Here we have a unary operator called  square  that squares each element of the input sequence. The sum of squares is then computed using a standard  plus  reduction. Like the slower version of SAXPY transformation.we could implement  norm  with multiple passes: first a  transform  using  square  or perhaps just  multiples  and then a  plus  reduction over a temporary array . However this would be unnecessarily wasteful and considerably slower. By fusing the square operation with the reduction kernel we again have a highly optimized implementation which offers the same performance as hand-written kernels.",
            "title": "reduction"
        },
        {
            "location": "/PCL/#prefix-sums",
            "text": "Parrallel prefix-sums, or scan operations, are important building blocks in many parallel algorithms such as stream compaction and radix sort. Consider the following source code which illustrates an inclusive scan operation using the default  plus  oeprator.  #include <thrust/scan.h>\n\nint data[6]  = {1, 0, 2, 2, 1, 3};\nthrust::inclusive_scan(data, data+6, data); //in-place scan\n//data is now { 1, 1, 3, 5, 6, 9}  In an inclusive scan each element of the output is the corresponding  partial sum  of the input range. For example , $data[2] = data[0] + data[1] + data[2]$ . An exclusive scan is similar, but shifted by one place to the right.  #include <thrust/scan.h>\nint data[6] = {1, 0, 2, 2, 1, 3};\nthrust::exclusive_scan(data, data+6, data); //in-place scan\n//data is now {0, 1, 1, 3, 5, 6}  so now $data[2] = data[1] + data[2]$. As these examples show \uff0c inclusive_scan  and  exclusive_scan  are permitted to be performed in-place. Thrust also provides the functions  transform_inclusive_scan  and  transform_exclusive_scan  which apply a unary function to the input sequence before performing the scan.",
            "title": "Prefix-Sums"
        },
        {
            "location": "/PCL/#reodering",
            "text": "Thrust provides support for partitioning\u5206\u533a and stream compaction\u6d41\u538b\u7f29 through the following algorithms.  copy_if  copy elements that pass a predicate test  partition  reorder elements according to predicate (true value precede false value)  remove  and  remove_if : remove elements that fail a predicate test  unique  : remove consecutive duplicates within a sequence",
            "title": "Reodering"
        },
        {
            "location": "/PCL/#sorting",
            "text": "Thrust offers several functions to sort data or rearrange data according to a given criterion\u6807\u51c6. The  thrust::sort  and  thrust::stable_sort  functions are direct analogs of  sort  and  stable_sort  in STL.  #include <thrust/sort.h>\n...\nconst int N=6;\nint A[N] = {1, 4, 2, 8, 5, 7};\n\nthrust::sort{A, A+N}\n//A is now {1, 2, 4, 5, 7, 8}  In addition, Thrust provides  thrust::sort_by_key  and  thrust::stable_sort_by_key  , which sort key-value pairs stored in separate places.  #include <thrust/sort.h>\n...\nconst int N = 6;\nint keys[N] = {1, 4, 2, 8, 5, 7};\nchar values[N] = {'a', 'b', 'c', 'd', 'e', 'f'}\nthrust::sort_by_key(keys, keys+N, values);\n//keys is now {1, 2, 4, 5, 7, 8}\n//value is now {'a', 'c', 'b', 'e', 'f', 'd'}",
            "title": "Sorting"
        },
        {
            "location": "/PCL/#fancy-iterators",
            "text": "Fancy iterators perform a variety of valuabe purposes.  Constant iterator  Arguably the simplest of the bunch,  constant_iterator  is simply an iterator that returns the same value whenever we access it. In the following example we initialize a constant iterator with the value 10.  #include <thrust/iterator/constant_iterator.h>\n...\n//create iterators\nthrust::constant_iterator<int> first(10);\nthrust::constant_iterator<int> last = first+3;\n\nfirst[0]; //returns 10\nfirst[1]; //returns 10\nfirst[100]; //returns 10\n\n//sum of [first, last)\nthrust::reduce(first, last); //return 30(i.e. 3*10)  Counting iterator  If a sequence of increasing value is required. the  counting_iterator  is the appropriate choice. Here we initialize a  counting_iterator  with the value 10 and access it like an array.  #include <thrust/iterator/counting_iterator.h>\n...\n// create iterators\nthrust::counting_iterator<int> first(10);\nthrust::counting_iterator<int> last = first + 3;\n\nfirst[0]   // returns 10\nfirst[1]   // returns 11\nfirst[100] // returns 110\n\n// sum of [first, last)\nthrust::reduce(first, last);   // returns 33 (i.e. 10 + 11 + 12)  while  constant_iterator  and  counting_iterator  act as array, they don't actually require any memory storage.Whenever we dereference one of these iterators it generates the appropriate value on-the-fly and returns it to the calling function.  transform_iterator  #include <thrust/iterator/transform_iterator.h>\n// initialize vector\nthrust::device_vector<int> vec(3);\nvec[0] = 10; vec[1] = 20; vec[2] = 30;\n\n// create iterator (type omitted)\n...\nfirst = thrust::make_transform_iterator(vec.begin(), negate<int>());\n...\nlast  = thrust::make_transform_iterator(vec.end(),   negate<int>());\n\nfirst[0]   // returns -10\nfirst[1]   // returns -20\nfirst[2]   // returns -30\n\n// sum of [first, last)\nthrust::reduce(first, last);   // returns -60 (i.e. -10 + -20 + -30)  avoid creating a variable to store  first  and  last  // sum of [first, last)\nthrust::reduce(thrust::make_transform_iterator(vec.begin(), negate<int>()),\n               thrust::make_transform_iterator(vec.end(),   negate<int>()));  permutation_iterator  permutation_iterator  is similar: it allows us to fuse gather and scatter operations with Thrust algorithms, or even other fancy iterators. The following example shows how to fuse a gather operation with a reduction.  #include <thrust/iterator/permutation_iterator.h>\n\n...\n\n// gather locations\nthrust::device_vector<int> map(4);\nmap[0] = 3;\nmap[1] = 1;\nmap[2] = 0;\nmap[3] = 5;\n\n// array to gather from\nthrust::device_vector<int> source(6);\nsource[0] = 10;\nsource[1] = 20;\nsource[2] = 30;\nsource[3] = 40;\nsource[4] = 50;\nsource[5] = 60;\n\n// fuse gather with reduction: \n//   sum = source[map[0]] + source[map[1]] + ...\nint sum = thrust::reduce(thrust::make_permutation_iterator(source.begin(), map.begin()),\n                         thrust::make_permutation_iterator(source.begin(), map.end()));  Here we have used the  make_permutation_iterator  function to simplify the construction of the  permutation_iterators . The first argument to  make_permutation_iterator  is the source array of the gather operation and the second is the list of map indices. Note that we pass in  source.begin()  for the first argument in both cases, but vary the second argument to define the beginning and end of the sequence.  zip_iterator  Keep reading, we\u2019ve saved the best iterator for last! The  zip_iterator  is an extremely useful gadget: it takes multiple input sequences and yields a sequence of tuples. In this example we \u201czip\u201d together a sequence of int and a sequence of char into a sequence of  tuple<int,char>  and compute the tuple with the maximum value.  #include <thrust/iterator/zip_iterator.h>\n...\n// initialize vectors\nthrust::device_vector<int>  A(3);\nthrust::device_vector<char> B(3);\nA[0] = 10;  A[1] = 20;  A[2] = 30;\nB[0] = 'x'; B[1] = 'y'; B[2] = 'z';\n\n// create iterator (type omitted)\nfirst = thrust::make_zip_iterator(thrust::make_tuple(A.begin(), B.begin()));\nlast  = thrust::make_zip_iterator(thrust::make_tuple(A.end(),   B.end()));\n\nfirst[0]   // returns tuple(10, 'x')\nfirst[1]   // returns tuple(20, 'y')\nfirst[2]   // returns tuple(30, 'z')\n\n// maximum of [first, last)\nthrust::maximum< tuple<int,char> > binary_op;\nthrust::tuple<int,char> init = first[0];\nthrust::reduce(first, last, init, binary_op); // returns tuple(30, 'z')  What makes  zip_iterator  so useful is that most algorithms accept either one, or occasionally two, input sequences. The  zip_iterator  allows us to combine many independent sequences into a single sequence of tuples, which can be processed by a broad set of algorithms.  Refer to the  arbitrary_transformation  example to see how to implement a ternary transformation with  zip_iterator  and  for_each . A simple extension of this example would allow you to compute transformations with multiple output sequences as well.  In addition to convenience,  zip_iterator  allows us to implement programs more efficiently. For example, storing 3d points as an array of  float3  in CUDA is generally a bad idea, since array accesses are not properly coalesced. With  zip_iterator  we can store the three coordinates in three separate arrays, which does permit coalesced memory access. In this case, we use  zip_iterator  to create a virtual array of 3d vectors which we can feed in to Thrust algorithms. Refer to the  dot_products_with_zip  example for additional details.",
            "title": "Fancy Iterators"
        },
        {
            "location": "/PCL/#__shared__",
            "text": "__shared__ int local_offset[BLOCK_SIZE_X];  Think of shared memory as an explicitly managed cache - it's only useful if you need to access data more than once.either within the same thread or from different threads within the same block. If you're only accessing data once then shared memory isn't going to help you.",
            "title": "__shared__"
        },
        {
            "location": "/PPO/",
            "text": "Tensorflow \u7b97\u6cd5\u529f\u80fd\n\n\n\n\n\u8bb0\u5f55\u4eea\u8bbe\u7f6e\n\n\n\u968f\u673a\u79cd\u5b50\u8bbe\u7f6e\n\n\n\u73af\u5883\u5b9e\u4f8b\u5316\n\n\n\u4e3a\u8ba1\u7b97\u56fe\u5236\u4f5c\u5360\u4f4d\u7b26\n\n\nactor_critic \u901a\u8fc7\u4f5c\u4e3a\u53c2\u6570\u4f20\u9012\u7ed9\u7b97\u6cd5\u51fd\u6570\u6765\u6784\u5efa\u53c2\u4e0e\u8005\u8bc4\u8bba\u8ba1\u7b97\u56fe\n\n\n\u5b9e\u4f8b\u5316\u4f53\u9a8c\u7f13\u51b2\u533a\n\n\n\u5efa\u7acb\u7279\u5b9a\u4e8e\u7b97\u6cd5\u7684\u635f\u5931\u51fd\u6570\u548c\u8bca\u65ad\u7684\u8ba1\u7b97\u56fe\n\n\n\u8fdb\u884c\u57f9\u8bad\n\n\n\u8fdb\u884ctf\u4f1a\u8bdd\u5e76\u521d\u59cb\u5316\u53c2\u6570\n\n\n\u901a\u8fc7\u8bb0\u5f55\u5668\u8bbe\u7f6e\u6a21\u578b\u4fdd\u5b58\n\n\n\u5b9a\u4e49\u8fd0\u884c\u7b97\u6cd5\u4e3b\u5faa\u73af\u6240\u9700\u7684\u529f\u80fd\uff08\u4f8b\u5982\uff0c\u53d6\u51b3\u4e8e\u7b97\u6cd5\uff0c\u6838\u5fc3\u66f4\u65b0\u529f\u80fd\uff0c\u83b7\u53d6\u64cd\u4f5c\u529f\u80fd\u548c\u6d4b\u8bd5\u4ee3\u7406\u529f\u80fd\uff09\n\n\n\u8fd0\u884c\u7b97\u6cd5\u7684\u4e3b\u5faa\u73af\uff1a\n\n\n\u5728\u73af\u5883\u4e2d\u8fd0\u884c\u4ee3\u7406\n\n\n\u6839\u636e\u7b97\u6cd5\u7684\u4e3b\u8981\u65b9\u7a0b\u5f0f\u5b9a\u65f6\u66f4\u65b0\u4ee3\u7406\u53c2\u6570\n\n\n\u8bb0\u5f55\u5173\u952e\u6027\u80fd\u6307\u6807\u5e76\u4fdd\u5b58\u4ee3\u7406\n\n\n\n\n\n\n\n\n\u6838\u5fc3\u6587\u4ef6\n\n\n\u6838\u5fc3\u6587\u4ef6\u4e0d\u50cf\u7b97\u6cd5\u6587\u4ef6\u90a3\u6837\u7d27\u5bc6\u5730\u9644\u7740\u5728\u6a21\u7248\u4e0a\uff0c\u4f46\u5177\u6709\u4e00\u4e9b\u8fd1\u4f3c\u7684\u7ed3\u6784\uff1a\n\n\n\n\n\u4ec5tensorflow:\u4e0e\u5236\u4f5c\u548c\u7ba1\u7406\u5360\u4f4d\u7b26\u6709\u5173\u7684\u529f\u80fd\n\n\n\u7528\u4e8e\u5efa\u7acbactor_critic \u7279\u5b9a\u7b97\u6cd5\u7684\u65b9\u6cd5\u76f8\u5173\u7684\u8ba1\u7b97\u56fe\u90e8\u5206\u7684\u529f\u80fd\n\n\n\u4efb\u4f55\u5176\u4ed6\u6709\u7528\u7684\u529f\u80fd\n\n\n\u4e8e\u7b97\u6cd5\u517c\u5bb9\u7684MLP actor_critic \u7684\u5b9e\u73b0\uff0c\u5176\u4e2d\u7b56\u7565\u548c\u503c\u51fd\u6570\u90fd\u7531\u7b80\u5355\u7684MLP\u8868\u793a\u3002\n\n\n\n\n\u4f53\u9a8c\u6df1\u5ea6RL\u7684\u6700\u4f73\u65b9\u6cd5\u4e4b\u4e00\u662f\u8fd0\u884c\u7b97\u6cd5\uff0c\u5e76\u67e5\u770b\u5b83\u4eec\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u7684\u6267\u884c\u60c5\u51b5\u3002Spinning Up\u968f\u9644spinup/run.py,\u8fd9\u662f\u4e00\u4e2a\u65b9\u4fbf\u7684\u5de5\u5177\uff0c\u53ef\u4ee5\u8ba9\u4f60\u4ece\u547d\u4ee4\u884c\u8f7b\u677e\u7684\u542f\u52a8\u5404\u79cd\u597d\u7b97\u6cd5\uff08\u53ef\u4ee5\u9009\u62e9\u8d85\u53c2\u6570\uff09\n\n\n\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\n\n\nPPO\u53d7\u5230\u4e0eTRPO\u76f8\u540c\u95ee\u9898\u7684\u6fc0\u52b1\uff1a\u6211\u4eec\u5982\u4f55\u624d\u80fd\u4f7f\u7528\u5f53\u524d\u62e5\u6709\u7684\u6570\u636e\u5728\u7b56\u7565\u4e0a\u91c7\u53d6\u6700\u5927\u53ef\u80fd\u7684\u6539\u8fdb\u6b65\u9aa4\uff0c\u800c\u53c8\u4e0d\u4f1a\u8d70\u7684\u592a\u8fdc\u800c\u610f\u5916\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002TRPO\u5c1d\u8bd5\u4f7f\u7528\u590d\u6742\u7684\u4e8c\u9636\u65b9\u6cd5\u89e3\u51b3\u6b64\u95ee\u9898\u3002PPO\u662f\u4e00\u9636\u65b9\u6cd5\u7cfb\u5217\uff0c\u5b83\u4f7f\u7528\u5176\u4ed6\u4e00\u4e9b\u6280\u5de7\u6765\u4f7f\u65b0\u7b56\u7565\u63a5\u8fd1\u4e8e\u65e7\u7b56\u7565\u3002PPO\u65b9\u6cd5\u660e\u663e\u66f4\u6613\u4e8e\u5b9e\u73b0\uff0c\u5e76\u4e14\u4ece\u7ecf\u9a8c\u4e0a\u770b\uff0c\u5176\u6027\u80fd\u81f3\u5c11\u4e0eTRPO\u76f8\u540c\u3002PPO\u6709\u4e24\u79cd\u4e3b\u8981\u53d8\u4f53\uff1aPPO-Penalty\uff0c PPO-Clip\u3002\n\n\nPPO-Penalty \u5927\u7ea6\u89e3\u51b3\u4e86\u50cfTRPO\u8fd9\u6837\u53d7KL\u7ea6\u675f\u7684\u66f4\u65b0\u3002\u4f46\u662f\u4f1a\u60e9\u7f5a\u76ee\u6807\u51fd\u6570\u4e2d\u7684KL\u6563\u5ea6\uff0c\u800c\u4e0d\u4f7f\u5176\u6210\u4e3a\u786c\u7ea6\u675f\uff0c\u5e76\u4f1a\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u81ea\u52a8\u8c03\u6574\u60e9\u7f5a\u7cfb\u6570\uff0c\u4ee5\u4f7f\u5176\u9002\u5f53\u7f29\u653e\u3002\n\n\nPPO-Clip \u5728\u76ee\u6807\u4e2d\u6ca1\u6709KL\u6563\u5ea6\u9879\uff0c \u4e5f\u6ca1\u6709\u4efb\u4f55\u7ea6\u675f\u3002\u53d6\u800c\u4ee3\u4e4b\u7684\u662f\u4f9d\u9760\u5bf9\u76ee\u6807\u51fd\u6570\u7684\u4e13\u95e8\u88c1\u526a\u6765\u6d88\u9664\u65b0\u653f\u7b56\u8fdc\u79bb\u65e7\u653f\u7b56\u7684\u52a8\u673a\u3002\n\n\n\u8981\u95fb\u901f\u89c8\n\n\n\n\nPPO\u662f\u4e00\u79cd\u57fa\u4e8e\u7b56\u7565\u7684\u7b97\u6cd5\n\n\nPPO\u53ef\u4ee5\u5e94\u7528\u4e8e\u5177\u6709\u79bb\u6563\u7684\u6216\u8fde\u7eed\u7684\u52a8\u4f5c\u7a7a\u95f4\u7684\u73af\u5883\n\n\nPPO\u7684Spinningup\u5b9e\u73b0\u652f\u6301\u4e0eMPI\u5e76\u884c\u5316\n\n\n\n\nPPO\u66f4\u65b0\u7b56\u7565\u516c\u5f0f\n\n\n\n\n\u901a\u5e38\u91c7\u53d6\u591a\u4e2a\u6b65\u9aa4\uff08\u901a\u5e38\u662f\u5c0f\u6279\u91cf\uff09SGD\u6765\u6700\u5927\u5316\u76ee\u6807\n\n\n$$\nL(s,\\alpha,\\theta_k,\\theta)=min\\left(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)}A^{\\pi\\theta_k}(s,\\alpha),clip(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)},1-\\epsilon,1+\\epsilon)A^{\\pi\\theta_k}(s,\\alpha)\\right)\n\n\n$$\n\n\n$\\epsilon$\u662f\u4e00\u4e2a\u8d85\u53c2\u6570\u7c97\u7565\u5730\u8bf4\u51fa\u65b0\u653f\u7b56\u88ab\u5141\u8bb8\u4e0e\u65e7\u653f\u7b56\u76f8\u8ddd\u591a\u8fdc\n\n\n\u8fd9\u662f\u4e00\u4e2a\u5f88\u590d\u6742\u7684\u65b9\u7a0b\u5f0f\uff0c\u7b2c\u4e00\u773c\u5f88\u96be\u8bf4\u660e\u767d\u5b83\u5728\u505a\u4ec0\u4e48\uff0c\u6216\u8005\u5b83\u5982\u4f55\u5e2e\u52a9\u65b0\u7b56\u7565\u9760\u8fd1\u65e7\u7b56\u7565\u3002\u4e8b\u5b9e\u8bc1\u660e\u6709\u4e00\u4e2a\u76f8\u5f53\u7b80\u5316\u7684\u7248\u672c\uff0c\u8f83\u5bb9\u6613\u7406\u89e3\uff0c\u4e5f\u662f\u6211\u4eec\u5728\u4ee3\u7801\u4e2d\u5b9e\u73b0\u7684\u7248\u672c\n\n\n$$\nL(s,\\alpha,\\theta_k,\\theta) = min\\left(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)}A^{\\pi\\theta_k}(s,\\alpha), g\\left(\\epsilon,A^{\\pi\\theta_k}(s,\\alpha)\\right)\\right)\n\n\n$$\n\n\n$$\ng(\\epsilon,A)=\\left{\n\\begin{matrix}(1+\\epsilon)A\\quad A\\geq0&\\\\(1-\\epsilon)A\\quad A<0\\end{matrix}\n\\right.\n\n\n$$\n\n\n\u4e3a\u4e86\u5f04\u6e05\u695a\u4ece\u4e2d\u5f97\u5230\u7684\u76f4\u89c9\uff0c\u8ba9\u6211\u770b\u4e00\u4e0b\u5355\u4e2a\u7684 \u72b6\u6001\u2014\u52a8\u4f5c \u5bf9\uff0c\u5e76\u8003\u8651\u6848\u4f8b\n\n\n\u4f18\u52bf\u662f\u79ef\u6781\n\u7684\uff0c\u5047\u8bbe\u8be5\u72b6\u6001\u2014\u52a8\u4f5c \u5bf9\u7684\u4f18\u52bf\u4e3a\u6b63\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b83\u5bf9\u76ee\u6807\u7684\u8d21\u732e\u51cf\u5c0f\u4e3a\uff1a\n\n\n$$\nL(s,\\alpha,\\theta,\\theta_k)=min\\left(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)},(1+\\epsilon)\\right)A^{\\pi\\theta_k}(s,\\alpha)\n\n\n$$\n\n\n\u56e0\u4e3a\u4f18\u52bf\u662f\u79ef\u6781\u7684\uff0c\u6240\u4ee5\u5982\u679c\u91c7\u53d6\u884c\u52a8\u7684\u53ef\u80fd\u6027\u66f4\u5927\uff08\u4e5f\u5c31\u662f$\\pi_{\\theta}(\\alpha|s)$\u589e\u52a0\uff09\uff0c\u76ee\u6807\u5c31\u4f1a\u589e\u52a0\u3002\u4f46\u662fterm\u4e2d\u7684min\u9650\u5236\u4e86\u53ef\u4ee5\u589e\u52a0\u591a\u5c11\u76ee\u6807\u3002\u4e00\u65e6$\\pi_\\theta(\\alpha|s)>(1+\\epsilon)\\pi_{\\theta_k}(\\alpha|s)$,\u8fd9\u65f6min\u8e22\u8fdb\uff0cterm\u5230\u8fbe$(1+\\epsilon)A^{\\pi\\theta_k}(s,\\alpha)$,\u56e0\u6b64\u7528\u8fc7\u8fdc\u79bb\u65e7\u653f\u7b56\uff0c\u65b0\u653f\u7b56\u4e0d\u4f1a\u83b7\u76ca\u3002\n\n\n\u4f18\u52bf\u662f\u8d1f\u9762\n\u7684\uff0c\u5047\u8bbe\u8be5\u72b6\u6001\u2014\u52a8\u4f5c \u5bf9\u7684\u4f18\u52bf\u662f\u8d1f\u7684\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b83\u5bf9\u76ee\u6807\u7684\u8d21\u732e\u51cf\u5c0f\u4e3a\uff1a\n\n\n$$\nL(s,\\alpha,\\theta,\\theta_k)=max\\left(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)},(1-\\epsilon)\\right)A^{\\pi\\theta_k}(s,\\alpha)\n\n\n$$\n\n\n\u56e0\u4e3a\u4f18\u52bf\u662f\u8d1f\u7684\uff0c\u6240\u4ee5\u5982\u679c \u91c7\u53d6\u884c\u52a8\u7684\u53ef\u80fd\u6027\u66f4\u5c0f\uff08\u4e5f\u5c31\u662f$\\pi_\\theta(\\alpha|s)$\u51cf\u5c0f\uff09\uff0c\u76ee\u6807\u5c31\u4f1a\u589e\u52a0\u3002\u4f46\u662fterm\u4e2d\u7684max\u9650\u5236\u4e86\u53ef\u4ee5\u589e\u52a0\u591a\u5c11\u76ee\u6807\u3002\u4e00\u65e6$\\pi_\\theta(\\alpha|s)<(1-\\epsilon)\\pi_{\\theta_k}(\\alpha|s)$,\u8fd9\u65f6max\u8e22\u8fdb\uff0cterm\u5230\u8fbe$(1-\\epsilon)A^{\\pi\\theta_k}(s,\\alpha)$,\u56e0\u6b64\uff0c\u518d\u6b21\uff0c\u8fdc\u79bb\u65e7\u653f\u7b56\uff0c\u4e0d\u4f1a\u4f7f\u65b0\u653f\u7b56\u53d7\u76ca\u3002\n\n\n\u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u901a\u8fc7\u53bb\u9664\u4f7f\u653f\u7b56\u53d1\u751f\u91cd\u5927\u53d8\u5316\u7684\u6fc0\u52b1\uff0c\u88c1\u526a\u5145\u5f53\u7740\u6b63\u5219\u5316\u5668\u3002\u800c\u8d85\u53c2\u6570$\\epsilon$\u5219\u5bf9\u5e94\u4e8e\u65b0\u653f\u7b56\u4e0e\u65e7\u653f\u7b56\u53ef\u4ee5\u6709\u591a\u8fdc\uff0c\u540c\u65f6\u4ecd\u4f7f\u76ee\u6807\u53d7\u76ca\u3002\n\n\n\u63a2\u7d22\u4e0e\u53d1\u73b0\n\n\nPPO\u6309\u57fa\u4e8e\u7b56\u7565\u65b9\u5f0f\u8bad\u7ec3\u968f\u673a\u7b56\u7565\uff0c\u8fd9\u610f\u5473\u7740\u5b83\u4f1a\u6839\u636e\u5176\u968f\u673a\u7b56\u7565\u7684\u6700\u65b0\u7248\u672c\u901a\u8fc7\u91c7\u6837\u64cd\u4f5c\u6765\u8fdb\u884c\u63a2\u7d22\u3002\u52a8\u4f5c\u9009\u62e9\u7684\u968f\u673a\u6027\u53d6\u51b3\u4e8e\u521d\u59cb\u6761\u4ef6\u548c\u8bad\u7ec3\u6b65\u9aa4\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u4ee3\u7406\u9010\u6b65\u51cf\u5c11\u968f\u673a\u6027\uff0c\u56e0\u4e3a\u66f4\u65b0\u89c4\u5219\u9f13\u52b1\u5b83\u5229\u7528\u5df2\u7ecf\u53d1\u73b0\u7684\u5956\u52b1\u3002\u8fd9\u53ef\u80fd\u5bfc\u81f4\u7b56\u7565\u9677\u5165\u5c40\u90e8\u6700\u4f18\u72b6\u6001\u3002\n\n\n\u4f2a\u4ee3\u7801\n\n\n\n\nPPO-CLIP\n\n\n\n\n\u8f93\u5165\uff1a\u521d\u59cb\u7b56\u7565\u53c2\u6570$\\theta_0$,\u521d\u59cb\u503c\u529f\u80fd\u53c2\u6570$\\phi_0$\n\n\nfor k = 0, 1, 2, ... do\n\n\n\u5728\u73af\u5883\u4e2d\u901a\u8fc7\u6267\u884c\u7b56\u7565$\\pi_k=\\pi(\\theta_k)$\uff0c\u6536\u96c6\u4e00\u7ec4\u8f68\u8ff9$D_k={\\tau_i}$\n\n\n\u7b97\u5956\u52b1$\\hat{R}_i$\n\n\n\u57fa\u4e8e\u5f53\u524d\u7684value function $V_{\\phi k}$,\u8ba1\u7b97advantage estimates $\\hat{A}_t$\n\n\n\u901a\u8fc7\u6700\u5927\u5316PPO-Clip\u76ee\u6807\uff0c\u66f4\u65b0\u7b56\u7565\n\n\n$$\n\\theta_{k+1}=arg\\ \\underset{\\theta}{max}\\frac{1}{|D_k|T}\\sum_{\\tau\\in D_k}\\sum^T_{t=0}min\\left(\\frac{\\pi_\\theta(\\alpha_t|s_t)}{\\pi_{\\theta_k}(\\alpha_t|s_t)}A^{\\pi\\theta_k}(s_t,\\alpha_t),g(\\epsilon,A^{\\pi\\theta_k}(s_t,\\alpha_t))\\right)\n\n\n$$\n\n\n\u901a\u5e38\u901a\u8fc7\u4e0eAdam\u4e00\u8d77\u7684\u968f\u673a\u68af\u5ea6\u4e0a\u5347\n\n\n\u901a\u8fc7\u5747\u65b9\u8bef\u5dee\u56de\u5f52\u62df\u5408value function\n$$\n\\phi_{k+1}=arg\\ \\underset{\\phi}{min}\\frac{1}{|D_k|T}\\sum_{\\tau\\in D_k}\\sum^T_{t=0}(V_\\phi(s_t)-\\hat{R}_t)^2\n\n\n$$\n\n\n\u7279\u522b\u662f\u901a\u8fc7\u4e00\u4e9b\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\n\n\nend for",
            "title": "PPO"
        },
        {
            "location": "/PPO/#tensorflow",
            "text": "\u8bb0\u5f55\u4eea\u8bbe\u7f6e  \u968f\u673a\u79cd\u5b50\u8bbe\u7f6e  \u73af\u5883\u5b9e\u4f8b\u5316  \u4e3a\u8ba1\u7b97\u56fe\u5236\u4f5c\u5360\u4f4d\u7b26  actor_critic \u901a\u8fc7\u4f5c\u4e3a\u53c2\u6570\u4f20\u9012\u7ed9\u7b97\u6cd5\u51fd\u6570\u6765\u6784\u5efa\u53c2\u4e0e\u8005\u8bc4\u8bba\u8ba1\u7b97\u56fe  \u5b9e\u4f8b\u5316\u4f53\u9a8c\u7f13\u51b2\u533a  \u5efa\u7acb\u7279\u5b9a\u4e8e\u7b97\u6cd5\u7684\u635f\u5931\u51fd\u6570\u548c\u8bca\u65ad\u7684\u8ba1\u7b97\u56fe  \u8fdb\u884c\u57f9\u8bad  \u8fdb\u884ctf\u4f1a\u8bdd\u5e76\u521d\u59cb\u5316\u53c2\u6570  \u901a\u8fc7\u8bb0\u5f55\u5668\u8bbe\u7f6e\u6a21\u578b\u4fdd\u5b58  \u5b9a\u4e49\u8fd0\u884c\u7b97\u6cd5\u4e3b\u5faa\u73af\u6240\u9700\u7684\u529f\u80fd\uff08\u4f8b\u5982\uff0c\u53d6\u51b3\u4e8e\u7b97\u6cd5\uff0c\u6838\u5fc3\u66f4\u65b0\u529f\u80fd\uff0c\u83b7\u53d6\u64cd\u4f5c\u529f\u80fd\u548c\u6d4b\u8bd5\u4ee3\u7406\u529f\u80fd\uff09  \u8fd0\u884c\u7b97\u6cd5\u7684\u4e3b\u5faa\u73af\uff1a  \u5728\u73af\u5883\u4e2d\u8fd0\u884c\u4ee3\u7406  \u6839\u636e\u7b97\u6cd5\u7684\u4e3b\u8981\u65b9\u7a0b\u5f0f\u5b9a\u65f6\u66f4\u65b0\u4ee3\u7406\u53c2\u6570  \u8bb0\u5f55\u5173\u952e\u6027\u80fd\u6307\u6807\u5e76\u4fdd\u5b58\u4ee3\u7406     \u6838\u5fc3\u6587\u4ef6  \u6838\u5fc3\u6587\u4ef6\u4e0d\u50cf\u7b97\u6cd5\u6587\u4ef6\u90a3\u6837\u7d27\u5bc6\u5730\u9644\u7740\u5728\u6a21\u7248\u4e0a\uff0c\u4f46\u5177\u6709\u4e00\u4e9b\u8fd1\u4f3c\u7684\u7ed3\u6784\uff1a   \u4ec5tensorflow:\u4e0e\u5236\u4f5c\u548c\u7ba1\u7406\u5360\u4f4d\u7b26\u6709\u5173\u7684\u529f\u80fd  \u7528\u4e8e\u5efa\u7acbactor_critic \u7279\u5b9a\u7b97\u6cd5\u7684\u65b9\u6cd5\u76f8\u5173\u7684\u8ba1\u7b97\u56fe\u90e8\u5206\u7684\u529f\u80fd  \u4efb\u4f55\u5176\u4ed6\u6709\u7528\u7684\u529f\u80fd  \u4e8e\u7b97\u6cd5\u517c\u5bb9\u7684MLP actor_critic \u7684\u5b9e\u73b0\uff0c\u5176\u4e2d\u7b56\u7565\u548c\u503c\u51fd\u6570\u90fd\u7531\u7b80\u5355\u7684MLP\u8868\u793a\u3002   \u4f53\u9a8c\u6df1\u5ea6RL\u7684\u6700\u4f73\u65b9\u6cd5\u4e4b\u4e00\u662f\u8fd0\u884c\u7b97\u6cd5\uff0c\u5e76\u67e5\u770b\u5b83\u4eec\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u7684\u6267\u884c\u60c5\u51b5\u3002Spinning Up\u968f\u9644spinup/run.py,\u8fd9\u662f\u4e00\u4e2a\u65b9\u4fbf\u7684\u5de5\u5177\uff0c\u53ef\u4ee5\u8ba9\u4f60\u4ece\u547d\u4ee4\u884c\u8f7b\u677e\u7684\u542f\u52a8\u5404\u79cd\u597d\u7b97\u6cd5\uff08\u53ef\u4ee5\u9009\u62e9\u8d85\u53c2\u6570\uff09",
            "title": "Tensorflow \u7b97\u6cd5\u529f\u80fd"
        },
        {
            "location": "/PPO/#_1",
            "text": "PPO\u53d7\u5230\u4e0eTRPO\u76f8\u540c\u95ee\u9898\u7684\u6fc0\u52b1\uff1a\u6211\u4eec\u5982\u4f55\u624d\u80fd\u4f7f\u7528\u5f53\u524d\u62e5\u6709\u7684\u6570\u636e\u5728\u7b56\u7565\u4e0a\u91c7\u53d6\u6700\u5927\u53ef\u80fd\u7684\u6539\u8fdb\u6b65\u9aa4\uff0c\u800c\u53c8\u4e0d\u4f1a\u8d70\u7684\u592a\u8fdc\u800c\u610f\u5916\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002TRPO\u5c1d\u8bd5\u4f7f\u7528\u590d\u6742\u7684\u4e8c\u9636\u65b9\u6cd5\u89e3\u51b3\u6b64\u95ee\u9898\u3002PPO\u662f\u4e00\u9636\u65b9\u6cd5\u7cfb\u5217\uff0c\u5b83\u4f7f\u7528\u5176\u4ed6\u4e00\u4e9b\u6280\u5de7\u6765\u4f7f\u65b0\u7b56\u7565\u63a5\u8fd1\u4e8e\u65e7\u7b56\u7565\u3002PPO\u65b9\u6cd5\u660e\u663e\u66f4\u6613\u4e8e\u5b9e\u73b0\uff0c\u5e76\u4e14\u4ece\u7ecf\u9a8c\u4e0a\u770b\uff0c\u5176\u6027\u80fd\u81f3\u5c11\u4e0eTRPO\u76f8\u540c\u3002PPO\u6709\u4e24\u79cd\u4e3b\u8981\u53d8\u4f53\uff1aPPO-Penalty\uff0c PPO-Clip\u3002  PPO-Penalty \u5927\u7ea6\u89e3\u51b3\u4e86\u50cfTRPO\u8fd9\u6837\u53d7KL\u7ea6\u675f\u7684\u66f4\u65b0\u3002\u4f46\u662f\u4f1a\u60e9\u7f5a\u76ee\u6807\u51fd\u6570\u4e2d\u7684KL\u6563\u5ea6\uff0c\u800c\u4e0d\u4f7f\u5176\u6210\u4e3a\u786c\u7ea6\u675f\uff0c\u5e76\u4f1a\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u81ea\u52a8\u8c03\u6574\u60e9\u7f5a\u7cfb\u6570\uff0c\u4ee5\u4f7f\u5176\u9002\u5f53\u7f29\u653e\u3002  PPO-Clip \u5728\u76ee\u6807\u4e2d\u6ca1\u6709KL\u6563\u5ea6\u9879\uff0c \u4e5f\u6ca1\u6709\u4efb\u4f55\u7ea6\u675f\u3002\u53d6\u800c\u4ee3\u4e4b\u7684\u662f\u4f9d\u9760\u5bf9\u76ee\u6807\u51fd\u6570\u7684\u4e13\u95e8\u88c1\u526a\u6765\u6d88\u9664\u65b0\u653f\u7b56\u8fdc\u79bb\u65e7\u653f\u7b56\u7684\u52a8\u673a\u3002",
            "title": "\u8fd1\u7aef\u7b56\u7565\u4f18\u5316"
        },
        {
            "location": "/PPO/#_2",
            "text": "PPO\u662f\u4e00\u79cd\u57fa\u4e8e\u7b56\u7565\u7684\u7b97\u6cd5  PPO\u53ef\u4ee5\u5e94\u7528\u4e8e\u5177\u6709\u79bb\u6563\u7684\u6216\u8fde\u7eed\u7684\u52a8\u4f5c\u7a7a\u95f4\u7684\u73af\u5883  PPO\u7684Spinningup\u5b9e\u73b0\u652f\u6301\u4e0eMPI\u5e76\u884c\u5316   PPO\u66f4\u65b0\u7b56\u7565\u516c\u5f0f   \u901a\u5e38\u91c7\u53d6\u591a\u4e2a\u6b65\u9aa4\uff08\u901a\u5e38\u662f\u5c0f\u6279\u91cf\uff09SGD\u6765\u6700\u5927\u5316\u76ee\u6807  $$\nL(s,\\alpha,\\theta_k,\\theta)=min\\left(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)}A^{\\pi\\theta_k}(s,\\alpha),clip(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)},1-\\epsilon,1+\\epsilon)A^{\\pi\\theta_k}(s,\\alpha)\\right)  $$  $\\epsilon$\u662f\u4e00\u4e2a\u8d85\u53c2\u6570\u7c97\u7565\u5730\u8bf4\u51fa\u65b0\u653f\u7b56\u88ab\u5141\u8bb8\u4e0e\u65e7\u653f\u7b56\u76f8\u8ddd\u591a\u8fdc  \u8fd9\u662f\u4e00\u4e2a\u5f88\u590d\u6742\u7684\u65b9\u7a0b\u5f0f\uff0c\u7b2c\u4e00\u773c\u5f88\u96be\u8bf4\u660e\u767d\u5b83\u5728\u505a\u4ec0\u4e48\uff0c\u6216\u8005\u5b83\u5982\u4f55\u5e2e\u52a9\u65b0\u7b56\u7565\u9760\u8fd1\u65e7\u7b56\u7565\u3002\u4e8b\u5b9e\u8bc1\u660e\u6709\u4e00\u4e2a\u76f8\u5f53\u7b80\u5316\u7684\u7248\u672c\uff0c\u8f83\u5bb9\u6613\u7406\u89e3\uff0c\u4e5f\u662f\u6211\u4eec\u5728\u4ee3\u7801\u4e2d\u5b9e\u73b0\u7684\u7248\u672c  $$\nL(s,\\alpha,\\theta_k,\\theta) = min\\left(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)}A^{\\pi\\theta_k}(s,\\alpha), g\\left(\\epsilon,A^{\\pi\\theta_k}(s,\\alpha)\\right)\\right)  $$  $$\ng(\\epsilon,A)=\\left{ \\begin{matrix}(1+\\epsilon)A\\quad A\\geq0&\\\\(1-\\epsilon)A\\quad A<0\\end{matrix} \\right.  $$  \u4e3a\u4e86\u5f04\u6e05\u695a\u4ece\u4e2d\u5f97\u5230\u7684\u76f4\u89c9\uff0c\u8ba9\u6211\u770b\u4e00\u4e0b\u5355\u4e2a\u7684 \u72b6\u6001\u2014\u52a8\u4f5c \u5bf9\uff0c\u5e76\u8003\u8651\u6848\u4f8b  \u4f18\u52bf\u662f\u79ef\u6781 \u7684\uff0c\u5047\u8bbe\u8be5\u72b6\u6001\u2014\u52a8\u4f5c \u5bf9\u7684\u4f18\u52bf\u4e3a\u6b63\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b83\u5bf9\u76ee\u6807\u7684\u8d21\u732e\u51cf\u5c0f\u4e3a\uff1a  $$\nL(s,\\alpha,\\theta,\\theta_k)=min\\left(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)},(1+\\epsilon)\\right)A^{\\pi\\theta_k}(s,\\alpha)  $$  \u56e0\u4e3a\u4f18\u52bf\u662f\u79ef\u6781\u7684\uff0c\u6240\u4ee5\u5982\u679c\u91c7\u53d6\u884c\u52a8\u7684\u53ef\u80fd\u6027\u66f4\u5927\uff08\u4e5f\u5c31\u662f$\\pi_{\\theta}(\\alpha|s)$\u589e\u52a0\uff09\uff0c\u76ee\u6807\u5c31\u4f1a\u589e\u52a0\u3002\u4f46\u662fterm\u4e2d\u7684min\u9650\u5236\u4e86\u53ef\u4ee5\u589e\u52a0\u591a\u5c11\u76ee\u6807\u3002\u4e00\u65e6$\\pi_\\theta(\\alpha|s)>(1+\\epsilon)\\pi_{\\theta_k}(\\alpha|s)$,\u8fd9\u65f6min\u8e22\u8fdb\uff0cterm\u5230\u8fbe$(1+\\epsilon)A^{\\pi\\theta_k}(s,\\alpha)$,\u56e0\u6b64\u7528\u8fc7\u8fdc\u79bb\u65e7\u653f\u7b56\uff0c\u65b0\u653f\u7b56\u4e0d\u4f1a\u83b7\u76ca\u3002  \u4f18\u52bf\u662f\u8d1f\u9762 \u7684\uff0c\u5047\u8bbe\u8be5\u72b6\u6001\u2014\u52a8\u4f5c \u5bf9\u7684\u4f18\u52bf\u662f\u8d1f\u7684\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b83\u5bf9\u76ee\u6807\u7684\u8d21\u732e\u51cf\u5c0f\u4e3a\uff1a  $$\nL(s,\\alpha,\\theta,\\theta_k)=max\\left(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)},(1-\\epsilon)\\right)A^{\\pi\\theta_k}(s,\\alpha)  $$  \u56e0\u4e3a\u4f18\u52bf\u662f\u8d1f\u7684\uff0c\u6240\u4ee5\u5982\u679c \u91c7\u53d6\u884c\u52a8\u7684\u53ef\u80fd\u6027\u66f4\u5c0f\uff08\u4e5f\u5c31\u662f$\\pi_\\theta(\\alpha|s)$\u51cf\u5c0f\uff09\uff0c\u76ee\u6807\u5c31\u4f1a\u589e\u52a0\u3002\u4f46\u662fterm\u4e2d\u7684max\u9650\u5236\u4e86\u53ef\u4ee5\u589e\u52a0\u591a\u5c11\u76ee\u6807\u3002\u4e00\u65e6$\\pi_\\theta(\\alpha|s)<(1-\\epsilon)\\pi_{\\theta_k}(\\alpha|s)$,\u8fd9\u65f6max\u8e22\u8fdb\uff0cterm\u5230\u8fbe$(1-\\epsilon)A^{\\pi\\theta_k}(s,\\alpha)$,\u56e0\u6b64\uff0c\u518d\u6b21\uff0c\u8fdc\u79bb\u65e7\u653f\u7b56\uff0c\u4e0d\u4f1a\u4f7f\u65b0\u653f\u7b56\u53d7\u76ca\u3002  \u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u901a\u8fc7\u53bb\u9664\u4f7f\u653f\u7b56\u53d1\u751f\u91cd\u5927\u53d8\u5316\u7684\u6fc0\u52b1\uff0c\u88c1\u526a\u5145\u5f53\u7740\u6b63\u5219\u5316\u5668\u3002\u800c\u8d85\u53c2\u6570$\\epsilon$\u5219\u5bf9\u5e94\u4e8e\u65b0\u653f\u7b56\u4e0e\u65e7\u653f\u7b56\u53ef\u4ee5\u6709\u591a\u8fdc\uff0c\u540c\u65f6\u4ecd\u4f7f\u76ee\u6807\u53d7\u76ca\u3002",
            "title": "\u8981\u95fb\u901f\u89c8"
        },
        {
            "location": "/PPO/#_3",
            "text": "PPO\u6309\u57fa\u4e8e\u7b56\u7565\u65b9\u5f0f\u8bad\u7ec3\u968f\u673a\u7b56\u7565\uff0c\u8fd9\u610f\u5473\u7740\u5b83\u4f1a\u6839\u636e\u5176\u968f\u673a\u7b56\u7565\u7684\u6700\u65b0\u7248\u672c\u901a\u8fc7\u91c7\u6837\u64cd\u4f5c\u6765\u8fdb\u884c\u63a2\u7d22\u3002\u52a8\u4f5c\u9009\u62e9\u7684\u968f\u673a\u6027\u53d6\u51b3\u4e8e\u521d\u59cb\u6761\u4ef6\u548c\u8bad\u7ec3\u6b65\u9aa4\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u4ee3\u7406\u9010\u6b65\u51cf\u5c11\u968f\u673a\u6027\uff0c\u56e0\u4e3a\u66f4\u65b0\u89c4\u5219\u9f13\u52b1\u5b83\u5229\u7528\u5df2\u7ecf\u53d1\u73b0\u7684\u5956\u52b1\u3002\u8fd9\u53ef\u80fd\u5bfc\u81f4\u7b56\u7565\u9677\u5165\u5c40\u90e8\u6700\u4f18\u72b6\u6001\u3002",
            "title": "\u63a2\u7d22\u4e0e\u53d1\u73b0"
        },
        {
            "location": "/PPO/#_4",
            "text": "PPO-CLIP   \u8f93\u5165\uff1a\u521d\u59cb\u7b56\u7565\u53c2\u6570$\\theta_0$,\u521d\u59cb\u503c\u529f\u80fd\u53c2\u6570$\\phi_0$  for k = 0, 1, 2, ... do  \u5728\u73af\u5883\u4e2d\u901a\u8fc7\u6267\u884c\u7b56\u7565$\\pi_k=\\pi(\\theta_k)$\uff0c\u6536\u96c6\u4e00\u7ec4\u8f68\u8ff9$D_k={\\tau_i}$  \u7b97\u5956\u52b1$\\hat{R}_i$  \u57fa\u4e8e\u5f53\u524d\u7684value function $V_{\\phi k}$,\u8ba1\u7b97advantage estimates $\\hat{A}_t$  \u901a\u8fc7\u6700\u5927\u5316PPO-Clip\u76ee\u6807\uff0c\u66f4\u65b0\u7b56\u7565  $$\n\\theta_{k+1}=arg\\ \\underset{\\theta}{max}\\frac{1}{|D_k|T}\\sum_{\\tau\\in D_k}\\sum^T_{t=0}min\\left(\\frac{\\pi_\\theta(\\alpha_t|s_t)}{\\pi_{\\theta_k}(\\alpha_t|s_t)}A^{\\pi\\theta_k}(s_t,\\alpha_t),g(\\epsilon,A^{\\pi\\theta_k}(s_t,\\alpha_t))\\right)  $$  \u901a\u5e38\u901a\u8fc7\u4e0eAdam\u4e00\u8d77\u7684\u968f\u673a\u68af\u5ea6\u4e0a\u5347  \u901a\u8fc7\u5747\u65b9\u8bef\u5dee\u56de\u5f52\u62df\u5408value function\n$$\n\\phi_{k+1}=arg\\ \\underset{\\phi}{min}\\frac{1}{|D_k|T}\\sum_{\\tau\\in D_k}\\sum^T_{t=0}(V_\\phi(s_t)-\\hat{R}_t)^2  $$  \u7279\u522b\u662f\u901a\u8fc7\u4e00\u4e9b\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5  end for",
            "title": "\u4f2a\u4ee3\u7801"
        },
        {
            "location": "/Qt/",
            "text": "qt install\n\n\n.bashrc\n\n\nexport QTDIR=/home/pmjd/NeDisk/Qt5.12.0/5.12.0/gcc_64\nexport PATH=$QTDIR/bin:$PATH\nexport LD_LIBRARY_PATH=$QTDIR/lib:$LD_LIBRARY_PATH\nexport QT_QPA_PLATFORM_PLUGIN_PATH=$QTDIR/plugins/platforms\nexport PATH=/home/pmjd/NeDisk/Qt5.12.0/Tools/QtCreator/bin:$PATH\n\n\n\ndouble to Qstring\n\n\ndouble valueAsDouble = 1.2;\nQString valueAsString = QString::number(valueAsDouble);\n\n\n\nQString to int\n\n\nQString str = \"10\";\nint n = str.toInt();\n\n\n\nQString to double\n\n\nbool ok(false);\nQString s(\"9338.712001\");\ndouble b = s.toDouble(&ok);\n\n\n\nregex \u63a7\u5236 lineEdit \u8f93\u5165\u5230 [-5\uff0c5]\n\n\nQRegExp rx(\"^(-?[0-4]\\\\.[0-9][0-9])|(-?5)$\");\nQRegExpValidator *pReg = new QRegExpValidator(rx, this);\nxpianyi_t->setValidator(pReg);\n\n\n\nregex \u63a7\u5236 lineEdit \u8f93\u5165 [-180,180]\n\n\nQRegExp rx_(\"^(-?180)|(-?1[0-7][0-9])|(-?[1-9][0-9])|(-?[1-9])|0$\");\nQRegExpValidator *pReg_ = new QRegExpValidator(rx_, this);\nxxuanzhuan_t->setValidator(pReg_);\n\n\n\ncv::Mat to QImage\n\n\ncv::Mat CaptureThread::QImage2cvMat(QImage image)\n{\n    cv::Mat mat;\n    switch(image.format())\n    {\n    case QImage::Format_ARGB32:\n    case QImage::Format_RGB32:\n    case QImage::Format_ARGB32_Premultiplied:\n        mat = cv::Mat(image.height(), image.width(), CV_8UC4, (void*)image.constBits(), image.bytesPerLine());\n        break;\n    case QImage::Format_RGB888:\n        mat = cv::Mat(image.height(), image.width(), CV_8UC3, (void*)image.constBits(), image.bytesPerLine());\n        cv::cvtColor(mat, mat, CV_BGR2RGB);\n        break;\n    case QImage::Format_Indexed8:\n        mat = cv::Mat(image.height(), image.width(), CV_8UC1, (void*)image.constBits(), image.bytesPerLine());\n        break;\n    }\n    return mat;\n}\n\n\n\nqt \u6587\u4ef6\u5939\u9009\u62e9\n\n\nvoid MainWindow::on_toolButton_clicked()\n{\n    QString srcDirPath = QFileDialog::getExistingDirectory(this, tr(\"\u9009\u62e9\u6587\u4ef6\u5939\"),\"/\");\n    if (srcDirPath.length() > 0)\n    {\n        ui->lineEdit_3->setText(srcDirPath);\n    }\n}\n\n\n\nqt \u6587\u4ef6\u9009\u62e9\n\n\nQString path = QFileDialog::getOpenFileName(NULL, tr(\"Open File\"), \".\", tr(\"Text Files(*.cfg)\"));\n    if (path.length() > 0)\n    {\n        ui->lineEdit->setText(path);\n        std::string pathfile = __FILE__;\n        std::size_t find = pathfile.find_last_of(\"/\");\n        std::cout << find << std::endl;\n        pathfile = pathfile.substr(0,find);\n        std::string cmd =\"code --user-data-dir \" + pathfile + \" \" + path.toStdString();\n        std::thread t(run_cmd,cmd);\n        t.detach();\n    }  \n\n\n\nQImage save jpg\n\n\npic.save(\"/home/promote/pictures/1.jpg\",\"JPG\");\n\n\n\n\u8bbe\u7f6e\u8fd0\u884c\u56fe\u6807\n\n\nsetWindowIcon(QIcon(\":/images/camera.png\"));",
            "title": "Qt"
        },
        {
            "location": "/Qt/#qt-install",
            "text": ".bashrc  export QTDIR=/home/pmjd/NeDisk/Qt5.12.0/5.12.0/gcc_64\nexport PATH=$QTDIR/bin:$PATH\nexport LD_LIBRARY_PATH=$QTDIR/lib:$LD_LIBRARY_PATH\nexport QT_QPA_PLATFORM_PLUGIN_PATH=$QTDIR/plugins/platforms\nexport PATH=/home/pmjd/NeDisk/Qt5.12.0/Tools/QtCreator/bin:$PATH",
            "title": "qt install"
        },
        {
            "location": "/Qt/#double-to-qstring",
            "text": "double valueAsDouble = 1.2;\nQString valueAsString = QString::number(valueAsDouble);",
            "title": "double to Qstring"
        },
        {
            "location": "/Qt/#qstring-to-int",
            "text": "QString str = \"10\";\nint n = str.toInt();",
            "title": "QString to int"
        },
        {
            "location": "/Qt/#qstring-to-double",
            "text": "bool ok(false);\nQString s(\"9338.712001\");\ndouble b = s.toDouble(&ok);",
            "title": "QString to double"
        },
        {
            "location": "/Qt/#regex-lineedit-55",
            "text": "QRegExp rx(\"^(-?[0-4]\\\\.[0-9][0-9])|(-?5)$\");\nQRegExpValidator *pReg = new QRegExpValidator(rx, this);\nxpianyi_t->setValidator(pReg);",
            "title": "regex \u63a7\u5236 lineEdit \u8f93\u5165\u5230 [-5\uff0c5]"
        },
        {
            "location": "/Qt/#regex-lineedit-180180",
            "text": "QRegExp rx_(\"^(-?180)|(-?1[0-7][0-9])|(-?[1-9][0-9])|(-?[1-9])|0$\");\nQRegExpValidator *pReg_ = new QRegExpValidator(rx_, this);\nxxuanzhuan_t->setValidator(pReg_);",
            "title": "regex \u63a7\u5236 lineEdit \u8f93\u5165 [-180,180]"
        },
        {
            "location": "/Qt/#cvmat-to-qimage",
            "text": "cv::Mat CaptureThread::QImage2cvMat(QImage image)\n{\n    cv::Mat mat;\n    switch(image.format())\n    {\n    case QImage::Format_ARGB32:\n    case QImage::Format_RGB32:\n    case QImage::Format_ARGB32_Premultiplied:\n        mat = cv::Mat(image.height(), image.width(), CV_8UC4, (void*)image.constBits(), image.bytesPerLine());\n        break;\n    case QImage::Format_RGB888:\n        mat = cv::Mat(image.height(), image.width(), CV_8UC3, (void*)image.constBits(), image.bytesPerLine());\n        cv::cvtColor(mat, mat, CV_BGR2RGB);\n        break;\n    case QImage::Format_Indexed8:\n        mat = cv::Mat(image.height(), image.width(), CV_8UC1, (void*)image.constBits(), image.bytesPerLine());\n        break;\n    }\n    return mat;\n}",
            "title": "cv::Mat to QImage"
        },
        {
            "location": "/Qt/#qt",
            "text": "void MainWindow::on_toolButton_clicked()\n{\n    QString srcDirPath = QFileDialog::getExistingDirectory(this, tr(\"\u9009\u62e9\u6587\u4ef6\u5939\"),\"/\");\n    if (srcDirPath.length() > 0)\n    {\n        ui->lineEdit_3->setText(srcDirPath);\n    }\n}",
            "title": "qt \u6587\u4ef6\u5939\u9009\u62e9"
        },
        {
            "location": "/Qt/#qt_1",
            "text": "QString path = QFileDialog::getOpenFileName(NULL, tr(\"Open File\"), \".\", tr(\"Text Files(*.cfg)\"));\n    if (path.length() > 0)\n    {\n        ui->lineEdit->setText(path);\n        std::string pathfile = __FILE__;\n        std::size_t find = pathfile.find_last_of(\"/\");\n        std::cout << find << std::endl;\n        pathfile = pathfile.substr(0,find);\n        std::string cmd =\"code --user-data-dir \" + pathfile + \" \" + path.toStdString();\n        std::thread t(run_cmd,cmd);\n        t.detach();\n    }",
            "title": "qt \u6587\u4ef6\u9009\u62e9"
        },
        {
            "location": "/Qt/#qimage-save-jpg",
            "text": "pic.save(\"/home/promote/pictures/1.jpg\",\"JPG\");",
            "title": "QImage save jpg"
        },
        {
            "location": "/Qt/#_1",
            "text": "setWindowIcon(QIcon(\":/images/camera.png\"));",
            "title": "\u8bbe\u7f6e\u8fd0\u884c\u56fe\u6807"
        },
        {
            "location": "/README/",
            "text": "util_wiki\n\n\n\u95ee\u9898\u89e3\u51b3\u65b9\u6cd5",
            "title": "README"
        },
        {
            "location": "/README/#util_wiki",
            "text": "\u95ee\u9898\u89e3\u51b3\u65b9\u6cd5",
            "title": "util_wiki"
        },
        {
            "location": "/RL/",
            "text": "what to learn in model-free RL\n\n\nThere are two main approaches to representing and training agents with model-free RL:\n\n\nPolicy Optimization\n\n\nMethods in this family represent a policy explicitly as $\\pi_\\theta(\\alpha,s)$.They optimize the parameters $\\theta$ either directly by gradient ascent on the performance objective $J(\\pi_\\theta)$ ,or indirectly, by maximizing local approximation of $J(\\pi_\\theta)$ ,This optimiation is almost always performed on-policy,which means that each update only uses data collected while acting according to the most recent version of the policy.Policy optimization also usually invovles learning an approximator $V_\\phi(s)$ for the on-policy value function $V^\\pi(s)$ ,which gets used in figuring out how to update the policy.\n\n\nA couple of examples of policy optimization methods are:\n\n\n\n\nA2C/A3C, which performs gradient ascent to directly maximize performance.\n\n\n\n\nA2C/A3C\n\n\nWe propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.\n\n\nDeep RL algorithms based on experience replay have achieved unprecedented success in challenging domains such as Atari 2600.However experience replay has serval drawbacks : it uses more memory and computation per real interaction; and it requires off-policy learning algorithms that can update from data generated by an older policy.\n\n\nThe best of the proposed methods, asynchronous advantage actor-critic(A3C), also mastered a variety of continuous motor control tasks as well as learned general strategies for exploring 3D mazes purely from visual inputs.\n\n\nRelated Work\n\n\nThe General Reinforcement Learning Architecture (Gorila) of performs asynchronous training of reinforcement learning agents in a distributed setting.In Gorila, each process contains an actor that acts in its own copy of the environment, a seperate replay memory, and a learner that samples data from the replay memory, and a learner that samples data from the replay memory and computes gradients of the DQN loss with respect to the policy parameters. The gradients are asynchronously sent to a central parameter server which updates a central copy of the model. The updated policy parameters are sent to the actor-learners at fixed intervals.By using 100 separate actor-learner processes and 30 parameter server instances, a total of 130 machines, Gorila was able to significantly outperform DQN over 49 Atari games. On many games Gorila reached the score achieved by DQN over 20 times faster than DQN. We also note that a similar way of parallelizing DQN was proposed by Chavez.\n\n\nIn earlier work Li applied the Map Reduce framework to parallelizing batch reinforcement learning methods with linear function approximation.Parallelism was used to speed up large matrix operations but not to paralleize the collection of experience or stabilize learning. Grounds proposed a parallel vision of the Sarsa algorithm that uses multiple separate actor-learners to accelerate training .Each actor-learner learns separately and periodically sends updates to weights that have changed significantly to the other learners using peer-to-peer communication.\n\n\nT studied convergence properties of Q-learning in the asynchronous optimization setting. These results show that Q-learning is still guaranteed to converage when some of the information is outdated as long as outdated information is always eventually discarded and several other technical assumptions are satisfied. Even earlier, studied the related problem of distributed dynamic programming.\n\n\nAnother related area of work is in evolutionary methods, which are often straightforward to parallelize by distributing fitness evaluations over multiple machines or threads.Such parallel evolutionary approaches have recently been applied to some visual reinforcement learning tasks.In one example, Koutnik evolved convolutional neural network controllers for the TORCS driving simulator by performing fitness evaluations on 8 CPU cores in parallel.\n\n\nReinforcement Learning Background\n\n\nWe consider the standard reinforcement learning setting where an agent interacts with an environment $\\varepsilon$ over a number of discrete time steps.At each time step $t$ ,the agent receives a state $s_t$ and selects an action $a_t$ from some set of possible actions $A$ according to its policy $\\pi$ ,where $\\pi$ is a mapping from states $s_t$ to actions $a_t$ . In return the agent receives the next state $s_{t+1}$ and receives a scalar reward $\\tau_t$ . The process continues until the agent reaches a terminal state after  which the process restarts. The return $R_t=\\sum^{\\infty}\n{k=0}\\gamma^k\\tau\n{k+1}$ is the total accumulated return from time step $t$ with discount factor $\\gamma\\in(0,1]$ .The goal of the agent is to maximize the expected return from each state $s_t$.\n\n\nThe action value $Q^\\pi(s,a)=\\mathbb{E}[R_t|s_t=s,a]$ is the expected return for selecting action $a$ in the state $s$ and following policy $\\pi$ . The optimal value function $Q^*(s,a)=max_\\pi Q^\\pi(s,a)$ gives the maximum action value for state $s$ and action $a$ achievable by any policy. Similarly, the value of the state $s$ and action $a$ achievable by any policy. Similarly, the value of state $s$ under policy $\\pi$ is defined as $V^\\pi(s)=\\mathbb{E}[R_t|s_t=s]$ and is simply the expected return for following policy $\\pi$ from state $s$.\n\n\nIn value-based model-free reinforcement learning methods, the action value function is represented using a function approximator, such as a neural network. Let $Q(s,a;\\theta) $ be an approximate action-value function with parameters $\\theta$ . The updates to $\\theta$ can be derived from a variety of reinforcement learning algorithms.One example of such an algorithm is Q-learning, which aims to directly approximate the optimal action value function: $Q^*(s,a)\\approx Q(s,a,\\theta)$ . In one-step Q-learning, the parameters $\\theta$ of the action value function $Q(s,a,\\theta)$ are learned by iteratively minimizing a sequence of loss functions, where the $i$th loss function defined as\n\n\n$$\nL_i(\\theta_i)=\\mathbb{E}\\left(\\tau+\\gamma\\underset{a'}{max}Q(s',a';\\theta_{i-1})-Q(s,a;\\theta_i\\right)^2\n\n\n$$\n\n\nwhere $s'$ is the state encountered after state $s$.\n\n\nWe refer to the above method as one-step Q-learning because it updates the action value Q(s,a) toward the onestep return $\\tau+\\gamma\\underset{a'}{max}Q(s',a';\\theta)$.One drawback of using one-step methods is that obtaining a reward $\\tau$ only directly affects the value of the state action pair $s,a$ that led to the reward. The values of other state action pairs are affected only indirectly through the updated value $Q(s,a)$ . This can make the learning process slow since many updates are required the propagate a reward to the relevant preceding states and actions.\n\n\nOne way of propagating rewards faster is by using n-step returns.In $n$-step Q-learning, $Q(s,a)$ is updated toward the $n$-step return defined as $\\tau_t+\\gamma\\tau_{t+1}+...+\\gamma^{n-1}\\tau_{t+n-1}+\\underset{a}{max}\\gamma^nQ(s_{t+n},a)$ . This results in a single reward $\\tau$ directly affecting the values of $n$ preceding state action pairs.This makes the process of propagating rewards to relevant state-action pairs potentially much more efficient.\n\n\nIn construct to value-based methods, policy-based model-free methods directly parameterize the policy $\\pi(a|s;\\theta)$ and update the parameters $\\theta$ by performing, typically approximate, gradient ascent on $\\mathbb{E}[R_t]$ . One example of such a method is the REINFORCE family of algorithms due to Williams. Standard REINFORCE updates the policy parameters $\\theta$ in the direction $\\triangledown_\\theta log\\pi(a_t|s_t;\\theta)R_t$,which is an unbiased estimate of $\\triangledown_\\theta\\mathbb{E}[R_t]$. It is possible to reduce the variance of this estimate while keeping it unbiased by substracting a learned  function of the state   $b_t(s_t)$, known as a baseline, from the return. The resulting gradient is $\\triangledown_\\theta log\\pi(a_t|s_t;\\theta)(R_t-b_t(s_t)).$\n\n\nA learned estimate of the value function is commonly used as the baseline $b_t(s_t)\\approx V^\\pi(s_t)$ leading to a much lower variance estimate of the policy gradient. When an aprroximate value function is used as the baseline, the quantity $R_t-b_t$ used to scale the policy gradient can be seen as an estimate of the advantage of action $a_t$ in state $s_t$, or $A(a_t,s_t)=Q(a_t,s_t)-V(s_t)$ ,because $R_t$ is an estimate of $Q^\\pi(a_t,s_t)$ and $b_t$ is an estimate of $V^\\pi(s_t)$. This approach can be viewed as an actor-critic architecture where the policy $\\pi$ is the actor and the baseline $b_t$ is the critic.\n\n\nAsynchronous RL Framework\n\n\nWe now present multi-threaded asynchronous variants of one-step Sarsa, one-step Q-learning, n-step Q-learning, and advantage actor-critic. The aim in designing these methods was to find RL algorithms that can train deep neural network policies reliably and without large resource requirements. While the underlying RL methods are quite different , with actor-critic being an on-policy policy search method and Q-learning being an off-policy value-based method ,we use two main ideas to make all four algorithms practical given our design goal.\n\n\nFirst, we use asynchronous actor-learners , similarly to the Gorila framework , but instead of using sparate machines and a parameter server , we use multiple CPU threads  on a single machine. Keeping the learners on a single machine removes the communication costs of sending gradients and parameters and enables us to use Hogwild! style updates for training.\n\n\nSecond ,we make the observation that multiple actors-learners running in parallel are likely to be exploring different parts of the environment.\n\n\nasynchronous one-step Q-learning pseudocode for each actor-learner thread\n\n\n//Assume global shared \u03b8,$\\theta^-$, and counter $T=0$.\n\n\nInitialize thread step counter $t\\leftarrow 0$\n\n\nInitialize target network weights $\\theta^-\\leftarrow\\theta$\n\n\nInitialize network gradients $d\\theta\\leftarrow 0$\n\n\nGet initial state $s$\n\n\nrepeat\n\n\n$\\$Take action",
            "title": "RL"
        },
        {
            "location": "/RL/#what-to-learn-in-model-free-rl",
            "text": "There are two main approaches to representing and training agents with model-free RL:",
            "title": "what to learn in model-free RL"
        },
        {
            "location": "/RL/#policy-optimization",
            "text": "Methods in this family represent a policy explicitly as $\\pi_\\theta(\\alpha,s)$.They optimize the parameters $\\theta$ either directly by gradient ascent on the performance objective $J(\\pi_\\theta)$ ,or indirectly, by maximizing local approximation of $J(\\pi_\\theta)$ ,This optimiation is almost always performed on-policy,which means that each update only uses data collected while acting according to the most recent version of the policy.Policy optimization also usually invovles learning an approximator $V_\\phi(s)$ for the on-policy value function $V^\\pi(s)$ ,which gets used in figuring out how to update the policy.  A couple of examples of policy optimization methods are:   A2C/A3C, which performs gradient ascent to directly maximize performance.",
            "title": "Policy Optimization"
        },
        {
            "location": "/RL/#a2ca3c",
            "text": "We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.  Deep RL algorithms based on experience replay have achieved unprecedented success in challenging domains such as Atari 2600.However experience replay has serval drawbacks : it uses more memory and computation per real interaction; and it requires off-policy learning algorithms that can update from data generated by an older policy.  The best of the proposed methods, asynchronous advantage actor-critic(A3C), also mastered a variety of continuous motor control tasks as well as learned general strategies for exploring 3D mazes purely from visual inputs.",
            "title": "A2C/A3C"
        },
        {
            "location": "/RL/#related-work",
            "text": "The General Reinforcement Learning Architecture (Gorila) of performs asynchronous training of reinforcement learning agents in a distributed setting.In Gorila, each process contains an actor that acts in its own copy of the environment, a seperate replay memory, and a learner that samples data from the replay memory, and a learner that samples data from the replay memory and computes gradients of the DQN loss with respect to the policy parameters. The gradients are asynchronously sent to a central parameter server which updates a central copy of the model. The updated policy parameters are sent to the actor-learners at fixed intervals.By using 100 separate actor-learner processes and 30 parameter server instances, a total of 130 machines, Gorila was able to significantly outperform DQN over 49 Atari games. On many games Gorila reached the score achieved by DQN over 20 times faster than DQN. We also note that a similar way of parallelizing DQN was proposed by Chavez.  In earlier work Li applied the Map Reduce framework to parallelizing batch reinforcement learning methods with linear function approximation.Parallelism was used to speed up large matrix operations but not to paralleize the collection of experience or stabilize learning. Grounds proposed a parallel vision of the Sarsa algorithm that uses multiple separate actor-learners to accelerate training .Each actor-learner learns separately and periodically sends updates to weights that have changed significantly to the other learners using peer-to-peer communication.  T studied convergence properties of Q-learning in the asynchronous optimization setting. These results show that Q-learning is still guaranteed to converage when some of the information is outdated as long as outdated information is always eventually discarded and several other technical assumptions are satisfied. Even earlier, studied the related problem of distributed dynamic programming.  Another related area of work is in evolutionary methods, which are often straightforward to parallelize by distributing fitness evaluations over multiple machines or threads.Such parallel evolutionary approaches have recently been applied to some visual reinforcement learning tasks.In one example, Koutnik evolved convolutional neural network controllers for the TORCS driving simulator by performing fitness evaluations on 8 CPU cores in parallel.",
            "title": "Related Work"
        },
        {
            "location": "/RL/#reinforcement-learning-background",
            "text": "We consider the standard reinforcement learning setting where an agent interacts with an environment $\\varepsilon$ over a number of discrete time steps.At each time step $t$ ,the agent receives a state $s_t$ and selects an action $a_t$ from some set of possible actions $A$ according to its policy $\\pi$ ,where $\\pi$ is a mapping from states $s_t$ to actions $a_t$ . In return the agent receives the next state $s_{t+1}$ and receives a scalar reward $\\tau_t$ . The process continues until the agent reaches a terminal state after  which the process restarts. The return $R_t=\\sum^{\\infty} {k=0}\\gamma^k\\tau {k+1}$ is the total accumulated return from time step $t$ with discount factor $\\gamma\\in(0,1]$ .The goal of the agent is to maximize the expected return from each state $s_t$.  The action value $Q^\\pi(s,a)=\\mathbb{E}[R_t|s_t=s,a]$ is the expected return for selecting action $a$ in the state $s$ and following policy $\\pi$ . The optimal value function $Q^*(s,a)=max_\\pi Q^\\pi(s,a)$ gives the maximum action value for state $s$ and action $a$ achievable by any policy. Similarly, the value of the state $s$ and action $a$ achievable by any policy. Similarly, the value of state $s$ under policy $\\pi$ is defined as $V^\\pi(s)=\\mathbb{E}[R_t|s_t=s]$ and is simply the expected return for following policy $\\pi$ from state $s$.  In value-based model-free reinforcement learning methods, the action value function is represented using a function approximator, such as a neural network. Let $Q(s,a;\\theta) $ be an approximate action-value function with parameters $\\theta$ . The updates to $\\theta$ can be derived from a variety of reinforcement learning algorithms.One example of such an algorithm is Q-learning, which aims to directly approximate the optimal action value function: $Q^*(s,a)\\approx Q(s,a,\\theta)$ . In one-step Q-learning, the parameters $\\theta$ of the action value function $Q(s,a,\\theta)$ are learned by iteratively minimizing a sequence of loss functions, where the $i$th loss function defined as  $$\nL_i(\\theta_i)=\\mathbb{E}\\left(\\tau+\\gamma\\underset{a'}{max}Q(s',a';\\theta_{i-1})-Q(s,a;\\theta_i\\right)^2  $$  where $s'$ is the state encountered after state $s$.  We refer to the above method as one-step Q-learning because it updates the action value Q(s,a) toward the onestep return $\\tau+\\gamma\\underset{a'}{max}Q(s',a';\\theta)$.One drawback of using one-step methods is that obtaining a reward $\\tau$ only directly affects the value of the state action pair $s,a$ that led to the reward. The values of other state action pairs are affected only indirectly through the updated value $Q(s,a)$ . This can make the learning process slow since many updates are required the propagate a reward to the relevant preceding states and actions.  One way of propagating rewards faster is by using n-step returns.In $n$-step Q-learning, $Q(s,a)$ is updated toward the $n$-step return defined as $\\tau_t+\\gamma\\tau_{t+1}+...+\\gamma^{n-1}\\tau_{t+n-1}+\\underset{a}{max}\\gamma^nQ(s_{t+n},a)$ . This results in a single reward $\\tau$ directly affecting the values of $n$ preceding state action pairs.This makes the process of propagating rewards to relevant state-action pairs potentially much more efficient.  In construct to value-based methods, policy-based model-free methods directly parameterize the policy $\\pi(a|s;\\theta)$ and update the parameters $\\theta$ by performing, typically approximate, gradient ascent on $\\mathbb{E}[R_t]$ . One example of such a method is the REINFORCE family of algorithms due to Williams. Standard REINFORCE updates the policy parameters $\\theta$ in the direction $\\triangledown_\\theta log\\pi(a_t|s_t;\\theta)R_t$,which is an unbiased estimate of $\\triangledown_\\theta\\mathbb{E}[R_t]$. It is possible to reduce the variance of this estimate while keeping it unbiased by substracting a learned  function of the state   $b_t(s_t)$, known as a baseline, from the return. The resulting gradient is $\\triangledown_\\theta log\\pi(a_t|s_t;\\theta)(R_t-b_t(s_t)).$  A learned estimate of the value function is commonly used as the baseline $b_t(s_t)\\approx V^\\pi(s_t)$ leading to a much lower variance estimate of the policy gradient. When an aprroximate value function is used as the baseline, the quantity $R_t-b_t$ used to scale the policy gradient can be seen as an estimate of the advantage of action $a_t$ in state $s_t$, or $A(a_t,s_t)=Q(a_t,s_t)-V(s_t)$ ,because $R_t$ is an estimate of $Q^\\pi(a_t,s_t)$ and $b_t$ is an estimate of $V^\\pi(s_t)$. This approach can be viewed as an actor-critic architecture where the policy $\\pi$ is the actor and the baseline $b_t$ is the critic.",
            "title": "Reinforcement Learning Background"
        },
        {
            "location": "/RL/#asynchronous-rl-framework",
            "text": "We now present multi-threaded asynchronous variants of one-step Sarsa, one-step Q-learning, n-step Q-learning, and advantage actor-critic. The aim in designing these methods was to find RL algorithms that can train deep neural network policies reliably and without large resource requirements. While the underlying RL methods are quite different , with actor-critic being an on-policy policy search method and Q-learning being an off-policy value-based method ,we use two main ideas to make all four algorithms practical given our design goal.  First, we use asynchronous actor-learners , similarly to the Gorila framework , but instead of using sparate machines and a parameter server , we use multiple CPU threads  on a single machine. Keeping the learners on a single machine removes the communication costs of sending gradients and parameters and enables us to use Hogwild! style updates for training.  Second ,we make the observation that multiple actors-learners running in parallel are likely to be exploring different parts of the environment.  asynchronous one-step Q-learning pseudocode for each actor-learner thread  //Assume global shared \u03b8,$\\theta^-$, and counter $T=0$.  Initialize thread step counter $t\\leftarrow 0$  Initialize target network weights $\\theta^-\\leftarrow\\theta$  Initialize network gradients $d\\theta\\leftarrow 0$  Get initial state $s$  repeat  $\\$Take action",
            "title": "Asynchronous RL Framework"
        },
        {
            "location": "/ROS2/",
            "text": "\u6e90\u7801\u7f16\u8bd1\n\n\nlocale  # check for UTF-8\n\nsudo apt update && sudo apt install locales\nsudo locale-gen en_US en_US.UTF-8\nsudo update-locale LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8\nexport LANG=en_US.UTF-8\n\nlocale  # verify settings\n\nsudo apt update && sudo apt install curl gnupg2 lsb-release\nsudo curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key  -o /usr/share/keyrings/ros-archive-keyring.gpg\n\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/ros2.list > /dev/null\n\nsudo apt update && sudo apt install -y \\\n  build-essential \\\n  cmake \\\n  git \\\n  libbullet-dev \\\n  python3-colcon-common-extensions \\\n  python3-flake8 \\\n  python3-pip \\\n  python3-pytest-cov \\\n  python3-rosdep \\\n  python3-setuptools \\\n  python3-vcstool \\\n  wget\n# install some pip packages needed for testing\npython3 -m pip install -U \\\n  argcomplete \\\n  flake8-blind-except \\\n  flake8-builtins \\\n  flake8-class-newline \\\n  flake8-comprehensions \\\n  flake8-deprecated \\\n  flake8-docstrings \\\n  flake8-import-order \\\n  flake8-quotes \\\n  pytest-repeat \\\n  pytest-rerunfailures \\\n  pytest\n# install Fast-RTPS dependencies\nsudo apt install --no-install-recommends -y \\\n  libasio-dev \\\n  libtinyxml2-dev\n# install Cyclone DDS dependencies\nsudo apt install --no-install-recommends -y \\\n  libcunit1-dev",
            "title": "ROS2"
        },
        {
            "location": "/ROS2/#_1",
            "text": "locale  # check for UTF-8\n\nsudo apt update && sudo apt install locales\nsudo locale-gen en_US en_US.UTF-8\nsudo update-locale LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8\nexport LANG=en_US.UTF-8\n\nlocale  # verify settings\n\nsudo apt update && sudo apt install curl gnupg2 lsb-release\nsudo curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key  -o /usr/share/keyrings/ros-archive-keyring.gpg\n\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/ros2.list > /dev/null\n\nsudo apt update && sudo apt install -y \\\n  build-essential \\\n  cmake \\\n  git \\\n  libbullet-dev \\\n  python3-colcon-common-extensions \\\n  python3-flake8 \\\n  python3-pip \\\n  python3-pytest-cov \\\n  python3-rosdep \\\n  python3-setuptools \\\n  python3-vcstool \\\n  wget\n# install some pip packages needed for testing\npython3 -m pip install -U \\\n  argcomplete \\\n  flake8-blind-except \\\n  flake8-builtins \\\n  flake8-class-newline \\\n  flake8-comprehensions \\\n  flake8-deprecated \\\n  flake8-docstrings \\\n  flake8-import-order \\\n  flake8-quotes \\\n  pytest-repeat \\\n  pytest-rerunfailures \\\n  pytest\n# install Fast-RTPS dependencies\nsudo apt install --no-install-recommends -y \\\n  libasio-dev \\\n  libtinyxml2-dev\n# install Cyclone DDS dependencies\nsudo apt install --no-install-recommends -y \\\n  libcunit1-dev",
            "title": "\u6e90\u7801\u7f16\u8bd1"
        },
        {
            "location": "/Ros/",
            "text": "Ros create msg\n\n\n\n\n\u5728node\u8282\u70b9\u6587\u4ef6\u5939\u4e0b\u521b\u5efamsg\u6587\u4ef6\u5939\n\n\n\u5728msg\u6587\u4ef6\u5939\u4e0b\u521b\u5efa **.msg\u6587\u4ef6\n\n\n\n\ndetect.msg\n\n\nbool begin\nstring first_name\nstring last_name\nuint8 age\nuint32 score\nint64 stateLabel\nint64 CIPV\nint64 trackID\nint64 trackFrameNum\nint64 obstacleType\nfloat32 real3DRightX\nfloat32 real3DLeftX\nfloat32 width\nfloat32 real3DLowY\nfloat32 real3DUpY\nfloat32 height\nfloat32 HMW\nfloat32 fuzzyCollisionTimeZ\nfloat32 longitudinalZ\nfloat32 speedOfLongitudinal\nfloat32 lateralX\nfloat32 speedOfLateral\nfloat32 currentSpeed\n\n\n\ndetect_array.msg\n\n\ndectect []\n\n\n\npackage.xml\n\n\n<build_depend>message_generation</build_depend>\n<exec_depend>message_runtime</exec_depend>\n\n\n\nCMakelists.txt\n\n\nfind_package(catkin REQUIRED COMPONENTS\n   roscpp\n   rospy\n   std_msgs\n   message_generation\n)\n\ncatkin_package(\n  ...\n  CATKIN_DEPENDS message_runtime ...\n  ...)\n\nadd_message_files(\n  FILES\n  detect.msg\n  detect_array.msg\n)\n\n\ngenerate_messages(\n  DEPENDENCIES\n  std_msgs\n)\n\n\n\n\n\n\n\n\u5305\u542b\u5934\u6587\u4ef6\n\n\n\n\n#include \"smart_eye/detect.h\"\n#include \"smart_eye/detect_array.h\"\n\nmsg_pub = nh_.advertise<smart_eye::detect_array>(\"detect_array\", 10);\n\nsmart_eye::detect msg;\nsmart_eye::detect_array msg_array;\nmsg.stateLabel = stateLabel;\nmsg.CIPV = CIPV;\nmsg.trackID = trackID;\nmsg.trackFrameNum = trackFrameNum;\nmsg.obstacleType = obstacleType;\nmsg.real3DRightX = real3DRightX;\nmsg.real3DLeftX = real3DLeftX;\nmsg.width = width;\nmsg.real3DLowY = real3DLowY;\nmsg.real3DUpY = real3DUpY;\nmsg.height = height;\nmsg.HMW = HMW;\nmsg.fuzzyCollisionTimeZ = fuzzyCollisionTimeZ;\nmsg.longitudinalZ = longitudinalZ;\nmsg.speedOfLongitudinal = speedOfLongitudinal;\nmsg.lateralX = lateralX;\nmsg.speedOfLateral = speedOfLateral;\nmsg.currentSpeed = currentSpeed;\nmsg_array.push(msg);\nmsg_pub.publish(msg_array);\n\n\n\n\u5f15\u7528\u5176\u4ed6\u5305\u7684\uff4d\uff53\uff47\n\n\n\u9996\u5148cmakelists.txt\n\n\nfind_package(catkin REQUIRED COMPONENTS\n  message_generation\n  pcl_ros\n  roscpp\n  rospy\n  std_msgs\n  wjj\n)\n\ncatkin_package(\n INCLUDE_DIRS include\n CATKIN_DEPENDS message_runtime pcl_ros roscpp rospy std_msgs wjj\n)\n\nadd_dependencies(teaching \n${catkin_EXPORTED_TARGETS}\nwjj_gencpp)\n\n\n\n\u518d\u8005package.xml\n\n\n <build_depend>wjj</build_depend>\n <exec_depend>wjj</exec_depend>\n\n\n\n\u6700\u540e\u4f7f\u7528\n\n\n#include <wjj/SaeJ1939.h>\n\n\n\nsudo rosdep init &rosdep update\u5931\u8d25\n\n\n\u8be5\u89e3\u51b3\u65b9\u6848\u662f\u9488\u5bf9\u7531\u4e8e\u4ee5\u4e0b\u4e24\u4e2a\u65e0\u6cd5\u6b63\u5e38\u8bbf\u95ee\uff0c\u4f46\u53ef\u4ee5ping\u901a\uff0c\u4e8e\u662f\u4fee\u6539hosts\u6587\u4ef6\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e24\u4e2a\u7f51\u5740\u7684IP\u5730\u5740\u5b9e\u73b0\u8bbf\u95ee\u3002\n\n\n'sudo gedit /etc/hosts'\n\n\n\u6dfb\u52a0\n\n\n199.232.28.133  raw.githubusercontent.com\n151.101.228.133  raw.github.com\n\n\n\n\n\u4fee\u6539\u5b8c\u6210\u540e\uff0c\u5728\u7ec8\u7aef\u6267\u884c\n\n\nsudo rosdep init\nredep update\n\n\n\nvscode python ros debug\n\n\n\u9996\u5148\n\n\ncatkin_make -DCMAKE_BUILD_TYPE=DEBUG\n\n\n\n\u5176\u6b21\u70b9\u51fbdebug\u6309\u626d,\u518d\u70b9\u51fb`create a launch.json file\n\n\n\n\n\u9009\u62e9\nShow\n\n\n\n\n\u9009\u62e9 \nAdd Configuration..\n\n\n\n\n\u518d\u6dfb\u52a0 \nRos Attach\n \n\n\n\nlaunch.json\n\n\n{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"ROS: Attach\",\n            \"type\": \"ros\",\n            \"request\": \"attach\"\n        }\n\n    ]\n}\n\n\n\n\uff44ebug\u65f6\u9047\u5230\u8def\u5f84\u95ee\u9898,\u4f8b\u5982rosmsg\u8def\u5f84\uff0c\u6700\u597d\u5728\u6587\u4ef6\u5c5e\u6027\u91cc\u590d\u5236\u8def\u5f84\uff0c\u4e0d\u7136\u5bb9\u6613\u51fa\u9519\n\n\nimport sys\nsys.path.append(\"/home/pmjd/Downloads/catkin_ws/devel/lib/python2.7/dist-packages\")\n\n\n\nError: package 'teleop_twist_keyboard' not found\n\n\nYou need to download the teleop_twist_keyboard from the github to your ~/catkin_ws/src folder. Steps:\n\n\n1) \ncd ~/catkin_ws/src\n\n2) \ngit clone https://github.com/ros-teleop/teleop_twist_keyboard\n\n\nSpawn service failed. Exiting.\n\n\nexport ROS_MASTER_URI=http://promote-OMEN-by-HP-Laptop-17-cb1xxx:11311/\n\n\n\ncreateQuaternionFromRPY\n\n\nstatic geometry_msgs::Quaternion createQuaternionFromRPY(double roll, double pitch, double yaw) {\ngeometry_msgs::Quaternion q;\ndouble t0 = cos(yaw * 0.5);\ndouble t1 = sin(yaw * 0.5);\ndouble t2 = cos(roll * 0.5);\ndouble t3 = sin(roll * 0.5);\ndouble t4 = cos(pitch * 0.5);\ndouble t5 = sin(pitch * 0.5);\nq.w = t0 * t2 * t4 + t1 * t3 * t5;\nq.x = t0 * t3 * t4 - t1 * t2 * t5;\nq.y = t0 * t2 * t5 + t1 * t3 * t4;\nq.z = t1 * t2 * t4 - t0 * t3 * t5;\nreturn q;\n}\n\n\nlaunch \u542f\u52a8 rviz\n\n\n<launch>\n  <node type=\"rviz\" name=\"rviz\" pkg=\"rviz\" args=\"-d $(find package_name)/rviz/config_file.rviz\" />\n</launch>\n\n\n\nrosdep\u5931\u8d25\n\n\n\u9996\u5148\nsudo rosdep init\n\n\n\u8fd9\u4e00\u6b65\u4f1a\u5728\n/etc/ros/rosdep/sources.list.d/\n\u76ee\u5f55\u4e0b\u65b0\u5efa\n20-default.list\n\n\n# os-specific listings first\nyaml https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/osx-homebrew.yaml osx\n\n# generic\nyaml https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/base.yaml\nyaml https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/python.yaml\nyaml https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/ruby.yaml\ngbpdistro https://raw.githubusercontent.com/ros/rosdistro/master/releases/fuerte.yaml fuerte\n\n# newer distributions (Groovy, Hydro, ...) must not be listed anymore, they are being fetched from the rosdistro index.yaml instead\n\n\n\n\n\u6211\u4eec\u9700\u8981\u4f9d\u6b21\u4e0b\u8f7d\nosx-homebrew.yaml\n\u7b49\u8fd9\u51e0\u4e2a\nyaml\n\u6587\u4ef6,\n\u4e0b\u8f7d\u5de5\u5177\n, \u5b58\u653e\u5728/home/promote/Downloads\u76ee\u5f55\u4e0b\n\n\n\u7136\u540e\u66f4\u6539\n20-default.list\n\u4e3a\n\n\n# os-specific listings first\nyaml file:///home/promote/Downloads/2021-04-01-14-01-21-master-osx-homebrew.yaml osx\n\n# generic\nyaml file:///home/promote/Downloads/2021-04-01-14-02-25-master-base.yaml\nyaml file:///home/promote/Downloads/2021-04-01-14-05-35-master-python.yaml\nyaml file:///home/promote/Downloads/2021-04-01-14-06-50-master-ruby.yaml\ngbpdistro file:///home/promote/Downloads/2021-04-01-14-07-41-master-fuerte.yaml fuerte\n\n# newer distributions (Groovy, Hydro, ...) must not be listed anymore, they are being fetched from the rosdistro index.yaml instead\n\n\n\n\n\u8fd8\u9700\u8981\u4f7f\u7528\nmate-search-tool\n \u5728\n/usr/lib/\n\u76ee\u5f55\u4e0b\u627e\u5230\u5305\u542b\nDEFAULT_INDEX_URL\n \u7684\npy\n\u6587\u4ef6\u3002\n/usr/lib/python2.7/dist-packages/rosdistro/__init__.py\n\n\n\u627e\u5230\u4ee3\u7801\u884c\n\n\nDEFAULT_INDEX_URL = 'https://raw.githubusercontent.com/ros/rosdistro/master/index-v4.yaml'\n\n\n\n\u540c\u6837\u6211\u4eec\u4e0b\u8f7d\nindex-v4.yaml\n\u6587\u4ef6\u81f3\n/home/promote/Downloads\n\uff0c\u628a\u6b64\u884c\u4ee3\u7801\u6539\u4e3a\n\n\n# DEFAULT_INDEX_URL = 'https://raw.githubusercontent.com/ros/rosdistro/master/index-v4.yaml'\nDEFAULT_INDEX_URL = 'file:/home/promote/Downloads/2021-04-01-14-51-42-rosdistro-index-v4.yaml'\n\n\n\n\n\u8fd9\u65f6\u518d\u8fd0\u884c\nrosdep update\n\n\n\u4f1a\u63d0\u793a\nNo such file or directory: '/home/promote/Downloads/dashing/distribution.yaml'\n\n\n\u6211\u4eec\u9700\u8981\u4e0b\u8f7d\nhttps://raw.githubusercontent.com/ros/rosdistro/master/dashing/distribution.yaml\n\n\n\u5728\n/home/promote/Downloads\n\u76ee\u5f55\u4e0b\u65b0\u5efa\u6587\u4ef6\u5939\ndashing\n\uff0c\u5e76\u628a\u4e0b\u8f7d\u7684\nyaml\n\u6587\u4ef6\u653e\u5165\ndashing\n\u6587\u4ef6\u5939\u4e0b\u91cd\u547d\u540d\u4e3a\ndistribution.yaml\n\n\n\u91cd\u590d\u4e0a\u4e00\u6b65\uff0c\u4f9d\u6b21\u4e0b\u8f7d\u5b8c\ndashing, kinetic, melodic, rolling, noetic, foxy\n\u7b49\n\n\n\u518d\u8fd0\u884c\nrosdep update\n\u5c31\u6210\u529f\u4e86\n\n\nros node \u6253\u5305\n\n\n\u5b89\u88c5\u4f9d\u8d56\n\n\nInstall \nbloom\n:\n\n\nsudo apt-get install python-bloom\n\n\n\nor (recommended)\n\n\nsudo pip install -U bloom\n\n\n\nInstall fakeroot:\n\n\nsudo apt-get install fakeroot\n\n\n\n\u51c6\u5907\n\n\nTo make a debian folder structure from the ROS package you must cd into the package to be in the same folder where \npackage.xml\n file is.\n\n\n\u751f\u6210debian\u5305\n\n\nbloom-generate rosdebian --os-name ubuntu --os-version trusty --ros-distro indigo\n\n\n\nros\u603b\u662f\u94fe\u63a5anaconda\u4e0b\u7684\u5e93\u6587\u4ef6\n\n\nexport LD_LIBRARY_PATH=\"\"\n\n\n\nUse parent frame_id Display\n\n\n\n\n\u5728launch\u6587\u4ef6\u91cc\u6dfb\u52a0\n\n\n\n\n<node pkg=\"tf\"  type=\"static_transform_publisher\" name=\"base_to_map\" args=\"1000 0 0 0 0 0 /base_link /map 50\" >\n\n<node pkg=\"ros2qt\" type=\"ros2qt\" name=\"ros2qt\" >\n    <rosparam>\n        use_sim_time: true\n    </rosparam>\n</node>\n<node pkg=\"ros2qt\" type=\"myclock.py\" name =\"myclock\"/>\n\n\n\nmyclock.py\n\n\n#!/usr/bin/python\nimport rospy\nfrom rosgraph_msgs.msg import Clock\nimport time\n\ndef simtime_talker():\n    rospy.set_param('use_sim_time',False)\n    pub1 = rospy.Publisher('clock',Clock, queue_size=10)\n    rospy.init_node('talker', anonymous=True)\n    rate = rospy.Rate(10) # 10hz\n    sim_speed_multiplier = 10    \n    sim_clock = Clock()\n    zero_time = rospy.get_time()\n    while not rospy.is_shutdown():\n        sim_clock.clock = rospy.Time.from_sec(sim_speed_multiplier*(rospy.get_time() - zero_time))\n        rospy.loginfo(sim_clock)\n        pub1.publish(sim_clock)\n        rate.sleep()\nif __name__ == '__main__':\n    try:\n        simtime_talker()\n    except rospy.ROSInterruptException:\n        pass\n\n\n\n\n\n\u5728\u7ec8\u7aef\u5efa\u7acb\u7236\u5b50\u5173\u7cfb\n\n\n\n\nrosrun tf stat_transform_publisher 0 0 1 0 0 0 1 base_link rslidar 50\n\n\n\n\n\n\u5373\u53ef\u5728rviz\u91cc\u8bbe\u7f6e\nfixed frame\n \u4e3a \nbase_link\n,\u89c2\u770b\u70b9\u4e91\u6570\u636e\n\n\n\n\n\u5982\u4f55\u53d1\u5e03tf\u8f6c\u6362\u5173\u7cfb\n\n\n\n\nUse \nstatic_transform_publisher\n -- this one is a ready to use node, that you can run from command line.\n\n\n\n\nrosrun tf stat_transform_publisher 0 0 1 0 0 0  world rslidar 50\n\n\n\n\n\nUse \ntf::TransformBroadcaster\n -- to create transforms in a code.\n\n\n\n\n\nvoid QNode::biaodingCallback(const PointCloud::ConstPtr& msg)\n{\n    frame_name = msg->header.frame_id;\n    static tf::TransformBroadcaster br;\n    tf::Transform transform;\n    transform.setOrigin( tf::Vector3(x_pianyi, y_pianyi, z_pianyi) );\n    tf::Quaternion q;\n    q.setRPY(x_xuanzhuan, y_xuanzhuan, z_xuanzhuan);\n    transform.setRotation(q);\n    br.sendTransform(tf::StampedTransform(transform, ros::Time::now(), \"base_link\", frame_name));\n}\n\n\n\nroslaunch \u5305\u542b roslaunch\u6587\u4ef6\n\n\n<include file=\"$(find robot_description)/launch/urdf_test.launch\" /> \n\n\n\nroslaunch \u5305\u542byaml \u6587\u4ef6\n\n\n<rosparam file=\"$(find ldtest)/config/para.yaml\" command=\"load\" />\n\n\n\npara.yaml\n\n\nx_offset: -1.45\ny_offset: 0.56\nz_offset: 2.95\nx_jiaodu: -1.5\ny_jiaodu: 44.3\nz_jiaodu: 4.0\nstone2car_x: -1.25\nstone2car_y: 5.35\nstone2car_z: -1.0\ndipan_init_jiaodu: 91.4\nremote: true\n\n\n\nroslaunch py node\n\n\n<node name=\"LidarTool\" pkg=\"ldtest\"\n    type=\"LidarTool.py\" required=\"false\"\n    output=\"screen\"\n/>",
            "title": "Ros"
        },
        {
            "location": "/Ros/#ros-create-msg",
            "text": "\u5728node\u8282\u70b9\u6587\u4ef6\u5939\u4e0b\u521b\u5efamsg\u6587\u4ef6\u5939  \u5728msg\u6587\u4ef6\u5939\u4e0b\u521b\u5efa **.msg\u6587\u4ef6   detect.msg  bool begin\nstring first_name\nstring last_name\nuint8 age\nuint32 score\nint64 stateLabel\nint64 CIPV\nint64 trackID\nint64 trackFrameNum\nint64 obstacleType\nfloat32 real3DRightX\nfloat32 real3DLeftX\nfloat32 width\nfloat32 real3DLowY\nfloat32 real3DUpY\nfloat32 height\nfloat32 HMW\nfloat32 fuzzyCollisionTimeZ\nfloat32 longitudinalZ\nfloat32 speedOfLongitudinal\nfloat32 lateralX\nfloat32 speedOfLateral\nfloat32 currentSpeed  detect_array.msg  dectect []  package.xml  <build_depend>message_generation</build_depend>\n<exec_depend>message_runtime</exec_depend>  CMakelists.txt  find_package(catkin REQUIRED COMPONENTS\n   roscpp\n   rospy\n   std_msgs\n   message_generation\n)\n\ncatkin_package(\n  ...\n  CATKIN_DEPENDS message_runtime ...\n  ...)\n\nadd_message_files(\n  FILES\n  detect.msg\n  detect_array.msg\n)\n\n\ngenerate_messages(\n  DEPENDENCIES\n  std_msgs\n)   \u5305\u542b\u5934\u6587\u4ef6   #include \"smart_eye/detect.h\"\n#include \"smart_eye/detect_array.h\"\n\nmsg_pub = nh_.advertise<smart_eye::detect_array>(\"detect_array\", 10);\n\nsmart_eye::detect msg;\nsmart_eye::detect_array msg_array;\nmsg.stateLabel = stateLabel;\nmsg.CIPV = CIPV;\nmsg.trackID = trackID;\nmsg.trackFrameNum = trackFrameNum;\nmsg.obstacleType = obstacleType;\nmsg.real3DRightX = real3DRightX;\nmsg.real3DLeftX = real3DLeftX;\nmsg.width = width;\nmsg.real3DLowY = real3DLowY;\nmsg.real3DUpY = real3DUpY;\nmsg.height = height;\nmsg.HMW = HMW;\nmsg.fuzzyCollisionTimeZ = fuzzyCollisionTimeZ;\nmsg.longitudinalZ = longitudinalZ;\nmsg.speedOfLongitudinal = speedOfLongitudinal;\nmsg.lateralX = lateralX;\nmsg.speedOfLateral = speedOfLateral;\nmsg.currentSpeed = currentSpeed;\nmsg_array.push(msg);\nmsg_pub.publish(msg_array);",
            "title": "Ros create msg"
        },
        {
            "location": "/Ros/#msg",
            "text": "\u9996\u5148cmakelists.txt  find_package(catkin REQUIRED COMPONENTS\n  message_generation\n  pcl_ros\n  roscpp\n  rospy\n  std_msgs\n  wjj\n)\n\ncatkin_package(\n INCLUDE_DIRS include\n CATKIN_DEPENDS message_runtime pcl_ros roscpp rospy std_msgs wjj\n)\n\nadd_dependencies(teaching \n${catkin_EXPORTED_TARGETS}\nwjj_gencpp)  \u518d\u8005package.xml   <build_depend>wjj</build_depend>\n <exec_depend>wjj</exec_depend>  \u6700\u540e\u4f7f\u7528  #include <wjj/SaeJ1939.h>",
            "title": "\u5f15\u7528\u5176\u4ed6\u5305\u7684\uff4d\uff53\uff47"
        },
        {
            "location": "/Ros/#sudo-rosdep-init-rosdep-update",
            "text": "\u8be5\u89e3\u51b3\u65b9\u6848\u662f\u9488\u5bf9\u7531\u4e8e\u4ee5\u4e0b\u4e24\u4e2a\u65e0\u6cd5\u6b63\u5e38\u8bbf\u95ee\uff0c\u4f46\u53ef\u4ee5ping\u901a\uff0c\u4e8e\u662f\u4fee\u6539hosts\u6587\u4ef6\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e24\u4e2a\u7f51\u5740\u7684IP\u5730\u5740\u5b9e\u73b0\u8bbf\u95ee\u3002  'sudo gedit /etc/hosts'  \u6dfb\u52a0  199.232.28.133  raw.githubusercontent.com\n151.101.228.133  raw.github.com  \u4fee\u6539\u5b8c\u6210\u540e\uff0c\u5728\u7ec8\u7aef\u6267\u884c  sudo rosdep init\nredep update",
            "title": "sudo rosdep init &amp;rosdep update\u5931\u8d25"
        },
        {
            "location": "/Ros/#vscode-python-ros-debug",
            "text": "\u9996\u5148  catkin_make -DCMAKE_BUILD_TYPE=DEBUG  \u5176\u6b21\u70b9\u51fbdebug\u6309\u626d,\u518d\u70b9\u51fb`create a launch.json file   \u9009\u62e9 Show   \u9009\u62e9  Add Configuration..   \u518d\u6dfb\u52a0  Ros Attach    launch.json  {\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"ROS: Attach\",\n            \"type\": \"ros\",\n            \"request\": \"attach\"\n        }\n\n    ]\n}  \uff44ebug\u65f6\u9047\u5230\u8def\u5f84\u95ee\u9898,\u4f8b\u5982rosmsg\u8def\u5f84\uff0c\u6700\u597d\u5728\u6587\u4ef6\u5c5e\u6027\u91cc\u590d\u5236\u8def\u5f84\uff0c\u4e0d\u7136\u5bb9\u6613\u51fa\u9519  import sys\nsys.path.append(\"/home/pmjd/Downloads/catkin_ws/devel/lib/python2.7/dist-packages\")",
            "title": "vscode python ros debug"
        },
        {
            "location": "/Ros/#error-package-teleop_twist_keyboard-not-found",
            "text": "You need to download the teleop_twist_keyboard from the github to your ~/catkin_ws/src folder. Steps:  1)  cd ~/catkin_ws/src \n2)  git clone https://github.com/ros-teleop/teleop_twist_keyboard",
            "title": "Error: package 'teleop_twist_keyboard' not found"
        },
        {
            "location": "/Ros/#spawn-service-failed-exiting",
            "text": "export ROS_MASTER_URI=http://promote-OMEN-by-HP-Laptop-17-cb1xxx:11311/",
            "title": "Spawn service failed. Exiting."
        },
        {
            "location": "/Ros/#createquaternionfromrpy",
            "text": "static geometry_msgs::Quaternion createQuaternionFromRPY(double roll, double pitch, double yaw) {\ngeometry_msgs::Quaternion q;\ndouble t0 = cos(yaw * 0.5);\ndouble t1 = sin(yaw * 0.5);\ndouble t2 = cos(roll * 0.5);\ndouble t3 = sin(roll * 0.5);\ndouble t4 = cos(pitch * 0.5);\ndouble t5 = sin(pitch * 0.5);\nq.w = t0 * t2 * t4 + t1 * t3 * t5;\nq.x = t0 * t3 * t4 - t1 * t2 * t5;\nq.y = t0 * t2 * t5 + t1 * t3 * t4;\nq.z = t1 * t2 * t4 - t0 * t3 * t5;\nreturn q;\n}",
            "title": "createQuaternionFromRPY"
        },
        {
            "location": "/Ros/#launch-rviz",
            "text": "<launch>\n  <node type=\"rviz\" name=\"rviz\" pkg=\"rviz\" args=\"-d $(find package_name)/rviz/config_file.rviz\" />\n</launch>",
            "title": "launch \u542f\u52a8 rviz"
        },
        {
            "location": "/Ros/#rosdep",
            "text": "\u9996\u5148 sudo rosdep init  \u8fd9\u4e00\u6b65\u4f1a\u5728 /etc/ros/rosdep/sources.list.d/ \u76ee\u5f55\u4e0b\u65b0\u5efa 20-default.list  # os-specific listings first\nyaml https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/osx-homebrew.yaml osx\n\n# generic\nyaml https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/base.yaml\nyaml https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/python.yaml\nyaml https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/ruby.yaml\ngbpdistro https://raw.githubusercontent.com/ros/rosdistro/master/releases/fuerte.yaml fuerte\n\n# newer distributions (Groovy, Hydro, ...) must not be listed anymore, they are being fetched from the rosdistro index.yaml instead  \u6211\u4eec\u9700\u8981\u4f9d\u6b21\u4e0b\u8f7d osx-homebrew.yaml \u7b49\u8fd9\u51e0\u4e2a yaml \u6587\u4ef6, \u4e0b\u8f7d\u5de5\u5177 , \u5b58\u653e\u5728/home/promote/Downloads\u76ee\u5f55\u4e0b  \u7136\u540e\u66f4\u6539 20-default.list \u4e3a  # os-specific listings first\nyaml file:///home/promote/Downloads/2021-04-01-14-01-21-master-osx-homebrew.yaml osx\n\n# generic\nyaml file:///home/promote/Downloads/2021-04-01-14-02-25-master-base.yaml\nyaml file:///home/promote/Downloads/2021-04-01-14-05-35-master-python.yaml\nyaml file:///home/promote/Downloads/2021-04-01-14-06-50-master-ruby.yaml\ngbpdistro file:///home/promote/Downloads/2021-04-01-14-07-41-master-fuerte.yaml fuerte\n\n# newer distributions (Groovy, Hydro, ...) must not be listed anymore, they are being fetched from the rosdistro index.yaml instead  \u8fd8\u9700\u8981\u4f7f\u7528 mate-search-tool  \u5728 /usr/lib/ \u76ee\u5f55\u4e0b\u627e\u5230\u5305\u542b DEFAULT_INDEX_URL  \u7684 py \u6587\u4ef6\u3002 /usr/lib/python2.7/dist-packages/rosdistro/__init__.py  \u627e\u5230\u4ee3\u7801\u884c  DEFAULT_INDEX_URL = 'https://raw.githubusercontent.com/ros/rosdistro/master/index-v4.yaml'  \u540c\u6837\u6211\u4eec\u4e0b\u8f7d index-v4.yaml \u6587\u4ef6\u81f3 /home/promote/Downloads \uff0c\u628a\u6b64\u884c\u4ee3\u7801\u6539\u4e3a  # DEFAULT_INDEX_URL = 'https://raw.githubusercontent.com/ros/rosdistro/master/index-v4.yaml'\nDEFAULT_INDEX_URL = 'file:/home/promote/Downloads/2021-04-01-14-51-42-rosdistro-index-v4.yaml'  \u8fd9\u65f6\u518d\u8fd0\u884c rosdep update  \u4f1a\u63d0\u793a No such file or directory: '/home/promote/Downloads/dashing/distribution.yaml'  \u6211\u4eec\u9700\u8981\u4e0b\u8f7d https://raw.githubusercontent.com/ros/rosdistro/master/dashing/distribution.yaml  \u5728 /home/promote/Downloads \u76ee\u5f55\u4e0b\u65b0\u5efa\u6587\u4ef6\u5939 dashing \uff0c\u5e76\u628a\u4e0b\u8f7d\u7684 yaml \u6587\u4ef6\u653e\u5165 dashing \u6587\u4ef6\u5939\u4e0b\u91cd\u547d\u540d\u4e3a distribution.yaml  \u91cd\u590d\u4e0a\u4e00\u6b65\uff0c\u4f9d\u6b21\u4e0b\u8f7d\u5b8c dashing, kinetic, melodic, rolling, noetic, foxy \u7b49  \u518d\u8fd0\u884c rosdep update \u5c31\u6210\u529f\u4e86",
            "title": "rosdep\u5931\u8d25"
        },
        {
            "location": "/Ros/#ros-node",
            "text": "",
            "title": "ros node \u6253\u5305"
        },
        {
            "location": "/Ros/#_1",
            "text": "Install  bloom :  sudo apt-get install python-bloom  or (recommended)  sudo pip install -U bloom  Install fakeroot:  sudo apt-get install fakeroot",
            "title": "\u5b89\u88c5\u4f9d\u8d56"
        },
        {
            "location": "/Ros/#_2",
            "text": "To make a debian folder structure from the ROS package you must cd into the package to be in the same folder where  package.xml  file is.",
            "title": "\u51c6\u5907"
        },
        {
            "location": "/Ros/#debian",
            "text": "bloom-generate rosdebian --os-name ubuntu --os-version trusty --ros-distro indigo",
            "title": "\u751f\u6210debian\u5305"
        },
        {
            "location": "/Ros/#rosanaconda",
            "text": "export LD_LIBRARY_PATH=\"\"",
            "title": "ros\u603b\u662f\u94fe\u63a5anaconda\u4e0b\u7684\u5e93\u6587\u4ef6"
        },
        {
            "location": "/Ros/#use-parent-frame_id-display",
            "text": "\u5728launch\u6587\u4ef6\u91cc\u6dfb\u52a0   <node pkg=\"tf\"  type=\"static_transform_publisher\" name=\"base_to_map\" args=\"1000 0 0 0 0 0 /base_link /map 50\" >\n\n<node pkg=\"ros2qt\" type=\"ros2qt\" name=\"ros2qt\" >\n    <rosparam>\n        use_sim_time: true\n    </rosparam>\n</node>\n<node pkg=\"ros2qt\" type=\"myclock.py\" name =\"myclock\"/>  myclock.py  #!/usr/bin/python\nimport rospy\nfrom rosgraph_msgs.msg import Clock\nimport time\n\ndef simtime_talker():\n    rospy.set_param('use_sim_time',False)\n    pub1 = rospy.Publisher('clock',Clock, queue_size=10)\n    rospy.init_node('talker', anonymous=True)\n    rate = rospy.Rate(10) # 10hz\n    sim_speed_multiplier = 10    \n    sim_clock = Clock()\n    zero_time = rospy.get_time()\n    while not rospy.is_shutdown():\n        sim_clock.clock = rospy.Time.from_sec(sim_speed_multiplier*(rospy.get_time() - zero_time))\n        rospy.loginfo(sim_clock)\n        pub1.publish(sim_clock)\n        rate.sleep()\nif __name__ == '__main__':\n    try:\n        simtime_talker()\n    except rospy.ROSInterruptException:\n        pass   \u5728\u7ec8\u7aef\u5efa\u7acb\u7236\u5b50\u5173\u7cfb   rosrun tf stat_transform_publisher 0 0 1 0 0 0 1 base_link rslidar 50   \u5373\u53ef\u5728rviz\u91cc\u8bbe\u7f6e fixed frame  \u4e3a  base_link ,\u89c2\u770b\u70b9\u4e91\u6570\u636e",
            "title": "Use parent frame_id Display"
        },
        {
            "location": "/Ros/#tf",
            "text": "Use  static_transform_publisher  -- this one is a ready to use node, that you can run from command line.   rosrun tf stat_transform_publisher 0 0 1 0 0 0  world rslidar 50   Use  tf::TransformBroadcaster  -- to create transforms in a code.   \nvoid QNode::biaodingCallback(const PointCloud::ConstPtr& msg)\n{\n    frame_name = msg->header.frame_id;\n    static tf::TransformBroadcaster br;\n    tf::Transform transform;\n    transform.setOrigin( tf::Vector3(x_pianyi, y_pianyi, z_pianyi) );\n    tf::Quaternion q;\n    q.setRPY(x_xuanzhuan, y_xuanzhuan, z_xuanzhuan);\n    transform.setRotation(q);\n    br.sendTransform(tf::StampedTransform(transform, ros::Time::now(), \"base_link\", frame_name));\n}",
            "title": "\u5982\u4f55\u53d1\u5e03tf\u8f6c\u6362\u5173\u7cfb"
        },
        {
            "location": "/Ros/#roslaunch-roslaunch",
            "text": "<include file=\"$(find robot_description)/launch/urdf_test.launch\" />",
            "title": "roslaunch \u5305\u542b roslaunch\u6587\u4ef6"
        },
        {
            "location": "/Ros/#roslaunch-yaml",
            "text": "<rosparam file=\"$(find ldtest)/config/para.yaml\" command=\"load\" />  para.yaml  x_offset: -1.45\ny_offset: 0.56\nz_offset: 2.95\nx_jiaodu: -1.5\ny_jiaodu: 44.3\nz_jiaodu: 4.0\nstone2car_x: -1.25\nstone2car_y: 5.35\nstone2car_z: -1.0\ndipan_init_jiaodu: 91.4\nremote: true",
            "title": "roslaunch \u5305\u542byaml \u6587\u4ef6"
        },
        {
            "location": "/Ros/#roslaunch-py-node",
            "text": "<node name=\"LidarTool\" pkg=\"ldtest\"\n    type=\"LidarTool.py\" required=\"false\"\n    output=\"screen\"\n/>",
            "title": "roslaunch py node"
        },
        {
            "location": "/Ros\u5efa\u6a21/",
            "text": "Ros\u5efa\u6a21\n\n\nrobot_model\n\n\nCreate your own urdf file\n\n\ndescription: in this tutorial you start creating your own urdf robot description file.\n\n\n\n\nCreate the tree structure\n  in this tutorial we will create the URDF description of the \"robot\" shown in the image below.\n  \n\n  The robot in the image is a tree structure.Let's start very simple,and create a description of that tree structure,without worrying about the dimensions etc.Fire up your favorite text editor,and create a file called my_robot.urdf:\n\n\n\n\n   1 <robot name=\"test_robot\">\n   2   <link name=\"link1\" />\n   3   <link name=\"link2\" />\n   4   <link name=\"link3\" />\n   5   <link name=\"link4\" />\n   6 \n   7   <joint name=\"joint1\" type=\"continuous\">\n   8     <parent link=\"link1\"/>\n   9     <child link=\"link2\"/>\n  10   </joint>\n  11 \n  12   <joint name=\"joint2\" type=\"continuous\">\n  13     <parent link=\"link1\"/>\n  14     <child link=\"link3\"/>\n  15   </joint>\n  16 \n  17   <joint name=\"joint3\" type=\"continuous\">\n  18     <parent link=\"link3\"/>\n  19     <child link=\"link4\"/>\n  20   </joint>\n  21 </robot>\n\n\n\nSo,just creating the structure is very simple.Now let's see if can get this urdf file parsed.There is a simple command line tool that will parse a urdf file for you, and tell you if the syntax is correct.\nyou might need to install, urdfdom as an upstream,ROS independent package:\\\n\n\nsudo apt-get install liburdfdom-tools\n\n\n\nNow run the check command:\n\n\nrosmake urdfdom_model\ncheck_urdf my_robot.urdf   \n\n\n\nAdd the dimensions\n\n\nSo now that we have the basic tree structure.let's add the appropriate dimensions.As you notice in the robot image,the reference frame of each link (in green) is located at the bottom of the link.and is identical to the reference frame of the joint.So,to add dimensions to our tree,all we habe to specify is the offset from a link to the joint(s) of its children.To accomplish this,we will add the field\n to each of the joints.\nLet's look at the second joint.Joint2 is offset in the Y-direction from link1,a little offset in the negative X-direction from link1, and it is rotated 90 degrees around the Z-axis.So,we need to add the following\n element:\n\n\n<origin xyz=\"-2 5 0\" rpy=\"0 0 1.57\"/>\n\n\n\nIf you repeat this for all the elements our URDF will look like this:\n\n\n   1 <robot name=\"test_robot\">\n   2   <link name=\"link1\" />\n   3   <link name=\"link2\" />\n   4   <link name=\"link3\" />\n   5   <link name=\"link4\" />\n   6 \n   7 \n   8   <joint name=\"joint1\" type=\"continuous\">\n   9     <parent link=\"link1\"/>\n  10     <child link=\"link2\"/>\n  11     <origin xyz=\"5 3 0\" rpy=\"0 0 0\" />\n  12   </joint>\n  13 \n  14   <joint name=\"joint2\" type=\"continuous\">\n  15     <parent link=\"link1\"/>\n  16     <child link=\"link3\"/>\n  17     <origin xyz=\"-2 5 0\" rpy=\"0 0 1.57\" />\n  18   </joint>\n  19 \n  20   <joint name=\"joint3\" type=\"continuous\">\n  21     <parent link=\"link3\"/>\n  22     <child link=\"link4\"/>\n  23     <origin xyz=\"5 0 0\" rpy=\"0 0 -1.57\" />\n  24   </joint>\n  25 </robot>\n\n\n\nCompleting the Kinematics\n\n\nWhat we didn't specify yet is around which axis the points rotate.Once we add that,we actually have a full kinematic model of this robot!All we need to do is add the \n element to each joint.The axis specifies the rotational axis in the local frame.\nSo, If you look at joint2, you see it rotates around the positive Y-axis.So, simple add the following xml to the joint element:\n\n\n<axis xyz=\"0 1 0\" />\n\n\n\nSimilarly,joint1 is rotating around the following axis:\n\n\n<axis xyz=\"-0.707 0.707 0\"/>\n\n\n\nNote that it is a good idea to normalize the axis.\nIf we add this to all the joints of the robot, our URDF looks like this:\n\n\n   1 <robot name=\"test_robot\">\n   2   <link name=\"link1\" />\n   3   <link name=\"link2\" />\n   4   <link name=\"link3\" />\n   5   <link name=\"link4\" />\n   6 \n   7   <joint name=\"joint1\" type=\"continuous\">\n   8     <parent link=\"link1\"/>\n   9     <child link=\"link2\"/>\n  10     <origin xyz=\"5 3 0\" rpy=\"0 0 0\" />\n  11     <axis xyz=\"-0.9 0.15 0\" />\n  12   </joint>\n  13 \n  14   <joint name=\"joint2\" type=\"continuous\">\n  15     <parent link=\"link1\"/>\n  16     <child link=\"link3\"/>\n  17     <origin xyz=\"-2 5 0\" rpy=\"0 0 1.57\" />\n  18     <axis xyz=\"-0.707 0.707 0\" />\n  19   </joint>\n  20 \n  21   <joint name=\"joint3\" type=\"continuous\">\n  22     <parent link=\"link3\"/>\n  23     <child link=\"link4\"/>\n  24     <origin xyz=\"5 0 0\" rpy=\"0 0 -1.57\" />\n  25     <axis xyz=\"0.707 -0.707 0\" />\n  26   </joint>\n  27 </robot>\n\n\n\nUpdate your file my_robot.urdf and run it through the parser.\n\n\ncheck_urdf my_robot.urdf\n\n\n\nThat's it.you created your first URDF robot description!Now you can try to visualize the URDF using graphiz:\n\n\nurdf_to_graphiz my_robot.urdf\n\n\n\nand open the generated file with your favorite pdf viewer:\n\n\nevince test_robot.pdf\n\n\n\n\n\nParse a urdf file\n\n\nDescription: This tutorial teachs you how to use the urdf parser\n\n\nReading a URDF file\n\n\nThis tutorial starts off where the previous one ended.You should still habe your my_robot.urdf file with a description of the robot shown before below.\nLet's first create a package with a dependency on the urdf parser in our snadbox:\n\n\ncd ~/catkin_ws/src\ncatkin_create_pkg robot_description urdf roscpp rospy tf sensor_msgs std_msgs\ncd robot_description\n\n\n\nNow create a /urdf folder to store the urdf file we just created:\n\n\nmkdir urdf\ncd urdf\n\n\n\nThis follows the convention of always storing your robot's URDF file in a ROS package named MYROBOT_description and within a subfolder named /urdf.Other standard subfolders of your robot's description package include /meshes, /media and /cad,like so:\\\n\n\n/MYROBOT_description\n  package.xml\n  CMakeLists.txt\n  /urdf\n  /meshes\n  /materials\n  /cad\n\n\n\nUsing the robot state publisher on your own robot\n\n\nDescription: This tutorial explains how you can publish the state of your robot to tf, using the robot state publisher.\n\n\nWhen you are working with a robot that has many relevant frames.it becomes quite a task to publish them all to tf.The robot state publisher is a tool that will do this job for you.\n\n\nThe robot state publisher helps you to broadcast the state of your robot to the tf transform library.The robot state publisher internally has a kinematic\u8fd0\u52a8\u5b66 model of the robot; so given the joint positions of the robot,the robot state publisher can compute and broadcast the 3D pose of each link in the robot.\nYou can use the robot state publisher as a standalone ROS node or as a library:\n\n\n1.Running as a ROS node\n\n\n1.1 robot_state_pubisher\n\n\nThe easiest way to run the robot state publisher is as a node.For normal users,this is the recommanded usage.You need two things to run the robot state publisher:\n\n\n\n\n\n\na urdf xml robot description loaded on the Parameter Server.\n\n\nA source that publishes the joint positions as a sensor_msgs/JointState\n\n\n\n\n\n\n1.1.1 Subscribed topics\n\n\njoint_states(Sensor_msgs/JointState)\n\n\n\n\njoint position information\n\n\n\n\n1.1.2 Parameters\n\n\nrobot_description(urdf map)tf_prefix(string)\n\n\n\n\nSet the tf prefix for namespace-aware publishing of transform\npublish_frequency(double)\nPublish frequency of state publisher,default:50Hz\n\n\n\n\n1.2 Example launch file\n\n\n<launch>\n  <param name = \"robot_description\" textfile = \"$(find mypackage)/urdf/robotmodel.xml\"/>\n  <node pkg = \"robot_state_publisher\" type=\"robot_state_publisher\" name = \"rob_st_pub\">\n  <remap from=\"robot_state_publisher\" to=\"my_robot_description\"/>\n  <remap from=\"joint_states\" to=\"different_joint_states\"/>\n  </node>\n</launch>\n\n\n\n2.Runing as a library\n\n\nAdvanced users can also run the robot state publisher as a library,from within their own c++ code.After you include the header:\n\n#include <robot_state_publisher/robot_state_publisher.h>\n\nall you need is the constructor which takes in a KDL tree\n\nRobotStatePublisher(const KDL::Tree& tree);\n\nand now, everytime you want to publish the state of your robot, you call the publishTransforms functions:\n\n\n//Publish moving joints\nvoid publishTransfroms(const std::map<std::string, double>& joint_positions,\n  const ros::Time& time);\n\n//publish fixed joints\nvoid publishFixedTransforms();\n\n\n\nThe first argument is a map with joint names and joint positions, and the second argument is the time at which the joint positions were recorded. It is okay if the map does not contain all the joint names. It is also okay if the map contains some joints names that are not part of the kinematic model. But note if you don't tell the joint state publisher about some of the joints in your kinematic model, then your tf tree will not be complete.\n\n\nUsing urdf with robot_state_publisher\n\n\nCreate the URDF File\n\n\nPublishing the State\n\n\n\n\ncd %TOP_DIR_YOUR_CATKIN_WS%/src\n\n\n\n\nThen fire your favourite editor and paste the following code into the src/state_publisher.cpp file:\n\n\n   1 #include <string>\n   2 #include <ros/ros.h>\n   3 #include <sensor_msgs/JointState.h>\n   4 #include <tf/transform_broadcaster.h>\n   5 \n   6 int main(int argc, char** argv) {\n   7     ros::init(argc, argv, \"state_publisher\");\n   8     ros::NodeHandle n;\n   9     ros::Publisher joint_pub = n.advertise<sensor_msgs::JointState>(\"joint_states\", 1);\n  10     tf::TransformBroadcaster broadcaster;\n  11     ros::Rate loop_rate(30);\n  12 \n  13     const double degree = M_PI/180;\n  14 \n  15     // robot state\n  16     double tilt = 0, tinc = degree, swivel=0, angle=0, height=0, hinc=0.005;\n  17 \n  18     // message declarations\n  19     geometry_msgs::TransformStamped odom_trans;\n  20     sensor_msgs::JointState joint_state;\n  21     odom_trans.header.frame_id = \"odom\";\n  22     odom_trans.child_frame_id = \"axis\";\n  23 \n  24     while (ros::ok()) {\n  25         //update joint_state\n  26         joint_state.header.stamp = ros::Time::now();\n  27         joint_state.name.resize(3);\n  28         joint_state.position.resize(3);\n  29         joint_state.name[0] =\"swivel\";\n  30         joint_state.position[0] = swivel;\n  31         joint_state.name[1] =\"tilt\";\n  32         joint_state.position[1] = tilt;\n  33         joint_state.name[2] =\"periscope\";\n  34         joint_state.position[2] = height;\n  35 \n  36 \n  37         // update transform\n  38         // (moving in a circle with radius=2)\n  39         odom_trans.header.stamp = ros::Time::now();\n  40         odom_trans.transform.translation.x = cos(angle)*2;\n  41         odom_trans.transform.translation.y = sin(angle)*2;\n  42         odom_trans.transform.translation.z = .7;\n  43         odom_trans.transform.rotation = tf::createQuaternionMsgFromYaw(angle+M_PI/2);\n  44 \n  45         //send the joint state and transform\n  46         joint_pub.publish(joint_state);\n  47         broadcaster.sendTransform(odom_trans);\n  48 \n  49         // Create new robot state\n  50         tilt += tinc;\n  51         if (tilt<-.5 || tilt>0) tinc *= -1;\n  52         height += hinc;\n  53         if (height>.2 || height<0) hinc *= -1;\n  54         swivel += degree;\n  55         angle += degree/4;\n  56 \n  57         // This will adjust as needed per iteration\n  58         loop_rate.sleep();\n  59     }\n  60 \n  61 \n  62     return 0;\n  63 }\n\n\n\nLaunch File\n\n\nThis launch file assumes you are using the package name \"r2d2\" and node name \"state_publisher\" and you have saved this urdf to the \"r2d2\" package.\n\n\n   1 <launch>\n   2         <param name=\"robot_description\" command=\"cat $(find r2d2)/model.xml\" />\n   3         <node name=\"robot_state_publisher\" pkg=\"robot_state_publisher\" type=\"robot_state_publisher\" />\n   4         <node name=\"state_publisher\" pkg=\"robot_description\" type=\"state_publisher\" />\n   5 </launch>\n\n\n\nViewing the Reult\n\n\nFirst we have to edit the CMakeLists.txt in the package where we saved the above source code.Make sure to add the tf dependency in addition to the other dependencies:\n\n\n\n\nfind_package(catkin REQUIRED COMPONENTS roscpp rospy std_msgs tf)\n\n\n\n\nNotice that roscpp is used to parse the code that we wrote and generate the state_publisher node. We also have to add the following to the end of the CMakelists.txt in order to generate the state_publisher node:\n\n\n\n\ninclude_directories(include ${catkin_INCLUDE_DIRS})\\\nadd_executable(state_publisher src/state_publisher.cpp)\\\ntarget_link_libraries(state_publisher ${catkin_LIBRARIES})\n\n\n\n\nNow we should go to the directory of the workspace and build it using:\n\n\n\n\ncatkin_make\n\n\n\n\nNow launch the package (assuming that our launch file is named display.launch):\n\n\n\n\nroslaunch r2d2 display.launch\n\n\n\n\nRun rviz in a new terminal using:\n\n\n\n\nrosrun rviz rviz\n\n\n\n\nChoose odom as your fixed frame (under Global Options). Then choose \"Add Display\" and add a Robot Model Display and a TF Display\n\n\n\nBuilding a Visual Robot Model with URDF from Scratch\n\n\nDescription:learn how to build a visual model of a robot that you can view in rviz\n\n\nBefore continuing,make sure you have the joint_state_publisher package installed.\n\n\n\n\nsudo apt-get install ros-melodic-joint-state-publisher\n\n\n\n\n1.One Shape\n\n\nFirst, we're just going to explore one simple shape.Here's about as simple as a urdf as you can make.\n\n\n<?xml version=\"1.0\"?>\n<robot name=\"myfirst\">\n  <link name=\"base_link\">\n    <visual>\n      <geometry>\n        <cylinder length=\"0.6\" radius=\"0.2\"/>\n      </geometry>\n    </visual>\n  </link>\n</robot>\n\n\n\nTo translate the XML into English, this is a robot with the name myfirst, that contains only one link (a.k.a. part), whose visual component is just a cylinder 0.6 meters long with a 0.2 meter radius. This may seem like a lot of enclosing tags for a simple \u201chello world\u201d type example, but it will get more complicated, trust me.\\\n\n\nGazebo \u5b89\u88c5\n\n\nsudo apt-get install ros-melodic-gazebo-ros-pkgs ros-melodic-gazebo-ros-control\n\n\nros link \u63d0\u793a\u6ca1\u6709\u5305\u542b\u5728gazebo\n\n\n\u53ef\u80fd\u6709\u4e24\u4e2a\u539f\u56e0\uff0c\u4e00\u4e2a\u662f\u5b83\u81ea\u8eab\u6ca1\u6709\u7269\u7406\u5c5e\u6027\uff0c\u53e6\u5916\u4e00\u4e2a\u662f\u5b83\u7684\u8fde\u63a5\u5bf9\u8c61\u6ca1\u6709\u7269\u7406\u5c5e\u6027\n\n\nDenavit-Hartenberg parameters\n\n\nIn mechanical engineering, the Denavit-Hartenberg parameters(also called DH parameters) are the four parameters associated with a particular convention for attaching reference frames to the links of a spatial kinematic chain or robot manipulator.\n\n\nJacques Denavit and Richard Hartenberg introduced this convention in 1955 in order to standardlize the coordinate frames for spatial linkages.\n\n\nDenavit-Hartenberg convention\n\nA commonly used convention for selecting frames of reference in robotics applications is Denavit and Hartenberg(D-H) convention. In this convention, coordinate frames are attached to the joints between two links such that one transformation is associated with the joint,[Z], and the second is associated with the link,[X]. The coordinate transformations along a serial robot consisting of n links from the kinematics equations of the robot.\n\n\n\n\nwhere [T] is the transformation locating the end-link.\n\n\nIn order to determine the coordinate transformations [Z] and [X], the joints connecting the links are modeled as either hinged or sliding joints, each of which have a unique line S in space that forms the joint axis and define the relative movement of the two links.A typical serial robot is characterized by a sequence of six lines Si,i=1,......6, one for each joint in the robot.For each sequence of lines Si and Si+1, there is a common normal line Ai,j+1.The system of six joint axes Si",
            "title": "Ros\u5efa\u6a21"
        },
        {
            "location": "/Ros\u5efa\u6a21/#ros",
            "text": "robot_model",
            "title": "Ros\u5efa\u6a21"
        },
        {
            "location": "/Ros\u5efa\u6a21/#create-your-own-urdf-file",
            "text": "",
            "title": "Create your own urdf file"
        },
        {
            "location": "/Ros\u5efa\u6a21/#description-in-this-tutorial-you-start-creating-your-own-urdf-robot-description-file",
            "text": "Create the tree structure\n  in this tutorial we will create the URDF description of the \"robot\" shown in the image below.\n   \n  The robot in the image is a tree structure.Let's start very simple,and create a description of that tree structure,without worrying about the dimensions etc.Fire up your favorite text editor,and create a file called my_robot.urdf:      1 <robot name=\"test_robot\">\n   2   <link name=\"link1\" />\n   3   <link name=\"link2\" />\n   4   <link name=\"link3\" />\n   5   <link name=\"link4\" />\n   6 \n   7   <joint name=\"joint1\" type=\"continuous\">\n   8     <parent link=\"link1\"/>\n   9     <child link=\"link2\"/>\n  10   </joint>\n  11 \n  12   <joint name=\"joint2\" type=\"continuous\">\n  13     <parent link=\"link1\"/>\n  14     <child link=\"link3\"/>\n  15   </joint>\n  16 \n  17   <joint name=\"joint3\" type=\"continuous\">\n  18     <parent link=\"link3\"/>\n  19     <child link=\"link4\"/>\n  20   </joint>\n  21 </robot>  So,just creating the structure is very simple.Now let's see if can get this urdf file parsed.There is a simple command line tool that will parse a urdf file for you, and tell you if the syntax is correct.\nyou might need to install, urdfdom as an upstream,ROS independent package:\\  sudo apt-get install liburdfdom-tools  Now run the check command:  rosmake urdfdom_model\ncheck_urdf my_robot.urdf",
            "title": "description: in this tutorial you start creating your own urdf robot description file."
        },
        {
            "location": "/Ros\u5efa\u6a21/#add-the-dimensions",
            "text": "So now that we have the basic tree structure.let's add the appropriate dimensions.As you notice in the robot image,the reference frame of each link (in green) is located at the bottom of the link.and is identical to the reference frame of the joint.So,to add dimensions to our tree,all we habe to specify is the offset from a link to the joint(s) of its children.To accomplish this,we will add the field  to each of the joints.\nLet's look at the second joint.Joint2 is offset in the Y-direction from link1,a little offset in the negative X-direction from link1, and it is rotated 90 degrees around the Z-axis.So,we need to add the following  element:  <origin xyz=\"-2 5 0\" rpy=\"0 0 1.57\"/>  If you repeat this for all the elements our URDF will look like this:     1 <robot name=\"test_robot\">\n   2   <link name=\"link1\" />\n   3   <link name=\"link2\" />\n   4   <link name=\"link3\" />\n   5   <link name=\"link4\" />\n   6 \n   7 \n   8   <joint name=\"joint1\" type=\"continuous\">\n   9     <parent link=\"link1\"/>\n  10     <child link=\"link2\"/>\n  11     <origin xyz=\"5 3 0\" rpy=\"0 0 0\" />\n  12   </joint>\n  13 \n  14   <joint name=\"joint2\" type=\"continuous\">\n  15     <parent link=\"link1\"/>\n  16     <child link=\"link3\"/>\n  17     <origin xyz=\"-2 5 0\" rpy=\"0 0 1.57\" />\n  18   </joint>\n  19 \n  20   <joint name=\"joint3\" type=\"continuous\">\n  21     <parent link=\"link3\"/>\n  22     <child link=\"link4\"/>\n  23     <origin xyz=\"5 0 0\" rpy=\"0 0 -1.57\" />\n  24   </joint>\n  25 </robot>",
            "title": "Add the dimensions"
        },
        {
            "location": "/Ros\u5efa\u6a21/#completing-the-kinematics",
            "text": "What we didn't specify yet is around which axis the points rotate.Once we add that,we actually have a full kinematic model of this robot!All we need to do is add the   element to each joint.The axis specifies the rotational axis in the local frame.\nSo, If you look at joint2, you see it rotates around the positive Y-axis.So, simple add the following xml to the joint element:  <axis xyz=\"0 1 0\" />  Similarly,joint1 is rotating around the following axis:  <axis xyz=\"-0.707 0.707 0\"/>  Note that it is a good idea to normalize the axis.\nIf we add this to all the joints of the robot, our URDF looks like this:     1 <robot name=\"test_robot\">\n   2   <link name=\"link1\" />\n   3   <link name=\"link2\" />\n   4   <link name=\"link3\" />\n   5   <link name=\"link4\" />\n   6 \n   7   <joint name=\"joint1\" type=\"continuous\">\n   8     <parent link=\"link1\"/>\n   9     <child link=\"link2\"/>\n  10     <origin xyz=\"5 3 0\" rpy=\"0 0 0\" />\n  11     <axis xyz=\"-0.9 0.15 0\" />\n  12   </joint>\n  13 \n  14   <joint name=\"joint2\" type=\"continuous\">\n  15     <parent link=\"link1\"/>\n  16     <child link=\"link3\"/>\n  17     <origin xyz=\"-2 5 0\" rpy=\"0 0 1.57\" />\n  18     <axis xyz=\"-0.707 0.707 0\" />\n  19   </joint>\n  20 \n  21   <joint name=\"joint3\" type=\"continuous\">\n  22     <parent link=\"link3\"/>\n  23     <child link=\"link4\"/>\n  24     <origin xyz=\"5 0 0\" rpy=\"0 0 -1.57\" />\n  25     <axis xyz=\"0.707 -0.707 0\" />\n  26   </joint>\n  27 </robot>  Update your file my_robot.urdf and run it through the parser.  check_urdf my_robot.urdf  That's it.you created your first URDF robot description!Now you can try to visualize the URDF using graphiz:  urdf_to_graphiz my_robot.urdf  and open the generated file with your favorite pdf viewer:  evince test_robot.pdf",
            "title": "Completing the Kinematics"
        },
        {
            "location": "/Ros\u5efa\u6a21/#parse-a-urdf-file",
            "text": "",
            "title": "Parse a urdf file"
        },
        {
            "location": "/Ros\u5efa\u6a21/#description-this-tutorial-teachs-you-how-to-use-the-urdf-parser",
            "text": "",
            "title": "Description: This tutorial teachs you how to use the urdf parser"
        },
        {
            "location": "/Ros\u5efa\u6a21/#reading-a-urdf-file",
            "text": "This tutorial starts off where the previous one ended.You should still habe your my_robot.urdf file with a description of the robot shown before below.\nLet's first create a package with a dependency on the urdf parser in our snadbox:  cd ~/catkin_ws/src\ncatkin_create_pkg robot_description urdf roscpp rospy tf sensor_msgs std_msgs\ncd robot_description  Now create a /urdf folder to store the urdf file we just created:  mkdir urdf\ncd urdf  This follows the convention of always storing your robot's URDF file in a ROS package named MYROBOT_description and within a subfolder named /urdf.Other standard subfolders of your robot's description package include /meshes, /media and /cad,like so:\\  /MYROBOT_description\n  package.xml\n  CMakeLists.txt\n  /urdf\n  /meshes\n  /materials\n  /cad",
            "title": "Reading a URDF file"
        },
        {
            "location": "/Ros\u5efa\u6a21/#using-the-robot-state-publisher-on-your-own-robot",
            "text": "",
            "title": "Using the robot state publisher on your own robot"
        },
        {
            "location": "/Ros\u5efa\u6a21/#description-this-tutorial-explains-how-you-can-publish-the-state-of-your-robot-to-tf-using-the-robot-state-publisher",
            "text": "When you are working with a robot that has many relevant frames.it becomes quite a task to publish them all to tf.The robot state publisher is a tool that will do this job for you. \nThe robot state publisher helps you to broadcast the state of your robot to the tf transform library.The robot state publisher internally has a kinematic\u8fd0\u52a8\u5b66 model of the robot; so given the joint positions of the robot,the robot state publisher can compute and broadcast the 3D pose of each link in the robot.\nYou can use the robot state publisher as a standalone ROS node or as a library:",
            "title": "Description: This tutorial explains how you can publish the state of your robot to tf, using the robot state publisher."
        },
        {
            "location": "/Ros\u5efa\u6a21/#1running-as-a-ros-node",
            "text": "",
            "title": "1.Running as a ROS node"
        },
        {
            "location": "/Ros\u5efa\u6a21/#11-robot_state_pubisher",
            "text": "The easiest way to run the robot state publisher is as a node.For normal users,this is the recommanded usage.You need two things to run the robot state publisher:    a urdf xml robot description loaded on the Parameter Server.  A source that publishes the joint positions as a sensor_msgs/JointState",
            "title": "1.1 robot_state_pubisher"
        },
        {
            "location": "/Ros\u5efa\u6a21/#111-subscribed-topics",
            "text": "joint_states(Sensor_msgs/JointState)   joint position information",
            "title": "1.1.1 Subscribed topics"
        },
        {
            "location": "/Ros\u5efa\u6a21/#112-parameters",
            "text": "robot_description(urdf map)tf_prefix(string)   Set the tf prefix for namespace-aware publishing of transform\npublish_frequency(double)\nPublish frequency of state publisher,default:50Hz",
            "title": "1.1.2 Parameters"
        },
        {
            "location": "/Ros\u5efa\u6a21/#12-example-launch-file",
            "text": "<launch>\n  <param name = \"robot_description\" textfile = \"$(find mypackage)/urdf/robotmodel.xml\"/>\n  <node pkg = \"robot_state_publisher\" type=\"robot_state_publisher\" name = \"rob_st_pub\">\n  <remap from=\"robot_state_publisher\" to=\"my_robot_description\"/>\n  <remap from=\"joint_states\" to=\"different_joint_states\"/>\n  </node>\n</launch>",
            "title": "1.2 Example launch file"
        },
        {
            "location": "/Ros\u5efa\u6a21/#2runing-as-a-library",
            "text": "Advanced users can also run the robot state publisher as a library,from within their own c++ code.After you include the header: #include <robot_state_publisher/robot_state_publisher.h> \nall you need is the constructor which takes in a KDL tree RobotStatePublisher(const KDL::Tree& tree); \nand now, everytime you want to publish the state of your robot, you call the publishTransforms functions:  //Publish moving joints\nvoid publishTransfroms(const std::map<std::string, double>& joint_positions,\n  const ros::Time& time);\n\n//publish fixed joints\nvoid publishFixedTransforms();  The first argument is a map with joint names and joint positions, and the second argument is the time at which the joint positions were recorded. It is okay if the map does not contain all the joint names. It is also okay if the map contains some joints names that are not part of the kinematic model. But note if you don't tell the joint state publisher about some of the joints in your kinematic model, then your tf tree will not be complete.",
            "title": "2.Runing as a library"
        },
        {
            "location": "/Ros\u5efa\u6a21/#using-urdf-with-robot_state_publisher",
            "text": "",
            "title": "Using urdf with robot_state_publisher"
        },
        {
            "location": "/Ros\u5efa\u6a21/#create-the-urdf-file",
            "text": "",
            "title": "Create the URDF File"
        },
        {
            "location": "/Ros\u5efa\u6a21/#publishing-the-state",
            "text": "cd %TOP_DIR_YOUR_CATKIN_WS%/src   Then fire your favourite editor and paste the following code into the src/state_publisher.cpp file:     1 #include <string>\n   2 #include <ros/ros.h>\n   3 #include <sensor_msgs/JointState.h>\n   4 #include <tf/transform_broadcaster.h>\n   5 \n   6 int main(int argc, char** argv) {\n   7     ros::init(argc, argv, \"state_publisher\");\n   8     ros::NodeHandle n;\n   9     ros::Publisher joint_pub = n.advertise<sensor_msgs::JointState>(\"joint_states\", 1);\n  10     tf::TransformBroadcaster broadcaster;\n  11     ros::Rate loop_rate(30);\n  12 \n  13     const double degree = M_PI/180;\n  14 \n  15     // robot state\n  16     double tilt = 0, tinc = degree, swivel=0, angle=0, height=0, hinc=0.005;\n  17 \n  18     // message declarations\n  19     geometry_msgs::TransformStamped odom_trans;\n  20     sensor_msgs::JointState joint_state;\n  21     odom_trans.header.frame_id = \"odom\";\n  22     odom_trans.child_frame_id = \"axis\";\n  23 \n  24     while (ros::ok()) {\n  25         //update joint_state\n  26         joint_state.header.stamp = ros::Time::now();\n  27         joint_state.name.resize(3);\n  28         joint_state.position.resize(3);\n  29         joint_state.name[0] =\"swivel\";\n  30         joint_state.position[0] = swivel;\n  31         joint_state.name[1] =\"tilt\";\n  32         joint_state.position[1] = tilt;\n  33         joint_state.name[2] =\"periscope\";\n  34         joint_state.position[2] = height;\n  35 \n  36 \n  37         // update transform\n  38         // (moving in a circle with radius=2)\n  39         odom_trans.header.stamp = ros::Time::now();\n  40         odom_trans.transform.translation.x = cos(angle)*2;\n  41         odom_trans.transform.translation.y = sin(angle)*2;\n  42         odom_trans.transform.translation.z = .7;\n  43         odom_trans.transform.rotation = tf::createQuaternionMsgFromYaw(angle+M_PI/2);\n  44 \n  45         //send the joint state and transform\n  46         joint_pub.publish(joint_state);\n  47         broadcaster.sendTransform(odom_trans);\n  48 \n  49         // Create new robot state\n  50         tilt += tinc;\n  51         if (tilt<-.5 || tilt>0) tinc *= -1;\n  52         height += hinc;\n  53         if (height>.2 || height<0) hinc *= -1;\n  54         swivel += degree;\n  55         angle += degree/4;\n  56 \n  57         // This will adjust as needed per iteration\n  58         loop_rate.sleep();\n  59     }\n  60 \n  61 \n  62     return 0;\n  63 }",
            "title": "Publishing the State"
        },
        {
            "location": "/Ros\u5efa\u6a21/#launch-file",
            "text": "This launch file assumes you are using the package name \"r2d2\" and node name \"state_publisher\" and you have saved this urdf to the \"r2d2\" package.     1 <launch>\n   2         <param name=\"robot_description\" command=\"cat $(find r2d2)/model.xml\" />\n   3         <node name=\"robot_state_publisher\" pkg=\"robot_state_publisher\" type=\"robot_state_publisher\" />\n   4         <node name=\"state_publisher\" pkg=\"robot_description\" type=\"state_publisher\" />\n   5 </launch>",
            "title": "Launch File"
        },
        {
            "location": "/Ros\u5efa\u6a21/#viewing-the-reult",
            "text": "First we have to edit the CMakeLists.txt in the package where we saved the above source code.Make sure to add the tf dependency in addition to the other dependencies:   find_package(catkin REQUIRED COMPONENTS roscpp rospy std_msgs tf)   Notice that roscpp is used to parse the code that we wrote and generate the state_publisher node. We also have to add the following to the end of the CMakelists.txt in order to generate the state_publisher node:   include_directories(include ${catkin_INCLUDE_DIRS})\\\nadd_executable(state_publisher src/state_publisher.cpp)\\\ntarget_link_libraries(state_publisher ${catkin_LIBRARIES})   Now we should go to the directory of the workspace and build it using:   catkin_make   Now launch the package (assuming that our launch file is named display.launch):   roslaunch r2d2 display.launch   Run rviz in a new terminal using:   rosrun rviz rviz   Choose odom as your fixed frame (under Global Options). Then choose \"Add Display\" and add a Robot Model Display and a TF Display",
            "title": "Viewing the Reult"
        },
        {
            "location": "/Ros\u5efa\u6a21/#building-a-visual-robot-model-with-urdf-from-scratch",
            "text": "",
            "title": "Building a Visual Robot Model with URDF from Scratch"
        },
        {
            "location": "/Ros\u5efa\u6a21/#descriptionlearn-how-to-build-a-visual-model-of-a-robot-that-you-can-view-in-rviz",
            "text": "Before continuing,make sure you have the joint_state_publisher package installed.   sudo apt-get install ros-melodic-joint-state-publisher",
            "title": "Description:learn how to build a visual model of a robot that you can view in rviz"
        },
        {
            "location": "/Ros\u5efa\u6a21/#1one-shape",
            "text": "First, we're just going to explore one simple shape.Here's about as simple as a urdf as you can make.  <?xml version=\"1.0\"?>\n<robot name=\"myfirst\">\n  <link name=\"base_link\">\n    <visual>\n      <geometry>\n        <cylinder length=\"0.6\" radius=\"0.2\"/>\n      </geometry>\n    </visual>\n  </link>\n</robot>  To translate the XML into English, this is a robot with the name myfirst, that contains only one link (a.k.a. part), whose visual component is just a cylinder 0.6 meters long with a 0.2 meter radius. This may seem like a lot of enclosing tags for a simple \u201chello world\u201d type example, but it will get more complicated, trust me.\\",
            "title": "1.One Shape"
        },
        {
            "location": "/Ros\u5efa\u6a21/#gazebo",
            "text": "sudo apt-get install ros-melodic-gazebo-ros-pkgs ros-melodic-gazebo-ros-control",
            "title": "Gazebo \u5b89\u88c5"
        },
        {
            "location": "/Ros\u5efa\u6a21/#ros-link-gazebo",
            "text": "\u53ef\u80fd\u6709\u4e24\u4e2a\u539f\u56e0\uff0c\u4e00\u4e2a\u662f\u5b83\u81ea\u8eab\u6ca1\u6709\u7269\u7406\u5c5e\u6027\uff0c\u53e6\u5916\u4e00\u4e2a\u662f\u5b83\u7684\u8fde\u63a5\u5bf9\u8c61\u6ca1\u6709\u7269\u7406\u5c5e\u6027",
            "title": "ros link \u63d0\u793a\u6ca1\u6709\u5305\u542b\u5728gazebo"
        },
        {
            "location": "/Ros\u5efa\u6a21/#denavit-hartenberg-parameters",
            "text": "In mechanical engineering, the Denavit-Hartenberg parameters(also called DH parameters) are the four parameters associated with a particular convention for attaching reference frames to the links of a spatial kinematic chain or robot manipulator.  Jacques Denavit and Richard Hartenberg introduced this convention in 1955 in order to standardlize the coordinate frames for spatial linkages.  Denavit-Hartenberg convention \nA commonly used convention for selecting frames of reference in robotics applications is Denavit and Hartenberg(D-H) convention. In this convention, coordinate frames are attached to the joints between two links such that one transformation is associated with the joint,[Z], and the second is associated with the link,[X]. The coordinate transformations along a serial robot consisting of n links from the kinematics equations of the robot.   where [T] is the transformation locating the end-link.  In order to determine the coordinate transformations [Z] and [X], the joints connecting the links are modeled as either hinged or sliding joints, each of which have a unique line S in space that forms the joint axis and define the relative movement of the two links.A typical serial robot is characterized by a sequence of six lines Si,i=1,......6, one for each joint in the robot.For each sequence of lines Si and Si+1, there is a common normal line Ai,j+1.The system of six joint axes Si",
            "title": "Denavit-Hartenberg parameters"
        },
        {
            "location": "/agx/",
            "text": "\u5b89\u88c5numba==0.53.1\n\n\ngit clone https://github.com/wjakob/tbb.git\ncd tbb/build\ncmake ..\nmake -j\nsudo make install",
            "title": "Agx"
        },
        {
            "location": "/agx/#numba0531",
            "text": "git clone https://github.com/wjakob/tbb.git\ncd tbb/build\ncmake ..\nmake -j\nsudo make install",
            "title": "\u5b89\u88c5numba==0.53.1"
        },
        {
            "location": "/arm_docker/",
            "text": "ARCH anaconda\n\n\nanaconda\n\n\n\u5b89\u88c5docker\n\n\nsudo apt-get remove docker docker-ce docker-io \nsudo apt-get install debootstrap\nsudo apt install docker.io\nsystemctl restart docker\n\n\n\n\u6784\u5efadocker\n\n\nln -sf /usr/local/cuda-10.0/targets/aarch64-linux/lib/libcusparse.so /usr/lib/libcusparse.so.10\ndocker pull arm64v8/ubuntu:18.04\napt-get update\napt-get install python3 python3-pip\npip3 install --upgrade pip\napt-get install libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev zip libjpeg8-dev liblapack-dev libblas-dev gfortran pkg-config\nln -sf /usr/lib/aarch64-linux-gnu/hdf5/serial/libhdf5.so /usr/lib/libhdf5.so\n\npip install -U numpy==1.16.1\npip install -U h5py==2.10.0\n\npip install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v45 tensorflow==2.3.1 -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com\nln -sf /usr/local/cuda-10.0/targets/aarch64-linux/lib/libcudart.so /usr/lib/libcudart.so.10.2\nln -sf /usr/lib/aarch64-linux-gnu/libcuda.so /usr/lib/libcuda.so.1\nln -sf /usr/lib/aarch64-linux-gnu/tegra/lib* /usr/lib/\nln -sf /usr/local/cuda-10.0/targets/aarch64-linux/lib/libcublas.so /usr/lib/libcublas.so.10\nln -sf /usr/local/cuda-10.0/targets/aarch64-linux/lib/libcusparse.so /usr/lib/libcusparse.so.10\nln -sf /usr/local/cuda-10.0/targets/aarch64-linux/lib/libcusolver.so /usr/lib/libcusolver.so.10\nln -sf /usr/local/cuda-10.0/targets/aarch64-linux/lib/libcurand.so /usr/lib/libcurand.so.10\nln -sf /usr/local/cuda-10.0/targets/aarch64-linux/lib/libcufft.so /usr/lib/libcufft.so.10\n\n\n\n\n\u6309ros\u5b98\u7f51\u5b89\u88c5\nros\n\n\nNGC\n\n\ndocker pull nvcr.io/nvidia/l4t-tensorflow:r32.4.3-tf1.15-py3\ndocker pull nvcr.io/nvidia/l4t-tensorflow:r32.4.3-tf2.2-py3\n\ndocker run -it --net=host --gpus all --runtime nvidia -e DISPLAY=$DISPLAY -v /tmp/.X11-unix/:/tmp/.X11-unix nvcr.io/nvidia/l4t-tensorflow:r32.4.3-tf2.2-py3\n\n\n\ndocker pull nvcr.io/nvidia/l4t-base:r32.4.3\n\nimport tensorflow as tf\nprint(tf.test.is_gpu_available())\n\n\n\n\n\u5b89\u88c5vosk\n\n\n pip3 install https://hub.fastgit.org/alphacep/vosk-api/releases/download/0.3.17/vosk-0.3.17-cp36-cp36m-linux_aarch64.whl\n\n\n\ngerman_transliterate\n\n\npip install git+https://github.com/repodiac/german_transliterate\n\npip install git+https://hub.fastgit.org/repodiac/german_transliterate\n\n\n\n\nwget https://hub.fastgit.org/Qengineering/TensorFlow-Addons-Jetson-Nano/raw/main/tensorflow_addons-0.13.0.dev0-cp36-cp36m-linux_aarch64.whl\n\npip install tensorflow_addons-0.13.0.dev0-cp36-cp36m-linux_aarch64.whl\n\napt install llvm-9-dev\n\nln -s /usr/bin/llvm-config-9 /usr/bin/llvm-config\n\n pip install librosa\n\npip install pypinyin\n\npip install g2p-en -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com\n\n\n\nSklearn - cannot allocate memory in static TLS block\n\n\nexport LD_PRELOAD=/usr/local/lib/python3.6/dist-packages/scikit_learn.libs/libgomp-d22c30c5.so.1.0.0\n\n\n\nAGX sounddevice\n\n\npython3 -m sounddevice #\u5217\u4e3e\u58f0\u97f3\u8bbe\u5907 sd.default.device = id\u53f7\napt-get install alsa-utils #\u5b89\u88c5alsa\naplay -l #\u67e5\u770b\u8bbe\u5907\n// \u53ef\u4ee5\u4f7f\u7528 aplay \u6765\u6d4b\u8bd5, \u54ea\u4e2a\u51fa\u58f0\u97f3\u5c31\u662f\u54ea\u4e2a\u4e86 ...\naplay -D hw:0,3 /usr/share/sounds/alsa/test.wav\naplay -D hw:0,7 /usr/share/sounds/alsa/test.wav\naplay -D hw:0,8 /usr/share/sounds/alsa/test.wav\n\n\n\n\ndocker \u53d1\u58f0\n\n\ndocker run --device /dev/snd ....\n\n\u7a0b\u5e8f\u91cc\uff0csd.default.device = 2\n\n\n\ndocker \u5f55\u97f3\n\n\narecord -D hw:2,0 -c 1 -r 48000 -f S16_LE -d 10 cap.wav",
            "title": "Arm docker"
        },
        {
            "location": "/arm_docker/#arch-anaconda",
            "text": "anaconda",
            "title": "ARCH anaconda"
        },
        {
            "location": "/arm_docker/#docker",
            "text": "sudo apt-get remove docker docker-ce docker-io \nsudo apt-get install debootstrap\nsudo apt install docker.io\nsystemctl restart docker",
            "title": "\u5b89\u88c5docker"
        },
        {
            "location": "/arm_docker/#docker_1",
            "text": "ln -sf /usr/local/cuda-10.0/targets/aarch64-linux/lib/libcusparse.so /usr/lib/libcusparse.so.10\ndocker pull arm64v8/ubuntu:18.04\napt-get update\napt-get install python3 python3-pip\npip3 install --upgrade pip\napt-get install libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev zip libjpeg8-dev liblapack-dev libblas-dev gfortran pkg-config\nln -sf /usr/lib/aarch64-linux-gnu/hdf5/serial/libhdf5.so /usr/lib/libhdf5.so\n\npip install -U numpy==1.16.1\npip install -U h5py==2.10.0\n\npip install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v45 tensorflow==2.3.1 -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com\nln -sf /usr/local/cuda-10.0/targets/aarch64-linux/lib/libcudart.so /usr/lib/libcudart.so.10.2\nln -sf /usr/lib/aarch64-linux-gnu/libcuda.so /usr/lib/libcuda.so.1\nln -sf /usr/lib/aarch64-linux-gnu/tegra/lib* /usr/lib/\nln -sf /usr/local/cuda-10.0/targets/aarch64-linux/lib/libcublas.so /usr/lib/libcublas.so.10\nln -sf /usr/local/cuda-10.0/targets/aarch64-linux/lib/libcusparse.so /usr/lib/libcusparse.so.10\nln -sf /usr/local/cuda-10.0/targets/aarch64-linux/lib/libcusolver.so /usr/lib/libcusolver.so.10\nln -sf /usr/local/cuda-10.0/targets/aarch64-linux/lib/libcurand.so /usr/lib/libcurand.so.10\nln -sf /usr/local/cuda-10.0/targets/aarch64-linux/lib/libcufft.so /usr/lib/libcufft.so.10  \u6309ros\u5b98\u7f51\u5b89\u88c5 ros  NGC  docker pull nvcr.io/nvidia/l4t-tensorflow:r32.4.3-tf1.15-py3\ndocker pull nvcr.io/nvidia/l4t-tensorflow:r32.4.3-tf2.2-py3\n\ndocker run -it --net=host --gpus all --runtime nvidia -e DISPLAY=$DISPLAY -v /tmp/.X11-unix/:/tmp/.X11-unix nvcr.io/nvidia/l4t-tensorflow:r32.4.3-tf2.2-py3  docker pull nvcr.io/nvidia/l4t-base:r32.4.3\n\nimport tensorflow as tf\nprint(tf.test.is_gpu_available())",
            "title": "\u6784\u5efadocker"
        },
        {
            "location": "/arm_docker/#vosk",
            "text": "pip3 install https://hub.fastgit.org/alphacep/vosk-api/releases/download/0.3.17/vosk-0.3.17-cp36-cp36m-linux_aarch64.whl",
            "title": "\u5b89\u88c5vosk"
        },
        {
            "location": "/arm_docker/#german_transliterate",
            "text": "pip install git+https://github.com/repodiac/german_transliterate\n\npip install git+https://hub.fastgit.org/repodiac/german_transliterate  wget https://hub.fastgit.org/Qengineering/TensorFlow-Addons-Jetson-Nano/raw/main/tensorflow_addons-0.13.0.dev0-cp36-cp36m-linux_aarch64.whl\n\npip install tensorflow_addons-0.13.0.dev0-cp36-cp36m-linux_aarch64.whl\n\napt install llvm-9-dev\n\nln -s /usr/bin/llvm-config-9 /usr/bin/llvm-config\n\n pip install librosa\n\npip install pypinyin\n\npip install g2p-en -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com",
            "title": "german_transliterate"
        },
        {
            "location": "/arm_docker/#sklearn-cannot-allocate-memory-in-static-tls-block",
            "text": "export LD_PRELOAD=/usr/local/lib/python3.6/dist-packages/scikit_learn.libs/libgomp-d22c30c5.so.1.0.0",
            "title": "Sklearn - cannot allocate memory in static TLS block"
        },
        {
            "location": "/arm_docker/#agx-sounddevice",
            "text": "python3 -m sounddevice #\u5217\u4e3e\u58f0\u97f3\u8bbe\u5907 sd.default.device = id\u53f7\napt-get install alsa-utils #\u5b89\u88c5alsa\naplay -l #\u67e5\u770b\u8bbe\u5907\n// \u53ef\u4ee5\u4f7f\u7528 aplay \u6765\u6d4b\u8bd5, \u54ea\u4e2a\u51fa\u58f0\u97f3\u5c31\u662f\u54ea\u4e2a\u4e86 ...\naplay -D hw:0,3 /usr/share/sounds/alsa/test.wav\naplay -D hw:0,7 /usr/share/sounds/alsa/test.wav\naplay -D hw:0,8 /usr/share/sounds/alsa/test.wav  docker \u53d1\u58f0  docker run --device /dev/snd ....\n\n\u7a0b\u5e8f\u91cc\uff0csd.default.device = 2  docker \u5f55\u97f3  arecord -D hw:2,0 -c 1 -r 48000 -f S16_LE -d 10 cap.wav",
            "title": "AGX sounddevice"
        },
        {
            "location": "/c++/",
            "text": "string \u524d\u8865\u96f6\n\n\n#include <iostream>\n#include <sstream>\n#include <iomanip>\nusing namespace std;\nvoid main()\n{\nint num = 1024;\nstringstream ss;\nss << setw(5) << setfill('0') << num ;\nstring str;\nss >> str;         //\u5c06\u5b57\u7b26\u6d41\u4f20\u7ed9 str\n//str = ss.str();  //\u4e5f\u53ef\u4ee5\ncout << str;\n}\n\n\n\n\u5f97\u5230\u5f53\u524d\u65f6\u95f4\n\n\n#include <iostream>\n#include <ctime>\n\nint main ()\n{\n  time_t rawtime;\n  struct tm * timeinfo;\n  char buffer[80];\n\n  time (&rawtime);\n  timeinfo = localtime(&rawtime);\n\n  strftime(buffer,sizeof(buffer),\"%d-%m-%Y %H:%M:%S\",timeinfo);\n  std::string str(buffer);\n\n  std::cout << str;\n\n  return 0;\n}\n\n\nstd::string get_time()\n{\n    time_t rawtime;\n    struct tm * timeinfo;\n    char buffer[80];\n\n    time (&rawtime);\n    timeinfo = localtime(&rawtime);\n\n    strftime(buffer,sizeof(buffer),\"%d%m%Y%H%M%S\",timeinfo);\n    std::string str(buffer);\n\n    std::cout << str;\n    return str;\n}\n\n\n\n\u5199\u5165\u6587\u4ef6append\n\n\n#include <fstream>\n\nint main() {  \n  std::ofstream outfile;\n\n  outfile.open(\"test.txt\", std::ios_base::app); // append instead of overwrite\n  outfile << \"Data\"; \n  return 0;\n}\n\n\n\nfloat to string\n\n\nto_string(float)\n\n\nstring to float\n\n\nstof(str)\n\n\n\u8bfb\u53d6\u6587\u6863\u6bcf\u4e00\u884c\u5185\u5bb9\n\n\nfstream newfile;\nnewfile.open(learning_file,ios::in);\nif (newfile.is_open())\n{\n    string tp;\n    while(getline(newfile, tp))\n    {\n        cout << tp << \"\\n\";\n    }\n    newfile.close();\n}\n\n\n\n\u6587\u4ef6\u6e05\u7a7a\n\n\nfstream newfile;\nnewfile.open(learning_file,ios::out);\nnewfile.close();\n\n\n\n\u5b57\u7b26\u4e32\uff53\uff50\uff4c\uff49\uff54\n\n\n#include <boost/algorithm/string.hpp>\n\nstd::string text = \"Let me split this into words\";\nstd::vector<std::string> results;\n\nboost::split(results, text, [](char c){return c == ' ';});\n\n\n\nCMakeLists.txt \u5f15\u5165\uff42\uff4f\uff4f\uff53\uff54\n\n\nfind_package(Boost COMPONENTS program_options filesystem REQUIRED )\ninclude_directories(${Boost_INCLUDE_DIRS})\nlink_directories(${Boost_LIBRARY_DIRS})\ntarget_link_libraries(learning ${catkin_LIBRARIES} ${Boost_LIBRARIES})\n\n\n\nexplicit\n\n\n#include <iostream> \n\nusing namespace std; \n\nclass Complex \n{ \nprivate: \n    double real; \n    double imag; \n\npublic: \n    // Default constructor \n    Complex(double r = 0.0, double i = 0.0) : real(r), imag(i) {} \n\n    // A method to compare two Complex numbers \n    bool operator == (Complex rhs) { \n    return (real == rhs.real && imag == rhs.imag)? true : false; \n    } \n}; \n\nint main() \n{ \n    // a Complex object \n    Complex com1(3.0, 0.0); \n\n    if (com1 == 3.0) \n    cout << \"Same\"; \n    else\n    cout << \"Not Same\"; \n    return 0; \n} \n\n\n\n\u7a0b\u5e8f\u8f93\u51fa\u3000\nSame\n\n\u5728C ++\u4e2d\uff0c\u5982\u679c\u7c7b\u5177\u6709\u53ef\u4ee5\u7528\u5355\u4e2a\u53c2\u6570\u8c03\u7528\u7684\u6784\u9020\u51fd\u6570\uff0c\u5219\u6b64\u6784\u9020\u51fd\u6570\u5c06\u6210\u4e3a\u8f6c\u6362\u6784\u9020\u51fd\u6570\uff0c\u56e0\u4e3a\u8fd9\u6837\u7684\u6784\u9020\u51fd\u6570\u5141\u8bb8\u5c06\u5355\u4e2a\u53c2\u6570\u8f6c\u6362\u4e3a\u6b63\u5728\u6784\u9020\u7684\u7c7b\u3002\n\u6211\u4eec\u53ef\u4ee5\u907f\u514d\u8fd9\u79cd\u9690\u5f0f\u8f6c\u6362\uff0c\u56e0\u4e3a\u5b83\u4eec\u53ef\u80fd\u5bfc\u81f4\u610f\u5916\u7684\u7ed3\u679c\u3002\u6211\u4eec\u53ef\u4ee5\u5728explicit\u5173\u952e\u5b57\u7684\u5e2e\u52a9\u4e0b\u4f7f\u6784\u9020\u51fd\u6570\u663e\u5f0f\u5316\u3002\\\n\n\nusing namespace std; \n\nclass Complex \n{ \nprivate: \n    double real; \n    double imag; \n\npublic: \n    // Default constructor \n    explicit Complex(double r = 0.0, double i = 0.0) : real(r), imag(i) {} \n\n    // A method to compare two Complex numbers \n    bool operator== (Complex rhs) { \n       return (real == rhs.real && imag == rhs.imag)? true : false; \n    } \n}; \n\nint main() \n{ \n    // a Complex object \n    Complex com1(3.0, 0.0); \n\n    if (com1 == (Complex)3.0) \n       cout << \"Same\"; \n    else\n       cout << \"Not Same\"; \n     return 0; \n} \n\n\n\nEigen \u5b89\u88c5\n\n\n\u4e0b\u8f7d\u5e76\u89e3\u538b\n\n\ncd eigen-3.3.9/\nmkdir build\ncmake ..\nsudo make install\n\n\n\n\u56e0\u4e3aeigen3\u9ed8\u8ba4\u5b89\u88c5\u5728/usr/local/include\u4e0b\u9762\uff0c\u6240\u4ee5\u9700\u8981\u521b\u5efa\u8f6f\u94fe\u63a5\n\n\ncd /usr/local/include/\nsudo ln -sf eigen3/Eigen Eigen\nsudo ln -sf eigen3/unsupported unsupported\n\n\n\nUse Eigen in cmake program\n\n\n```bash\nfind_package(Eigen3 3.3 REQUIRED)\n...\ninclude_directories(${EIGEN3_INCLUDE_DIR})\n...\ntarget_link_libraries(example Eigen3::Eigen)\n\n\n\ndouble to string \u4fdd\u7559\u5c0f\u6570\u70b9\u540e1\u4f4d\n\n\nstd::string doubleToString(const double &val)\n\n{\n\n    char* chCode;\n\n    chCode = new char[20];\n\n    sprintf(chCode, \"%.2lf\", val);\n\n    std::string str(chCode);\n\n    delete[]chCode;\n\n    return str;\n\n}\n\n\n\n\u8bfb\u53d6\u7f51\u53e3\u4fe1\u606f\n\n\n#include <arpa/inet.h>\n#include <sys/socket.h>\n#include <netdb.h>\n#include <ifaddrs.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include <linux/if_link.h>\n#include <iostream>\n\nstd::string my_get_WIP()\n{\n    struct ifaddrs *ifaddr, *ifa;\n    int family, s, n;\n    char host[NI_MAXHOST];\n    if (getifaddrs(&ifaddr) == -1) {\n        perror(\"getifaddrs\");\n        exit(EXIT_FAILURE);\n    }\n    std::string result;\n    for (ifa = ifaddr, n = 0; ifa != NULL; ifa = ifa->ifa_next, n++) {\n        if (ifa->ifa_addr == NULL)\n            continue;\n        family = ifa->ifa_addr->sa_family;\n\n        if (family == AF_INET || family == AF_INET6) {\n            s = getnameinfo(ifa->ifa_addr,\n                    (family == AF_INET) ? sizeof(struct sockaddr_in) :\n                                            sizeof(struct sockaddr_in6),\n                    host, NI_MAXHOST,\n                    NULL, 0, NI_NUMERICHOST);\n            if (s != 0) {\n                printf(\"getnameinfo() failed: %s\\n\", gai_strerror(s));\n                exit(EXIT_FAILURE);\n            }\n\n        } else if (family == AF_PACKET && ifa->ifa_data != NULL) {\n            struct rtnl_link_stats *stats = (struct rtnl_link_stats *)ifa->ifa_data;\n        }\n        std::string host_str = std::string(host);\n        if(host_str.find(\"192.168.1.\") != std::string::npos)\n        {\n            result = std::string(ifa->ifa_name) +  \":\" + host_str;\n        }\n    }\n\n    freeifaddrs(ifaddr);\n    return result;\n    exit(EXIT_SUCCESS);\n\n}\n\n\n\n\u5224\u65ad\u5b57\u7b26\u4e32\u662f\u5426\u542b\u6709\u67d0\u5b57\u7b26\n\n\nstd::size_t find = str.find(\":\");\nif(find != std::string::npos)\n{\n    std::cout << \u201cfind\u201d << std::endl;\n}\n\n\n\nchar to string\n\n\n#include <iostream>\n#include <string>\n#include <sstream>\n\nstd::string char_string(char c)\n{\n    std::string s;\n    std::stringstream ss;\n    ss << c;\n    ss >> s;                // or, use `s = ss.str()`\n    std::cout << s << std::endl;\n    return s;\n}\n\nint main()\n{\n    char c = 'A';\n    char_string(c);\n    return 0;\n}\n\n\n\nget current executation path\n\n\n#include<stdio.h>\n#include<unistd.h> \n#include<stdlib.h>\n#include <iostream>\n#include <string>\n#include <sstream>\n\nstd::string char_string(char c)\n{\n    std::string s;\n    std::stringstream ss;\n    ss << c;\n    ss >> s;                // or, use `s = ss.str()`\n    std::cout << s << std::endl;\n    return s;\n}\n\nstd::string get_execute_path()\n{\n    char *buf;\n    buf=(char *)malloc(100*sizeof(char));\n    getcwd(buf,100);\n    return char_string(buf);\n}\n\nstd::string get \n\nmain() {\nchar *buf;\nbuf=(char *)malloc(100*sizeof(char));\ngetcwd(buf,100);\nprintf(\"\\n %s \\n\",buf);\n}\n\n\n\n\nget current working directory\n\n\n    printf(\"Current working dir: %s\\n\", get_current_dir_name());\n\n\n\nget current file path\n\n\nstd::string get_current_path()\n{\n    std::string path = __FILE__;\n    return path;\n}\n\n\n\nget current file's direct\n\n\nstd::string get_current_folder()\n{\n    std::string pathfile = __FILE__;\n    std::size_t find = pathfile.find_last_of(\"/\");\n    std::cout << find << std::endl;\n    pathfile = pathfile.substr(0,find);\n    return pathfile;\n}\n\n\n\n\u83b7\u53d6\u76ee\u5f55\u4e0b\u7684\u6587\u4ef6\n\n\n#include <iostream>\n#include <dirent.h>\n\nbool get_filelist_from_dir(std::string _path, std::vector<std::string>& _files)\n{\n    DIR* dir;   \n    dir = opendir(_path.c_str());\n    struct dirent* ptr;\n    std::vector<std::string> file;\n    while((ptr = readdir(dir)) != NULL)\n    {\n        if(ptr->d_name[0] == '.') {continue;}\n        file.push_back(ptr->d_name);\n    }\n    closedir(dir);\n    sort(file.begin(), file.end());\n    _files = file;\n}\n\n\n\n\n\u5199\u5165\u6587\u4ef6\n\n\n#include <iostream>\n#include <fstream>\nusing namespace std;\n\nvoid main()\n{\n    ofstream in;\n    in.open(\"com.txt\",ios::trunc); //ios::trunc\u8868\u793a\u5728\u6253\u5f00\u6587\u4ef6\u524d\u5c06\u6587\u4ef6\u6e05\u7a7a,\u7531\u4e8e\u662f\u5199\u5165,\u6587\u4ef6\u4e0d\u5b58\u5728\u5219\u521b\u5efa\n    int i;\n    char a='a';\n    for(i=1;i<=26;i++)//\u5c0626\u4e2a\u6570\u5b57\u53ca\u82f1\u6587\u5b57\u6bcd\u5199\u5165\u6587\u4ef6\n    {\n    if(i<10)\n    {\n        in<<\"0\"<<i<<\"\\t\"<<a<<\"\\n\";\n        a++;\n    }\n    else\n    {\n        in<<i<<\"\\t\"<<a<<\"\\n\";\n        a++;\n    }\n    }\n    in.close();//\u5173\u95ed\u6587\u4ef6\n}\n\n\n\n\u4ea7\u751f\u968f\u673a\u6570\n\n\n#include <iostream>\n#include <cstdlib>\n#include <ctime>\nusing namespace std;\n\nint main()\n{\n    srand((int)time(0));  // \u4ea7\u751f\u968f\u673a\u79cd\u5b50  \u628a0\u6362\u6210NULL\u4e5f\u884c\n    for (int i = 0; i < 10; i++)\n    {\n        cout << rand()%100<< \" \";\n    }\n    return 0;\n}",
            "title": "C++"
        },
        {
            "location": "/c++/#string",
            "text": "#include <iostream>\n#include <sstream>\n#include <iomanip>\nusing namespace std;\nvoid main()\n{\nint num = 1024;\nstringstream ss;\nss << setw(5) << setfill('0') << num ;\nstring str;\nss >> str;         //\u5c06\u5b57\u7b26\u6d41\u4f20\u7ed9 str\n//str = ss.str();  //\u4e5f\u53ef\u4ee5\ncout << str;\n}",
            "title": "string \u524d\u8865\u96f6"
        },
        {
            "location": "/c++/#_1",
            "text": "#include <iostream>\n#include <ctime>\n\nint main ()\n{\n  time_t rawtime;\n  struct tm * timeinfo;\n  char buffer[80];\n\n  time (&rawtime);\n  timeinfo = localtime(&rawtime);\n\n  strftime(buffer,sizeof(buffer),\"%d-%m-%Y %H:%M:%S\",timeinfo);\n  std::string str(buffer);\n\n  std::cout << str;\n\n  return 0;\n}\n\n\nstd::string get_time()\n{\n    time_t rawtime;\n    struct tm * timeinfo;\n    char buffer[80];\n\n    time (&rawtime);\n    timeinfo = localtime(&rawtime);\n\n    strftime(buffer,sizeof(buffer),\"%d%m%Y%H%M%S\",timeinfo);\n    std::string str(buffer);\n\n    std::cout << str;\n    return str;\n}",
            "title": "\u5f97\u5230\u5f53\u524d\u65f6\u95f4"
        },
        {
            "location": "/c++/#append",
            "text": "#include <fstream>\n\nint main() {  \n  std::ofstream outfile;\n\n  outfile.open(\"test.txt\", std::ios_base::app); // append instead of overwrite\n  outfile << \"Data\"; \n  return 0;\n}",
            "title": "\u5199\u5165\u6587\u4ef6append"
        },
        {
            "location": "/c++/#float-to-string",
            "text": "to_string(float)",
            "title": "float to string"
        },
        {
            "location": "/c++/#string-to-float",
            "text": "stof(str)",
            "title": "string to float"
        },
        {
            "location": "/c++/#_2",
            "text": "fstream newfile;\nnewfile.open(learning_file,ios::in);\nif (newfile.is_open())\n{\n    string tp;\n    while(getline(newfile, tp))\n    {\n        cout << tp << \"\\n\";\n    }\n    newfile.close();\n}",
            "title": "\u8bfb\u53d6\u6587\u6863\u6bcf\u4e00\u884c\u5185\u5bb9"
        },
        {
            "location": "/c++/#_3",
            "text": "fstream newfile;\nnewfile.open(learning_file,ios::out);\nnewfile.close();",
            "title": "\u6587\u4ef6\u6e05\u7a7a"
        },
        {
            "location": "/c++/#split",
            "text": "#include <boost/algorithm/string.hpp>\n\nstd::string text = \"Let me split this into words\";\nstd::vector<std::string> results;\n\nboost::split(results, text, [](char c){return c == ' ';});",
            "title": "\u5b57\u7b26\u4e32\uff53\uff50\uff4c\uff49\uff54"
        },
        {
            "location": "/c++/#cmakeliststxt-boost",
            "text": "find_package(Boost COMPONENTS program_options filesystem REQUIRED )\ninclude_directories(${Boost_INCLUDE_DIRS})\nlink_directories(${Boost_LIBRARY_DIRS})\ntarget_link_libraries(learning ${catkin_LIBRARIES} ${Boost_LIBRARIES})",
            "title": "CMakeLists.txt \u5f15\u5165\uff42\uff4f\uff4f\uff53\uff54"
        },
        {
            "location": "/c++/#explicit",
            "text": "#include <iostream> \n\nusing namespace std; \n\nclass Complex \n{ \nprivate: \n    double real; \n    double imag; \n\npublic: \n    // Default constructor \n    Complex(double r = 0.0, double i = 0.0) : real(r), imag(i) {} \n\n    // A method to compare two Complex numbers \n    bool operator == (Complex rhs) { \n    return (real == rhs.real && imag == rhs.imag)? true : false; \n    } \n}; \n\nint main() \n{ \n    // a Complex object \n    Complex com1(3.0, 0.0); \n\n    if (com1 == 3.0) \n    cout << \"Same\"; \n    else\n    cout << \"Not Same\"; \n    return 0; \n}   \u7a0b\u5e8f\u8f93\u51fa\u3000 Same \n\u5728C ++\u4e2d\uff0c\u5982\u679c\u7c7b\u5177\u6709\u53ef\u4ee5\u7528\u5355\u4e2a\u53c2\u6570\u8c03\u7528\u7684\u6784\u9020\u51fd\u6570\uff0c\u5219\u6b64\u6784\u9020\u51fd\u6570\u5c06\u6210\u4e3a\u8f6c\u6362\u6784\u9020\u51fd\u6570\uff0c\u56e0\u4e3a\u8fd9\u6837\u7684\u6784\u9020\u51fd\u6570\u5141\u8bb8\u5c06\u5355\u4e2a\u53c2\u6570\u8f6c\u6362\u4e3a\u6b63\u5728\u6784\u9020\u7684\u7c7b\u3002\n\u6211\u4eec\u53ef\u4ee5\u907f\u514d\u8fd9\u79cd\u9690\u5f0f\u8f6c\u6362\uff0c\u56e0\u4e3a\u5b83\u4eec\u53ef\u80fd\u5bfc\u81f4\u610f\u5916\u7684\u7ed3\u679c\u3002\u6211\u4eec\u53ef\u4ee5\u5728explicit\u5173\u952e\u5b57\u7684\u5e2e\u52a9\u4e0b\u4f7f\u6784\u9020\u51fd\u6570\u663e\u5f0f\u5316\u3002\\  using namespace std; \n\nclass Complex \n{ \nprivate: \n    double real; \n    double imag; \n\npublic: \n    // Default constructor \n    explicit Complex(double r = 0.0, double i = 0.0) : real(r), imag(i) {} \n\n    // A method to compare two Complex numbers \n    bool operator== (Complex rhs) { \n       return (real == rhs.real && imag == rhs.imag)? true : false; \n    } \n}; \n\nint main() \n{ \n    // a Complex object \n    Complex com1(3.0, 0.0); \n\n    if (com1 == (Complex)3.0) \n       cout << \"Same\"; \n    else\n       cout << \"Not Same\"; \n     return 0; \n}",
            "title": "explicit"
        },
        {
            "location": "/c++/#eigen",
            "text": "\u4e0b\u8f7d\u5e76\u89e3\u538b  cd eigen-3.3.9/\nmkdir build\ncmake ..\nsudo make install  \u56e0\u4e3aeigen3\u9ed8\u8ba4\u5b89\u88c5\u5728/usr/local/include\u4e0b\u9762\uff0c\u6240\u4ee5\u9700\u8981\u521b\u5efa\u8f6f\u94fe\u63a5  cd /usr/local/include/\nsudo ln -sf eigen3/Eigen Eigen\nsudo ln -sf eigen3/unsupported unsupported",
            "title": "Eigen \u5b89\u88c5"
        },
        {
            "location": "/c++/#use-eigen-in-cmake-program",
            "text": "```bash\nfind_package(Eigen3 3.3 REQUIRED)\n...\ninclude_directories(${EIGEN3_INCLUDE_DIR})\n...\ntarget_link_libraries(example Eigen3::Eigen)",
            "title": "Use Eigen in cmake program"
        },
        {
            "location": "/c++/#double-to-string-1",
            "text": "std::string doubleToString(const double &val)\n\n{\n\n    char* chCode;\n\n    chCode = new char[20];\n\n    sprintf(chCode, \"%.2lf\", val);\n\n    std::string str(chCode);\n\n    delete[]chCode;\n\n    return str;\n\n}",
            "title": "double to string \u4fdd\u7559\u5c0f\u6570\u70b9\u540e1\u4f4d"
        },
        {
            "location": "/c++/#_4",
            "text": "#include <arpa/inet.h>\n#include <sys/socket.h>\n#include <netdb.h>\n#include <ifaddrs.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include <linux/if_link.h>\n#include <iostream>\n\nstd::string my_get_WIP()\n{\n    struct ifaddrs *ifaddr, *ifa;\n    int family, s, n;\n    char host[NI_MAXHOST];\n    if (getifaddrs(&ifaddr) == -1) {\n        perror(\"getifaddrs\");\n        exit(EXIT_FAILURE);\n    }\n    std::string result;\n    for (ifa = ifaddr, n = 0; ifa != NULL; ifa = ifa->ifa_next, n++) {\n        if (ifa->ifa_addr == NULL)\n            continue;\n        family = ifa->ifa_addr->sa_family;\n\n        if (family == AF_INET || family == AF_INET6) {\n            s = getnameinfo(ifa->ifa_addr,\n                    (family == AF_INET) ? sizeof(struct sockaddr_in) :\n                                            sizeof(struct sockaddr_in6),\n                    host, NI_MAXHOST,\n                    NULL, 0, NI_NUMERICHOST);\n            if (s != 0) {\n                printf(\"getnameinfo() failed: %s\\n\", gai_strerror(s));\n                exit(EXIT_FAILURE);\n            }\n\n        } else if (family == AF_PACKET && ifa->ifa_data != NULL) {\n            struct rtnl_link_stats *stats = (struct rtnl_link_stats *)ifa->ifa_data;\n        }\n        std::string host_str = std::string(host);\n        if(host_str.find(\"192.168.1.\") != std::string::npos)\n        {\n            result = std::string(ifa->ifa_name) +  \":\" + host_str;\n        }\n    }\n\n    freeifaddrs(ifaddr);\n    return result;\n    exit(EXIT_SUCCESS);\n\n}",
            "title": "\u8bfb\u53d6\u7f51\u53e3\u4fe1\u606f"
        },
        {
            "location": "/c++/#_5",
            "text": "std::size_t find = str.find(\":\");\nif(find != std::string::npos)\n{\n    std::cout << \u201cfind\u201d << std::endl;\n}",
            "title": "\u5224\u65ad\u5b57\u7b26\u4e32\u662f\u5426\u542b\u6709\u67d0\u5b57\u7b26"
        },
        {
            "location": "/c++/#char-to-string",
            "text": "#include <iostream>\n#include <string>\n#include <sstream>\n\nstd::string char_string(char c)\n{\n    std::string s;\n    std::stringstream ss;\n    ss << c;\n    ss >> s;                // or, use `s = ss.str()`\n    std::cout << s << std::endl;\n    return s;\n}\n\nint main()\n{\n    char c = 'A';\n    char_string(c);\n    return 0;\n}",
            "title": "char to string"
        },
        {
            "location": "/c++/#get-current-executation-path",
            "text": "#include<stdio.h>\n#include<unistd.h> \n#include<stdlib.h>\n#include <iostream>\n#include <string>\n#include <sstream>\n\nstd::string char_string(char c)\n{\n    std::string s;\n    std::stringstream ss;\n    ss << c;\n    ss >> s;                // or, use `s = ss.str()`\n    std::cout << s << std::endl;\n    return s;\n}\n\nstd::string get_execute_path()\n{\n    char *buf;\n    buf=(char *)malloc(100*sizeof(char));\n    getcwd(buf,100);\n    return char_string(buf);\n}\n\nstd::string get \n\nmain() {\nchar *buf;\nbuf=(char *)malloc(100*sizeof(char));\ngetcwd(buf,100);\nprintf(\"\\n %s \\n\",buf);\n}",
            "title": "get current executation path"
        },
        {
            "location": "/c++/#get-current-working-directory",
            "text": "printf(\"Current working dir: %s\\n\", get_current_dir_name());",
            "title": "get current working directory"
        },
        {
            "location": "/c++/#get-current-file-path",
            "text": "std::string get_current_path()\n{\n    std::string path = __FILE__;\n    return path;\n}",
            "title": "get current file path"
        },
        {
            "location": "/c++/#get-current-files-direct",
            "text": "std::string get_current_folder()\n{\n    std::string pathfile = __FILE__;\n    std::size_t find = pathfile.find_last_of(\"/\");\n    std::cout << find << std::endl;\n    pathfile = pathfile.substr(0,find);\n    return pathfile;\n}",
            "title": "get current file's direct"
        },
        {
            "location": "/c++/#_6",
            "text": "#include <iostream>\n#include <dirent.h>\n\nbool get_filelist_from_dir(std::string _path, std::vector<std::string>& _files)\n{\n    DIR* dir;   \n    dir = opendir(_path.c_str());\n    struct dirent* ptr;\n    std::vector<std::string> file;\n    while((ptr = readdir(dir)) != NULL)\n    {\n        if(ptr->d_name[0] == '.') {continue;}\n        file.push_back(ptr->d_name);\n    }\n    closedir(dir);\n    sort(file.begin(), file.end());\n    _files = file;\n}",
            "title": "\u83b7\u53d6\u76ee\u5f55\u4e0b\u7684\u6587\u4ef6"
        },
        {
            "location": "/c++/#_7",
            "text": "#include <iostream>\n#include <fstream>\nusing namespace std;\n\nvoid main()\n{\n    ofstream in;\n    in.open(\"com.txt\",ios::trunc); //ios::trunc\u8868\u793a\u5728\u6253\u5f00\u6587\u4ef6\u524d\u5c06\u6587\u4ef6\u6e05\u7a7a,\u7531\u4e8e\u662f\u5199\u5165,\u6587\u4ef6\u4e0d\u5b58\u5728\u5219\u521b\u5efa\n    int i;\n    char a='a';\n    for(i=1;i<=26;i++)//\u5c0626\u4e2a\u6570\u5b57\u53ca\u82f1\u6587\u5b57\u6bcd\u5199\u5165\u6587\u4ef6\n    {\n    if(i<10)\n    {\n        in<<\"0\"<<i<<\"\\t\"<<a<<\"\\n\";\n        a++;\n    }\n    else\n    {\n        in<<i<<\"\\t\"<<a<<\"\\n\";\n        a++;\n    }\n    }\n    in.close();//\u5173\u95ed\u6587\u4ef6\n}",
            "title": "\u5199\u5165\u6587\u4ef6"
        },
        {
            "location": "/c++/#_8",
            "text": "#include <iostream>\n#include <cstdlib>\n#include <ctime>\nusing namespace std;\n\nint main()\n{\n    srand((int)time(0));  // \u4ea7\u751f\u968f\u673a\u79cd\u5b50  \u628a0\u6362\u6210NULL\u4e5f\u884c\n    for (int i = 0; i < 10; i++)\n    {\n        cout << rand()%100<< \" \";\n    }\n    return 0;\n}",
            "title": "\u4ea7\u751f\u968f\u673a\u6570"
        },
        {
            "location": "/caffe/",
            "text": "caffe paython \u73af\u5883\u914d\u7f6e\n\n\n\n\nexport PYTHONPATH=$HOME/Downloads/caffe/python:$PYTHONPATH\n\n\n\n\ncaffe \u5b89\u88c5\n\n\n\n\ngit clone https://github.com/BVLC/caffe.git\n\n\n\n\n\u5b89\u88c5\u4f9d\u8d56\n\n\nsudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler\nsudo apt-get install --no-install-recommends libboost-all-dev\n\nsudo apt-get install libopenblas-dev\n\nsudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev\n\n\n\nMakefile.config\n\n\n## Refer to http://caffe.berkeleyvision.org/installation.html\n# Contributions simplifying and improving our build system are welcome!\n\n# cuDNN acceleration switch (uncomment to build with cuDNN).\n# USE_CUDNN := 1\n\n# CPU-only switch (uncomment to build without GPU support).\n# CPU_ONLY := 1\n\n# uncomment to disable IO dependencies and corresponding data layers\n# USE_OPENCV := 0\n# USE_LEVELDB := 0\n# USE_LMDB := 0\n\n# uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)\n#   You should not set this flag if you will be reading LMDBs with any\n#   possibility of simultaneous read and write\n# ALLOW_LMDB_NOLOCK := 1\n\n# Uncomment if you're using OpenCV 3\n OPENCV_VERSION := 3\n\n# To customize your choice of compiler, uncomment and set the following.\n# N.B. the default for Linux is g++ and the default for OSX is clang++\n# CUSTOM_CXX := g++\n\n# CUDA directory contains bin/ and lib/ directories that we need.\nCUDA_DIR := /usr/local/cuda\n# On Ubuntu 14.04, if cuda tools are installed via\n# \"sudo apt-get install nvidia-cuda-toolkit\" then use this instead:\n# CUDA_DIR := /usr\n\n# CUDA architecture setting: going with all of them.\n# For CUDA < 6.0, comment the lines after *_35 for compatibility.\nCUDA_ARCH := -gencode arch=compute_30,code=sm_30 \\\n             -gencode arch=compute_35,code=sm_35 \\\n             -gencode arch=compute_50,code=sm_50 \\\n             -gencode arch=compute_52,code=sm_52 \\\n             -gencode arch=compute_61,code=sm_61 \\\n             -gencode arch=compute_61,code=compute_61\n\n             # -gencode arch=compute_20,code=sm_20 \\\n             # -gencode arch=compute_20,code=sm_21 \\\n\n\n# BLAS choice:\n# atlas for ATLAS (default)\n# mkl for MKL\n# open for OpenBlas\n# BLAS := atlas\nBLAS := open\n# Custom (MKL/ATLAS/OpenBLAS) include and lib directories.\n# Leave commented to accept the defaults for your choice of BLAS\n# (which should work)!\n# BLAS_INCLUDE := /path/to/your/blas\n# BLAS_LIB := /path/to/your/blas\n\n# Homebrew puts openblas in a directory that is not on the standard search path\n# BLAS_INCLUDE := $(shell brew --prefix openblas)/include\n# BLAS_LIB := $(shell brew --prefix openblas)/lib\n\n# This is required only if you will compile the matlab interface.\n# MATLAB directory should contain the mex binary in /bin.\n# MATLAB_DIR := /usr/local\n# MATLAB_DIR := /Applications/MATLAB_R2012b.app\n\n# NOTE: this is required only if you will compile the python interface.\n# We need to be able to find Python.h and numpy/arrayobject.h.\nPYTHON_INCLUDE := /usr/include/python2.7 \\\n        /usr/lib/python2.7/dist-packages/numpy/core/include\n# Anaconda Python distribution is quite popular. Include path:\n# Verify anaconda location, sometimes it's in root.\n# ANACONDA_HOME := $(HOME)/anaconda2\n# PYTHON_INCLUDE := $(ANACONDA_HOME)/include \\\n        $(ANACONDA_HOME)/include/python2.7 \\\n        $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include \\\n\n# Uncomment to use Python 3 (default is Python 2)\n# PYTHON_LIBRARIES := boost_python3 python3.5m\n# PYTHON_INCLUDE := /usr/include/python3.5m \\\n#                 /usr/lib/python3.5/dist-packages/numpy/core/include\n\n# We need to be able to find libpythonX.X.so or .dylib.\nPYTHON_LIB := /usr/lib\n# PYTHON_LIB := $(ANACONDA_HOME)/lib\n\n# Homebrew installs numpy in a non standard path (keg only)\n# PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include\n# PYTHON_LIB += $(shell brew --prefix numpy)/lib\n\n# Uncomment to support layers written in Python (will link against Python libs)\n# WITH_PYTHON_LAYER := 1\n\n# Whatever else you find you need goes here.\nINCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial/\nLIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib  /usr/lib/x86_64-linux-gnu/hdf5/serial\n\n# If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies\n# INCLUDE_DIRS += $(shell brew --prefix)/include\n# LIBRARY_DIRS += $(shell brew --prefix)/lib\n\n# Uncomment to use `pkg-config` to specify OpenCV library paths.\n# (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)\n# USE_PKG_CONFIG := 1\n\n# N.B. both build and distribute dirs are cleared on `make clean`\nBUILD_DIR := build\nDISTRIBUTE_DIR := distribute\n\n# Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171\n# DEBUG := 1\n\n# The ID of the GPU that 'make runtest' will use to run unit tests.\nTEST_GPUID := 0\n\n# enable pretty build (comment to see full commands)\nQ ?= @\n\n\n\n\u5b89\u88c5\n\n\n\n\nsudo make -j8\\\nsudo make distribute\n\n\n\n\n\u5e94\u7528\n\n\nCMakeLists.txt\n\n\nset(CAFFE_PATH \"$ENV{HOME}/Disk/caffe/distribute\")\nif (EXISTS \"${CAFFE_PATH}\")\n    include_directories(\n            include\n            ${catkin_INCLUDE_DIRS}\n            ${OpenCV_INCLUDE_DIRS}\n            ${CAFFE_PATH}/include\n    )\n    target_link_libraries(lidar_cnn_seg_detect\n            ${catkin_LIBRARIES}\n            ${OpenCV_LIBRARIES}\n            ${CUDA_LIBRARIES}\n            ${CAFFE_PATH}/lib/libcaffe.so\n            glog\n            )\n\n\n\n\ncaffe python layer\n\n\nlayer\n\n\ntuozhan\n\n\nThe models are defined in plaintext protocol buffer schema (prototxt) while the learned models are serialized as binary protocol buffer (binaryproto) .caffemodel files.\n\n\n\n\ncaffe \u8bad\u7ec3\u9700\u8981\u6570\u636e\u96c6\uff0c\u7f51\u7edc.prototxt \u548c solver.prototxt\n\n\n\n\ncaffe \u7f51\u7edc\u53ef\u89c6\u5316\n\n\n\n\npython draw_net.py --rankdir TB test.prototxt test.png",
            "title": "Caffe"
        },
        {
            "location": "/caffe/#caffe-paython",
            "text": "export PYTHONPATH=$HOME/Downloads/caffe/python:$PYTHONPATH",
            "title": "caffe paython \u73af\u5883\u914d\u7f6e"
        },
        {
            "location": "/caffe/#caffe",
            "text": "git clone https://github.com/BVLC/caffe.git",
            "title": "caffe \u5b89\u88c5"
        },
        {
            "location": "/caffe/#_1",
            "text": "sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler\nsudo apt-get install --no-install-recommends libboost-all-dev\n\nsudo apt-get install libopenblas-dev\n\nsudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev",
            "title": "\u5b89\u88c5\u4f9d\u8d56"
        },
        {
            "location": "/caffe/#makefileconfig",
            "text": "## Refer to http://caffe.berkeleyvision.org/installation.html\n# Contributions simplifying and improving our build system are welcome!\n\n# cuDNN acceleration switch (uncomment to build with cuDNN).\n# USE_CUDNN := 1\n\n# CPU-only switch (uncomment to build without GPU support).\n# CPU_ONLY := 1\n\n# uncomment to disable IO dependencies and corresponding data layers\n# USE_OPENCV := 0\n# USE_LEVELDB := 0\n# USE_LMDB := 0\n\n# uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)\n#   You should not set this flag if you will be reading LMDBs with any\n#   possibility of simultaneous read and write\n# ALLOW_LMDB_NOLOCK := 1\n\n# Uncomment if you're using OpenCV 3\n OPENCV_VERSION := 3\n\n# To customize your choice of compiler, uncomment and set the following.\n# N.B. the default for Linux is g++ and the default for OSX is clang++\n# CUSTOM_CXX := g++\n\n# CUDA directory contains bin/ and lib/ directories that we need.\nCUDA_DIR := /usr/local/cuda\n# On Ubuntu 14.04, if cuda tools are installed via\n# \"sudo apt-get install nvidia-cuda-toolkit\" then use this instead:\n# CUDA_DIR := /usr\n\n# CUDA architecture setting: going with all of them.\n# For CUDA < 6.0, comment the lines after *_35 for compatibility.\nCUDA_ARCH := -gencode arch=compute_30,code=sm_30 \\\n             -gencode arch=compute_35,code=sm_35 \\\n             -gencode arch=compute_50,code=sm_50 \\\n             -gencode arch=compute_52,code=sm_52 \\\n             -gencode arch=compute_61,code=sm_61 \\\n             -gencode arch=compute_61,code=compute_61\n\n             # -gencode arch=compute_20,code=sm_20 \\\n             # -gencode arch=compute_20,code=sm_21 \\\n\n\n# BLAS choice:\n# atlas for ATLAS (default)\n# mkl for MKL\n# open for OpenBlas\n# BLAS := atlas\nBLAS := open\n# Custom (MKL/ATLAS/OpenBLAS) include and lib directories.\n# Leave commented to accept the defaults for your choice of BLAS\n# (which should work)!\n# BLAS_INCLUDE := /path/to/your/blas\n# BLAS_LIB := /path/to/your/blas\n\n# Homebrew puts openblas in a directory that is not on the standard search path\n# BLAS_INCLUDE := $(shell brew --prefix openblas)/include\n# BLAS_LIB := $(shell brew --prefix openblas)/lib\n\n# This is required only if you will compile the matlab interface.\n# MATLAB directory should contain the mex binary in /bin.\n# MATLAB_DIR := /usr/local\n# MATLAB_DIR := /Applications/MATLAB_R2012b.app\n\n# NOTE: this is required only if you will compile the python interface.\n# We need to be able to find Python.h and numpy/arrayobject.h.\nPYTHON_INCLUDE := /usr/include/python2.7 \\\n        /usr/lib/python2.7/dist-packages/numpy/core/include\n# Anaconda Python distribution is quite popular. Include path:\n# Verify anaconda location, sometimes it's in root.\n# ANACONDA_HOME := $(HOME)/anaconda2\n# PYTHON_INCLUDE := $(ANACONDA_HOME)/include \\\n        $(ANACONDA_HOME)/include/python2.7 \\\n        $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include \\\n\n# Uncomment to use Python 3 (default is Python 2)\n# PYTHON_LIBRARIES := boost_python3 python3.5m\n# PYTHON_INCLUDE := /usr/include/python3.5m \\\n#                 /usr/lib/python3.5/dist-packages/numpy/core/include\n\n# We need to be able to find libpythonX.X.so or .dylib.\nPYTHON_LIB := /usr/lib\n# PYTHON_LIB := $(ANACONDA_HOME)/lib\n\n# Homebrew installs numpy in a non standard path (keg only)\n# PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include\n# PYTHON_LIB += $(shell brew --prefix numpy)/lib\n\n# Uncomment to support layers written in Python (will link against Python libs)\n# WITH_PYTHON_LAYER := 1\n\n# Whatever else you find you need goes here.\nINCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial/\nLIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib  /usr/lib/x86_64-linux-gnu/hdf5/serial\n\n# If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies\n# INCLUDE_DIRS += $(shell brew --prefix)/include\n# LIBRARY_DIRS += $(shell brew --prefix)/lib\n\n# Uncomment to use `pkg-config` to specify OpenCV library paths.\n# (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)\n# USE_PKG_CONFIG := 1\n\n# N.B. both build and distribute dirs are cleared on `make clean`\nBUILD_DIR := build\nDISTRIBUTE_DIR := distribute\n\n# Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171\n# DEBUG := 1\n\n# The ID of the GPU that 'make runtest' will use to run unit tests.\nTEST_GPUID := 0\n\n# enable pretty build (comment to see full commands)\nQ ?= @",
            "title": "Makefile.config"
        },
        {
            "location": "/caffe/#_2",
            "text": "sudo make -j8\\\nsudo make distribute",
            "title": "\u5b89\u88c5"
        },
        {
            "location": "/caffe/#_3",
            "text": "CMakeLists.txt  set(CAFFE_PATH \"$ENV{HOME}/Disk/caffe/distribute\")\nif (EXISTS \"${CAFFE_PATH}\")\n    include_directories(\n            include\n            ${catkin_INCLUDE_DIRS}\n            ${OpenCV_INCLUDE_DIRS}\n            ${CAFFE_PATH}/include\n    )\n    target_link_libraries(lidar_cnn_seg_detect\n            ${catkin_LIBRARIES}\n            ${OpenCV_LIBRARIES}\n            ${CUDA_LIBRARIES}\n            ${CAFFE_PATH}/lib/libcaffe.so\n            glog\n            )",
            "title": "\u5e94\u7528"
        },
        {
            "location": "/caffe/#caffe-python-layer",
            "text": "layer  tuozhan",
            "title": "caffe python layer"
        },
        {
            "location": "/caffe/#the-models-are-defined-in-plaintext-protocol-buffer-schema-prototxt-while-the-learned-models-are-serialized-as-binary-protocol-buffer-binaryproto-caffemodel-files",
            "text": "caffe \u8bad\u7ec3\u9700\u8981\u6570\u636e\u96c6\uff0c\u7f51\u7edc.prototxt \u548c solver.prototxt",
            "title": "The models are defined in plaintext protocol buffer schema (prototxt) while the learned models are serialized as binary protocol buffer (binaryproto) .caffemodel files."
        },
        {
            "location": "/caffe/#caffe_1",
            "text": "python draw_net.py --rankdir TB test.prototxt test.png",
            "title": "caffe \u7f51\u7edc\u53ef\u89c6\u5316"
        },
        {
            "location": "/cmake/",
            "text": "\u5f15\u7528so\u6587\u4ef6\n\n\ntarget_link_libraries(vision_qt ${QT_LIBRARIES} \n${CMAKE_CURRENT_SOURCE_DIR}/lib/x64/libMVSDK.so\n${catkin_LIBRARIES})\n\n\n\ncmake \u6392\u9664 include \u8def\u5f84\n\n\nLIST(APPEND CMAKE_CXX_IMPLICIT_INCLUDE_DIRECTORIES \"/usr/include/opencv4\")\nLIST(APPEND CMAKE_C_IMPLICIT_INCLUDE_DIRECTORIES \"/usr/include/opencv4\")\n\n\n\n\u89e3\u9664\u6392\u9664\n\n\nunset(CMAKE_C_IMPLICIT_INCLUDE_DIRECTORIES)\nunset(CMAKE_CXX_IMPLICIT_INCLUDE_DIRECTORIES)\n\n\n\n\u6392\u9664\u9ed8\u8ba4\u8def\u5f84\u5bfb\u627epkg\n\n\nfind_package(OpenCV 3 REQUIRED NO_DEFAULT_PATH)\n\n\n\nubuntu\u5b89\u88c5\u6700\u65b0\u7684cmake\n\n\nDownload\n\n\n\u4e0b\u8f7dsource\u6587\u4ef6\uff0c\u89e3\u538bcmake\u6587\u4ef6\u5939\n\n\ncd cmake\n./configure\nmake -j8\nsudo make install\ncmake --version",
            "title": "Cmake"
        },
        {
            "location": "/cmake/#so",
            "text": "target_link_libraries(vision_qt ${QT_LIBRARIES} \n${CMAKE_CURRENT_SOURCE_DIR}/lib/x64/libMVSDK.so\n${catkin_LIBRARIES})",
            "title": "\u5f15\u7528so\u6587\u4ef6"
        },
        {
            "location": "/cmake/#cmake-include",
            "text": "LIST(APPEND CMAKE_CXX_IMPLICIT_INCLUDE_DIRECTORIES \"/usr/include/opencv4\")\nLIST(APPEND CMAKE_C_IMPLICIT_INCLUDE_DIRECTORIES \"/usr/include/opencv4\")",
            "title": "cmake \u6392\u9664 include \u8def\u5f84"
        },
        {
            "location": "/cmake/#_1",
            "text": "unset(CMAKE_C_IMPLICIT_INCLUDE_DIRECTORIES)\nunset(CMAKE_CXX_IMPLICIT_INCLUDE_DIRECTORIES)",
            "title": "\u89e3\u9664\u6392\u9664"
        },
        {
            "location": "/cmake/#pkg",
            "text": "find_package(OpenCV 3 REQUIRED NO_DEFAULT_PATH)",
            "title": "\u6392\u9664\u9ed8\u8ba4\u8def\u5f84\u5bfb\u627epkg"
        },
        {
            "location": "/cmake/#ubuntucmake",
            "text": "Download  \u4e0b\u8f7dsource\u6587\u4ef6\uff0c\u89e3\u538bcmake\u6587\u4ef6\u5939  cd cmake\n./configure\nmake -j8\nsudo make install\ncmake --version",
            "title": "ubuntu\u5b89\u88c5\u6700\u65b0\u7684cmake"
        },
        {
            "location": "/cuda/",
            "text": "cuda \u5b89\u88c5\n\n\n.bashrc\u91cc\u6dfb\u52a0\n\n\nexport PATH=/usr/local/cuda-10.0/bin${PATH:+:${PATH}}\nexport LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\n\n\n\ncudnn.h: No such file or directory\n\n\n\u5728 /home/promote/NeDisk/anaconda3/pkgs/cudnn-7.6.5-cuda10.0_0/include \u76ee\u5f55\u4e0b\u62f7\u8d1d\u4e00\u4e2acudnn.h\u5230\u9879\u76ee\u76ee\u5f55\u91cc\u5c31\u53ef\u4ee5\u4e86\u3002\n\n\ncould not find -lcudnn\n\n\nLDFLAGS+= -L/home/promote/NeDisk/anaconda3/pkgs/cudnn-7.6.5-cuda10.0_0/lib -lcudnn\n\n\n\n\u5b89\u88c5 libcudnn.so \u548c cudnn.h\n\n\nconda install cudatoolkit==10.0.130 \nconda install cudnn==7.6.5\n\n\n\ncannot find -lcuda\n\n\nlocate libcuda\n\u627e\u5230 /usr/lib/x86_64-linux-gnu/libcuda.so.1\n\nsudo ln -s /usr/lib/x86_64-linux-gnu/libcuda.so.1 /usr/local/cuda/lib64/libcuda.so",
            "title": "Cuda"
        },
        {
            "location": "/cuda/#cuda",
            "text": ".bashrc\u91cc\u6dfb\u52a0  export PATH=/usr/local/cuda-10.0/bin${PATH:+:${PATH}}\nexport LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}",
            "title": "cuda \u5b89\u88c5"
        },
        {
            "location": "/cuda/#cudnnh-no-such-file-or-directory",
            "text": "\u5728 /home/promote/NeDisk/anaconda3/pkgs/cudnn-7.6.5-cuda10.0_0/include \u76ee\u5f55\u4e0b\u62f7\u8d1d\u4e00\u4e2acudnn.h\u5230\u9879\u76ee\u76ee\u5f55\u91cc\u5c31\u53ef\u4ee5\u4e86\u3002",
            "title": "cudnn.h: No such file or directory"
        },
        {
            "location": "/cuda/#could-not-find-lcudnn",
            "text": "LDFLAGS+= -L/home/promote/NeDisk/anaconda3/pkgs/cudnn-7.6.5-cuda10.0_0/lib -lcudnn",
            "title": "could not find -lcudnn"
        },
        {
            "location": "/cuda/#libcudnnso-cudnnh",
            "text": "conda install cudatoolkit==10.0.130 \nconda install cudnn==7.6.5",
            "title": "\u5b89\u88c5 libcudnn.so \u548c cudnn.h"
        },
        {
            "location": "/cuda/#cannot-find-lcuda",
            "text": "locate libcuda\n\u627e\u5230 /usr/lib/x86_64-linux-gnu/libcuda.so.1\n\nsudo ln -s /usr/lib/x86_64-linux-gnu/libcuda.so.1 /usr/local/cuda/lib64/libcuda.so",
            "title": "cannot find -lcuda"
        },
        {
            "location": "/deeplearning/",
            "text": "LeNet\n\n\n\n\n\n\nuse convolution to extract spatial features.\n\n\nsubsample using spatial average of maps.\n\n\nnon-linearity in the form of tanh or sigmoids.\n\n\nmulti-layer neural network(MLP) as final classifier\n\n\nsparse connection matrix between layers to avoid large computational cost\n\n\n\n\n\n\nAlexNet\n\n\n\n\nuse of rectified linear units(ReLU) as non-linearities.\n\n\nuse of dropout technique to selectively ignore single neurons during training , a way to avoid overfitting of the model.\n\n\noverlapping max pooling, avoiding the averaging effects of average pooling.\n\n\nusing GPUs to reduce training time\n\n\n\n\nxception\n\n\nComputing the mean and std of dataset\n\n\nimport tensorflow as tf\nfrom PIL import ImageStat\n\nclass Stats(ImageStat.Stat):\n    def __add__(self, other):\n        return Stats(list(map(add, self.h, other.h)))\n\nloader = DataLoader(dataset, batch_size=10, num_workers=5)\n\nstatistics = None\nfor data in loader:\n    for b in range(data.shape[0]):\n        if statistics is None:\n            statistics = Stats(tf.to_pil_image(data[b]))\n        else:\n            statistics += Stats(tf.to_pil_image(data[b]))\n\nprint(f'mean:{statistics.mean}, std:{statistics.stddev}')\n\n\n\npyx to so\n\n\npython setup.py build_ext --inplace\n\n\n\u5f7b\u5e95\u5220\u9664cuda\n\n\nsudo apt-get purge nvidia*\nsudo apt-get autoremove\nsudo apt-get autoclean\nsudo rm -rf /usr/local/cuda*\n\n\n\nSteps to Install PyTorch With CUDA 10.0\n\n\nconda install pytorch torchvision cudatoolkit=10.0 -c pytorch\n\\\n\npip install torchsummary\n\n\nconda install matplotlib\n\n\nconda install -c conda-forge matplotlib\n\n\nconda install tensorflow object_detection api\n\n\nconda install -c conda-forge tf_object_detection\n\n\nModuleNotFoundError: No module named 'deployment'\n\n\nfrom slim.deployment import model_deploy\n\n\nModuleNotFoundError: No module named 'slim'\n\n\nconda install -c conda-forge tf-slim\n\n\nModuleNotFoundError: No module named 'nets'\n\n\nchange \nfrome nets\n to \nfrom slim.nets\n\n\ncreating trainval.txt\n\n\nimport os\nimport random\nimport sys\n\nif len(sys.argv) < 2:\n    print(\"no directory specified, please input target directory\")\n    exit()\n\nroot_path = sys.argv[1]\n\nxmlfilepath = root_path + '/Annotations'\n\ntxtsavepath = root_path + '/ImageSets/Main'\n\nif not os.path.exists(root_path):\n    print(\"cannot find such directory: \" + root_path)\n    exit()\n\nif not os.path.exists(txtsavepath):\n    os.makedirs(txtsavepath)\n\ntrainval_percent = 0.9\ntrain_percent = 0.8\ntotal_xml = os.listdir(xmlfilepath)\nnum = len(total_xml)\nlist = range(num)\ntv = int(num * trainval_percent)\ntr = int(tv * train_percent)\ntrainval = random.sample(list, tv)\ntrain = random.sample(trainval, tr)\n\nprint(\"train and val size:\", tv)\nprint(\"train size:\", tr)\n\nftrainval = open(txtsavepath + '/trainval.txt', 'w')\nftest = open(txtsavepath + '/test.txt', 'w')\nftrain = open(txtsavepath + '/train.txt', 'w')\nfval = open(txtsavepath + '/val.txt', 'w')\n\nfor i in list:\n    name = total_xml[i][:-4] + '\\n'\n    if i in trainval:\n        ftrainval.write(name)\n        if i in train:\n            ftrain.write(name)\n        else:\n            fval.write(name)\n    else:\n        ftest.write(name)\n\nftrainval.close()\nftrain.close()\nfval.close()\nftest.close()\n\n\n\n\ncreating TFR datasets\n\n\nimport hashlib\nimport io\nimport logging\nimport os\nimport random\nimport re\n\nfrom lxml import etree\nimport PIL.Image\nimport tensorflow as tf\n\nfrom object_detection.utils import dataset_util\nfrom object_detection.utils import label_map_util\n\nflags = tf.app.flags\nflags.DEFINE_string('data_dir', '', 'Root directory to raw pet dataset.')\nflags.DEFINE_string('output_dir', '', 'Path to directory to output TFRecords.')\nflags.DEFINE_string('label_map_path', 'data/pet_label_map.pbtxt',\n                    'Path to label map proto')\nFLAGS = flags.FLAGS\n\n\ndef get_class_name_from_filename(file_name):\n  \"\"\"Gets the class name from a file.\n\n  Args:\n    file_name: The file name to get the class name from.\n               ie. \"american_pit_bull_terrier_105.jpg\"\n\n  Returns:\n    A string of the class name.\n  \"\"\"\n  print(file_name)\n  match = re.match(r'([A-Za-z_]+)(_[0-9]+\\.jpg)', file_name, re.I)\n  return match.groups()[0]\n\n\ndef dict_to_tf_example(data,\n                       label_map_dict,\n                       image_subdirectory,\n                       ignore_difficult_instances=False):\n  \"\"\"Convert XML derived dict to tf.Example proto.\n\n  Notice that this function normalizes the bounding box coordinates provided\n  by the raw data.\n\n  Args:\n    data: dict holding PASCAL XML fields for a single image (obtained by\n      running dataset_util.recursive_parse_xml_to_dict)\n    label_map_dict: A map from string label names to integers ids.\n    image_subdirectory: String specifying subdirectory within the\n      Pascal dataset directory holding the actual image data.\n    ignore_difficult_instances: Whether to skip difficult instances in the\n      dataset  (default: False).\n\n  Returns:\n    example: The converted tf.Example.\n\n  Raises:\n    ValueError: if the image pointed to by data['filename'] is not a valid JPEG\n  \"\"\"\n  img_path = os.path.join(image_subdirectory, data['filename'])\n  with tf.gfile.GFile(img_path, 'rb') as fid:\n    encoded_jpg = fid.read()\n  encoded_jpg_io = io.BytesIO(encoded_jpg)\n  image = PIL.Image.open(encoded_jpg_io)\n  if image.format != 'JPEG':\n    raise ValueError('Image format not JPEG')\n  key = hashlib.sha256(encoded_jpg).hexdigest()\n\n  width = int(data['size']['width'])\n  height = int(data['size']['height'])\n\n  xmin = []\n  ymin = []\n  xmax = []\n  ymax = []\n  classes = []\n  classes_text = []\n  truncated = []\n  poses = []\n  difficult_obj = []\n  for obj in data['object']:\n    difficult = bool(int(obj['difficult']))\n    if ignore_difficult_instances and difficult:\n      continue\n\n    difficult_obj.append(int(difficult))\n\n    xmin.append(float(obj['bndbox']['xmin']) / width)\n    ymin.append(float(obj['bndbox']['ymin']) / height)\n    xmax.append(float(obj['bndbox']['xmax']) / width)\n    ymax.append(float(obj['bndbox']['ymax']) / height)\n    class_name = get_class_name_from_filename(data['filename'])\n    classes_text.append(class_name.encode('utf8'))\n    classes.append(label_map_dict[class_name])\n    truncated.append(int(obj['truncated']))\n    poses.append(obj['pose'].encode('utf8'))\n\n  example = tf.train.Example(features=tf.train.Features(feature={\n      'image/height': dataset_util.int64_feature(height),\n      'image/width': dataset_util.int64_feature(width),\n      'image/filename': dataset_util.bytes_feature(\n          data['filename'].encode('utf8')),\n      'image/source_id': dataset_util.bytes_feature(\n          data['filename'].encode('utf8')),\n      'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')),\n      'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n      'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')),\n      'image/object/bbox/xmin': dataset_util.float_list_feature(xmin),\n      'image/object/bbox/xmax': dataset_util.float_list_feature(xmax),\n      'image/object/bbox/ymin': dataset_util.float_list_feature(ymin),\n      'image/object/bbox/ymax': dataset_util.float_list_feature(ymax),\n      'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n      'image/object/class/label': dataset_util.int64_list_feature(classes),\n      'image/object/difficult': dataset_util.int64_list_feature(difficult_obj),\n      'image/object/truncated': dataset_util.int64_list_feature(truncated),\n      'image/object/view': dataset_util.bytes_list_feature(poses),\n  }))\n  return example\n\n\ndef create_tf_record(output_filename,\n                     label_map_dict,\n                     annotations_dir,\n                     image_dir,\n                     examples):\n  \"\"\"Creates a TFRecord file from examples.\n\n  Args:\n    output_filename: Path to where output file is saved.\n    label_map_dict: The label map dictionary.\n    annotations_dir: Directory where annotation files are stored.\n    image_dir: Directory where image files are stored.\n    examples: Examples to parse and save to tf record.\n  \"\"\"\n  writer = tf.python_io.TFRecordWriter(output_filename)\n  for idx, example in enumerate(examples):\n    if idx % 100 == 0:\n      logging.info('On image %d of %d', idx, len(examples))\n    # path = os.path.join(annotations_dir, 'xmls', example + '.xml')\n    path = os.path.join(annotations_dir, example + '.xml')\n    if not os.path.exists(path):\n      logging.warning('Could not find %s, ignoring example.', path)\n      continue\n    with tf.gfile.GFile(path, 'r') as fid:\n      xml_str = fid.read()\n    xml = etree.fromstring(xml_str)\n    data = dataset_util.recursive_parse_xml_to_dict(xml)['annotation']\n\n    tf_example = dict_to_tf_example(data, label_map_dict, image_dir)\n    writer.write(tf_example.SerializeToString())\n\n  writer.close()\n\n\n# TODO: Add test for pet/PASCAL main files.\ndef main(_):\n  data_dir = FLAGS.data_dir\n  label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path)\n\n  logging.info('Reading from Pet dataset.')\n  image_dir = os.path.join(data_dir, 'images')\n  annotations_dir = os.path.join(data_dir, 'annotations')\n  examples_path = os.path.join(annotations_dir, 'trainval.txt')\n  examples_list = dataset_util.read_examples_list(examples_path)\n\n  # Test images are not included in the downloaded data set, so we shall perform\n  # our own split.\n  random.seed(42)\n  random.shuffle(examples_list)\n  num_examples = len(examples_list)\n  num_train = int(0.7 * num_examples)\n  train_examples = examples_list[:num_train]\n  val_examples = examples_list[num_train:]\n  logging.info('%d training and %d validation examples.',\n               len(train_examples), len(val_examples))\n\n  train_output_path = os.path.join(FLAGS.output_dir, 'pet_train.record')\n  val_output_path = os.path.join(FLAGS.output_dir, 'pet_val.record')\n  create_tf_record(train_output_path, label_map_dict, annotations_dir,\n                   image_dir, train_examples)\n  create_tf_record(val_output_path, label_map_dict, annotations_dir,\n                   image_dir, val_examples)\n\nif __name__ == '__main__':\n  tf.app.run()\n\n\n\n\npython object_detection/dataset_tools/create_pet_tf_record.py \\\n    --label_map_path=object_detection/data/pet_label_map.pbtxt \\\n    --data_dir=`pwd` \\\n    --output_dir=`pwd`\n\n\n\ncreate model config file\n\n\n# Faster R-CNN with Resnet-101 (v1), configuration for MSCOCO Dataset.\n# Users should configure the fine_tune_checkpoint field in the train config as\n# well as the label_map_path and input_path fields in the train_input_reader and\n# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n# should be configured.\n\nmodel {\n  faster_rcnn {\n    num_classes: 90\n    image_resizer {\n      keep_aspect_ratio_resizer {\n        min_dimension: 600\n        max_dimension: 1024\n      }\n    }\n    feature_extractor {\n      type: 'faster_rcnn_resnet101'\n      first_stage_features_stride: 16\n    }\n    first_stage_anchor_generator {\n      grid_anchor_generator {\n        scales: [0.25, 0.5, 1.0, 2.0]\n        aspect_ratios: [0.5, 1.0, 2.0]\n        height_stride: 16\n        width_stride: 16\n      }\n    }\n    first_stage_box_predictor_conv_hyperparams {\n      op: CONV\n      regularizer {\n        l2_regularizer {\n          weight: 0.0\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n          stddev: 0.01\n        }\n      }\n    }\n    first_stage_nms_score_threshold: 0.0\n    first_stage_nms_iou_threshold: 0.7\n    first_stage_max_proposals: 300\n    first_stage_localization_loss_weight: 2.0\n    first_stage_objectness_loss_weight: 1.0\n    initial_crop_size: 14\n    maxpool_kernel_size: 2\n    maxpool_stride: 2\n    second_stage_box_predictor {\n      mask_rcnn_box_predictor {\n        use_dropout: false\n        dropout_keep_probability: 1.0\n        fc_hyperparams {\n          op: FC\n          regularizer {\n            l2_regularizer {\n              weight: 0.0\n            }\n          }\n          initializer {\n            variance_scaling_initializer {\n              factor: 1.0\n              uniform: true\n              mode: FAN_AVG\n            }\n          }\n        }\n      }\n    }\n    second_stage_post_processing {\n      batch_non_max_suppression {\n        score_threshold: 0.0\n        iou_threshold: 0.6\n        max_detections_per_class: 100\n        max_total_detections: 300\n      }\n      score_converter: SOFTMAX\n    }\n    second_stage_localization_loss_weight: 2.0\n    second_stage_classification_loss_weight: 1.0\n  }\n}\n\ntrain_config: {\n  batch_size: 1\n  optimizer {\n    momentum_optimizer: {\n      learning_rate: {\n        manual_step_learning_rate {\n          initial_learning_rate: 0.0003\n          schedule {\n            step: 900000\n            learning_rate: .00003\n          }\n          schedule {\n            step: 1200000\n            learning_rate: .000003\n          }\n        }\n      }\n      momentum_optimizer_value: 0.9\n    }\n    use_moving_average: false\n  }\n  gradient_clipping_by_norm: 10.0\n  fine_tune_checkpoint: \"PATH_TO_BE_CONFIGURED/model.ckpt\"\n  from_detection_checkpoint: true\n  data_augmentation_options {\n    random_horizontal_flip {\n    }\n  }\n}\n\ntrain_input_reader: {\n  tf_record_input_reader {\n    input_path: \"PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100\"\n  }\n  label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\"\n}\n\neval_config: {\n  num_examples: 8000\n  # Note: The below line limits the evaluation process to 10 evaluations.\n  # Remove the below line to evaluate indefinitely.\n  max_evals: 10\n}\n\neval_input_reader: {\n  tf_record_input_reader {\n    input_path: \"PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010\"\n  }\n  label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\"\n  shuffle: false\n  num_readers: 1\n}\n\n\n\ntrain\n\n\npython train.py --logtostderr \n--train_dir=/home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/train_dir \n--pipeline_config_path=/home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/net.config\n\n\n\ntensorboard --logdir /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/train_dir/\n\n\n\nevaluator\n\n\npython new_eval.py --logtostderr \n--checkpoint_dir /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/train_dir \n--eval_dir /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/eval_dir \n--pipeline_config_path /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/net.config\n\n\n\ntensorboard --logdir /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/eval_dir/\n\n\n\n# new_eval.py\nimport functools\nimport os\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.python.util.deprecation import deprecated\nfrom object_detection.builders import dataset_builder\nfrom object_detection.builders import graph_rewriter_builder\nfrom object_detection.builders import model_builder\nfrom object_detection.legacy import evaluator\nfrom object_detection.utils import config_util\nfrom object_detection.utils import label_map_util\n\n\nfrom tensorflow.compat.v1 import ConfigProto\nfrom tensorflow.compat.v1 import InteractiveSession\nimport keras.backend as K\nconfig = ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = InteractiveSession(config=config)\nK.set_session(session)\n\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\nflags = tf.app.flags\nflags.DEFINE_boolean('eval_training_data', False,\n                     'If training data should be evaluated for this job.')\nflags.DEFINE_string(\n    'checkpoint_dir', '',\n    'Directory containing checkpoints to evaluate, typically '\n    'set to `train_dir` used in the training job.')\nflags.DEFINE_string('eval_dir', '', 'Directory to write eval summaries to.')\nflags.DEFINE_string(\n    'pipeline_config_path', '',\n    'Path to a pipeline_pb2.TrainEvalPipelineConfig config '\n    'file. If provided, other configs are ignored')\nflags.DEFINE_string('eval_config_path', '',\n                    'Path to an eval_pb2.EvalConfig config file.')\nflags.DEFINE_string('input_config_path', '',\n                    'Path to an input_reader_pb2.InputReader config file.')\nflags.DEFINE_string('model_config_path', '',\n                    'Path to a model_pb2.DetectionModel config file.')\nflags.DEFINE_boolean(\n    'run_once', False, 'Option to only run a single pass of '\n    'evaluation. Overrides the `max_evals` parameter in the '\n    'provided config.')\nFLAGS = flags.FLAGS\n\n\n@deprecated(None, 'Use object_detection/model_main.py.')\ndef main(unused_argv):\n  assert FLAGS.checkpoint_dir, '`checkpoint_dir` is missing.'\n  assert FLAGS.eval_dir, '`eval_dir` is missing.'\n  tf.gfile.MakeDirs(FLAGS.eval_dir)\n  if FLAGS.pipeline_config_path:\n    configs = config_util.get_configs_from_pipeline_file(\n        FLAGS.pipeline_config_path)\n    tf.gfile.Copy(\n        FLAGS.pipeline_config_path,\n        os.path.join(FLAGS.eval_dir, 'pipeline.config'),\n        overwrite=True)\n  else:\n    configs = config_util.get_configs_from_multiple_files(\n        model_config_path=FLAGS.model_config_path,\n        eval_config_path=FLAGS.eval_config_path,\n        eval_input_config_path=FLAGS.input_config_path)\n    for name, config in [('model.config', FLAGS.model_config_path),\n                         ('eval.config', FLAGS.eval_config_path),\n                         ('input.config', FLAGS.input_config_path)]:\n      tf.gfile.Copy(config, os.path.join(FLAGS.eval_dir, name), overwrite=True)\n\n  model_config = configs['model']\n  eval_config = configs['eval_config']\n  input_config = configs['eval_input_config']\n  if FLAGS.eval_training_data:\n    input_config = configs['train_input_config']\n\n  model_fn = functools.partial(\n      model_builder.build, model_config=model_config, is_training=False)\n\n  def get_next(config):\n    return dataset_builder.make_initializable_iterator(\n        dataset_builder.build(config)).get_next()\n\n  create_input_dict_fn = functools.partial(get_next, input_config)\n\n  categories = label_map_util.create_categories_from_labelmap(\n      input_config.label_map_path)\n\n  if FLAGS.run_once:\n    eval_config.max_evals = 1\n\n  graph_rewriter_fn = None\n  if 'graph_rewriter_config' in configs:\n    graph_rewriter_fn = graph_rewriter_builder.build(\n        configs['graph_rewriter_config'], is_training=False)\n\n  evaluator.evaluate(\n      create_input_dict_fn,\n      model_fn,\n      eval_config,\n      categories,\n      FLAGS.checkpoint_dir,\n      FLAGS.eval_dir,\n      graph_hook_fn=graph_rewriter_fn)\n\n\nif __name__ == '__main__':\n  tf.app.run()\n\n\n\n\nfrom object_detection import evaluator\n\n\nImportError: cannot import name 'evaluator'\n\\\n\nfrom object_detection.legacy import evaluator\n\n\ncontrol trainning steps\n\n\nnum_steps:600\n\n\ntensorflow freeze\n\n\n# From tensorflow/models/research/\nINPUT_TYPE=image_tensor\nPIPELINE_CONFIG_PATH={path to pipeline config file}\nTRAINED_CKPT_PREFIX={path to model.ckpt}\nEXPORT_DIR={path to folder that will be used for export}\npython object_detection/export_inference_graph.py \\\n    --input_type=${INPUT_TYPE} \\\n    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \\\n    --trained_checkpoint_prefix=${TRAINED_CKPT_PREFIX} \\\n    --output_directory=${EXPORT_DIR}\n\n\n\nmodel:save and load\n\n\nmodel.fit(x_train, y_train, epochs = 150, batch_size = 32,callbacks=[tensorboard_callback])\nmodel.save('./models/model.h5')\nmodel.save_weights('./models/weights.h5')\n\n\n\nmodel_path = './models/model.h5'\nmodel_weights_path = './models/weights.h5'\nmodel = load_model(model_path)\nmodel.load_weights(model_weights_path)\narray = model.predict(point_set)\n\n\n\nor\n\n\ntf.keras.models.save_model(\n    model,\n    'models/mymode',\n    overwrite=True,\n    include_optimizer=True\n  )\n\n\n\nmodel = tf.keras.models.load_model('./models/mymode')\n\n\n\ncudnn\u5931\u8d25\n\n\nfrom tensorflow.compat.v1 import ConfigProto\nfrom tensorflow.compat.v1 import InteractiveSession\nimport keras.backend as K\nK.set_image_data_format('channels_last')\nK.set_learning_phase(1)\nconfig = ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = InteractiveSession(config=config)\nK.set_session(session)\n\n\n\npb:save and load\n\n\ndef freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n  graph = session.graph\n  with graph.as_default():\n    freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n    output_names = output_names or []\n    output_names += [v.op.name for v in tf.global_variables()]\n    input_graph_def = graph.as_graph_def()\n    if clear_devices:\n      for node in input_graph_def.node:\n        node.device = ''\n    frozen_graph = tf.graph_util.convert_variables_to_constants(\n        session, input_graph_def, output_names, freeze_var_names)\n    return frozen_graph\n\n\nfrozen_graph = freeze_session(K.get_session(), output_names=[out.op.name for out in model.outputs])\ntf.io.write_graph(frozen_graph, './models', 'xor.pbtxt', as_text=True)\ntf.io.write_graph(frozen_graph, './models', 'xor.pb', as_text=False)\n\n\n\ndetection_graph = tf.Graph()\nwith detection_graph.as_default():\n  od_graph_def = tf.GraphDef()\n  with tf.gfile.GFile('xor.pb', 'rb') as fid:\n    serialized_graph = fid.read()\n    od_graph_def.ParseFromString(serialized_graph)\n    tf.import_graph_def(od_graph_def, name='')\n    input = tf.get_default_graph().get_tensor_by_name('input_1:0')\n    output = tf.get_default_graph().get_tensor_by_name('fc2/Softmax:0')\n\nwith detection_graph.as_default():\n  with tf.Session() as sess:\n    values =sess.run(output, feed_dict={input: point_set})\n    print(values)\n\n\n\n\u67e5\u770b\u6a21\u578b\u7684\u8f93\u5165\u5c42\u548c\u8f93\u51fa\u5c42\n\n\nprint('model.inputs :',model.inputs)\nprint('model.outputs : ',model.outputs)\n\n\n\noutput\n\n\nmodel.inputs : [<tf.Tensor 'input_1:0' shape=(?, 2600, 3) dtype=float32>]\nmodel.outputs :  [<tf.Tensor 'fc2/Softmax:0' shape=(?, 2) dtype=float32>]\n\n\n\n\u6240\u4ee5\u8f93\u5165\u5c42\u662f\ninput_1:0\n\u8f93\u51fa\u5c42\u662f\nfc2/Softmax:0\n\n\n\u5b89\u88c5\uff54\uff45\uff4e\uff53\uff4f\uff52RT\n\n\n\n\n\u4e0b\u8f7d\u4e0e\u60a8\u4f7f\u7528\u7684Ubuntu\u7248\u672c\u548cCPU\u67b6\u6784\u5339\u914d\u7684TensorRT\u672c\u5730repo\u6587\u4ef6\u3002\n\n\n\u4eceDebian\u672c\u5730repo\u8f6f\u4ef6\u5305\u5b89\u88c5TensorRT\u3002\n   \nos=\"ubuntu1x04\"\n   tag=\"cudax.x-trt7.x.x.x-ga-yyyymmdd\"\n   sudo dpkg -i nv-tensorrt-repo-${os}-${tag}_1-1_amd64.deb\n   sudo apt-key add /var/nv-tensorrt-repo-${tag}/7fa2af80.pub\n   sudo apt-get update\n   sudo apt-get install tensorrt\n\n\n\u5982\u679c\u4f7f\u7528Python 2.7\uff1a\\\n   \nsudo apt-get install python-libnvinfer-dev\n\\\n   \u5c06\u5b89\u88c5\u4ee5\u4e0b\u9644\u52a0\u8f6f\u4ef6\u5305\uff1a\\\n   \npython-libnvinfer\n\\\n   \u5982\u679c\u4f7f\u7528Python 3.x\uff1a\\\n   \nsudo apt-get install python3-libnvinfer-dev\n\\\n   \u5c06\u5b89\u88c5\u4ee5\u4e0b\u9644\u52a0\u8f6f\u4ef6\u5305\uff1a\\\n   \npython3-libnvinfer\n\\\n   \u5982\u679c\u60a8\u6253\u7b97\u5c06TensorRT\u4e0eTensorFlow\u7ed3\u5408\u4f7f\u7528\uff1a\\\n   \nsudo apt-get install uff-converter-tf\n\\\n   \u5982\u679c\u60a8\u8981\u8fd0\u884c\u9700\u8981ONNX\u7684\u793a\u4f8b \u56fe\u5f62\u5916\u79d1\u533b\u751f \u6216\u5c06Python\u6a21\u5757\u7528\u4e8e\u60a8\u81ea\u5df1\u7684\u9879\u76ee\uff0c\u8fd0\u884c\uff1a\\\n   \nsudo apt-get install onnx-graphsurgeon\n\\\n\n\n\n\nanaconda tensorRT\n\n\n\n\n\u4e0b\u8f7dtar\u6587\u4ef6\nTensorRT-7.0.0.11.Ubuntu-18.04.x86_64-gnu.cuda-10.0.cudnn7.6.tar.gz\n\n\n\n\n(wind1) star@xmatrix:~$ \n(wind1) star@xmatrix:~$ \n(wind1) star@xmatrix:~$ cd TensorRT\n(wind1) star@xmatrix:~/TensorRT$ \n(wind1) star@xmatrix:~/TensorRT$ \n(wind1) star@xmatrix:~/TensorRT$ \n(wind1) star@xmatrix:~/TensorRT$ ls\nTensorRT  TensorRT_1  TensorRT-7.0.0.11  TensorRT-7.0.0.11.Ubuntu-16.04.x86_64-gnu.cuda-10.0.cudnn7.6.tar.gz  TensorRT-7.0.0(1).tar.gz\n(wind1) star@xmatrix:~/TensorRT$ \n(wind1) star@xmatrix:~/TensorRT$ \n(wind1) star@xmatrix:~/TensorRT$ \n(wind1) star@xmatrix:~/TensorRT$ \n(wind1) star@xmatrix:~/TensorRT$ \n(wind1) star@xmatrix:~/TensorRT$ cd TensorRT-7.0.0.11\n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ ls\nbin  data  doc  graphsurgeon  include  lib  python  samples  targets  TensorRT-Release-Notes.pdf  uff\n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ cd python\n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ ls\ntensorrt-7.0.0.11-cp27-none-linux_x86_64.whl  tensorrt-7.0.0.11-cp35-none-linux_x86_64.whl  tensorrt-7.0.0.11-cp37-none-linux_x86_64.whl\ntensorrt-7.0.0.11-cp34-none-linux_x86_64.whl  tensorrt-7.0.0.11-cp36-none-linux_x86_64.whl\n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ pip --version\npip 19.3.1 from /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages/pip (python 3.6)\n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ pip3 --version\npip 19.3.1 from /usr/local/lib/python3.5/dist-packages/pip (python 3.5)\n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ python -m pip install tensorrt-7.0.0.11-cp36-none-linux_x86_64.whl\nProcessing ./tensorrt-7.0.0.11-cp36-none-linux_x86_64.whl\nInstalling collected packages: tensorrt\nSuccessfully installed tensorrt-7.0.0.11\n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ python\nPython 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) \n[GCC 7.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \n>>> \n>>> import tensorrt\n>>> \n>>> exit();\n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ cd ../\n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ ls\nbin  data  doc  graphsurgeon  include  lib  python  samples  targets  TensorRT-Release-Notes.pdf  uff\n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ cd uff\n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ ls\nuff-0.6.5-py2.py3-none-any.whl\n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ python -m pip install uff-0.6.5-py2.py3-none-any.whl\nProcessing ./uff-0.6.5-py2.py3-none-any.whl\nRequirement already satisfied: numpy>=1.11.0 in /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages (from uff==0.6.5) (1.16.4)\nRequirement already satisfied: protobuf>=3.3.0 in /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages (from uff==0.6.5) (3.11.2)\nRequirement already satisfied: six>=1.9 in /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages (from protobuf>=3.3.0->uff==0.6.5) (1.13.0)\nRequirement already satisfied: setuptools in /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages (from protobuf>=3.3.0->uff==0.6.5) (42.0.2.post20191203)\nInstalling collected packages: uff\nSuccessfully installed uff-0.6.5\n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ python\nPython 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) \n[GCC 7.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \n>>> \n>>> import tensorrt\n>>> import uff\nWARNING:tensorflow:From /home/star/anaconda3/envs/wind1/lib/python3.6\n\n7.0.0.11\n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n\n\n\nUsing UFF converter to convert the frozen tensorflow model to a UFF file\n\n\nconda activate wjj\n\\\n\npip install nvidia-pyindex\n\\\n\npip install uff\n\\\nyou need to find your uff installed path. \\\n\n\nimport uff\nprint(uff.__path__)\n\n\n\nAnd after locating it , in it\u2019s bin folder there should be a script named as \nconvert_to_uff.py\n. And now you need to open the terminal and simply type \npython3 convert_to_uff.py\n <\npath to the saved model\n>\\\nIn my case-->\\\n\npython3 convert_to_uff.py /home/models/catsAndDogs.pb\n\\\nAnd it will simply save the converted \n.uff\n in your \n.pb\n model location.\nAnd then this is how the next script should be done.\n\n\nNVIDIA\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\n\n\nNVIDIA\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\n\n\nCould not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\n\n\nUse allow_growth memory option in TensorFlow and Keras, before your code.\n\n\nFor Keras\n\n\nimport tensorflow as tf\nconfig = tf.compat.v1.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\n\nsess = tf.compat.v1.Session(config=config)\ntf.compat.v1.keras.backend.set_session(sess)\n\n\n\nFor TensorFlow\n\n\nimport tensorflow as tf\nconfig = tf.compat.v1.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.compat.v1.Session(config=config)\n\n\n\n\u8fd8\u6709\u4e00\u79cd\u60c5\u51b5,\u51cf\u5c11batch size\n\n\nAnchor boxes\n\n\nOne of the hardest concepts to grasp\u628a\u63e1 when learning about Convolutional Neural Networks for object detection is the idea of anchor boxes.It is also one of the most important parameters you can tune for improved performance on your dataset.In fact,if anchor oxes are  not tuned correctly,your neural network will never even know that certain\u67d0\u4e9b small,large or irregular\u4e0d\u89c4\u5219 objects exist and will never have a chance to detect them.Luckily, there are some simple steps you can take to make sure you do not fall into this trap\u9677\u9631.\n\n\nwhat are anchor boxes?\n\\\nwhen you use a neural network like yolo or ssd to predict multiple objects in a picture, the network is actually making thousands of predictions and only showing the ones that it decided were an object.The multiple predictions are output the following format:\\\nPrediction 1: (X,Y,Height,Width),Class\\\n...\\\nPrediction ~8000: (X,Y,Height,Width),Class\n\n\nWhere the (X,Y,Height,Width) is called the \"bounding box\", or box surrounding the objects.This box and the object class are labelled manually by human annotators.\n\n\nIn an extremely simplified example, imagine that we have a model that has two predictions and receives the following image:\\\n\n\n\nWe need to tell our network if each of its predictions is correct or not in order for it to be able to learn.But what do we tell the neural network it prediction should be? Should the predicted class be:\\\nPrediction 1:Pear\\\nPrediction 2:Apple\n\n\nOr should it be:\\\nPrediction 1:Apple\\\nPrediction 2:Pear\n\n\nWhat if the network predicts:\\\nPrediction 1:Apple\\\nPrediction 2:Apple\n\n\nWe need our network's two predictors to be able to tell whether it is their job to predict the pear or the apple.To do this there are a several tools.Predictors can specialize in certain size objects, objects with a certain aspect ratio(tall vs. wide),or objects in different parts of the image.Most networks use all three criteria\u6807\u51c6.In our example of the pear/apple image,we could have Prediction 1 be for objects on the left and Prediction 2 for objects on the right side of the image.Then we would have our answer for what the network should be predict:\\\nPrediction 1:Pear\\\nPrediction 2:Apple\n\n\nAnchor Boxes in Practice\\\nState of the art\u6700\u5148\u8fdb\u7684 object detection systems currently do the following:\\\n1. Create thousands of \"anchor boxes\" or \"prior boxes\" for each predictor that represent the ideal location, shape and size of the object it specializes\u4e13 in predicting.\n2. For each anchor box,calculate which object's  bounding box has the highest overlap divided by non-overlap.This is called Intersection Over Union or IOU.\n3. If the highest IOU is greater than 50%, tell the anchor box that it should detect the object that gave the highest IOU.\n4. Otherwise if the IOU is greater than 40%, tell the neural network that the true detection is ambiguous and not to learn from that example.\n5. If the highest IOU is less than 40%,then the anchor box should predict that there is no object.\n\n\nThis works well in practice and the thousands of predictors do a very good job of deciding whether their type of object apperars in an image.\n\n\nUsing the default anchor box configuration can create predictors that are too specialized and ojects that appear in the image may not achieve an IOU of 50% with any of the anchor boxes.In this case, the neural network will never know these objects existed and will never learn to predict them. We can tweak\u8c03\u6574 our anchor boxes to be much smaller\n\n\nIn xx net configuration, the smallest anchor box size is 32x32.This means that many objects smaller than this will go undetected. We can fix this by changing our default anchor box configurations. Reducing the smallest anchor box size, all of the things line up with at least one of our anchor boxes and our neural network can learn to detect them!\n\n\nImproving Anchor Box Configuration\n\\\nAs a general rule,you should ask yourself the following questions about your dataset before diving into training your model:\n1. What is the smallest size box I want to be able to detect?\n2. What is the largest size box I want to be able to detect?\n3. What are the shapes the box can take? For example, a car detector might have short and wide anchor boxes as long as there is no chance of the car or the camera being turned on its side.\n\n\nYou can get a rough estimate of these by actually calculating the most extreme\u6781\u7aef sizes\u3000and aspect ratios in the dataset.Yolo V3 uses K-means to estimate the ideal bounding boxes.Another option is to learn the anchor box configuration.\n\n\nOnce you have thought through these questions you can start designing your anchor boxes. Be sure to test them by encoding your ground truth bounding boxes\u5730\u9762\u771f\u503c\u8fb9\u754c\u6846 and then decoding them as though\u5c31\u50cf they were predictions from your model.You should be able to recover the ground truth bounding boxes.\n\n\nAutonomous self-learning systems\n\n\nPreface\n\n\nThis report describes the various processes that are part of the work on the main project at Oslo and Akershus University College(HIOA), department of engineering education,spring 2015.\n\n\nThe report deals with the development of a self-learning algorithm and a demonstrator in the form of a robot that will avoid static and dynamic objects in an environment that is constantly changing.The thesis\u8bba\u6587 is given by HIOA.The report addresses the theory behind the most well-known and used self-learning algorithms,and discusses the advantages,disadvantages and uses of theese.It also contains a description of the technical solution for the demonstrator, and the method used in this project.\n\n\nThe report is written within a topic that is considered new technical and is therefore assumed to be able to be used for futher research and/or learning within autonomous self-learning system.\n\n\nThe reader is expected to have basic knowledge in electronics and information technology.\n\n\nWe would like to thank our employer.Oslo and Akershus University College,for the opportunity to carry out the project and for financial support.We would also like to thank supervisor EZ for a good collaboration, as well as important and constructive guidance throughout the project period.\n\n\nSummary\n\n\nIn today's society, self-learning systems are an increasingly relevant topic.Systems that are not explicity programmed to perform a specific task, but are even able to adapt,can be very useful.\n\n\nThe system described in this thesis is realized with Q-learning by both the table method and the neural network.Q-learning is a learning algorithm based on experience.The algorithm involves an agent exploring his environment, where the environment is represented by a number of states. The agent experiments withs the environment by performing an action, and then observes the consequence of that action.The consequence is given in the form of a positive or negative reward. The goal of the method is to maximize the accumulated reward over time.\n\n\nAutonomous self-learning systems are becoming increasingly relevant because the system is able to adapt to partially or completely unknown situations.It learns from experience and needs less information at start-up as it acquires information along the way.In autonomous self-learning systems and self-propelled robots, avoiding obstacles is a key task.This report addresses a demonstrator of such a system, realized with Q-learning presented later in the reportk, and provides a description of the algorithm and results.\n\n\nTheory\n\n\nBefore looking at the structure of a self-learning system, one can advantageouly look at what the concept of learning is.Learning is often defined as a lasting change in behavior as a result of experience(St. Olavs Hospital,undated).The property of organisms\u751f\u7269\u4f53 that is defined as learning is one of the cornerstones of what is called intelligence which, among other things, is defined as an ability to acquire and apply knowledge and skills.Humans and animals are considered intelligent, among other things,based on their ability to learn from experience.\n\n\nmachine learning\n\n\nThis subchapter is based on the theory of S.M,2009.Machine learning is a form of artifical intelligence that focuses on the development of self-learning algorithms. In most cases,self-learning systems deal with parts of natural intellignece, including memory,adaptation and generalization\u6982\u62ec.Unlike traditional non-learning systems, the method makes it possible to construct a system that is able to expand , adapt to new information and learn a given task without being specifically programmed for this.For machine learning,this system is called an agent.By using menmory,an agent can recognize the last time it was in a similiar situation and what action it took.Based on the outcome from the previous time, it can, if it was correct,choose to repeat the action, or try something new.By generallizing, the agent can recognize similarities in different situations, and thus use experienct from one situation and apply this experience in another.\n\n\nIn order to realize\u5b9e\u73b0 this concept, machine learning uses principles\u539f\u7406 from statistics, mathematics, physics, neurology and biology.\n\n\n\uff37hen talking about machine learning and self-learning systems,algorithms are mainly the main product.The actual process in these algorithms can be compared to data mining\u6316\u6398.Data mining is a process that analyzes data from different perspectives and summarizes if into useful information.Both methods go through data to find patterns\u6a21\u5f0f,but instead of extractiong data for human interpretation,the information is used to improve the agent's understanding.For the agent to be able to learn, it must know how to improve , and whether it will get better or not.There are 15 more methods to solve this, which in turn provide servel main categories within machine learning:Supervised learning,unsupervised learning and reinforcement learning.\n\n\nSupervised Learning\n\n\nAn agent is given a training set with ,for example,pictures of a face and pictures without a face.The agent then prepared through a training process where it gives a forecast of what the picture is of. Whenever the forecast is incorrect, the agent is corrected.This process continues untill the model achieves a desired level of accuracy. Since the algorithm does not have a specific definition of what is a face and what is not, it must therefore learn this using examples.A good algorithm will eventually be able to estimate whether an image is of a face or not.The learning methods are best explained by examples:\n\n\nUnsupervised Learning\n\n\nWhen learning without supervision, information is not marked.This is ,the system is not told what is the image of a face and what is not.As a result, there is no correction or reward to indicate a potential solution, but the algorithm tries to identify the similarities between the images, swfor then categorize them and divide them into groups.\n\n\nReinforcement Learning\n\n\nIn reinforcement learning,the algorithm is told when the answer it gives is incorrect, but receives no suggestions on how to correct this.It must explore and try out different solutions untill it finds out how it gets the right answers.This is a kind of middle ground of supervised learning and unsupervised learning.Examples could be learning to play a board game or a robot that is going to learn to walk.Each time an agent performs an action,he or she recieves a reward or a penalty, based on how desirable\u53ef\u53d6\u7684 the outcome of the action is.For example.when an agent is trained to play a board game, he gets a positive reward for winning, and a negative reward for losing.All other cases give no reward.\\\n\n\\\nThe activity mentioned above can be represent as a sequence of state-action reward:\\\n\n\\\nThis means that the agent was in state s0, performed action a0,which resulted in it receiving reward r1 and ending up in state s1.Furthermore, it performed action a1, received reward r2, and ended up in state s2, and so on.\\\n\uff34his sequence is made up of experiences where experience is given as:\\\n\n\\\nExperience shows that the agent was in state s,performed action a,received reward r, and ended up in state s', and represented by (s,a,s',r)\n\n\nIn order for the agent to be able to learn from the sequences mentioned and thus call it an experience.It has a table called Q-table which acts as its memory.All data points stored in this table are called Q-values and represent how desirable\u53ef\u53d6\u7684 it is to perform a specific action in a specific state.\n\n\nOne experience adds one data point Q(s,a) in the table that represents the agents current estimate\u4f30\u8ba1 of the optimal\u6700\u4f73 Q value. It is this information that the agent uses to learn an optimal pattern of action.The size of the table depends on how many conditions and actions are included in the problem you are trying to solve, where the number of conditions gives the number of rows ,while the number of actions gives the number of colums.For example if you have 20 states and 3 actions, you will get a table of 20x3.\n\n\nBefore the agent begins to experiment, it knows nothing else what actions it is capable of performing, and the Q table is consequently empty.This is, all Q values are equal to 0.\n\n\nThe key to the method described above is to update the Q value that is applied to the agent when it performs an action in a given state in the current data point when it gains an experience.\n\n\nThis value is given by the Q function\\\n\n\\\nor more clearly:\\\n\n\n\n\n\n\u03b1\uff0d\uff0dlearning rate\\\n\u0393\uff0d\uff0ddiscount factor\\\nr\uff0d\uff0dreward\\\n\n\n\n\nThe learning rate dicates\u6307\u793a how much of previouly\u5148\u524d acquired learning should be replaced with new information. \nBy \u03b1=1,previous values are replaced with new information.while \u03b1=0 ,corresponds to no update.In other words,the agent will konw by \u03b1=1,assume that the last reward and resulting state are representative of future values.\n\n\nThe discount factor indicates the weight of all futher step rewards.If \u0393=0,the agent will only consider current rewards, while it will think more long-term\u957f\u671f and strive\u52aa\u529b for higher future rewards as \u0393 approaches 1.\n\n\nThe reward is defined when you create the program in the form of reward functions\u5956\u52b1\u662f\u5728\u521b\u5efa\u7a0b\u5e8f\u65f6\u4ee5\u5956\u52b1\u51fd\u6570\u7684\u5f62\u5f0f\u5b9a\u4e49\u7684,and can be positive or negative. What this reward value is defined as is not critical.On the other hand, It is important that there is a clear distinction\u533a\u522b between the reward for good deeds\u884c\u4e3a and the reward for bad deeds.\n\n\nWhen a new experience is added to the algorithm, a new Q value is estimated, and the old value of the Q table for the last experience is updated with the new one.\n\n\nAssume that the agent is a mouse that is placed in a room divded into six states, see below Figure.In state s6 there is a piece of cheese,while in state s5 there is a cat.If the agent finds the piece of cheese, it receives a reward of 10, while it receives a reward of -10 if it moves to the state where the cat is.These two states are called terminal states.The agent explores the environment untill one of the terminal states is reached,before starting a new iteration(attempt)\\\n\n\n\n\uff34he agent can perform four actions: up,down,left,right.If it moves in a direction where there is a wall, it gets a reward of -1, and it remains in the same state. All other conditions have a value equal to 0.This is implemented in the algorithm using reward functions.\\\n\n\n\\\nThe agent has no information about his surroundings other than that there are six states and four acitons, what state he is in at any given time and any rewards it receives as actions are performed.Nor does it know what a reward or punishment is.The Q table is illustrated in Table 1.\n\n\nBy assuming the following sequences of experiences,(s,a,r,s') one can illustrate how a sequence updates the Q-table.\n\n\n\nFor the sake of illustration, the learning rate \u03b1 and the discount factor \u0393 set equal to 1 and all Q-values have an initial value equal to 0.\\\n\n\n\nAfter this sequence of experiences, the Q table with updated values will look like this:\\\n\n\\\nopp=up ned=down venstre=left h\u00d8yre=right\n\n\nEach experience will update the value of the state table for the performed state and action combination, and the Q values will converge\u6c47\u805a to optimal values over time.The more experiments the agent conducts\u884c\u4e3a, the better the estimates in the Qtable.In theory, the agent will eventually achieve an optimized Q-table, and will be able to choose the shortest path to the cheese each time, regardless of the starting state.\n\n\nExploration or Utilize \u52d8\u63a2\u6216\u5f00\u53d1\n\n\nOne of the challenges with the Q-learning algorithm is that it does not directly tell what the agent should do, but rather functions as a tool that shows the agent what the optimal action is in a given condition.This is a challenge because the agent must explore the environment enough times to be able to build a solid foundation\u575a\u5b9e\u7684\u57fa\u7840 that provides a good estimate of the Q values.To address this, an exploration feature is implemented that determines the strategy\u6218\u7565 the agent will use to select actions, either explore or exploit.\n\n\nExploration:The agent is bold\u80c6\u5927 and does not necessary choose the best course of action, with the intension of establishing a better estimate of the Q values.\n\n\nUtilize: The agent utilizes the experience it has already built up, and selects the optimal action for the condition it is in. That is, the action that gives the highest Q value. It is said that the agent is greedy\u8d2a\u5a6a\n\n\nThe purpose of this feature is to establish a relationship between the two strategies. It is important to explore enough so that the agent builds a solid foundation of the Q-values, but it is also important to utilize the already acquired knowledge to ensure the highest possible reward.\n\n\nThere are a number of ways to accomplish this, but two of the most popular methods are the greedy feature and the Boltzmann distribution, popularly called \"softmax\":\n\n\n\u03b5-greedy\n\n\n\u03b5-greedy is a strategy based on always choosing the optimal action except for \u03b5 of the aisles and choosing a random action of the aisles.\\\n\n\\\nthere\\\n a* is optimal action with probability 1-\u03b5\\\n ar is random action with probability \u03b5\\\nwhere\\\n 0< \u03b5 < 1\n\n\nIt is possible to change over time. This allows the agent to choose a more random strategy at the beginning of the learning process, and then become more greedy as it acquires more knowledge.\n\n\nOne of the challenges with greed is that it considers all actions except the optimal one as equal.\u5b83\u5c06\u6700\u4f73\u884c\u52a8\u4e4b\u5916\u7684\u6240\u6709\u884c\u52a8\u90fd\u89c6\u4e3a\u5e73\u7b49 If, in addition to an optimal action, there are two good actions as well as additional actions that look less promising,\u8fd8\u6709\u4e24\u4e2a\u597d\u7684\u52a8\u4f5c\u4ee5\u53ca\u770b\u8d77\u6765\u4e0d\u592a\u6709\u524d\u9014\u7684\u5176\u4ed6\u52a8\u4f5c\uff0c it will make sense\u6709\u610f\u4e49 for the agent to choose one of the two better ones instead of spending time exploring the bad actions. Using greed is as likely to explore a bad action as to explore one of the two better ones. One way to solve this is to use the Boltzmann distribution \n\n\n2.3.2 Boltzmann distribution\nThis strategy is known as \"softmax\" action selection and involves selecting an action with a probability that depends on the Q value. The probability of selecting action a in state s is proportional to\u3000\n,which means that the agent in state s chooses action a with probability \\\n\n\n\nThe parameter T specifies how random values are to be selected.\u53c2\u6570T\u6307\u5b9a\u5982\u4f55\u9009\u62e9\u968f\u673a\u503c\u3002  When T has a high value, actions are chosen to approximately the same extent.\u9009\u62e9\u7684\u52a8\u4f5c\u7684\u7a0b\u5ea6\u5927\u81f4\u76f8\u540c As T decreases, the actions with the highest Q value are more likely to be selected, while the best action is always selected when T-> 0\n\n\nOne of the advantages of the Q-learning algorithm is that it is exploration insensitive\u5bf9\u63a2\u7d22\u4e0d\u654f\u611f. That is, the Q values will converge to optimal values regardless of the agent's behavior as long as information is collected. This is one of the reasons why Q-learning is one of the most widely used and effective methods in reinforcement learning. On the other hand, it is a prerequisite that all combinations of condition / action are tried out many enough times, which in most cases means that the exploration and exploitation aspect must be addressed as it is not always possible for an agent to experiment infinitely.\n\n\nThere are several methods for setting up an artificial neural network.One of the most common,which is also used in this project, is called a feedforward neural network, which means that the information is always fed from the input page and supplied further through the network.In other words,output from one layer + bias is used as input in the next.\n\n\nFuzzy Logic\u6a21\u7cca\u7406\u8bba\n\n\nThe theory of fuzzy logic is based on S.K.J(2007)\\\nFuzzy logic is a decision-making or management model based on diffuse\u3000\u6269\u6563\uff0c\u6563\u53d1,and input data classified as degrees of membership in a diffuse quantity. A diffuse set has no sharp boundaries, and is a set of values that belong to the same definition, where the definition is a diffuse term often described by an adjective\u5f62\u5bb9\u8bcd. For example: big potatoes or tall people.A diffuse set is also called a semantically\u8bed\u4e49\u4e0a variable or set.\n\n\nFuzzy logic classifies input data according to how large a membership the input data has to a set.The degree of membership is graded\u5206\u7ea7 between 0 and 1,and determined on the basis of a membership function\u6839\u636e\u96b6\u5c5e\u51fd\u6570\u786e\u5b9a.\uff34he member ship function is determined on the basis of knowledge of the field, and can be non-linear.Input data can have membership in servel different sets, but can only have full membership in one set.\n\n\nFuzzy logic behavior\n\n\n\n\nDiffusion\u6269\u6563: input data is graded according to the diffused quantities,and the diffused quantities are sent to subprocess 2\n\n\nLogical rules:the diffused quantites are tested against rules.The degree of fulfillment \u5c65\u884c from each rule is sent to sub-process 3.The rules can be implented with {and, if, or},or other methods such as an impact\u5f71\u54cd matrix.The rules describe the relationship between the diffused input and output.\n\n\nImplication: the received data from sub-process 2 is tested againist rules that grade the impact on the overall consequence of each of the received data.The ratings are then sent to sub-process 4.\n\n\nAggregation\u805a\u5408:all the consequence\u540e\u679c quantities are compiled into an aggregate quantity\u603b\u91cf,which is the union of the quantities from subprocess 3.\n\n\nA vdiffusion: The most representative element is extracted from the aggregate amount.This is usually the center of gravity\u91cd\u529b.That value will then be the control signal.\n\n\n\n\nThe Fuzzy logic method has a number of advantages in that it is able to process imprecise\u4e0d\u7cbe\u786e data and model non-linear relationships between input and output.The principle enables an accurate and realistic\u73b0\u5b9e\u7684 classification of data, as data is rarely\u5f88\u5c11 only true or false.\n\n\nTechnical solution\n\n\nAgents\n\n\nTwo versions of the agent were developed.The first version,which was later replaced, was built on the basis of drivetrain\u52a8\u529b\u603b\u6210 and mechanics\u673a\u68b0\u5e08 taken from 'WLtoys' model 'A969'.\nThe radio-controlled car in scale 1:18 was delivered\u5df2\u4ea4\u4ed8 to drive with ratio and battery.The car measured 285x140x105mm(LxWxH) and could run for about 10 minutes on a fully charged battery.\n\n\nSince the car was to be equiped with an Xbee module and communicate via bluetooth.the car's original reciever was removed.The steering servo\u8f6c\u5411\u4f3a\u670d was adapted to the car's receiver and was therefore replaced in favour of a PWM-controllable servo.To improve the agent's running time, the original battery(7.4V/1100mAh) was replaced with a 7.4V/5200mAh \"Fire Bull\" type battery, which would thoretically provide approximately five times as long.\n\n\nTo make room for all the electronics, a plate 1.5mm aluminum\u94dd was cut out and mounted on top of the car.The motor driver, microcontroller board and Xbee module were placed together with the motor and servo on the classis itself, while the battery, sensors and voltage regulator were mounted\u5b89\u88c5 on the new plate for easier access.\n\n\nAfter the mentioned changes, the agent had improved battery life, adjustable speed and the correct physical size, but it turned out not to have a good enough turning radius. It had particular problems in situations where it had to react in one direction, and then change direction.\n\n\nThe car's turning radius was tested before the conversion was started, and it was measured that the car need 70cm to turn 90 degrees to the left or right(figure 3-4).It thus needed an area of 140cm in diameter to turn 180 degrees.It was therefore assumed that a round arena\u7ade\u6280\u573a with a diameter of about four meters was needed.The area was built,and it was found that four meters was enough to be able to drive around without obstacles,but that it became too samll when car had to avoid objects.\n\n\nThe ultrasonic sensors are limited, through discretion\u901a\u8fc7\u659f\u914c,to integer values between 1cm and 120cm, If one is to create a look-up table for the entire spectrum\u8303\u56f4, the table will consist of 120^5 different combinations, and would with three possible actions end up with a look-up table of 120^5x3 number of places. This is a less good solution as the table would be very large(74 billion seats).A large lookup table means that the agent spends much more time learning all the combinations of conditions, and actions for these.In addition, such a solution is far too large to be implemented on a normal computer.This chanllenge is solved by discretizing\u79bb\u6563\u5316 or dividing, the continuous sensor area into smaller and more managerable state spaces.\n\n\n4.2.1 Condition space model\n\n\nAll sensors can measure distances up to 400 cm. To avoid a large state table and to streamline the learning process, the sensor values are discretized down to 144 states. For the project described in this report, this also simplifies the reward function.\n\n\n\n\nEach snsor is divided into four zones based on distance in centimeters. The division is shown in Figure 4.4\\\n\n\\\n\n\nA condition consists of four parameters. Two for the left side and two for the right side of the agent's field of view.A state\nis a combination of distance to the nearest obstacle and in which sector the obstacle is located .a sector describes the angle at which an object is located, relative to the agent.\n\n\nThe four parameter are defined by:\\\nk1:zone\u533a left side \u533a\u57df\u5de6\u4fa7\\\nk2:zone right side \u533a\u57df\u53f3\u4fa7\\\nk3:sector\u90e8\u95e8 left side \u6247\u533a\u5de6\u4fa7\\\nk4:sector right side \u6247\u533a\u53f3\u4fa7\\\nk5:observation of dynamic or static obstacle left side \u89c2\u5bdf\u52a8\u6001\u6216\u9759\u6001\u969c\u788d\u7269\u5de6\u4fa7\\\nk6:observation of dynamic or static obstacle right side \u89c2\u5bdf\u52a8\u6001\u6216\u9759\u6001\u969c\u788d\u7269\u53f3\u4fa7\\\nS(state space model) = k1 x k2 x k3 x k4 x (k5 x k6)\\\nThe agent can perceive\u611f\u77e5 two obstacles simultaneously, but not on the same page.With two obstacles on the same side,the algorithm will prioritize\u4f18\u5148 the obstacle that is in the zone with the lowest numbering. The division of the sectors is illustrated in Figure below\\\n\n\\\nsector\u6247\u533a\\\nThe sectors have four different values, 0,1,2,3.The sensors perceive\u611f\u77e5 an object when the zone value is less than 2.The sectors are symmetrically\u5bf9\u79f0\u5730 distributed\u5206\u5e03.so that the agent's right field of view is discretized in the same way as the left.Sensors 1,2 and 3 on the left side correspond to sensors 3,4 and 5 for the right side.\n\n\n4.2.11 Discretion for system with dynamic obstacles\n\n\nIn the simulation for a system with dynamic obstacles, the state space model is expanded with two additional parameters,k5 and k6. k5 indicates with the value 1 or 0 whether the obstacle detected on the left side is dynamic, while k6 does the same for the right side.The number 1 indicates a dynamic obstacle,while 0 indicates static.The state table has thus benn expanded to 576 states.,An object os classified as a dynamic obstacle in that the agent first, while standing still,performs two measurements in a row. Then the two measurements are compared, if the obstacle moves away from the agent, the first measurement will be positive number, and classified as a static obstacle.If on the other hand, the same calculation operation gives a negative number, the obstacle is classified as a dynamic obstacle and the agent must react accordingly.\n\n\n4.2.1.2 the state table\n\n\nThe state table is structured as a table where all combinations of states and actions are represented as a table point.The state table for the physical model has 144 rows, and three columns that present each of the possible actions(left,right, straightfoward). A section of the condition table is shown in Table 7.\n\n\n4.2.2 Reward function\n\n\nThe reward feature determines the agent's immmediate reward after each experience.When implementing the reward function, a distinction is made between good actions that give a positive reward value,and bad actions that give a negative reward value.The reward function used for Q-learning with the table method is inspired by and shown under equation.\\\n\n\n\nThe reward function, r1 is defined to give a positive reward value when the agent drives straight ahead, while for turns a small negative reward value is given. The negative value is given to prioritize \u4f18\u5148 driving straight ahead.\n\n\nThe parameter r2 gives a positve reward when the total change in the sensor values is positive. That is , the agent moves away from an obstacle. If the agent moves towards an obstacle , it becomes total the change in sensor values is negatie, and the agent recieve a negative reward.\n\n\nThe function r3 gives the agent a negative reward value if it first turns to right and then to the left, or first to the left and then to right one after the other. This feature allows the agent to avoid an indecisive\u4f18\u67d4\u5be1\u65ad\u7684 sequence, where it only swings\u6447\u6446 back and forth.\n\n\n\uff34he function r gives the agent a reward value equal to the sum of r1,r2 and r3 if it does not collide, while in the event of a collision it gets a negative reward value of -100.\n\n\n4.2.3 Exploration function\nThe exploration function determines the agent's strategy for choosing actions. It can either explore the state space or take advantage of the experiences it already has. The function ensures an appropriate balance between these strategies (see chapter 2.3).\n\n\nThe simulations for the exploration methods show that softmax gives the best results in the simulated model, and is thus used in the simulation. The parameter T is set equal to 24, and is reduced by 5% of its total value for each individual experiment.In the physical model, epsilon is used because it provides a better overview of how much coincidence there is for the agent to take an arbitrary action, and epsilon is set at a 5% chance of doing a random action.\n\n\n4.2.4 Implementation\n\n\nThe implementation of Q-learning with the table method is based on the same algorithm and reward function in the simulation and for the physical model.The different between the simulation and the physical agent is the exploration function and an additional extension of the state space in the simulation(see the discretization\u79bb\u6563\u5316 chapter).In the simulation, you have the choice between static and dynamic obstacles, and to adapt the agent to dynamic obstacle, the Q table is expanded.\n\n\n4.2.4.1 The Q-Learning algorithm with the table method\n\n\nThe Q-Learning algorithm with the table method\nAt startup:\n - initialize state space S\n - initialize the action list A\n - initialize the Q table Q(s,a)(empty table) based on the state space S and the action list A\n\n\nFor each attempt:\n 1) Observe initial sensor values\n 2) Discretize the sensor values and set it equal to s(initial mode)\n 3) Select an action a based on the exploration function and perform the action\n 4) Observe the agent's sensor values after the operation\n 5) Discretize the sensor values and set it equal to s'(Next mode)\n 6) Give the agent the reward r based on the reward function\n 7) Update the Q table\n 8) Set s=s'(Now state)\n 9) Keep last action, prev_a =a (Used in the reward function)\n 10) Repeat the operation from point iii)(a new step /experience ) If the agent has not reached its terminal state(collision or maximum number of steps). Start a new attempt if the agent has reached its terminal state(Restart from i).\n\n\n4.2.4.2 Simulated model\n\n\nIn the simulation, a user interface is created that shows the user the simulation of the agent, two different graphs, buttons for different functions and information panel.The information panel show the user relevant\u76f8\u5173\u7684 values from the simulation.One graph shows an average of the number of steps for every tenth attempt, while the other shows the agent's accumulated reward.One step in the program corresponds to an experienced situation,whether it has been experienced before or is new.A new attempt is initialiated each time the agent has moved the maximum number of steps for the current attempt, or has collided with an obstacle .The maximum number of steps can be changed, but is normally 400-600.The dimensions of the simulation are scaled 1\uff1a10[cm]\n\n\nThe ultrasonic sensors on the physical agent are limited to integer values from 1 to 120cm in the code of the agents microcontrollers.Distance less than 8 cm id defined as a collision. The initial state of the physical agent is an arbitrary position in the environment.The only requirement is that the initial state is not a terminal state(collision).\n\n\nThe commands\n- command 1 : request sensor values\n- command 2: ask the agent to drive straight ahead\n- command 3: ask the agent to turn right\n- command 4: ask the agent to turn left\n- command 5: ask the agent to stop, sensor values <8 cm(collision)\n\n\n4.3 Q-Learning with neural network (Linear function approximation)\n\n\nQ-Learning with neural network is based on approximation of one or more linear Q-functions. Unlike Q-table learning.this method stores the Q functions in a neural network. By using this method ,discretization is not necessary, and the entire continuous sensor spectrum can be used. The behavior behind an artifical neuron is described in chapter 2.3.3\\\n\nneural network\n\\\n\n\\\ninput layer---------------------------------hidden layer-------------------------------------output layer\n\n\nThe figure above shows the neural network as it is implemented in this system. In the network, all the neurous, including the bias from one layer, are connected to each individual neuron in the next layer. The neural network has three layers.Layer One, the input layer, has five neurons (one for each sensor),layer two has fifteen neurons, and layer three has three neurons according to the number of actions in the action list.Layer 1 is the start of the network, and the sensor values are used as input.and these must be in the interval\u95f4\u9694[-1,1]\n\n\nThe weight matrix w(1) is used to weight the link between layer 1 and layer 2.means w(2) for layer 2 and layer 3.The matrices have a dimension of (number of neurons in layer L)x(number of neurons in Layer (l-1) + bias)\n\n\nw(1) has a dimension of 15x6 and w(2) dimension of 3x16. In the output layer, there are three neurons, and each individual neuron corresponds to a Q function.The number of neurons in the output layer is equal to the number of actions of the agent, and each individual neuron estimates a Q function with respect to an action in the agent's action list Equation(21)\\\n\n\n\nThese Q function estimate a Q value that is futher used to determine the optimal actions for the agent, where the optimal action for each condition is therefore defined as a = max(Q(s,a)). It is impossible to say which Q function are suitable for the system, since the neural network estimates the Q function based on the agent's experience. In order for the agent to achieve the highest total reward, it must therefore explore the environment long enough for the network to converge to an optimal function.\n\n\n\\\nThe figure shows the learning process of agent. The method needs initial values, and these are given by sensor values and the network's weight matrices.The weight matrices are initiallized with random values between 0 and 1, but not zero.If the weight matrices are initialized with a value equal to 0,all the neurons will have the same value,which means that the neural network will not be usable. This algorithm uses the agent's sensor values, as opposed to the table method which uses a discretization of values. The exploration function used is the same as for the table method, but the reward function is different. The purpose of this method is to find optimal Q functions by updating the weight machines after each experiment.\n\n\n4.3.1 Reward function\n\n\nSince the agent's state space on the neural network is continuous, an equally elaborate reward function is not required as for the table method.The reward function is defined by the agent receiving a positive reward if it drives straight ahead, and a negative reward in the event of a turn and a collision. The reward value r must be in the interval [-1,1], and the function as defined for the system is shown in equation.\\\n\n\n\n4.3.2 Feedforward\n\n\nAs described in Chapter 2 Theory, neural network in this project is a forward-connected neural network. The neurons are activated using the function of the hyperbolic forceps\u53cc\u66f2\u7ebf\u94b3 given in equation.\n\n\\\nThe activated values become output values in this layer, and are used as input for the next layer.The three neurons in the output layer correspond to the Q funcitons when all data is fed into the network.\n\n\n\\\n\n\\\nThe weight matrix w has m number of rows equal to the number of neurons,and columns n equal to the number of inputs.The matrix w(2) and the vector y are multiplied by each other and the product is used in the activation function described ealier in the chapter.The elements in both the weight matrix and the input vector are real numbers,and the value of the inputs is in the interval[-1,1].Equation(26-28) shows the Q functions of the three actions.\n\n\n\n\nThe calculations in the figures above can be abbreviated\u7f29\u5199 and shown in equation 27 in vector form.\\\n\n\\\nActivation of the hidden layer takes place in the same way as for the output layer, and the input to this layer is the sensor values.\\\n\n\nInstall Pytroch-Gpu\n\n\nversison:1.3.0\n\n\nconda install -c anaconda pytorch-gpu\n \n\n\nversion:1.8.0\n\n\nconda install -c conda-forge pytorch-gpu\n\n\n\u51cf\u5c11\u5185\u5b58\u5360\u7528\u7684\u4e00\u79cd\u65b9\u6cd5\n\n\nThe projected depth values in the original depth maps are float32 and the unit is meter (m). However, we don't want to save float32 because it took too much storage. A common technique is that we can convert it to uint16 by int(depth * 256). This keeps certain degree of accuracy but takes less storage. That's why we need to divide the value by 256.",
            "title": "Deeplearning"
        },
        {
            "location": "/deeplearning/#lenet",
            "text": "use convolution to extract spatial features.  subsample using spatial average of maps.  non-linearity in the form of tanh or sigmoids.  multi-layer neural network(MLP) as final classifier  sparse connection matrix between layers to avoid large computational cost",
            "title": "LeNet"
        },
        {
            "location": "/deeplearning/#alexnet",
            "text": "use of rectified linear units(ReLU) as non-linearities.  use of dropout technique to selectively ignore single neurons during training , a way to avoid overfitting of the model.  overlapping max pooling, avoiding the averaging effects of average pooling.  using GPUs to reduce training time",
            "title": "AlexNet"
        },
        {
            "location": "/deeplearning/#xception",
            "text": "",
            "title": "xception"
        },
        {
            "location": "/deeplearning/#computing-the-mean-and-std-of-dataset",
            "text": "import tensorflow as tf\nfrom PIL import ImageStat\n\nclass Stats(ImageStat.Stat):\n    def __add__(self, other):\n        return Stats(list(map(add, self.h, other.h)))\n\nloader = DataLoader(dataset, batch_size=10, num_workers=5)\n\nstatistics = None\nfor data in loader:\n    for b in range(data.shape[0]):\n        if statistics is None:\n            statistics = Stats(tf.to_pil_image(data[b]))\n        else:\n            statistics += Stats(tf.to_pil_image(data[b]))\n\nprint(f'mean:{statistics.mean}, std:{statistics.stddev}')",
            "title": "Computing the mean and std of dataset"
        },
        {
            "location": "/deeplearning/#pyx-to-so",
            "text": "python setup.py build_ext --inplace",
            "title": "pyx to so"
        },
        {
            "location": "/deeplearning/#cuda",
            "text": "sudo apt-get purge nvidia*\nsudo apt-get autoremove\nsudo apt-get autoclean\nsudo rm -rf /usr/local/cuda*",
            "title": "\u5f7b\u5e95\u5220\u9664cuda"
        },
        {
            "location": "/deeplearning/#steps-to-install-pytorch-with-cuda-100",
            "text": "conda install pytorch torchvision cudatoolkit=10.0 -c pytorch \\ pip install torchsummary",
            "title": "Steps to Install PyTorch With CUDA 10.0"
        },
        {
            "location": "/deeplearning/#conda-install-matplotlib",
            "text": "conda install -c conda-forge matplotlib",
            "title": "conda install matplotlib"
        },
        {
            "location": "/deeplearning/#conda-install-tensorflow-object_detection-api",
            "text": "conda install -c conda-forge tf_object_detection",
            "title": "conda install tensorflow object_detection api"
        },
        {
            "location": "/deeplearning/#modulenotfounderror-no-module-named-deployment",
            "text": "from slim.deployment import model_deploy",
            "title": "ModuleNotFoundError: No module named 'deployment'"
        },
        {
            "location": "/deeplearning/#modulenotfounderror-no-module-named-slim",
            "text": "conda install -c conda-forge tf-slim",
            "title": "ModuleNotFoundError: No module named 'slim'"
        },
        {
            "location": "/deeplearning/#modulenotfounderror-no-module-named-nets",
            "text": "change  frome nets  to  from slim.nets",
            "title": "ModuleNotFoundError: No module named 'nets'"
        },
        {
            "location": "/deeplearning/#creating-trainvaltxt",
            "text": "import os\nimport random\nimport sys\n\nif len(sys.argv) < 2:\n    print(\"no directory specified, please input target directory\")\n    exit()\n\nroot_path = sys.argv[1]\n\nxmlfilepath = root_path + '/Annotations'\n\ntxtsavepath = root_path + '/ImageSets/Main'\n\nif not os.path.exists(root_path):\n    print(\"cannot find such directory: \" + root_path)\n    exit()\n\nif not os.path.exists(txtsavepath):\n    os.makedirs(txtsavepath)\n\ntrainval_percent = 0.9\ntrain_percent = 0.8\ntotal_xml = os.listdir(xmlfilepath)\nnum = len(total_xml)\nlist = range(num)\ntv = int(num * trainval_percent)\ntr = int(tv * train_percent)\ntrainval = random.sample(list, tv)\ntrain = random.sample(trainval, tr)\n\nprint(\"train and val size:\", tv)\nprint(\"train size:\", tr)\n\nftrainval = open(txtsavepath + '/trainval.txt', 'w')\nftest = open(txtsavepath + '/test.txt', 'w')\nftrain = open(txtsavepath + '/train.txt', 'w')\nfval = open(txtsavepath + '/val.txt', 'w')\n\nfor i in list:\n    name = total_xml[i][:-4] + '\\n'\n    if i in trainval:\n        ftrainval.write(name)\n        if i in train:\n            ftrain.write(name)\n        else:\n            fval.write(name)\n    else:\n        ftest.write(name)\n\nftrainval.close()\nftrain.close()\nfval.close()\nftest.close()",
            "title": "creating trainval.txt"
        },
        {
            "location": "/deeplearning/#creating-tfr-datasets",
            "text": "import hashlib\nimport io\nimport logging\nimport os\nimport random\nimport re\n\nfrom lxml import etree\nimport PIL.Image\nimport tensorflow as tf\n\nfrom object_detection.utils import dataset_util\nfrom object_detection.utils import label_map_util\n\nflags = tf.app.flags\nflags.DEFINE_string('data_dir', '', 'Root directory to raw pet dataset.')\nflags.DEFINE_string('output_dir', '', 'Path to directory to output TFRecords.')\nflags.DEFINE_string('label_map_path', 'data/pet_label_map.pbtxt',\n                    'Path to label map proto')\nFLAGS = flags.FLAGS\n\n\ndef get_class_name_from_filename(file_name):\n  \"\"\"Gets the class name from a file.\n\n  Args:\n    file_name: The file name to get the class name from.\n               ie. \"american_pit_bull_terrier_105.jpg\"\n\n  Returns:\n    A string of the class name.\n  \"\"\"\n  print(file_name)\n  match = re.match(r'([A-Za-z_]+)(_[0-9]+\\.jpg)', file_name, re.I)\n  return match.groups()[0]\n\n\ndef dict_to_tf_example(data,\n                       label_map_dict,\n                       image_subdirectory,\n                       ignore_difficult_instances=False):\n  \"\"\"Convert XML derived dict to tf.Example proto.\n\n  Notice that this function normalizes the bounding box coordinates provided\n  by the raw data.\n\n  Args:\n    data: dict holding PASCAL XML fields for a single image (obtained by\n      running dataset_util.recursive_parse_xml_to_dict)\n    label_map_dict: A map from string label names to integers ids.\n    image_subdirectory: String specifying subdirectory within the\n      Pascal dataset directory holding the actual image data.\n    ignore_difficult_instances: Whether to skip difficult instances in the\n      dataset  (default: False).\n\n  Returns:\n    example: The converted tf.Example.\n\n  Raises:\n    ValueError: if the image pointed to by data['filename'] is not a valid JPEG\n  \"\"\"\n  img_path = os.path.join(image_subdirectory, data['filename'])\n  with tf.gfile.GFile(img_path, 'rb') as fid:\n    encoded_jpg = fid.read()\n  encoded_jpg_io = io.BytesIO(encoded_jpg)\n  image = PIL.Image.open(encoded_jpg_io)\n  if image.format != 'JPEG':\n    raise ValueError('Image format not JPEG')\n  key = hashlib.sha256(encoded_jpg).hexdigest()\n\n  width = int(data['size']['width'])\n  height = int(data['size']['height'])\n\n  xmin = []\n  ymin = []\n  xmax = []\n  ymax = []\n  classes = []\n  classes_text = []\n  truncated = []\n  poses = []\n  difficult_obj = []\n  for obj in data['object']:\n    difficult = bool(int(obj['difficult']))\n    if ignore_difficult_instances and difficult:\n      continue\n\n    difficult_obj.append(int(difficult))\n\n    xmin.append(float(obj['bndbox']['xmin']) / width)\n    ymin.append(float(obj['bndbox']['ymin']) / height)\n    xmax.append(float(obj['bndbox']['xmax']) / width)\n    ymax.append(float(obj['bndbox']['ymax']) / height)\n    class_name = get_class_name_from_filename(data['filename'])\n    classes_text.append(class_name.encode('utf8'))\n    classes.append(label_map_dict[class_name])\n    truncated.append(int(obj['truncated']))\n    poses.append(obj['pose'].encode('utf8'))\n\n  example = tf.train.Example(features=tf.train.Features(feature={\n      'image/height': dataset_util.int64_feature(height),\n      'image/width': dataset_util.int64_feature(width),\n      'image/filename': dataset_util.bytes_feature(\n          data['filename'].encode('utf8')),\n      'image/source_id': dataset_util.bytes_feature(\n          data['filename'].encode('utf8')),\n      'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')),\n      'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n      'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')),\n      'image/object/bbox/xmin': dataset_util.float_list_feature(xmin),\n      'image/object/bbox/xmax': dataset_util.float_list_feature(xmax),\n      'image/object/bbox/ymin': dataset_util.float_list_feature(ymin),\n      'image/object/bbox/ymax': dataset_util.float_list_feature(ymax),\n      'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n      'image/object/class/label': dataset_util.int64_list_feature(classes),\n      'image/object/difficult': dataset_util.int64_list_feature(difficult_obj),\n      'image/object/truncated': dataset_util.int64_list_feature(truncated),\n      'image/object/view': dataset_util.bytes_list_feature(poses),\n  }))\n  return example\n\n\ndef create_tf_record(output_filename,\n                     label_map_dict,\n                     annotations_dir,\n                     image_dir,\n                     examples):\n  \"\"\"Creates a TFRecord file from examples.\n\n  Args:\n    output_filename: Path to where output file is saved.\n    label_map_dict: The label map dictionary.\n    annotations_dir: Directory where annotation files are stored.\n    image_dir: Directory where image files are stored.\n    examples: Examples to parse and save to tf record.\n  \"\"\"\n  writer = tf.python_io.TFRecordWriter(output_filename)\n  for idx, example in enumerate(examples):\n    if idx % 100 == 0:\n      logging.info('On image %d of %d', idx, len(examples))\n    # path = os.path.join(annotations_dir, 'xmls', example + '.xml')\n    path = os.path.join(annotations_dir, example + '.xml')\n    if not os.path.exists(path):\n      logging.warning('Could not find %s, ignoring example.', path)\n      continue\n    with tf.gfile.GFile(path, 'r') as fid:\n      xml_str = fid.read()\n    xml = etree.fromstring(xml_str)\n    data = dataset_util.recursive_parse_xml_to_dict(xml)['annotation']\n\n    tf_example = dict_to_tf_example(data, label_map_dict, image_dir)\n    writer.write(tf_example.SerializeToString())\n\n  writer.close()\n\n\n# TODO: Add test for pet/PASCAL main files.\ndef main(_):\n  data_dir = FLAGS.data_dir\n  label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path)\n\n  logging.info('Reading from Pet dataset.')\n  image_dir = os.path.join(data_dir, 'images')\n  annotations_dir = os.path.join(data_dir, 'annotations')\n  examples_path = os.path.join(annotations_dir, 'trainval.txt')\n  examples_list = dataset_util.read_examples_list(examples_path)\n\n  # Test images are not included in the downloaded data set, so we shall perform\n  # our own split.\n  random.seed(42)\n  random.shuffle(examples_list)\n  num_examples = len(examples_list)\n  num_train = int(0.7 * num_examples)\n  train_examples = examples_list[:num_train]\n  val_examples = examples_list[num_train:]\n  logging.info('%d training and %d validation examples.',\n               len(train_examples), len(val_examples))\n\n  train_output_path = os.path.join(FLAGS.output_dir, 'pet_train.record')\n  val_output_path = os.path.join(FLAGS.output_dir, 'pet_val.record')\n  create_tf_record(train_output_path, label_map_dict, annotations_dir,\n                   image_dir, train_examples)\n  create_tf_record(val_output_path, label_map_dict, annotations_dir,\n                   image_dir, val_examples)\n\nif __name__ == '__main__':\n  tf.app.run()  python object_detection/dataset_tools/create_pet_tf_record.py \\\n    --label_map_path=object_detection/data/pet_label_map.pbtxt \\\n    --data_dir=`pwd` \\\n    --output_dir=`pwd`",
            "title": "creating TFR datasets"
        },
        {
            "location": "/deeplearning/#create-model-config-file",
            "text": "# Faster R-CNN with Resnet-101 (v1), configuration for MSCOCO Dataset.\n# Users should configure the fine_tune_checkpoint field in the train config as\n# well as the label_map_path and input_path fields in the train_input_reader and\n# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n# should be configured.\n\nmodel {\n  faster_rcnn {\n    num_classes: 90\n    image_resizer {\n      keep_aspect_ratio_resizer {\n        min_dimension: 600\n        max_dimension: 1024\n      }\n    }\n    feature_extractor {\n      type: 'faster_rcnn_resnet101'\n      first_stage_features_stride: 16\n    }\n    first_stage_anchor_generator {\n      grid_anchor_generator {\n        scales: [0.25, 0.5, 1.0, 2.0]\n        aspect_ratios: [0.5, 1.0, 2.0]\n        height_stride: 16\n        width_stride: 16\n      }\n    }\n    first_stage_box_predictor_conv_hyperparams {\n      op: CONV\n      regularizer {\n        l2_regularizer {\n          weight: 0.0\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n          stddev: 0.01\n        }\n      }\n    }\n    first_stage_nms_score_threshold: 0.0\n    first_stage_nms_iou_threshold: 0.7\n    first_stage_max_proposals: 300\n    first_stage_localization_loss_weight: 2.0\n    first_stage_objectness_loss_weight: 1.0\n    initial_crop_size: 14\n    maxpool_kernel_size: 2\n    maxpool_stride: 2\n    second_stage_box_predictor {\n      mask_rcnn_box_predictor {\n        use_dropout: false\n        dropout_keep_probability: 1.0\n        fc_hyperparams {\n          op: FC\n          regularizer {\n            l2_regularizer {\n              weight: 0.0\n            }\n          }\n          initializer {\n            variance_scaling_initializer {\n              factor: 1.0\n              uniform: true\n              mode: FAN_AVG\n            }\n          }\n        }\n      }\n    }\n    second_stage_post_processing {\n      batch_non_max_suppression {\n        score_threshold: 0.0\n        iou_threshold: 0.6\n        max_detections_per_class: 100\n        max_total_detections: 300\n      }\n      score_converter: SOFTMAX\n    }\n    second_stage_localization_loss_weight: 2.0\n    second_stage_classification_loss_weight: 1.0\n  }\n}\n\ntrain_config: {\n  batch_size: 1\n  optimizer {\n    momentum_optimizer: {\n      learning_rate: {\n        manual_step_learning_rate {\n          initial_learning_rate: 0.0003\n          schedule {\n            step: 900000\n            learning_rate: .00003\n          }\n          schedule {\n            step: 1200000\n            learning_rate: .000003\n          }\n        }\n      }\n      momentum_optimizer_value: 0.9\n    }\n    use_moving_average: false\n  }\n  gradient_clipping_by_norm: 10.0\n  fine_tune_checkpoint: \"PATH_TO_BE_CONFIGURED/model.ckpt\"\n  from_detection_checkpoint: true\n  data_augmentation_options {\n    random_horizontal_flip {\n    }\n  }\n}\n\ntrain_input_reader: {\n  tf_record_input_reader {\n    input_path: \"PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100\"\n  }\n  label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\"\n}\n\neval_config: {\n  num_examples: 8000\n  # Note: The below line limits the evaluation process to 10 evaluations.\n  # Remove the below line to evaluate indefinitely.\n  max_evals: 10\n}\n\neval_input_reader: {\n  tf_record_input_reader {\n    input_path: \"PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010\"\n  }\n  label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\"\n  shuffle: false\n  num_readers: 1\n}",
            "title": "create model config file"
        },
        {
            "location": "/deeplearning/#train",
            "text": "python train.py --logtostderr \n--train_dir=/home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/train_dir \n--pipeline_config_path=/home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/net.config  tensorboard --logdir /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/train_dir/",
            "title": "train"
        },
        {
            "location": "/deeplearning/#evaluator",
            "text": "python new_eval.py --logtostderr \n--checkpoint_dir /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/train_dir \n--eval_dir /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/eval_dir \n--pipeline_config_path /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/net.config  tensorboard --logdir /home/pmjd/Disk/Deep-Learning-master/tensorflow_toy_detector/eval_dir/  # new_eval.py\nimport functools\nimport os\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.python.util.deprecation import deprecated\nfrom object_detection.builders import dataset_builder\nfrom object_detection.builders import graph_rewriter_builder\nfrom object_detection.builders import model_builder\nfrom object_detection.legacy import evaluator\nfrom object_detection.utils import config_util\nfrom object_detection.utils import label_map_util\n\n\nfrom tensorflow.compat.v1 import ConfigProto\nfrom tensorflow.compat.v1 import InteractiveSession\nimport keras.backend as K\nconfig = ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = InteractiveSession(config=config)\nK.set_session(session)\n\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\nflags = tf.app.flags\nflags.DEFINE_boolean('eval_training_data', False,\n                     'If training data should be evaluated for this job.')\nflags.DEFINE_string(\n    'checkpoint_dir', '',\n    'Directory containing checkpoints to evaluate, typically '\n    'set to `train_dir` used in the training job.')\nflags.DEFINE_string('eval_dir', '', 'Directory to write eval summaries to.')\nflags.DEFINE_string(\n    'pipeline_config_path', '',\n    'Path to a pipeline_pb2.TrainEvalPipelineConfig config '\n    'file. If provided, other configs are ignored')\nflags.DEFINE_string('eval_config_path', '',\n                    'Path to an eval_pb2.EvalConfig config file.')\nflags.DEFINE_string('input_config_path', '',\n                    'Path to an input_reader_pb2.InputReader config file.')\nflags.DEFINE_string('model_config_path', '',\n                    'Path to a model_pb2.DetectionModel config file.')\nflags.DEFINE_boolean(\n    'run_once', False, 'Option to only run a single pass of '\n    'evaluation. Overrides the `max_evals` parameter in the '\n    'provided config.')\nFLAGS = flags.FLAGS\n\n\n@deprecated(None, 'Use object_detection/model_main.py.')\ndef main(unused_argv):\n  assert FLAGS.checkpoint_dir, '`checkpoint_dir` is missing.'\n  assert FLAGS.eval_dir, '`eval_dir` is missing.'\n  tf.gfile.MakeDirs(FLAGS.eval_dir)\n  if FLAGS.pipeline_config_path:\n    configs = config_util.get_configs_from_pipeline_file(\n        FLAGS.pipeline_config_path)\n    tf.gfile.Copy(\n        FLAGS.pipeline_config_path,\n        os.path.join(FLAGS.eval_dir, 'pipeline.config'),\n        overwrite=True)\n  else:\n    configs = config_util.get_configs_from_multiple_files(\n        model_config_path=FLAGS.model_config_path,\n        eval_config_path=FLAGS.eval_config_path,\n        eval_input_config_path=FLAGS.input_config_path)\n    for name, config in [('model.config', FLAGS.model_config_path),\n                         ('eval.config', FLAGS.eval_config_path),\n                         ('input.config', FLAGS.input_config_path)]:\n      tf.gfile.Copy(config, os.path.join(FLAGS.eval_dir, name), overwrite=True)\n\n  model_config = configs['model']\n  eval_config = configs['eval_config']\n  input_config = configs['eval_input_config']\n  if FLAGS.eval_training_data:\n    input_config = configs['train_input_config']\n\n  model_fn = functools.partial(\n      model_builder.build, model_config=model_config, is_training=False)\n\n  def get_next(config):\n    return dataset_builder.make_initializable_iterator(\n        dataset_builder.build(config)).get_next()\n\n  create_input_dict_fn = functools.partial(get_next, input_config)\n\n  categories = label_map_util.create_categories_from_labelmap(\n      input_config.label_map_path)\n\n  if FLAGS.run_once:\n    eval_config.max_evals = 1\n\n  graph_rewriter_fn = None\n  if 'graph_rewriter_config' in configs:\n    graph_rewriter_fn = graph_rewriter_builder.build(\n        configs['graph_rewriter_config'], is_training=False)\n\n  evaluator.evaluate(\n      create_input_dict_fn,\n      model_fn,\n      eval_config,\n      categories,\n      FLAGS.checkpoint_dir,\n      FLAGS.eval_dir,\n      graph_hook_fn=graph_rewriter_fn)\n\n\nif __name__ == '__main__':\n  tf.app.run()",
            "title": "evaluator"
        },
        {
            "location": "/deeplearning/#from-object_detection-import-evaluator",
            "text": "ImportError: cannot import name 'evaluator' \\ from object_detection.legacy import evaluator",
            "title": "from object_detection import evaluator"
        },
        {
            "location": "/deeplearning/#control-trainning-steps",
            "text": "num_steps:600",
            "title": "control trainning steps"
        },
        {
            "location": "/deeplearning/#tensorflow-freeze",
            "text": "# From tensorflow/models/research/\nINPUT_TYPE=image_tensor\nPIPELINE_CONFIG_PATH={path to pipeline config file}\nTRAINED_CKPT_PREFIX={path to model.ckpt}\nEXPORT_DIR={path to folder that will be used for export}\npython object_detection/export_inference_graph.py \\\n    --input_type=${INPUT_TYPE} \\\n    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \\\n    --trained_checkpoint_prefix=${TRAINED_CKPT_PREFIX} \\\n    --output_directory=${EXPORT_DIR}",
            "title": "tensorflow freeze"
        },
        {
            "location": "/deeplearning/#modelsave-and-load",
            "text": "model.fit(x_train, y_train, epochs = 150, batch_size = 32,callbacks=[tensorboard_callback])\nmodel.save('./models/model.h5')\nmodel.save_weights('./models/weights.h5')  model_path = './models/model.h5'\nmodel_weights_path = './models/weights.h5'\nmodel = load_model(model_path)\nmodel.load_weights(model_weights_path)\narray = model.predict(point_set)  or  tf.keras.models.save_model(\n    model,\n    'models/mymode',\n    overwrite=True,\n    include_optimizer=True\n  )  model = tf.keras.models.load_model('./models/mymode')",
            "title": "model:save and load"
        },
        {
            "location": "/deeplearning/#cudnn",
            "text": "from tensorflow.compat.v1 import ConfigProto\nfrom tensorflow.compat.v1 import InteractiveSession\nimport keras.backend as K\nK.set_image_data_format('channels_last')\nK.set_learning_phase(1)\nconfig = ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = InteractiveSession(config=config)\nK.set_session(session)",
            "title": "cudnn\u5931\u8d25"
        },
        {
            "location": "/deeplearning/#pbsave-and-load",
            "text": "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n  graph = session.graph\n  with graph.as_default():\n    freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n    output_names = output_names or []\n    output_names += [v.op.name for v in tf.global_variables()]\n    input_graph_def = graph.as_graph_def()\n    if clear_devices:\n      for node in input_graph_def.node:\n        node.device = ''\n    frozen_graph = tf.graph_util.convert_variables_to_constants(\n        session, input_graph_def, output_names, freeze_var_names)\n    return frozen_graph\n\n\nfrozen_graph = freeze_session(K.get_session(), output_names=[out.op.name for out in model.outputs])\ntf.io.write_graph(frozen_graph, './models', 'xor.pbtxt', as_text=True)\ntf.io.write_graph(frozen_graph, './models', 'xor.pb', as_text=False)  detection_graph = tf.Graph()\nwith detection_graph.as_default():\n  od_graph_def = tf.GraphDef()\n  with tf.gfile.GFile('xor.pb', 'rb') as fid:\n    serialized_graph = fid.read()\n    od_graph_def.ParseFromString(serialized_graph)\n    tf.import_graph_def(od_graph_def, name='')\n    input = tf.get_default_graph().get_tensor_by_name('input_1:0')\n    output = tf.get_default_graph().get_tensor_by_name('fc2/Softmax:0')\n\nwith detection_graph.as_default():\n  with tf.Session() as sess:\n    values =sess.run(output, feed_dict={input: point_set})\n    print(values)",
            "title": "pb:save and load"
        },
        {
            "location": "/deeplearning/#_1",
            "text": "print('model.inputs :',model.inputs)\nprint('model.outputs : ',model.outputs)  output  model.inputs : [<tf.Tensor 'input_1:0' shape=(?, 2600, 3) dtype=float32>]\nmodel.outputs :  [<tf.Tensor 'fc2/Softmax:0' shape=(?, 2) dtype=float32>]  \u6240\u4ee5\u8f93\u5165\u5c42\u662f input_1:0 \u8f93\u51fa\u5c42\u662f fc2/Softmax:0",
            "title": "\u67e5\u770b\u6a21\u578b\u7684\u8f93\u5165\u5c42\u548c\u8f93\u51fa\u5c42"
        },
        {
            "location": "/deeplearning/#tensorrt",
            "text": "\u4e0b\u8f7d\u4e0e\u60a8\u4f7f\u7528\u7684Ubuntu\u7248\u672c\u548cCPU\u67b6\u6784\u5339\u914d\u7684TensorRT\u672c\u5730repo\u6587\u4ef6\u3002  \u4eceDebian\u672c\u5730repo\u8f6f\u4ef6\u5305\u5b89\u88c5TensorRT\u3002\n    os=\"ubuntu1x04\"\n   tag=\"cudax.x-trt7.x.x.x-ga-yyyymmdd\"\n   sudo dpkg -i nv-tensorrt-repo-${os}-${tag}_1-1_amd64.deb\n   sudo apt-key add /var/nv-tensorrt-repo-${tag}/7fa2af80.pub\n   sudo apt-get update\n   sudo apt-get install tensorrt  \u5982\u679c\u4f7f\u7528Python 2.7\uff1a\\\n    sudo apt-get install python-libnvinfer-dev \\\n   \u5c06\u5b89\u88c5\u4ee5\u4e0b\u9644\u52a0\u8f6f\u4ef6\u5305\uff1a\\\n    python-libnvinfer \\\n   \u5982\u679c\u4f7f\u7528Python 3.x\uff1a\\\n    sudo apt-get install python3-libnvinfer-dev \\\n   \u5c06\u5b89\u88c5\u4ee5\u4e0b\u9644\u52a0\u8f6f\u4ef6\u5305\uff1a\\\n    python3-libnvinfer \\\n   \u5982\u679c\u60a8\u6253\u7b97\u5c06TensorRT\u4e0eTensorFlow\u7ed3\u5408\u4f7f\u7528\uff1a\\\n    sudo apt-get install uff-converter-tf \\\n   \u5982\u679c\u60a8\u8981\u8fd0\u884c\u9700\u8981ONNX\u7684\u793a\u4f8b \u56fe\u5f62\u5916\u79d1\u533b\u751f \u6216\u5c06Python\u6a21\u5757\u7528\u4e8e\u60a8\u81ea\u5df1\u7684\u9879\u76ee\uff0c\u8fd0\u884c\uff1a\\\n    sudo apt-get install onnx-graphsurgeon \\",
            "title": "\u5b89\u88c5\uff54\uff45\uff4e\uff53\uff4f\uff52RT"
        },
        {
            "location": "/deeplearning/#anaconda-tensorrt",
            "text": "\u4e0b\u8f7dtar\u6587\u4ef6 TensorRT-7.0.0.11.Ubuntu-18.04.x86_64-gnu.cuda-10.0.cudnn7.6.tar.gz   (wind1) star@xmatrix:~$ \n(wind1) star@xmatrix:~$ \n(wind1) star@xmatrix:~$ cd TensorRT\n(wind1) star@xmatrix:~/TensorRT$ \n(wind1) star@xmatrix:~/TensorRT$ \n(wind1) star@xmatrix:~/TensorRT$ \n(wind1) star@xmatrix:~/TensorRT$ ls\nTensorRT  TensorRT_1  TensorRT-7.0.0.11  TensorRT-7.0.0.11.Ubuntu-16.04.x86_64-gnu.cuda-10.0.cudnn7.6.tar.gz  TensorRT-7.0.0(1).tar.gz\n(wind1) star@xmatrix:~/TensorRT$ \n(wind1) star@xmatrix:~/TensorRT$ \n(wind1) star@xmatrix:~/TensorRT$ \n(wind1) star@xmatrix:~/TensorRT$ \n(wind1) star@xmatrix:~/TensorRT$ \n(wind1) star@xmatrix:~/TensorRT$ cd TensorRT-7.0.0.11\n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ ls\nbin  data  doc  graphsurgeon  include  lib  python  samples  targets  TensorRT-Release-Notes.pdf  uff\n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ cd python\n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ ls\ntensorrt-7.0.0.11-cp27-none-linux_x86_64.whl  tensorrt-7.0.0.11-cp35-none-linux_x86_64.whl  tensorrt-7.0.0.11-cp37-none-linux_x86_64.whl\ntensorrt-7.0.0.11-cp34-none-linux_x86_64.whl  tensorrt-7.0.0.11-cp36-none-linux_x86_64.whl\n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ pip --version\npip 19.3.1 from /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages/pip (python 3.6)\n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ pip3 --version\npip 19.3.1 from /usr/local/lib/python3.5/dist-packages/pip (python 3.5)\n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ python -m pip install tensorrt-7.0.0.11-cp36-none-linux_x86_64.whl\nProcessing ./tensorrt-7.0.0.11-cp36-none-linux_x86_64.whl\nInstalling collected packages: tensorrt\nSuccessfully installed tensorrt-7.0.0.11\n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ python\nPython 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) \n[GCC 7.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \n>>> \n>>> import tensorrt\n>>> \n>>> exit();\n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/python$ cd ../\n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ ls\nbin  data  doc  graphsurgeon  include  lib  python  samples  targets  TensorRT-Release-Notes.pdf  uff\n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11$ cd uff\n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ ls\nuff-0.6.5-py2.py3-none-any.whl\n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ python -m pip install uff-0.6.5-py2.py3-none-any.whl\nProcessing ./uff-0.6.5-py2.py3-none-any.whl\nRequirement already satisfied: numpy>=1.11.0 in /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages (from uff==0.6.5) (1.16.4)\nRequirement already satisfied: protobuf>=3.3.0 in /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages (from uff==0.6.5) (3.11.2)\nRequirement already satisfied: six>=1.9 in /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages (from protobuf>=3.3.0->uff==0.6.5) (1.13.0)\nRequirement already satisfied: setuptools in /home/star/anaconda3/envs/wind1/lib/python3.6/site-packages (from protobuf>=3.3.0->uff==0.6.5) (42.0.2.post20191203)\nInstalling collected packages: uff\nSuccessfully installed uff-0.6.5\n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ python\nPython 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) \n[GCC 7.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \n>>> \n>>> import tensorrt\n>>> import uff\nWARNING:tensorflow:From /home/star/anaconda3/envs/wind1/lib/python3.6\n\n7.0.0.11\n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$ \n(wind1) star@xmatrix:~/TensorRT/TensorRT-7.0.0.11/uff$",
            "title": "anaconda tensorRT"
        },
        {
            "location": "/deeplearning/#using-uff-converter-to-convert-the-frozen-tensorflow-model-to-a-uff-file",
            "text": "conda activate wjj \\ pip install nvidia-pyindex \\ pip install uff \\\nyou need to find your uff installed path. \\  import uff\nprint(uff.__path__)  And after locating it , in it\u2019s bin folder there should be a script named as  convert_to_uff.py . And now you need to open the terminal and simply type  python3 convert_to_uff.py  < path to the saved model >\\\nIn my case-->\\ python3 convert_to_uff.py /home/models/catsAndDogs.pb \\\nAnd it will simply save the converted  .uff  in your  .pb  model location.\nAnd then this is how the next script should be done.",
            "title": "Using UFF converter to convert the frozen tensorflow model to a UFF file"
        },
        {
            "location": "/deeplearning/#nvidia",
            "text": "NVIDIA\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6",
            "title": "NVIDIA\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6"
        },
        {
            "location": "/deeplearning/#could-not-create-cudnn-handle-cudnn_status_internal_error",
            "text": "Use allow_growth memory option in TensorFlow and Keras, before your code.  For Keras  import tensorflow as tf\nconfig = tf.compat.v1.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\n\nsess = tf.compat.v1.Session(config=config)\ntf.compat.v1.keras.backend.set_session(sess)  For TensorFlow  import tensorflow as tf\nconfig = tf.compat.v1.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.compat.v1.Session(config=config)  \u8fd8\u6709\u4e00\u79cd\u60c5\u51b5,\u51cf\u5c11batch size",
            "title": "Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR"
        },
        {
            "location": "/deeplearning/#anchor-boxes",
            "text": "One of the hardest concepts to grasp\u628a\u63e1 when learning about Convolutional Neural Networks for object detection is the idea of anchor boxes.It is also one of the most important parameters you can tune for improved performance on your dataset.In fact,if anchor oxes are  not tuned correctly,your neural network will never even know that certain\u67d0\u4e9b small,large or irregular\u4e0d\u89c4\u5219 objects exist and will never have a chance to detect them.Luckily, there are some simple steps you can take to make sure you do not fall into this trap\u9677\u9631.  what are anchor boxes? \\\nwhen you use a neural network like yolo or ssd to predict multiple objects in a picture, the network is actually making thousands of predictions and only showing the ones that it decided were an object.The multiple predictions are output the following format:\\\nPrediction 1: (X,Y,Height,Width),Class\\\n...\\\nPrediction ~8000: (X,Y,Height,Width),Class  Where the (X,Y,Height,Width) is called the \"bounding box\", or box surrounding the objects.This box and the object class are labelled manually by human annotators.  In an extremely simplified example, imagine that we have a model that has two predictions and receives the following image:\\  We need to tell our network if each of its predictions is correct or not in order for it to be able to learn.But what do we tell the neural network it prediction should be? Should the predicted class be:\\\nPrediction 1:Pear\\\nPrediction 2:Apple  Or should it be:\\\nPrediction 1:Apple\\\nPrediction 2:Pear  What if the network predicts:\\\nPrediction 1:Apple\\\nPrediction 2:Apple  We need our network's two predictors to be able to tell whether it is their job to predict the pear or the apple.To do this there are a several tools.Predictors can specialize in certain size objects, objects with a certain aspect ratio(tall vs. wide),or objects in different parts of the image.Most networks use all three criteria\u6807\u51c6.In our example of the pear/apple image,we could have Prediction 1 be for objects on the left and Prediction 2 for objects on the right side of the image.Then we would have our answer for what the network should be predict:\\\nPrediction 1:Pear\\\nPrediction 2:Apple  Anchor Boxes in Practice\\\nState of the art\u6700\u5148\u8fdb\u7684 object detection systems currently do the following:\\\n1. Create thousands of \"anchor boxes\" or \"prior boxes\" for each predictor that represent the ideal location, shape and size of the object it specializes\u4e13 in predicting.\n2. For each anchor box,calculate which object's  bounding box has the highest overlap divided by non-overlap.This is called Intersection Over Union or IOU.\n3. If the highest IOU is greater than 50%, tell the anchor box that it should detect the object that gave the highest IOU.\n4. Otherwise if the IOU is greater than 40%, tell the neural network that the true detection is ambiguous and not to learn from that example.\n5. If the highest IOU is less than 40%,then the anchor box should predict that there is no object.  This works well in practice and the thousands of predictors do a very good job of deciding whether their type of object apperars in an image.  Using the default anchor box configuration can create predictors that are too specialized and ojects that appear in the image may not achieve an IOU of 50% with any of the anchor boxes.In this case, the neural network will never know these objects existed and will never learn to predict them. We can tweak\u8c03\u6574 our anchor boxes to be much smaller  In xx net configuration, the smallest anchor box size is 32x32.This means that many objects smaller than this will go undetected. We can fix this by changing our default anchor box configurations. Reducing the smallest anchor box size, all of the things line up with at least one of our anchor boxes and our neural network can learn to detect them!  Improving Anchor Box Configuration \\\nAs a general rule,you should ask yourself the following questions about your dataset before diving into training your model:\n1. What is the smallest size box I want to be able to detect?\n2. What is the largest size box I want to be able to detect?\n3. What are the shapes the box can take? For example, a car detector might have short and wide anchor boxes as long as there is no chance of the car or the camera being turned on its side.  You can get a rough estimate of these by actually calculating the most extreme\u6781\u7aef sizes\u3000and aspect ratios in the dataset.Yolo V3 uses K-means to estimate the ideal bounding boxes.Another option is to learn the anchor box configuration.  Once you have thought through these questions you can start designing your anchor boxes. Be sure to test them by encoding your ground truth bounding boxes\u5730\u9762\u771f\u503c\u8fb9\u754c\u6846 and then decoding them as though\u5c31\u50cf they were predictions from your model.You should be able to recover the ground truth bounding boxes.",
            "title": "Anchor boxes"
        },
        {
            "location": "/deeplearning/#autonomous-self-learning-systems",
            "text": "",
            "title": "Autonomous self-learning systems"
        },
        {
            "location": "/deeplearning/#preface",
            "text": "This report describes the various processes that are part of the work on the main project at Oslo and Akershus University College(HIOA), department of engineering education,spring 2015.  The report deals with the development of a self-learning algorithm and a demonstrator in the form of a robot that will avoid static and dynamic objects in an environment that is constantly changing.The thesis\u8bba\u6587 is given by HIOA.The report addresses the theory behind the most well-known and used self-learning algorithms,and discusses the advantages,disadvantages and uses of theese.It also contains a description of the technical solution for the demonstrator, and the method used in this project.  The report is written within a topic that is considered new technical and is therefore assumed to be able to be used for futher research and/or learning within autonomous self-learning system.  The reader is expected to have basic knowledge in electronics and information technology.  We would like to thank our employer.Oslo and Akershus University College,for the opportunity to carry out the project and for financial support.We would also like to thank supervisor EZ for a good collaboration, as well as important and constructive guidance throughout the project period.",
            "title": "Preface"
        },
        {
            "location": "/deeplearning/#summary",
            "text": "In today's society, self-learning systems are an increasingly relevant topic.Systems that are not explicity programmed to perform a specific task, but are even able to adapt,can be very useful.  The system described in this thesis is realized with Q-learning by both the table method and the neural network.Q-learning is a learning algorithm based on experience.The algorithm involves an agent exploring his environment, where the environment is represented by a number of states. The agent experiments withs the environment by performing an action, and then observes the consequence of that action.The consequence is given in the form of a positive or negative reward. The goal of the method is to maximize the accumulated reward over time.  Autonomous self-learning systems are becoming increasingly relevant because the system is able to adapt to partially or completely unknown situations.It learns from experience and needs less information at start-up as it acquires information along the way.In autonomous self-learning systems and self-propelled robots, avoiding obstacles is a key task.This report addresses a demonstrator of such a system, realized with Q-learning presented later in the reportk, and provides a description of the algorithm and results.",
            "title": "Summary"
        },
        {
            "location": "/deeplearning/#theory",
            "text": "Before looking at the structure of a self-learning system, one can advantageouly look at what the concept of learning is.Learning is often defined as a lasting change in behavior as a result of experience(St. Olavs Hospital,undated).The property of organisms\u751f\u7269\u4f53 that is defined as learning is one of the cornerstones of what is called intelligence which, among other things, is defined as an ability to acquire and apply knowledge and skills.Humans and animals are considered intelligent, among other things,based on their ability to learn from experience.",
            "title": "Theory"
        },
        {
            "location": "/deeplearning/#machine-learning",
            "text": "This subchapter is based on the theory of S.M,2009.Machine learning is a form of artifical intelligence that focuses on the development of self-learning algorithms. In most cases,self-learning systems deal with parts of natural intellignece, including memory,adaptation and generalization\u6982\u62ec.Unlike traditional non-learning systems, the method makes it possible to construct a system that is able to expand , adapt to new information and learn a given task without being specifically programmed for this.For machine learning,this system is called an agent.By using menmory,an agent can recognize the last time it was in a similiar situation and what action it took.Based on the outcome from the previous time, it can, if it was correct,choose to repeat the action, or try something new.By generallizing, the agent can recognize similarities in different situations, and thus use experienct from one situation and apply this experience in another.  In order to realize\u5b9e\u73b0 this concept, machine learning uses principles\u539f\u7406 from statistics, mathematics, physics, neurology and biology.  \uff37hen talking about machine learning and self-learning systems,algorithms are mainly the main product.The actual process in these algorithms can be compared to data mining\u6316\u6398.Data mining is a process that analyzes data from different perspectives and summarizes if into useful information.Both methods go through data to find patterns\u6a21\u5f0f,but instead of extractiong data for human interpretation,the information is used to improve the agent's understanding.For the agent to be able to learn, it must know how to improve , and whether it will get better or not.There are 15 more methods to solve this, which in turn provide servel main categories within machine learning:Supervised learning,unsupervised learning and reinforcement learning.",
            "title": "machine learning"
        },
        {
            "location": "/deeplearning/#supervised-learning",
            "text": "An agent is given a training set with ,for example,pictures of a face and pictures without a face.The agent then prepared through a training process where it gives a forecast of what the picture is of. Whenever the forecast is incorrect, the agent is corrected.This process continues untill the model achieves a desired level of accuracy. Since the algorithm does not have a specific definition of what is a face and what is not, it must therefore learn this using examples.A good algorithm will eventually be able to estimate whether an image is of a face or not.The learning methods are best explained by examples:",
            "title": "Supervised Learning"
        },
        {
            "location": "/deeplearning/#unsupervised-learning",
            "text": "When learning without supervision, information is not marked.This is ,the system is not told what is the image of a face and what is not.As a result, there is no correction or reward to indicate a potential solution, but the algorithm tries to identify the similarities between the images, swfor then categorize them and divide them into groups.",
            "title": "Unsupervised Learning"
        },
        {
            "location": "/deeplearning/#reinforcement-learning",
            "text": "In reinforcement learning,the algorithm is told when the answer it gives is incorrect, but receives no suggestions on how to correct this.It must explore and try out different solutions untill it finds out how it gets the right answers.This is a kind of middle ground of supervised learning and unsupervised learning.Examples could be learning to play a board game or a robot that is going to learn to walk.Each time an agent performs an action,he or she recieves a reward or a penalty, based on how desirable\u53ef\u53d6\u7684 the outcome of the action is.For example.when an agent is trained to play a board game, he gets a positive reward for winning, and a negative reward for losing.All other cases give no reward.\\ \\\nThe activity mentioned above can be represent as a sequence of state-action reward:\\ \\\nThis means that the agent was in state s0, performed action a0,which resulted in it receiving reward r1 and ending up in state s1.Furthermore, it performed action a1, received reward r2, and ended up in state s2, and so on.\\\n\uff34his sequence is made up of experiences where experience is given as:\\ \\\nExperience shows that the agent was in state s,performed action a,received reward r, and ended up in state s', and represented by (s,a,s',r)  In order for the agent to be able to learn from the sequences mentioned and thus call it an experience.It has a table called Q-table which acts as its memory.All data points stored in this table are called Q-values and represent how desirable\u53ef\u53d6\u7684 it is to perform a specific action in a specific state.  One experience adds one data point Q(s,a) in the table that represents the agents current estimate\u4f30\u8ba1 of the optimal\u6700\u4f73 Q value. It is this information that the agent uses to learn an optimal pattern of action.The size of the table depends on how many conditions and actions are included in the problem you are trying to solve, where the number of conditions gives the number of rows ,while the number of actions gives the number of colums.For example if you have 20 states and 3 actions, you will get a table of 20x3.  Before the agent begins to experiment, it knows nothing else what actions it is capable of performing, and the Q table is consequently empty.This is, all Q values are equal to 0.  The key to the method described above is to update the Q value that is applied to the agent when it performs an action in a given state in the current data point when it gains an experience.  This value is given by the Q function\\ \\\nor more clearly:\\   \u03b1\uff0d\uff0dlearning rate\\\n\u0393\uff0d\uff0ddiscount factor\\\nr\uff0d\uff0dreward\\   The learning rate dicates\u6307\u793a how much of previouly\u5148\u524d acquired learning should be replaced with new information. \nBy \u03b1=1,previous values are replaced with new information.while \u03b1=0 ,corresponds to no update.In other words,the agent will konw by \u03b1=1,assume that the last reward and resulting state are representative of future values.  The discount factor indicates the weight of all futher step rewards.If \u0393=0,the agent will only consider current rewards, while it will think more long-term\u957f\u671f and strive\u52aa\u529b for higher future rewards as \u0393 approaches 1.  The reward is defined when you create the program in the form of reward functions\u5956\u52b1\u662f\u5728\u521b\u5efa\u7a0b\u5e8f\u65f6\u4ee5\u5956\u52b1\u51fd\u6570\u7684\u5f62\u5f0f\u5b9a\u4e49\u7684,and can be positive or negative. What this reward value is defined as is not critical.On the other hand, It is important that there is a clear distinction\u533a\u522b between the reward for good deeds\u884c\u4e3a and the reward for bad deeds.  When a new experience is added to the algorithm, a new Q value is estimated, and the old value of the Q table for the last experience is updated with the new one.  Assume that the agent is a mouse that is placed in a room divded into six states, see below Figure.In state s6 there is a piece of cheese,while in state s5 there is a cat.If the agent finds the piece of cheese, it receives a reward of 10, while it receives a reward of -10 if it moves to the state where the cat is.These two states are called terminal states.The agent explores the environment untill one of the terminal states is reached,before starting a new iteration(attempt)\\  \uff34he agent can perform four actions: up,down,left,right.If it moves in a direction where there is a wall, it gets a reward of -1, and it remains in the same state. All other conditions have a value equal to 0.This is implemented in the algorithm using reward functions.\\  \\\nThe agent has no information about his surroundings other than that there are six states and four acitons, what state he is in at any given time and any rewards it receives as actions are performed.Nor does it know what a reward or punishment is.The Q table is illustrated in Table 1.  By assuming the following sequences of experiences,(s,a,r,s') one can illustrate how a sequence updates the Q-table.  For the sake of illustration, the learning rate \u03b1 and the discount factor \u0393 set equal to 1 and all Q-values have an initial value equal to 0.\\  After this sequence of experiences, the Q table with updated values will look like this:\\ \\\nopp=up ned=down venstre=left h\u00d8yre=right  Each experience will update the value of the state table for the performed state and action combination, and the Q values will converge\u6c47\u805a to optimal values over time.The more experiments the agent conducts\u884c\u4e3a, the better the estimates in the Qtable.In theory, the agent will eventually achieve an optimized Q-table, and will be able to choose the shortest path to the cheese each time, regardless of the starting state.",
            "title": "Reinforcement Learning"
        },
        {
            "location": "/deeplearning/#exploration-or-utilize",
            "text": "One of the challenges with the Q-learning algorithm is that it does not directly tell what the agent should do, but rather functions as a tool that shows the agent what the optimal action is in a given condition.This is a challenge because the agent must explore the environment enough times to be able to build a solid foundation\u575a\u5b9e\u7684\u57fa\u7840 that provides a good estimate of the Q values.To address this, an exploration feature is implemented that determines the strategy\u6218\u7565 the agent will use to select actions, either explore or exploit.  Exploration:The agent is bold\u80c6\u5927 and does not necessary choose the best course of action, with the intension of establishing a better estimate of the Q values.  Utilize: The agent utilizes the experience it has already built up, and selects the optimal action for the condition it is in. That is, the action that gives the highest Q value. It is said that the agent is greedy\u8d2a\u5a6a  The purpose of this feature is to establish a relationship between the two strategies. It is important to explore enough so that the agent builds a solid foundation of the Q-values, but it is also important to utilize the already acquired knowledge to ensure the highest possible reward.  There are a number of ways to accomplish this, but two of the most popular methods are the greedy feature and the Boltzmann distribution, popularly called \"softmax\":",
            "title": "Exploration or Utilize \u52d8\u63a2\u6216\u5f00\u53d1"
        },
        {
            "location": "/deeplearning/#-greedy",
            "text": "\u03b5-greedy is a strategy based on always choosing the optimal action except for \u03b5 of the aisles and choosing a random action of the aisles.\\ \\\nthere\\\n a* is optimal action with probability 1-\u03b5\\\n ar is random action with probability \u03b5\\\nwhere\\\n 0< \u03b5 < 1  It is possible to change over time. This allows the agent to choose a more random strategy at the beginning of the learning process, and then become more greedy as it acquires more knowledge.  One of the challenges with greed is that it considers all actions except the optimal one as equal.\u5b83\u5c06\u6700\u4f73\u884c\u52a8\u4e4b\u5916\u7684\u6240\u6709\u884c\u52a8\u90fd\u89c6\u4e3a\u5e73\u7b49 If, in addition to an optimal action, there are two good actions as well as additional actions that look less promising,\u8fd8\u6709\u4e24\u4e2a\u597d\u7684\u52a8\u4f5c\u4ee5\u53ca\u770b\u8d77\u6765\u4e0d\u592a\u6709\u524d\u9014\u7684\u5176\u4ed6\u52a8\u4f5c\uff0c it will make sense\u6709\u610f\u4e49 for the agent to choose one of the two better ones instead of spending time exploring the bad actions. Using greed is as likely to explore a bad action as to explore one of the two better ones. One way to solve this is to use the Boltzmann distribution   2.3.2 Boltzmann distribution\nThis strategy is known as \"softmax\" action selection and involves selecting an action with a probability that depends on the Q value. The probability of selecting action a in state s is proportional to\u3000 ,which means that the agent in state s chooses action a with probability \\  The parameter T specifies how random values are to be selected.\u53c2\u6570T\u6307\u5b9a\u5982\u4f55\u9009\u62e9\u968f\u673a\u503c\u3002  When T has a high value, actions are chosen to approximately the same extent.\u9009\u62e9\u7684\u52a8\u4f5c\u7684\u7a0b\u5ea6\u5927\u81f4\u76f8\u540c As T decreases, the actions with the highest Q value are more likely to be selected, while the best action is always selected when T-> 0  One of the advantages of the Q-learning algorithm is that it is exploration insensitive\u5bf9\u63a2\u7d22\u4e0d\u654f\u611f. That is, the Q values will converge to optimal values regardless of the agent's behavior as long as information is collected. This is one of the reasons why Q-learning is one of the most widely used and effective methods in reinforcement learning. On the other hand, it is a prerequisite that all combinations of condition / action are tried out many enough times, which in most cases means that the exploration and exploitation aspect must be addressed as it is not always possible for an agent to experiment infinitely.  There are several methods for setting up an artificial neural network.One of the most common,which is also used in this project, is called a feedforward neural network, which means that the information is always fed from the input page and supplied further through the network.In other words,output from one layer + bias is used as input in the next.",
            "title": "\u03b5-greedy"
        },
        {
            "location": "/deeplearning/#fuzzy-logic",
            "text": "The theory of fuzzy logic is based on S.K.J(2007)\\\nFuzzy logic is a decision-making or management model based on diffuse\u3000\u6269\u6563\uff0c\u6563\u53d1,and input data classified as degrees of membership in a diffuse quantity. A diffuse set has no sharp boundaries, and is a set of values that belong to the same definition, where the definition is a diffuse term often described by an adjective\u5f62\u5bb9\u8bcd. For example: big potatoes or tall people.A diffuse set is also called a semantically\u8bed\u4e49\u4e0a variable or set.  Fuzzy logic classifies input data according to how large a membership the input data has to a set.The degree of membership is graded\u5206\u7ea7 between 0 and 1,and determined on the basis of a membership function\u6839\u636e\u96b6\u5c5e\u51fd\u6570\u786e\u5b9a.\uff34he member ship function is determined on the basis of knowledge of the field, and can be non-linear.Input data can have membership in servel different sets, but can only have full membership in one set.",
            "title": "Fuzzy Logic\u6a21\u7cca\u7406\u8bba"
        },
        {
            "location": "/deeplearning/#fuzzy-logic-behavior",
            "text": "Diffusion\u6269\u6563: input data is graded according to the diffused quantities,and the diffused quantities are sent to subprocess 2  Logical rules:the diffused quantites are tested against rules.The degree of fulfillment \u5c65\u884c from each rule is sent to sub-process 3.The rules can be implented with {and, if, or},or other methods such as an impact\u5f71\u54cd matrix.The rules describe the relationship between the diffused input and output.  Implication: the received data from sub-process 2 is tested againist rules that grade the impact on the overall consequence of each of the received data.The ratings are then sent to sub-process 4.  Aggregation\u805a\u5408:all the consequence\u540e\u679c quantities are compiled into an aggregate quantity\u603b\u91cf,which is the union of the quantities from subprocess 3.  A vdiffusion: The most representative element is extracted from the aggregate amount.This is usually the center of gravity\u91cd\u529b.That value will then be the control signal.   The Fuzzy logic method has a number of advantages in that it is able to process imprecise\u4e0d\u7cbe\u786e data and model non-linear relationships between input and output.The principle enables an accurate and realistic\u73b0\u5b9e\u7684 classification of data, as data is rarely\u5f88\u5c11 only true or false.",
            "title": "Fuzzy logic behavior"
        },
        {
            "location": "/deeplearning/#technical-solution",
            "text": "",
            "title": "Technical solution"
        },
        {
            "location": "/deeplearning/#agents",
            "text": "Two versions of the agent were developed.The first version,which was later replaced, was built on the basis of drivetrain\u52a8\u529b\u603b\u6210 and mechanics\u673a\u68b0\u5e08 taken from 'WLtoys' model 'A969'.\nThe radio-controlled car in scale 1:18 was delivered\u5df2\u4ea4\u4ed8 to drive with ratio and battery.The car measured 285x140x105mm(LxWxH) and could run for about 10 minutes on a fully charged battery.  Since the car was to be equiped with an Xbee module and communicate via bluetooth.the car's original reciever was removed.The steering servo\u8f6c\u5411\u4f3a\u670d was adapted to the car's receiver and was therefore replaced in favour of a PWM-controllable servo.To improve the agent's running time, the original battery(7.4V/1100mAh) was replaced with a 7.4V/5200mAh \"Fire Bull\" type battery, which would thoretically provide approximately five times as long.  To make room for all the electronics, a plate 1.5mm aluminum\u94dd was cut out and mounted on top of the car.The motor driver, microcontroller board and Xbee module were placed together with the motor and servo on the classis itself, while the battery, sensors and voltage regulator were mounted\u5b89\u88c5 on the new plate for easier access.  After the mentioned changes, the agent had improved battery life, adjustable speed and the correct physical size, but it turned out not to have a good enough turning radius. It had particular problems in situations where it had to react in one direction, and then change direction.  The car's turning radius was tested before the conversion was started, and it was measured that the car need 70cm to turn 90 degrees to the left or right(figure 3-4).It thus needed an area of 140cm in diameter to turn 180 degrees.It was therefore assumed that a round arena\u7ade\u6280\u573a with a diameter of about four meters was needed.The area was built,and it was found that four meters was enough to be able to drive around without obstacles,but that it became too samll when car had to avoid objects.  The ultrasonic sensors are limited, through discretion\u901a\u8fc7\u659f\u914c,to integer values between 1cm and 120cm, If one is to create a look-up table for the entire spectrum\u8303\u56f4, the table will consist of 120^5 different combinations, and would with three possible actions end up with a look-up table of 120^5x3 number of places. This is a less good solution as the table would be very large(74 billion seats).A large lookup table means that the agent spends much more time learning all the combinations of conditions, and actions for these.In addition, such a solution is far too large to be implemented on a normal computer.This chanllenge is solved by discretizing\u79bb\u6563\u5316 or dividing, the continuous sensor area into smaller and more managerable state spaces.",
            "title": "Agents"
        },
        {
            "location": "/deeplearning/#421-condition-space-model",
            "text": "All sensors can measure distances up to 400 cm. To avoid a large state table and to streamline the learning process, the sensor values are discretized down to 144 states. For the project described in this report, this also simplifies the reward function.   Each snsor is divided into four zones based on distance in centimeters. The division is shown in Figure 4.4\\ \\  A condition consists of four parameters. Two for the left side and two for the right side of the agent's field of view.A state\nis a combination of distance to the nearest obstacle and in which sector the obstacle is located .a sector describes the angle at which an object is located, relative to the agent.  The four parameter are defined by:\\\nk1:zone\u533a left side \u533a\u57df\u5de6\u4fa7\\\nk2:zone right side \u533a\u57df\u53f3\u4fa7\\\nk3:sector\u90e8\u95e8 left side \u6247\u533a\u5de6\u4fa7\\\nk4:sector right side \u6247\u533a\u53f3\u4fa7\\\nk5:observation of dynamic or static obstacle left side \u89c2\u5bdf\u52a8\u6001\u6216\u9759\u6001\u969c\u788d\u7269\u5de6\u4fa7\\\nk6:observation of dynamic or static obstacle right side \u89c2\u5bdf\u52a8\u6001\u6216\u9759\u6001\u969c\u788d\u7269\u53f3\u4fa7\\\nS(state space model) = k1 x k2 x k3 x k4 x (k5 x k6)\\\nThe agent can perceive\u611f\u77e5 two obstacles simultaneously, but not on the same page.With two obstacles on the same side,the algorithm will prioritize\u4f18\u5148 the obstacle that is in the zone with the lowest numbering. The division of the sectors is illustrated in Figure below\\ \\\nsector\u6247\u533a\\\nThe sectors have four different values, 0,1,2,3.The sensors perceive\u611f\u77e5 an object when the zone value is less than 2.The sectors are symmetrically\u5bf9\u79f0\u5730 distributed\u5206\u5e03.so that the agent's right field of view is discretized in the same way as the left.Sensors 1,2 and 3 on the left side correspond to sensors 3,4 and 5 for the right side.",
            "title": "4.2.1 Condition space model"
        },
        {
            "location": "/deeplearning/#4211-discretion-for-system-with-dynamic-obstacles",
            "text": "In the simulation for a system with dynamic obstacles, the state space model is expanded with two additional parameters,k5 and k6. k5 indicates with the value 1 or 0 whether the obstacle detected on the left side is dynamic, while k6 does the same for the right side.The number 1 indicates a dynamic obstacle,while 0 indicates static.The state table has thus benn expanded to 576 states.,An object os classified as a dynamic obstacle in that the agent first, while standing still,performs two measurements in a row. Then the two measurements are compared, if the obstacle moves away from the agent, the first measurement will be positive number, and classified as a static obstacle.If on the other hand, the same calculation operation gives a negative number, the obstacle is classified as a dynamic obstacle and the agent must react accordingly.",
            "title": "4.2.11 Discretion for system with dynamic obstacles"
        },
        {
            "location": "/deeplearning/#4212-the-state-table",
            "text": "The state table is structured as a table where all combinations of states and actions are represented as a table point.The state table for the physical model has 144 rows, and three columns that present each of the possible actions(left,right, straightfoward). A section of the condition table is shown in Table 7.",
            "title": "4.2.1.2 the state table"
        },
        {
            "location": "/deeplearning/#422-reward-function",
            "text": "The reward feature determines the agent's immmediate reward after each experience.When implementing the reward function, a distinction is made between good actions that give a positive reward value,and bad actions that give a negative reward value.The reward function used for Q-learning with the table method is inspired by and shown under equation.\\  The reward function, r1 is defined to give a positive reward value when the agent drives straight ahead, while for turns a small negative reward value is given. The negative value is given to prioritize \u4f18\u5148 driving straight ahead.  The parameter r2 gives a positve reward when the total change in the sensor values is positive. That is , the agent moves away from an obstacle. If the agent moves towards an obstacle , it becomes total the change in sensor values is negatie, and the agent recieve a negative reward.  The function r3 gives the agent a negative reward value if it first turns to right and then to the left, or first to the left and then to right one after the other. This feature allows the agent to avoid an indecisive\u4f18\u67d4\u5be1\u65ad\u7684 sequence, where it only swings\u6447\u6446 back and forth.  \uff34he function r gives the agent a reward value equal to the sum of r1,r2 and r3 if it does not collide, while in the event of a collision it gets a negative reward value of -100.  4.2.3 Exploration function\nThe exploration function determines the agent's strategy for choosing actions. It can either explore the state space or take advantage of the experiences it already has. The function ensures an appropriate balance between these strategies (see chapter 2.3).  The simulations for the exploration methods show that softmax gives the best results in the simulated model, and is thus used in the simulation. The parameter T is set equal to 24, and is reduced by 5% of its total value for each individual experiment.In the physical model, epsilon is used because it provides a better overview of how much coincidence there is for the agent to take an arbitrary action, and epsilon is set at a 5% chance of doing a random action.",
            "title": "4.2.2 Reward function"
        },
        {
            "location": "/deeplearning/#424-implementation",
            "text": "The implementation of Q-learning with the table method is based on the same algorithm and reward function in the simulation and for the physical model.The different between the simulation and the physical agent is the exploration function and an additional extension of the state space in the simulation(see the discretization\u79bb\u6563\u5316 chapter).In the simulation, you have the choice between static and dynamic obstacles, and to adapt the agent to dynamic obstacle, the Q table is expanded.",
            "title": "4.2.4 Implementation"
        },
        {
            "location": "/deeplearning/#4241-the-q-learning-algorithm-with-the-table-method",
            "text": "The Q-Learning algorithm with the table method\nAt startup:\n - initialize state space S\n - initialize the action list A\n - initialize the Q table Q(s,a)(empty table) based on the state space S and the action list A  For each attempt:\n 1) Observe initial sensor values\n 2) Discretize the sensor values and set it equal to s(initial mode)\n 3) Select an action a based on the exploration function and perform the action\n 4) Observe the agent's sensor values after the operation\n 5) Discretize the sensor values and set it equal to s'(Next mode)\n 6) Give the agent the reward r based on the reward function\n 7) Update the Q table\n 8) Set s=s'(Now state)\n 9) Keep last action, prev_a =a (Used in the reward function)\n 10) Repeat the operation from point iii)(a new step /experience ) If the agent has not reached its terminal state(collision or maximum number of steps). Start a new attempt if the agent has reached its terminal state(Restart from i).",
            "title": "4.2.4.1 The Q-Learning algorithm with the table method"
        },
        {
            "location": "/deeplearning/#4242-simulated-model",
            "text": "In the simulation, a user interface is created that shows the user the simulation of the agent, two different graphs, buttons for different functions and information panel.The information panel show the user relevant\u76f8\u5173\u7684 values from the simulation.One graph shows an average of the number of steps for every tenth attempt, while the other shows the agent's accumulated reward.One step in the program corresponds to an experienced situation,whether it has been experienced before or is new.A new attempt is initialiated each time the agent has moved the maximum number of steps for the current attempt, or has collided with an obstacle .The maximum number of steps can be changed, but is normally 400-600.The dimensions of the simulation are scaled 1\uff1a10[cm]  The ultrasonic sensors on the physical agent are limited to integer values from 1 to 120cm in the code of the agents microcontrollers.Distance less than 8 cm id defined as a collision. The initial state of the physical agent is an arbitrary position in the environment.The only requirement is that the initial state is not a terminal state(collision).  The commands\n- command 1 : request sensor values\n- command 2: ask the agent to drive straight ahead\n- command 3: ask the agent to turn right\n- command 4: ask the agent to turn left\n- command 5: ask the agent to stop, sensor values <8 cm(collision)",
            "title": "4.2.4.2 Simulated model"
        },
        {
            "location": "/deeplearning/#43-q-learning-with-neural-network-linear-function-approximation",
            "text": "Q-Learning with neural network is based on approximation of one or more linear Q-functions. Unlike Q-table learning.this method stores the Q functions in a neural network. By using this method ,discretization is not necessary, and the entire continuous sensor spectrum can be used. The behavior behind an artifical neuron is described in chapter 2.3.3\\ neural network \\ \\\ninput layer---------------------------------hidden layer-------------------------------------output layer  The figure above shows the neural network as it is implemented in this system. In the network, all the neurous, including the bias from one layer, are connected to each individual neuron in the next layer. The neural network has three layers.Layer One, the input layer, has five neurons (one for each sensor),layer two has fifteen neurons, and layer three has three neurons according to the number of actions in the action list.Layer 1 is the start of the network, and the sensor values are used as input.and these must be in the interval\u95f4\u9694[-1,1]  The weight matrix w(1) is used to weight the link between layer 1 and layer 2.means w(2) for layer 2 and layer 3.The matrices have a dimension of (number of neurons in layer L)x(number of neurons in Layer (l-1) + bias)  w(1) has a dimension of 15x6 and w(2) dimension of 3x16. In the output layer, there are three neurons, and each individual neuron corresponds to a Q function.The number of neurons in the output layer is equal to the number of actions of the agent, and each individual neuron estimates a Q function with respect to an action in the agent's action list Equation(21)\\  These Q function estimate a Q value that is futher used to determine the optimal actions for the agent, where the optimal action for each condition is therefore defined as a = max(Q(s,a)). It is impossible to say which Q function are suitable for the system, since the neural network estimates the Q function based on the agent's experience. In order for the agent to achieve the highest total reward, it must therefore explore the environment long enough for the network to converge to an optimal function.  \\\nThe figure shows the learning process of agent. The method needs initial values, and these are given by sensor values and the network's weight matrices.The weight matrices are initiallized with random values between 0 and 1, but not zero.If the weight matrices are initialized with a value equal to 0,all the neurons will have the same value,which means that the neural network will not be usable. This algorithm uses the agent's sensor values, as opposed to the table method which uses a discretization of values. The exploration function used is the same as for the table method, but the reward function is different. The purpose of this method is to find optimal Q functions by updating the weight machines after each experiment.",
            "title": "4.3 Q-Learning with neural network (Linear function approximation)"
        },
        {
            "location": "/deeplearning/#431-reward-function",
            "text": "Since the agent's state space on the neural network is continuous, an equally elaborate reward function is not required as for the table method.The reward function is defined by the agent receiving a positive reward if it drives straight ahead, and a negative reward in the event of a turn and a collision. The reward value r must be in the interval [-1,1], and the function as defined for the system is shown in equation.\\",
            "title": "4.3.1 Reward function"
        },
        {
            "location": "/deeplearning/#432-feedforward",
            "text": "As described in Chapter 2 Theory, neural network in this project is a forward-connected neural network. The neurons are activated using the function of the hyperbolic forceps\u53cc\u66f2\u7ebf\u94b3 given in equation. \\\nThe activated values become output values in this layer, and are used as input for the next layer.The three neurons in the output layer correspond to the Q funcitons when all data is fed into the network.  \\ \\\nThe weight matrix w has m number of rows equal to the number of neurons,and columns n equal to the number of inputs.The matrix w(2) and the vector y are multiplied by each other and the product is used in the activation function described ealier in the chapter.The elements in both the weight matrix and the input vector are real numbers,and the value of the inputs is in the interval[-1,1].Equation(26-28) shows the Q functions of the three actions.   The calculations in the figures above can be abbreviated\u7f29\u5199 and shown in equation 27 in vector form.\\ \\\nActivation of the hidden layer takes place in the same way as for the output layer, and the input to this layer is the sensor values.\\",
            "title": "4.3.2 Feedforward"
        },
        {
            "location": "/deeplearning/#install-pytroch-gpu",
            "text": "versison:1.3.0  conda install -c anaconda pytorch-gpu    version:1.8.0  conda install -c conda-forge pytorch-gpu",
            "title": "Install Pytroch-Gpu"
        },
        {
            "location": "/deeplearning/#_2",
            "text": "The projected depth values in the original depth maps are float32 and the unit is meter (m). However, we don't want to save float32 because it took too much storage. A common technique is that we can convert it to uint16 by int(depth * 256). This keeps certain degree of accuracy but takes less storage. That's why we need to divide the value by 256.",
            "title": "\u51cf\u5c11\u5185\u5b58\u5360\u7528\u7684\u4e00\u79cd\u65b9\u6cd5"
        },
        {
            "location": "/deeplearning_work_station/",
            "text": "\u4e24\u4e2a\u9f13\u98ce\u5f0fGPU\\\n   SUS GeForce RTX 2080 Ti 11G Turbo Edition GDA\\\n   \n\u6dd8\u5b9d\u94fe\u63a5\n\\\n   \n\n\n\u4e00\u4e2a20\u7ebf\u7a0bcpu\\\n   \u82f1\u7279\u5c14\u9177\u777fi9-9820X Skylake X 10\u68383.3Ghz\\\n   \n\u6dd8\u5b9d\u94fe\u63a5\n\n   \n\n\nx299\u4e3b\u677f\\\n   \u534e\u7855WS X299 SAGE LGA 2066 Intel X299\\\n   \n\u6dd8\u5b9d\u94fe\u63a5\n\n   \n\n\n\u4e3b\u673a\u58f3\\\n   \n\n   \n\u6dd8\u5b9d\u94fe\u63a5\n\n\n\u56fa\u6001\u786c\u76d8\\\n   \u00a0HP EX920 M.2 1TB PCIe NVMe NAND SSD\\\n   \n\u4eac\u4e1c\u94fe\u63a5\n\n   \n\n\n\u673a\u68b0\u786c\u76d8\uff13\uff34\uff22\\\n   \n\n\n128GB\u5185\u5b58\\\n   128GB RAM\uff08\u968f\u673a\u5b58\u53d6\u5b58\u50a8\u5668\uff09\\\n   \n\u6dd8\u5b9d\u94fe\u63a5\n\n   \n\n\n\u7535\u6e90\\\n   1600w PSU\\\n   \n\u6dd8\u5b9d\u94fe\u63a5\n\n   \n\n\n\u51b7\u5374\u5668\\\n    CPU\u98ce\u51b7\\\n    \n\u6dd8\u5b9d\u94fe\u63a5",
            "title": "Deeplearning work station"
        },
        {
            "location": "/github\u7f51\u9875\u4e0d\u663e\u793a\u56fe\u7247/",
            "text": "github\u4e0d\u663e\u793a\u56fe\u7247\n\n\n\u4fee\u6539/etc/hosts\uff0c\u653e\u5165\u4e0b\u9762\u5185\u5bb9\uff1a\n\n\n# GitHub Start \n140.82.113.3    github.com\n140.82.113.3    gist.github.com\n185.199.108.153    assets-cdn.github.com\n199.232.68.133    raw.githubusercontent.com\n199.232.68.133    gist.githubusercontent.com\n199.232.68.133    cloud.githubusercontent.com\n199.232.68.133    camo.githubusercontent.com\n199.232.68.133    avatars0.githubusercontent.com\n199.232.68.133    avatars1.githubusercontent.com\n199.232.68.133    avatars2.githubusercontent.com\n199.232.68.133    avatars3.githubusercontent.com\n151.101.184.133    avatars4.githubusercontent.com\n151.101.184.133    avatars5.githubusercontent.com\n151.101.184.133    avatars6.githubusercontent.com\n151.101.184.133    avatars7.githubusercontent.com\n151.101.184.133    avatars8.githubusercontent.com\n\n # GitHub End\n\n\n\n\n\u5176\u4e2d\u5404\u4e2a\uff49\uff50\u9700\u8981\u4ece\n\u7f51\u7ad9\u67e5\u627e",
            "title": "Github\u7f51\u9875\u4e0d\u663e\u793a\u56fe\u7247"
        },
        {
            "location": "/github\u7f51\u9875\u4e0d\u663e\u793a\u56fe\u7247/#github",
            "text": "\u4fee\u6539/etc/hosts\uff0c\u653e\u5165\u4e0b\u9762\u5185\u5bb9\uff1a  # GitHub Start \n140.82.113.3    github.com\n140.82.113.3    gist.github.com\n185.199.108.153    assets-cdn.github.com\n199.232.68.133    raw.githubusercontent.com\n199.232.68.133    gist.githubusercontent.com\n199.232.68.133    cloud.githubusercontent.com\n199.232.68.133    camo.githubusercontent.com\n199.232.68.133    avatars0.githubusercontent.com\n199.232.68.133    avatars1.githubusercontent.com\n199.232.68.133    avatars2.githubusercontent.com\n199.232.68.133    avatars3.githubusercontent.com\n151.101.184.133    avatars4.githubusercontent.com\n151.101.184.133    avatars5.githubusercontent.com\n151.101.184.133    avatars6.githubusercontent.com\n151.101.184.133    avatars7.githubusercontent.com\n151.101.184.133    avatars8.githubusercontent.com\n\n # GitHub End  \u5176\u4e2d\u5404\u4e2a\uff49\uff50\u9700\u8981\u4ece \u7f51\u7ad9\u67e5\u627e",
            "title": "github\u4e0d\u663e\u793a\u56fe\u7247"
        },
        {
            "location": "/jekyll/",
            "text": "sudo apt -y install ruby ruby-dev\nexport GEM_HOME=$HOME/gems\nexport PATH=$HOME/gems/bin:$PATH\nsource ~/.bashrc\ngem install jekyll bundler \njekyll new my-awesome-site \ncd my-awesome-site \nbundle install \nbundle exec jekyll serve",
            "title": "Jekyll"
        },
        {
            "location": "/lidar_demo\u6559\u7a0b/",
            "text": "lidar_demo\u6559\u7a0b\n\n\n\u7cfb\u7edf\u73af\u5883\n\n\n\n\nros melodic\n\n\n\n\nUser Guide\n\n\n\u7f16\u8bd1\n\n\n\u521b\u5efa\uff52\uff4f\uff53\u73af\u5883\u3000\uff5e/catkin_ws/src\\\n\u628alidar_demo.zip\u3000\u89e3\u538b\u5230\uff5e/catkin_ws/src\u76ee\u5f55\u4e0b\\\n\u9700\u8981\u628a\uff5e/catkin_ws/src/lidar_demo/src/lidar_demo.cpp\u91cc\n\n\nros::Subscriber sub = nh.subscribe<sensor_msgs::PointCloud2>(\"/points_raw\", 1, callback);\n\u4e2d\u7684\"/points_raw\" \u66f4\u6539\u4e3a\u96f7\u8fbe\u6570\u636e\u6e90\uff0c\u5373\u60f3\u8981\u8fdb\u884c\u805a\u7c7b\u8bc6\u522b\u7684\u70b9\u4e91\u7684\uff54\uff4f\uff50\uff49\uff43\n\n\n\n\u5f00\u59cb\u7f16\u8bd1\\\n\ncd ~/catkin_ws && catkin_make\n\n\n\u8fd0\u884c\n\n\nsource devel/setup.bash\n\\\n\u542f\u52a8\\\n\nroslaunch lidar_demo lidar_demo.launch\n\\\n\u5728\uff4c\uff41\uff55\uff4e\uff43\uff48\u6587\u4ef6\u91cc\u6709\uff15\u4e2a\u53c2\u6570\u53ef\u4ee5\u8c03\u8282\n\n\n <!-- \u96f7\u8fbe\u6570\u636e\u6e90-->\n<arg name=\"topic_name\" default=\"/points_raw\"/> \n <!-- rviz \u663e\u793a\u65f6 Fixed Frame \u5904\u7684\u540d\u79f0-->\n<arg name=\"frame_id\" default=\"velodyne\"/> \n <!-- \uff10.01 \u662f1\u5206\u7c73-->\n<arg name=\"Cluster_D\" default=\"0.75\"/>  \n<!-- \u8bc6\u522b\u7269\u4f53\u5305\u542b\u7684\u6700\u5c11\u70b9\u6570\uff0c\u5c11\u4e8e\u6b64\u70b9\u6570\u4e0d\u5212\u5206-->\n<arg name=\"Cluster_Min\" default=\"20\"/>  \n<!-- \u8bc6\u522b\u7269\u4f53\u5305\u542b\u7684\u6700\u591a\u70b9\u6570\uff0c\u591a\u4e8e\u6b64\u70b9\u6570\u4e0d\u5212\u5206 (100000)-->\n<arg name=\"Cluster_Max\" default=\"1000\"/> \n\n\n\nframe_id \u5fc5\u987b\u548c\uff52\uff56\uff49\uff5a\u91cc\u7684\uff46ixed frame \u4e00\u81f4\\\nCluster_D \u662f\u805a\u884c\u534a\u5f84\uff0c0.01\u662f\uff11\u5206\u7c73\\\nCluster_Min\u3000\u8bc6\u522b\u7269\u4f53\u5305\u542b\u7684\u6700\u5c11\u70b9\u6570\uff0c\u5c11\u4e8e\u6b64\u70b9\u6570\u4e0d\u5212\u5206\\\nCluster_Max\u3000\u8bc6\u522b\u7269\u4f53\u5305\u542b\u7684\u6700\u591a\u70b9\u6570\uff0c\u591a\u4e8e\u6b64\u70b9\u6570\u4e0d\u5212\u5206\\\n\uff23luster\u53c2\u6570\u6839\u636e\u6211\u4eec\u8bc6\u522b\u9700\u6c42\u53ef\u8c03",
            "title": "Lidar demo\u6559\u7a0b"
        },
        {
            "location": "/lidar_demo\u6559\u7a0b/#lidar_demo",
            "text": "",
            "title": "lidar_demo\u6559\u7a0b"
        },
        {
            "location": "/lidar_demo\u6559\u7a0b/#_1",
            "text": "ros melodic",
            "title": "\u7cfb\u7edf\u73af\u5883"
        },
        {
            "location": "/lidar_demo\u6559\u7a0b/#user-guide",
            "text": "",
            "title": "User Guide"
        },
        {
            "location": "/lidar_demo\u6559\u7a0b/#_2",
            "text": "\u521b\u5efa\uff52\uff4f\uff53\u73af\u5883\u3000\uff5e/catkin_ws/src\\\n\u628alidar_demo.zip\u3000\u89e3\u538b\u5230\uff5e/catkin_ws/src\u76ee\u5f55\u4e0b\\\n\u9700\u8981\u628a\uff5e/catkin_ws/src/lidar_demo/src/lidar_demo.cpp\u91cc  ros::Subscriber sub = nh.subscribe<sensor_msgs::PointCloud2>(\"/points_raw\", 1, callback);\n\u4e2d\u7684\"/points_raw\" \u66f4\u6539\u4e3a\u96f7\u8fbe\u6570\u636e\u6e90\uff0c\u5373\u60f3\u8981\u8fdb\u884c\u805a\u7c7b\u8bc6\u522b\u7684\u70b9\u4e91\u7684\uff54\uff4f\uff50\uff49\uff43  \u5f00\u59cb\u7f16\u8bd1\\ cd ~/catkin_ws && catkin_make",
            "title": "\u7f16\u8bd1"
        },
        {
            "location": "/lidar_demo\u6559\u7a0b/#_3",
            "text": "source devel/setup.bash \\\n\u542f\u52a8\\ roslaunch lidar_demo lidar_demo.launch \\\n\u5728\uff4c\uff41\uff55\uff4e\uff43\uff48\u6587\u4ef6\u91cc\u6709\uff15\u4e2a\u53c2\u6570\u53ef\u4ee5\u8c03\u8282   <!-- \u96f7\u8fbe\u6570\u636e\u6e90-->\n<arg name=\"topic_name\" default=\"/points_raw\"/> \n <!-- rviz \u663e\u793a\u65f6 Fixed Frame \u5904\u7684\u540d\u79f0-->\n<arg name=\"frame_id\" default=\"velodyne\"/> \n <!-- \uff10.01 \u662f1\u5206\u7c73-->\n<arg name=\"Cluster_D\" default=\"0.75\"/>  \n<!-- \u8bc6\u522b\u7269\u4f53\u5305\u542b\u7684\u6700\u5c11\u70b9\u6570\uff0c\u5c11\u4e8e\u6b64\u70b9\u6570\u4e0d\u5212\u5206-->\n<arg name=\"Cluster_Min\" default=\"20\"/>  \n<!-- \u8bc6\u522b\u7269\u4f53\u5305\u542b\u7684\u6700\u591a\u70b9\u6570\uff0c\u591a\u4e8e\u6b64\u70b9\u6570\u4e0d\u5212\u5206 (100000)-->\n<arg name=\"Cluster_Max\" default=\"1000\"/>   frame_id \u5fc5\u987b\u548c\uff52\uff56\uff49\uff5a\u91cc\u7684\uff46ixed frame \u4e00\u81f4\\\nCluster_D \u662f\u805a\u884c\u534a\u5f84\uff0c0.01\u662f\uff11\u5206\u7c73\\\nCluster_Min\u3000\u8bc6\u522b\u7269\u4f53\u5305\u542b\u7684\u6700\u5c11\u70b9\u6570\uff0c\u5c11\u4e8e\u6b64\u70b9\u6570\u4e0d\u5212\u5206\\\nCluster_Max\u3000\u8bc6\u522b\u7269\u4f53\u5305\u542b\u7684\u6700\u591a\u70b9\u6570\uff0c\u591a\u4e8e\u6b64\u70b9\u6570\u4e0d\u5212\u5206\\\n\uff23luster\u53c2\u6570\u6839\u636e\u6211\u4eec\u8bc6\u522b\u9700\u6c42\u53ef\u8c03",
            "title": "\u8fd0\u884c"
        },
        {
            "location": "/make/",
            "text": "\u6dfb\u52a0 pkg-config-path\n\n\nexport PKG_CONFIG_PATH='/home/promote/NeDisk/third_part/opencv-3.4.2/install_gcc7/lib/pkgconfig':$PKG_CONFIG_PATH \n\n\n\n\n\u6dfb\u52a0 include path\n\n\nCOMMON= -Iinclude/ -I3rdparty/stb/include -I/home/promote/NeDisk/third_part/opencv-3.4.2/install_gcc7/include \n\n\n\n\u6dfb\u52a0 lib path\n\n\nexport LD_LIBRARY_PATH=/home/promote/NeDisk/third_part/opencv-3.4.2/install_gcc7/lib:$LD_LIBRARY_PATH\n\n\n\nmakefile \u627e\u4e0d\u5230 -lxxxx\n\n\nLDFLAGS+= -L/home/promote/NeDisk/anaconda3/pkgs/cudnn-7.6.5-cuda10.0_0/lib -lcudnn",
            "title": "Make"
        },
        {
            "location": "/make/#pkg-config-path",
            "text": "export PKG_CONFIG_PATH='/home/promote/NeDisk/third_part/opencv-3.4.2/install_gcc7/lib/pkgconfig':$PKG_CONFIG_PATH",
            "title": "\u6dfb\u52a0 pkg-config-path"
        },
        {
            "location": "/make/#include-path",
            "text": "COMMON= -Iinclude/ -I3rdparty/stb/include -I/home/promote/NeDisk/third_part/opencv-3.4.2/install_gcc7/include",
            "title": "\u6dfb\u52a0 include path"
        },
        {
            "location": "/make/#lib-path",
            "text": "export LD_LIBRARY_PATH=/home/promote/NeDisk/third_part/opencv-3.4.2/install_gcc7/lib:$LD_LIBRARY_PATH",
            "title": "\u6dfb\u52a0 lib path"
        },
        {
            "location": "/make/#makefile-lxxxx",
            "text": "LDFLAGS+= -L/home/promote/NeDisk/anaconda3/pkgs/cudnn-7.6.5-cuda10.0_0/lib -lcudnn",
            "title": "makefile \u627e\u4e0d\u5230 -lxxxx"
        },
        {
            "location": "/python/",
            "text": "conda The following packages are not available from current channels:\n\n\nconda config --append channels conda-forge\n\nIt tells conda to also look on the conda-forge channel when you search for packages.\n\n\nconda \u521b\u5efa\u73af\u5883\n\n\nconda create -n xception_net python==3.6.5 numpy==1.17.4 scipy==1.3.3 h5py==2.10.0 Keras==2.3.1 tensorflow-gpu==1.15.0\n\n\nAs the comment at the top indicates, the output of\n\nconda list -e > requirements.txt\n\n\ncan be used to create a conda virtual environment with\n\nconda create --name <env> --file requirements.txt\n\n\ncudnn_status_internal_error tensorflow\n\n\nYou can try Allowing GPU memory growth with:\n\n\nimport tensorflow as tf\ngpu = tf.config.experimental.list_physical_devices('GPU')\ntf.config.experimental.set_memory_growth(gpu[0], True)\n\n\n\nvscode activate conda env\n\n\n{\n    \"ros.distro\": \"melodic\",\n    \"python.autoComplete.extraPaths\": [\n        \"/home/pmjd/Disk/anaconda3/envs/wjj/lib/python3.6/site-packages\"\n    ],\n    \"python.terminal.activateEnvInCurrentTerminal\": true,\n    \"python.condaPath\": \"/home/pmjd/Disk/anaconda3/bin/conda\",\n    \"python.defaultInterpreterPath\": \"/home/pmjd/Disk/anaconda3/envs/wjj/bin/python\"\n}\n\n\n\nNameError: name 'xrange' is not defined\n\n\ntry:\n    # Python 2\n    xrange\nexcept NameError:\n    # Python 3, xrange is now named range\n    xrange = range\n\n\n\nTensorFlow ValueError: Cannot feed value of shape (64, 64, 3) for Tensor u'Placeholder:0', which has shape '(?, 64, 64, 3)'\n\n\nimage has a shape of (64,64,3).\nYour input placeholder _x have a shape of (?, 64,64,3).\nThe problem is that you're feeding the placeholder with a value of a different shape.\nYou have to feed it with a value of (1, 64, 64, 3) = a batch of 1 image.\nJust reshape your image value to a batch with size one.\n\nnp.expand_dims(img, axis=0)\n\n\nopencv conda\n\n\npip install opencv-python\n\n\nopencv is not compatible with python 3. I had to install opencv3 for python 3. The marked answer in how could we install opencv on anaconda? explains how to install opencv(3) for anaconda:\n\n\nRun the following command:\n\n\nconda install -c https://conda.binstar.org/menpo opencv\n\n\nI realized that opencv3 is also available now, run the following command:\n\n\nconda install -c https://conda.binstar.org/menpo opencv3\n\n\nEdit on Aug 18, 2016: You may like to add the \"menpo\" channel permanently by:\n\n\nconda config --add channels menpo\n\n\nAnd then opencv can be installed by:\n\n\nconda install opencv (or opencv3)\n\n\nEdit on Aug 14, 2017: \"clinicalgraphics\" channel provides relatively newer vtk version for very recent python3\n\n\nconda install -c clinicalgraphics vtk\n\n\nEdit on April 16, 2020 (based on @AMC's comment): OpenCV can be installed through conda-forge (details see here)\n\n\nconda install -c conda-forge opencv\n\n\npointcloud2 to array\n\n\ndef pointcloud2_to_array(cloud_msg, squeeze=True):\n    dtype_list = fields_to_dtype(cloud_msg.fields, cloud_msg.point_step)\n    cloud_arr = np.fromstring(cloud_msg.data, dtype_list)\n\n    cloud_arr = cloud_arr[\n        [fname for fname, _type in dtype_list if not (fname[:len(DUMMY_FIELD_PREFIX)] == DUMMY_FIELD_PREFIX)]]\n\n    if squeeze and cloud_msg.height == 1:\n        return np.reshape(cloud_arr, (cloud_msg.width,))\n    else:\n        return np.reshape(cloud_arr, (cloud_msg.height, cloud_msg.width)) \n\n\n\nread pcd to array\n\n\nimport numpy as np \nimport open3d as o3d\n\npcd = o3d.io.read_point_cloud(\"pointcloud_path.pcd\")\nout_arr = np.asarray(pcd.points)  \nprint (\"output array from input list : \", out_arr)  \n\n\n\nkitti \u6570\u636e\u96c6\u4e0b\u8f7d\n\n\n\n\n\u56fe\u7247\u4e0b\u8f7d\n\n\n\n\n\n\nhttps://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_image_2.zip\n\n\n\n\n\n\n\u70b9\u4e91\u4e0b\u8f7d\n\n\n\n\n\n\nhttps://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_velodyne.zip\n\n\n\n\n\n\n\u6807\u7b7e\u4e0b\u8f7d\n\n\n\n\n\n\nhttps://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_label_2.zip\n\n\n\n\n\n\n\u77eb\u6b63\u6587\u4ef6\u4e0b\u8f7d\n\n\n\n\n\n\nhttps://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_calib.zip\n\n\n\n\npython \"//\" operator\n\n\nIn python 2.x\n\n\n>>> 10/3\n3\n>>> # To get a floating point number from integer division:\n>>> 10.0/3\n3.3333333333333335\n>>> float(10)/3\n3.3333333333333335\n\n\n\nIn python 3.x\n\n\n>>> 10/3\n3.3333333333333335\n>>> 10//3\n3\n\n\n\npython super()\n\n\n\u5185\u7f6e\u7684super()\u8fd4\u56de\u4e00\u4e2a\u4ee3\u7406\u5bf9\u8c61\uff08\u8d85\u7c7b\u7684\u4e34\u65f6\u5bf9\u8c61),\u8be5\u4ee3\u7406\u5bf9\u8c61\u5141\u8bb8\u6211\u4eec\u8bbf\u95ee\u57fa\u7c7b\u7684\u65b9\u6cd5\u3002\u5728python\u4e2d\uff0csuper()\u6709\u4e24\u4e2a\u4e3b\u8981\u7528\u4f8b\uff1a\n\n\n\n\n\u8ba9\u6211\u4eec\u907f\u514d\u663e\u793a\u4f7f\u7528\u57fa\u7c7b\u540d\u79f0\n\n\n\u5904\u7406\u591a\u91cd\u7ee7\u627f\\\n\n\n\n\n\u793a\u4f8b\uff11\uff1a\u5177\u6709\u5355\u7ee7\u627f\u7684super()\n\n\n\u5728\u5355\u7ee7\u627f\u7684\u60c5\u51b5\u4e0b\uff0c\u5b83\u5141\u8bb8\u6211\u4eec\u901a\u8fc7\u5f15\u7528\u57fa\u7c7bsuper()\n\n\nclass Mammal(object):\n  def __init__(self, mammalName):\n    print(mammalName, 'is a warm-blooded animal.')\n\nclass Dog(Mammal):\n  def __init__(self):\n    print('Dog has four legs.')\n    super().__init__('Dog')\n\nd1 = Dog()\n\n\n\n\u8f93\u51fa\n\n\nDog has four legs.\nDog is a warm-blooded animal.\n\n\n\n\u8be5super()\u5185\u5efa\u8fd4\u56de\u4e00\u4e2a\u4ee3\u7406\u5bf9\u8c61\uff0c\u66ff\u4ee3\u5bf9\u8c61\uff0c\u53ef\u4ee5\u901a\u8fc7\u59d4\u6258\u8c03\u7528\u57fa\u7c7b\u7684\u65b9\u6cd5\uff0c\u8fd9\u79f0\u4e3a\u201c\u95f4\u63a5\"(\u4f7f\u7528\u5f15\u7528\u57fa\u7840\u5bf9\u8c61\u7684\u80fd\u529bsuper())\n\u7531\u4e8e\u95f4\u63a5\u662f\u5728\u8fd0\u884c\u65f6\u8ba1\u7b97\u7684\uff0c\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u5728\u4e0d\u540c\u7684\u65f6\u95f4\u4f7f\u7528\u4e0d\u540c\u7684\u57fa\u7c7b\uff08\u5982\u679c\u9700\u8981\uff09\n\n\n\u793a\u4f8b\uff12\uff1a\u5177\u6709\u591a\u91cd\u7ee7\u627f\u7684super()\n\n\nclass Animal:\n  def __init__(self, Animal):\n    print(Animal, 'is an animal.');\n\nclass Mammal(Animal):\n  def __init__(self, mammalName):\n    print(mammalName, 'is a warm-blooded animal.')\n    super().__init__(mammalName)\n\nclass NonWingedMammal(Mammal):\n  def __init__(self, NonWingedMammal):\n    print(NonWingedMammal, \"can't fly.\")\n    super().__init__(NonWingedMammal)\n\nclass NonMarineMammal(Mammal):\n  def __init__(self, NonMarineMammal):\n    print(NonMarineMammal, \"can't swim.\")\n    super().__init__(NonMarineMammal)\n\nclass Dog(NonMarineMammal, NonWingedMammal):\n  def __init__(self):\n    print('Dog has 4 legs.');\n    super().__init__('Dog')\n\nd = Dog()\nprint('')\nbat = NonMarineMammal('Bat')\n\n\n\n\u8f93\u51fa\n\n\nDog has 4 legs.\nDog can't swim.\nDog can't flay.\nDog is a warm-blooded animal.\nDog is an animal.\n\nBat can't swim.\nBat is a warm-blooded animal.\nBat is an animal.\n\n\n\nMethod Resolution Order\u65b9\u6cd5\u89e3\u6790\u987a\u5e8f (MRO)\n\n\n>>> Dog.__mro__\n(<class 'Dog'>, \n<class 'NonMarineMammal'>, \n<class 'NonWingedMammal'>, \n<class 'Mammal'>, \n<class 'Animal'>, \n<class 'object'>)\n\n\n\nSplit method in python is outputing an index error\n\n\none of your lines must be empty\n\n\ndeque in python \uff50\uff59\uff54\uff48\uff4f\uff4e\u4e2d\u7684\u53cc\u7aef\u961f\u5217\n\n\n# Python code to demonstrate deque \nfrom collections import deque \n# Declaring deque \nqueue = deque(['name','age','DOB']) \nprint(queue) \n=========================================================\nOutput:\ndeque(['name', 'age', 'DOB'])\n\n\n\n\n\nappend() :- This function is used to insert the value in its argument to the right end of deque.\n\n\nappendleft() :- This function is used to insert the value in its argument to the left end of deque.\n\n\npop() :- This function is used to delete an argument from the right end of deque.\n\n\npopleft() :- This function is used to delete an argument from the left end of deque.\n\n\nindex(ele, beg, end) :- This function returns the first index of the value mentioned in arguments, starting searching from beg till end index.\n\n\ninsert(i, a) :- This function inserts the value mentioned in arguments(a) at index(i) specified in arguments.\n\n\nremove() :- This function removes the first occurrence of value mentioned in arguments.\n\n\nextend(iterable) :- This function is used to add multiple values at the right end of deque. The argument passed is an iterable.\n\n\nextendleft(iterable) :- This function is used to add multiple values at the left end of deque. The argument passed is an iterable. Order is reversed as a result of left appends.\n\n\nreverse() :- This function is used to reverse order of deque elements.\n\n\nrotate() :- This function rotates the deque by the number specified in arguments. If the number specified is negative, rotation occurs to left. Else rotation is to right.\n\n\n\n\npython random.sample()\n\n\n\n\npython pip \u4e0d\u80fd\u7528\n\n\nPIP_NO_CACHE_DIR=off pip install gym\n\n\nnum.linspace() in Python\n\n\nimport numpy as np\nprint(\"B\\n\", np.linspace(2.0, 3.0, num=5, retstep=True),\"\\n\")\n\nx = np.linspace(0, 2, 10)\nprint(\"A\\n\", np.sin(x))\n\n\n\nOutput\n\n\nB\n (array([ 2.  ,  2.25,  2.5 ,  2.75,  3.  ]), 0.25)\n\nA\n [ 0.          0.22039774  0.42995636  0.6183698   0.77637192  0.8961922\n  0.9719379   0.99988386  0.9786557   0.90929743]\n\n\n\npip \u4e0b\u8f7d\u63d0\u901f\n\n\n\u56fd\u5185\u6e90\uff1a\n\u65b0\u7248ubuntu\u8981\u6c42\u4f7f\u7528https\u6e90\uff0c\u8981\u6ce8\u610f\u3002\n\u6e05\u534e\uff1ahttps://pypi.tuna.tsinghua.edu.cn/simple\n\u963f\u91cc\u4e91\uff1ahttp://mirrors.aliyun.com/pypi/simple\n\u4e2d\u56fd\u79d1\u6280\u5927\u5b66 https://pypi.mirrors.ustc.edu.cn/simple\n\u534e\u4e2d\u7406\u5de5\u5927\u5b66\uff1ahttp://pypi.hustunique.com\n\u5c71\u4e1c\u7406\u5de5\u5927\u5b66\uff1ahttp://pypi.sdutlinux.org\n\u8c46\u74e3\uff1ahttp://pypi.douban.com/simple\n\u4e34\u65f6\u4f7f\u7528\uff1a\n\u53ef\u4ee5\u5728\u4f7f\u7528pip\u7684\u65f6\u5019\u52a0\u53c2\u6570\npip install -i https://pypi.tuna.tsinghua.edu.cn/simple\n\n\nPIL.Image\u8f6c\u6362\u6210OpenCV\u683c\u5f0f\n\n\nimport cv2\nfrom PIL import Image\nimport numpy\n\nimage = Image.open(\"plane.jpg\")\nimage.show()\nimg = cv2.cvtColor(numpy.asarray(image),cv2.COLOR_RGB2BGR)\ncv2.imshow(\"OpenCV\",img)\ncv2.waitKey()\n\n\n\nOpenCV\u8f6c\u6362\u6210PIL.Image\u683c\u5f0f\n\n\nimport cv2\nfrom PIL import Image\nimport numpy\n\nimg = cv2.imread(\"plane.jpg\")\ncv2.imshow(\"OpenCV\",img)\nimage = Image.fromarray(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\nimage.show()\ncv2.waitKey()\n\n\n\n\u6dfb\u52a0python\u8def\u5f84\n\n\nxport PYTHONPATH=$PYTHONPATH:/home/dell/Deep/DeepSpeech/training\n\n\n\nNumpy\n\n\nnumpy.amax()\n\n\n\n\nnumpy.argmax\n\n\nnumpy.argmax(a, axis=None, out=None)[source]Returns the indices of the maximum values along an axis.\nparameters\n\\\n\n\n\n\na: array_like\n,Input array.\n\n\naxis:int, optional\n, By default, the index is into the flattened array, otherwise along the specified axis.\n\n\noutarray, optional\n, If provided, the result will be inserted into this array. It should be of the appropriate shape and dtype.\n\n\n\n\nReturns\n\n\nindex_array:ndarray of ints\n\nArray of indices into the array. It has the same shape as a.shape with the dimension along axis removed.\n\n\n\n\n\n\nnp.squeeze\n\n\nFunction\n\n\nRemove dimensions of size 1 from ndarray\nYou can use \nnumpy.squeeze()\n to remove all dimensions of size 1 from the NumPy array \nndarray. squeeze()\n is also provided as a method of \nndarray\n.\n\n\nimport numpy as np\n\na = np.arange(6).reshape(1, 2, 1, 3, 1)\nprint(a)\n# [[[[[0]\n#     [1]\n#     [2]]]\n# \n# \n#   [[[3]\n#     [4]\n#     [5]]]]]\n\nprint(a.shape)\n# (1, 2, 1, 3, 1)\n\na_s = np.squeeze(a)\nprint(a_s)\n# [[0 1 2]\n#  [3 4 5]]\n\nprint(a_s.shape)\n# (2, 3)\n\n\n\nBy default, all dimensions with size \n1\n are removed, as in the example above.\n\n\nYou can specify the index of the dimension to be removed in the second argument \naxis\n of \nnumpy.squeeze()\n. Dimensions that are not the specified index are not removed.\n\n\nprint(a.shape)\n# (1, 2, 1, 3, 1)\nprint(np.squeeze(a, 0))\n# [[[[0]\n#    [1]\n#    [2]]]\n# \n# \n#  [[[3]\n#    [4]\n#    [5]]]]\n\nprint(np.squeeze(a, 0).shape)\n# (2, 1, 3, 1)\n\n\n\nAn error will occur if you specify a dimension whose size is not \n1\n or a dimension that does not exist.\n\n\naxis can also be specified as a negative value. \n-1\ncorresponds to the last dimension and can be specified by the position from the back.\n\n\nprint(np.squeeze(a, -1))\n# [[[[0 1 2]]\n# \n#   [[3 4 5]]]]\n\nprint(np.squeeze(a, -1).shape)\n# (1, 2, 1, 3)\n\nprint(np.squeeze(a, -3))\n# [[[[0]\n#    [1]\n#    [2]]\n# \n#   [[3]\n#    [4]\n#    [5]]]]\n\nprint(np.squeeze(a, -3).shape)\n# (1, 2, 3, 1)\n\n\n\n\u63d0\u53d6\u6587\u4ef6\u548c\u4e0d\u540c\u683c\u5f0f\u6587\u4ef6\n\n\n  # Get all filenames in the dataroot\n  filenames = os.listdir(dataset_root)\n  filenames = [_ for _ in filenames if _.endswith(\".h5\")]#\u63d0\u53d6h5\u6587\u4ef6\n\n\n\n\u6839\u636e\u6587\u4ef6\u540d\u7ee7\u7eed\u63d0\u53d6\u6587\u4ef6\n\n\n  ver1_ori = [\"front\", \"back\"]\n  filenames = [_ for _ in filenames if os.path.splitext(_)[0].split(\"_\")[-1] in ver1_ori]\n\n\n\n\u786e\u4fdd\u6587\u4ef6\u540d\u4e0d\u4e3a \n\u7a7a\n\n\n  assert len(filenames) > 0\n\n\n\n\u8fd4\u56de\u5b8c\u6574\u8def\u5f84\n\n\n  # Add to full data path\n  filenames_original = [os.path.join(dataset_root, _) for _ in filenames]\n\n\n\n\u67e5\u770b\u662f\u5426\u5c5e\u4e8e\u5df2\u77e5\u7c7b\n\n\n  # Check modalities\n  avail_modality = [\"rgb\", \"rgbd\"]\n  if not modality in avail_modality:\n      raise ValueError(\"[Error] Unsupported modality. Consider \", avail_modality)\n\n\n\n.pkl file\n\n\nYour \npkl\n file is, in fact, a serialized pickle file, which means it has been dumped using Python's \npickle\n module.\n\n\nimport pickle\nwith open('serialized.pkl', 'rb') as f:\n    data = pickle.load(f)\n\n\n\nNote \ngzip\n is only needed if the file is compressed:\n\n\nimport gzip\nimport pickle\nwith gzip.open('mnist.pkl.gz', 'rb') as f:\n    train_set, valid_set, test_set = pickle.load(f)\n\n\n\nWhere each set can be further divided \n(i.e. for the training set)\n:\n\n\ntrain_x, train_y = train_set\n\n\n\nIf you want to display the dataset\n\n\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nplt.imshow(train_x[0].reshape((28, 28)), cmap=cm.Greys_r)\nplt.show()\n\n\n\n\u5224\u65ad\u6587\u4ef6\u662f\u5426\u5b58\u5728\n\n\nif not os.path.exists(datapoint):\n  raise ValueError(\"[Error] File does not exist.\")\n\n\n\nh5 file Open\n\n\nIn order to open a HDF5 file with the \nh5py\n module you can use \nh5py.File(filename)\n.\n\n\nimport h5py\nfilename = \"vstoxx_data_31032014.h5\"\nh5 = h5py.File(filename,'r')\nfutures_data = h5['futures_data']  # VSTOXX futures data\noptions_data = h5['options_data']  # VSTOXX call option data\nh5.close()\n\n\n\nnumpy.random.uniform\n\n\nwe can get the random samples from uniform distribution and returns the random samples as numpy array by using this method.\n\n\n# import numpy\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Using uniform() method\ngfg = np.random.uniform(2.1, 5.5, 1000)\nplt.hist(gfg, bins = 100, density = True)\nplt.show()\n\n\n\nisinstance()\n\n\nThe \nisinstance()\n function returns \nTrue\n if the specified object is of the specified type, otherwise \nFalse\n.\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nobject\n\n\nRequired, An Object\n\n\n\n\n\n\ntype\n\n\nA type or class, or a tuple of types and/or classes\n\n\n\n\n\n\n\n\n\u5224\u65ad\u56fe\u50cf\u683c\u5f0f\n\n\ndef _is_pil_image(img):\n    if accimage is not None:\n        return isinstance(img, (Image.Image, accimage.Image))\n    else:\n        return isinstance(img, Image.Image)\n\n\n\nPIL\u56fe\u50cf\u4eae\u5ea6\u8c03\u6574\n\n\ndef adjust_brightness(img, brightness_factor):\n    \"\"\"Adjust brightness of an Image.\n    Args:\n        img (PIL Image): PIL Image to be adjusted.\n        brightness_factor (float):  How much to adjust the brightness. Can be\n            any non negative number. 0 gives a black image, 1 gives the\n            original image while 2 increases the brightness by a factor of 2.\n    Returns:\n        PIL Image: Brightness adjusted image.\n    \"\"\"\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    enhancer = ImageEnhance.Brightness(img)\n    img = enhancer.enhance(brightness_factor)\n    return img\n\n\n\nContrast adjusted image\n\n\ndef adjust_contrast(img, contrast_factor):\n    \"\"\"Adjust contrast of an Image.\n    Args:\n        img (PIL Image): PIL Image to be adjusted.\n        contrast_factor (float): How much to adjust the contrast. Can be any\n            non negative number. 0 gives a solid gray image, 1 gives the\n            original image while 2 increases the contrast by a factor of 2.\n    Returns:\n        PIL Image: Contrast adjusted image.\n    \"\"\"\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    enhancer = ImageEnhance.Contrast(img)\n    img = enhancer.enhance(contrast_factor)\n    return img\n\n\n\nSaturation adjusted image\n\n\ndef adjust_saturation(img, saturation_factor):\n    \"\"\"Adjust color saturation of an image.\n    Args:\n        img (PIL Image): PIL Image to be adjusted.\n        saturation_factor (float):  How much to adjust the saturation. 0 will\n            give a black and white image, 1 will give the original image while\n            2 will enhance the saturation by a factor of 2.\n    Returns:\n        PIL Image: Saturation adjusted image.\n    \"\"\"\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    enhancer = ImageEnhance.Color(img)\n    img = enhancer.enhance(saturation_factor)\n    return img\n\n\n\nHue adjusted image\n\n\ndef adjust_hue(img, hue_factor):\n  \"\"\"Adjust hue of an image.\n  The image hue is adjusted by converting the image to HSV and\n  cyclically shifting the intensities in the hue channel (H).\n  The image is then converted back to original image mode.\n  `hue_factor` is the amount of shift in H channel and must be in the\n  interval `[-0.5, 0.5]`.\n  See https://en.wikipedia.org/wiki/Hue for more details on Hue.\n  Args:\n      img (PIL Image): PIL Image to be adjusted.\n      hue_factor (float):  How much to shift the hue channel. Should be in\n          [-0.5, 0.5]. 0.5 and -0.5 give complete reversal of hue channel in\n          HSV space in positive and negative direction respectively.\n          0 means no shift. Therefore, both -0.5 and 0.5 will give an image\n          with complementary colors while 0 gives the original image.\n  Returns:\n      PIL Image: Hue adjusted image.\n  \"\"\"\n  if not(-0.5 <= hue_factor <= 0.5):\n      raise ValueError('hue_factor is not in [-0.5, 0.5].'.format(hue_factor))\n\n  if not _is_pil_image(img):\n      raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n  input_mode = img.mode\n  if input_mode in {'L', '1', 'I', 'F'}:\n      return img\n\n  h, s, v = img.convert('HSV').split()\n\n  np_h = np.array(h, dtype=np.uint8)\n  # uint8 addition take cares of rotation across boundaries\n  with np.errstate(over='ignore'):\n      np_h += np.uint8(hue_factor * 255)\n  h = Image.fromarray(np_h, 'L')\n\n  img = Image.merge('HSV', (h, s, v)).convert(input_mode)\n  return img\n\n\n\nAdjust gamma on IMG\n\n\ndef adjust_gamma(img, gamma, gain=1):\n  \"\"\"Perform gamma correction on an image.\n  Also known as Power Law Transform. Intensities in RGB mode are adjusted\n  based on the following equation:\n      I_out = 255 * gain * ((I_in / 255) ** gamma)\n  See https://en.wikipedia.org/wiki/Gamma_correction for more details.\n  Args:\n      img (PIL Image): PIL Image to be adjusted.\n      gamma (float): Non negative real number. gamma larger than 1 make the\n          shadows darker, while gamma smaller than 1 make dark regions\n          lighter.\n      gain (float): The constant multiplier.\n  \"\"\"\n  if not _is_pil_image(img):\n      raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n  if gamma < 0:\n      raise ValueError('Gamma should be a non-negative real number')\n\n  input_mode = img.mode\n  img = img.convert('RGB')\n\n  np_img = np.array(img, dtype=np.float32)\n  np_img = 255 * gain * ((np_img / 255) ** gamma)\n  np_img = np.uint8(np.clip(np_img, 0, 255))\n\n  img = Image.fromarray(np_img, 'RGB').convert(input_mode)\n  return img\n\n\n\nnp.clip(img, 0, 255)\n\n\nGiven an interval, values outside the interval are clipped to the interval edges. For example, if an interval of [0, 1] is specified, values smaller than 0 become 0, and values larger than 1 become 1.\n\n\nConvert np.ndarray img to tensor\n\n\nclass ToTensor(object):\n  \"\"\"Convert a ``numpy.ndarray`` to tensor.\n  Converts a numpy.ndarray (H x W x C) to a torch.FloatTensor of shape (C x H x W).\n  \"\"\"\n\n  def __call__(self, img):\n      \"\"\"Convert a ``numpy.ndarray`` to tensor.\n      Args:\n          img (numpy.ndarray): Image to be converted to tensor.\n      Returns:\n          Tensor: Converted image.\n      \"\"\"\n      if not(_is_numpy_image(img)):\n          raise TypeError('img should be ndarray. Got {}'.format(type(img)))\n\n      if isinstance(img, np.ndarray):\n          # handle numpy array\n          if img.ndim == 3:\n              img = torch.from_numpy(img.transpose((2, 0, 1)).copy())\n          elif img.ndim == 2:\n              img = torch.from_numpy(img.copy())\n          else:\n              raise RuntimeError('img should be ndarray with 2 or 3 dimensions. Got {}'.format(img.ndim))\n\n          # backward compatibility\n          # return img.float().div(255)\n          return img.float()\n\n\n\nmath.floor\n\n\nRound numbers down to the nearest integer\n\n\n#Import math library\nimport math\n# Round numbers down to the nearest integer\nprint(math.floor(0.6))\nprint(math.floor(1.4))\nprint(math.floor(5.3))\nprint(math.floor(-5.3))\nprint(math.floor(22.6))\nprint(math.floor(10.0))\n\n# 0\n# 1\n# 5\n# -6\n# 22\n# 10\n\n\n\nunsqueeze\n\n\nIf you look at the shape of the array before and after, you see that before it was (4,) and after it is (1, 4) \n(when second parameter is 0)\n and (4, 1) \n(when second parameter is 1)\n. So a 1 was inserted in the shape of the array at axis 0 or 1, depending on the value of the second parameter.\n\n\nThat is opposite of \nnp.squeeze()\n (nomenclature borrowed from MATLAB) which removes axes of size 1 (singletons).\n\n\n>>> x = torch.tensor([1, 2, 3, 4])\n>>> torch.unsqueeze(x, 0)\ntensor([[ 1,  2,  3,  4]])\n>>> torch.unsqueeze(x, 1)\ntensor([[ 1],\n        [ 2],\n        [ 3],\n        [ 4]])\n\n\n\nnp.transpose\n\n\nThe transpose() function is used to permute\u7f6e\u6362 the dimensions of an array.\n\n\nimport numpy as np\na = np.ones((2,3,4))\nprint(a.shape)\nprint(\"--------------------------------\")\nprint(np.transpose(a,(2,0,1)).shape)\nprint(\"--------------------------------\")\nprint(np.transpose(a,(2,1,0)).shape)\n\n# (2, 3, 4)\n# --------------------------------\n# (4, 2, 3)\n# --------------------------------\n# (4, 3, 2)\n# transpose\u91cc\u7684 0\u4ee3\u8868\u7b2c\u4e00\u7ef4\u6848\u4f8b\u4e2d\u662f2\uff0c 1\u4ee3\u8868\u7b2c\u4e8c\u7ef4\u6848\u4f8b\u4e2d\u662f3\uff0c 2\u4ee3\u8868\u7b2c\u4e09\u7ef4\u6848\u4f8b\u4e2d\u662f4.\n\n\n\nWhat does axis=0 do in Numpy's sum function\n\n\n\n\na = np.array([[1, 2, 3], [4, 5, 6]])\nnp.sum(a, axis = 0)\n# result array([5, 7, 9])\n\na = np.array([1, 2, 3])\nnp.sum(a, axis = 0)\n#result 6\n\n\n\nPad the points to 512\n\n\n  num_points = points.shape[0]\n  target_size = 512\n  output_points = np.repeat(points[-1, :][None, ...], target_size, axis=0)\n  output_points[:num_points, :] = points\n  output_labels = np.repeat(labels[-1, :][None, ...], target_size, axis=0)\n  output_labels[:num_points, :] = labels\n\n\n\nnumpy.random.shuffle()\n\n\nWith the help of \nnumpy.random.shuffle()\n method, we can get the random positioning of different integer values in the numpy array or we can say that all the values in an array will be shuffled randomly.\n\n\nOrderedDict\n\n\nAn OrderedDict is a dictionary subclass that remembers the \norder\n that keys were first \ninserted\n.\n\n\nImage to Array & Array to Image\n\n\nfrom PIL import Image\nimport numpy as np\nim = Image.open('1.jpg')\nim2arr = np.array(im) # im2arr.shape: height x width x channel\narr2im = Image.fromarray(im2arr)\n\n\n\npyton thread\n\n\nimport threading\nclass SubscribeThread(threading.Thread):\n    def __init__(self):\n        threading.Thread.__init__(self)\n\n    def callback(self,data):\n        rospy.loginfo(rospy.get_caller_id() + \"I heard %s\", data.data)\n\n    def run(self):\n        rospy.Subscriber(\"image_button\", String, self.callback)\n        rospy.spin()\n\nsubscribe_thread = SubscribeThread()\nsubscribe_thread.setDaemon(True)\nsubscribe_thread.start()\n\n\n\npython wheel arm\n\n\nwheel",
            "title": "Python"
        },
        {
            "location": "/python/#conda-the-following-packages-are-not-available-from-current-channels",
            "text": "conda config --append channels conda-forge \nIt tells conda to also look on the conda-forge channel when you search for packages.",
            "title": "conda The following packages are not available from current channels:"
        },
        {
            "location": "/python/#conda",
            "text": "conda create -n xception_net python==3.6.5 numpy==1.17.4 scipy==1.3.3 h5py==2.10.0 Keras==2.3.1 tensorflow-gpu==1.15.0  As the comment at the top indicates, the output of conda list -e > requirements.txt  can be used to create a conda virtual environment with conda create --name <env> --file requirements.txt",
            "title": "conda \u521b\u5efa\u73af\u5883"
        },
        {
            "location": "/python/#cudnn_status_internal_error-tensorflow",
            "text": "You can try Allowing GPU memory growth with:  import tensorflow as tf\ngpu = tf.config.experimental.list_physical_devices('GPU')\ntf.config.experimental.set_memory_growth(gpu[0], True)",
            "title": "cudnn_status_internal_error tensorflow"
        },
        {
            "location": "/python/#vscode-activate-conda-env",
            "text": "{\n    \"ros.distro\": \"melodic\",\n    \"python.autoComplete.extraPaths\": [\n        \"/home/pmjd/Disk/anaconda3/envs/wjj/lib/python3.6/site-packages\"\n    ],\n    \"python.terminal.activateEnvInCurrentTerminal\": true,\n    \"python.condaPath\": \"/home/pmjd/Disk/anaconda3/bin/conda\",\n    \"python.defaultInterpreterPath\": \"/home/pmjd/Disk/anaconda3/envs/wjj/bin/python\"\n}",
            "title": "vscode activate conda env"
        },
        {
            "location": "/python/#nameerror-name-xrange-is-not-defined",
            "text": "try:\n    # Python 2\n    xrange\nexcept NameError:\n    # Python 3, xrange is now named range\n    xrange = range",
            "title": "NameError: name 'xrange' is not defined"
        },
        {
            "location": "/python/#tensorflow-valueerror-cannot-feed-value-of-shape-64-64-3-for-tensor-uplaceholder0-which-has-shape-64-64-3",
            "text": "image has a shape of (64,64,3).\nYour input placeholder _x have a shape of (?, 64,64,3).\nThe problem is that you're feeding the placeholder with a value of a different shape.\nYou have to feed it with a value of (1, 64, 64, 3) = a batch of 1 image.\nJust reshape your image value to a batch with size one. np.expand_dims(img, axis=0)",
            "title": "TensorFlow ValueError: Cannot feed value of shape (64, 64, 3) for Tensor u'Placeholder:0', which has shape '(?, 64, 64, 3)'"
        },
        {
            "location": "/python/#opencv-conda",
            "text": "pip install opencv-python  opencv is not compatible with python 3. I had to install opencv3 for python 3. The marked answer in how could we install opencv on anaconda? explains how to install opencv(3) for anaconda:  Run the following command:  conda install -c https://conda.binstar.org/menpo opencv  I realized that opencv3 is also available now, run the following command:  conda install -c https://conda.binstar.org/menpo opencv3  Edit on Aug 18, 2016: You may like to add the \"menpo\" channel permanently by:  conda config --add channels menpo  And then opencv can be installed by:  conda install opencv (or opencv3)  Edit on Aug 14, 2017: \"clinicalgraphics\" channel provides relatively newer vtk version for very recent python3  conda install -c clinicalgraphics vtk  Edit on April 16, 2020 (based on @AMC's comment): OpenCV can be installed through conda-forge (details see here)  conda install -c conda-forge opencv",
            "title": "opencv conda"
        },
        {
            "location": "/python/#pointcloud2-to-array",
            "text": "def pointcloud2_to_array(cloud_msg, squeeze=True):\n    dtype_list = fields_to_dtype(cloud_msg.fields, cloud_msg.point_step)\n    cloud_arr = np.fromstring(cloud_msg.data, dtype_list)\n\n    cloud_arr = cloud_arr[\n        [fname for fname, _type in dtype_list if not (fname[:len(DUMMY_FIELD_PREFIX)] == DUMMY_FIELD_PREFIX)]]\n\n    if squeeze and cloud_msg.height == 1:\n        return np.reshape(cloud_arr, (cloud_msg.width,))\n    else:\n        return np.reshape(cloud_arr, (cloud_msg.height, cloud_msg.width))",
            "title": "pointcloud2 to array"
        },
        {
            "location": "/python/#read-pcd-to-array",
            "text": "import numpy as np \nimport open3d as o3d\n\npcd = o3d.io.read_point_cloud(\"pointcloud_path.pcd\")\nout_arr = np.asarray(pcd.points)  \nprint (\"output array from input list : \", out_arr)",
            "title": "read pcd to array"
        },
        {
            "location": "/python/#kitti",
            "text": "\u56fe\u7247\u4e0b\u8f7d    https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_image_2.zip    \u70b9\u4e91\u4e0b\u8f7d    https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_velodyne.zip    \u6807\u7b7e\u4e0b\u8f7d    https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_label_2.zip    \u77eb\u6b63\u6587\u4ef6\u4e0b\u8f7d    https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_calib.zip",
            "title": "kitti \u6570\u636e\u96c6\u4e0b\u8f7d"
        },
        {
            "location": "/python/#python-operator",
            "text": "In python 2.x  >>> 10/3\n3\n>>> # To get a floating point number from integer division:\n>>> 10.0/3\n3.3333333333333335\n>>> float(10)/3\n3.3333333333333335  In python 3.x  >>> 10/3\n3.3333333333333335\n>>> 10//3\n3",
            "title": "python \"//\" operator"
        },
        {
            "location": "/python/#python-super",
            "text": "\u5185\u7f6e\u7684super()\u8fd4\u56de\u4e00\u4e2a\u4ee3\u7406\u5bf9\u8c61\uff08\u8d85\u7c7b\u7684\u4e34\u65f6\u5bf9\u8c61),\u8be5\u4ee3\u7406\u5bf9\u8c61\u5141\u8bb8\u6211\u4eec\u8bbf\u95ee\u57fa\u7c7b\u7684\u65b9\u6cd5\u3002\u5728python\u4e2d\uff0csuper()\u6709\u4e24\u4e2a\u4e3b\u8981\u7528\u4f8b\uff1a   \u8ba9\u6211\u4eec\u907f\u514d\u663e\u793a\u4f7f\u7528\u57fa\u7c7b\u540d\u79f0  \u5904\u7406\u591a\u91cd\u7ee7\u627f\\",
            "title": "python super()"
        },
        {
            "location": "/python/#1super",
            "text": "\u5728\u5355\u7ee7\u627f\u7684\u60c5\u51b5\u4e0b\uff0c\u5b83\u5141\u8bb8\u6211\u4eec\u901a\u8fc7\u5f15\u7528\u57fa\u7c7bsuper()  class Mammal(object):\n  def __init__(self, mammalName):\n    print(mammalName, 'is a warm-blooded animal.')\n\nclass Dog(Mammal):\n  def __init__(self):\n    print('Dog has four legs.')\n    super().__init__('Dog')\n\nd1 = Dog()  \u8f93\u51fa  Dog has four legs.\nDog is a warm-blooded animal.  \u8be5super()\u5185\u5efa\u8fd4\u56de\u4e00\u4e2a\u4ee3\u7406\u5bf9\u8c61\uff0c\u66ff\u4ee3\u5bf9\u8c61\uff0c\u53ef\u4ee5\u901a\u8fc7\u59d4\u6258\u8c03\u7528\u57fa\u7c7b\u7684\u65b9\u6cd5\uff0c\u8fd9\u79f0\u4e3a\u201c\u95f4\u63a5\"(\u4f7f\u7528\u5f15\u7528\u57fa\u7840\u5bf9\u8c61\u7684\u80fd\u529bsuper())\n\u7531\u4e8e\u95f4\u63a5\u662f\u5728\u8fd0\u884c\u65f6\u8ba1\u7b97\u7684\uff0c\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u5728\u4e0d\u540c\u7684\u65f6\u95f4\u4f7f\u7528\u4e0d\u540c\u7684\u57fa\u7c7b\uff08\u5982\u679c\u9700\u8981\uff09",
            "title": "\u793a\u4f8b\uff11\uff1a\u5177\u6709\u5355\u7ee7\u627f\u7684super()"
        },
        {
            "location": "/python/#2super",
            "text": "class Animal:\n  def __init__(self, Animal):\n    print(Animal, 'is an animal.');\n\nclass Mammal(Animal):\n  def __init__(self, mammalName):\n    print(mammalName, 'is a warm-blooded animal.')\n    super().__init__(mammalName)\n\nclass NonWingedMammal(Mammal):\n  def __init__(self, NonWingedMammal):\n    print(NonWingedMammal, \"can't fly.\")\n    super().__init__(NonWingedMammal)\n\nclass NonMarineMammal(Mammal):\n  def __init__(self, NonMarineMammal):\n    print(NonMarineMammal, \"can't swim.\")\n    super().__init__(NonMarineMammal)\n\nclass Dog(NonMarineMammal, NonWingedMammal):\n  def __init__(self):\n    print('Dog has 4 legs.');\n    super().__init__('Dog')\n\nd = Dog()\nprint('')\nbat = NonMarineMammal('Bat')  \u8f93\u51fa  Dog has 4 legs.\nDog can't swim.\nDog can't flay.\nDog is a warm-blooded animal.\nDog is an animal.\n\nBat can't swim.\nBat is a warm-blooded animal.\nBat is an animal.",
            "title": "\u793a\u4f8b\uff12\uff1a\u5177\u6709\u591a\u91cd\u7ee7\u627f\u7684super()"
        },
        {
            "location": "/python/#method-resolution-order-mro",
            "text": ">>> Dog.__mro__\n(<class 'Dog'>, \n<class 'NonMarineMammal'>, \n<class 'NonWingedMammal'>, \n<class 'Mammal'>, \n<class 'Animal'>, \n<class 'object'>)",
            "title": "Method Resolution Order\u65b9\u6cd5\u89e3\u6790\u987a\u5e8f (MRO)"
        },
        {
            "location": "/python/#split-method-in-python-is-outputing-an-index-error",
            "text": "one of your lines must be empty",
            "title": "Split method in python is outputing an index error"
        },
        {
            "location": "/python/#deque-in-python-python",
            "text": "# Python code to demonstrate deque \nfrom collections import deque \n# Declaring deque \nqueue = deque(['name','age','DOB']) \nprint(queue) \n=========================================================\nOutput:\ndeque(['name', 'age', 'DOB'])   append() :- This function is used to insert the value in its argument to the right end of deque.  appendleft() :- This function is used to insert the value in its argument to the left end of deque.  pop() :- This function is used to delete an argument from the right end of deque.  popleft() :- This function is used to delete an argument from the left end of deque.  index(ele, beg, end) :- This function returns the first index of the value mentioned in arguments, starting searching from beg till end index.  insert(i, a) :- This function inserts the value mentioned in arguments(a) at index(i) specified in arguments.  remove() :- This function removes the first occurrence of value mentioned in arguments.  extend(iterable) :- This function is used to add multiple values at the right end of deque. The argument passed is an iterable.  extendleft(iterable) :- This function is used to add multiple values at the left end of deque. The argument passed is an iterable. Order is reversed as a result of left appends.  reverse() :- This function is used to reverse order of deque elements.  rotate() :- This function rotates the deque by the number specified in arguments. If the number specified is negative, rotation occurs to left. Else rotation is to right.",
            "title": "deque in python \uff50\uff59\uff54\uff48\uff4f\uff4e\u4e2d\u7684\u53cc\u7aef\u961f\u5217"
        },
        {
            "location": "/python/#python-randomsample",
            "text": "",
            "title": "python random.sample()"
        },
        {
            "location": "/python/#python-pip",
            "text": "PIP_NO_CACHE_DIR=off pip install gym",
            "title": "python pip \u4e0d\u80fd\u7528"
        },
        {
            "location": "/python/#numlinspace-in-python",
            "text": "import numpy as np\nprint(\"B\\n\", np.linspace(2.0, 3.0, num=5, retstep=True),\"\\n\")\n\nx = np.linspace(0, 2, 10)\nprint(\"A\\n\", np.sin(x))  Output  B\n (array([ 2.  ,  2.25,  2.5 ,  2.75,  3.  ]), 0.25)\n\nA\n [ 0.          0.22039774  0.42995636  0.6183698   0.77637192  0.8961922\n  0.9719379   0.99988386  0.9786557   0.90929743]",
            "title": "num.linspace() in Python"
        },
        {
            "location": "/python/#pip",
            "text": "\u56fd\u5185\u6e90\uff1a\n\u65b0\u7248ubuntu\u8981\u6c42\u4f7f\u7528https\u6e90\uff0c\u8981\u6ce8\u610f\u3002\n\u6e05\u534e\uff1ahttps://pypi.tuna.tsinghua.edu.cn/simple\n\u963f\u91cc\u4e91\uff1ahttp://mirrors.aliyun.com/pypi/simple\n\u4e2d\u56fd\u79d1\u6280\u5927\u5b66 https://pypi.mirrors.ustc.edu.cn/simple\n\u534e\u4e2d\u7406\u5de5\u5927\u5b66\uff1ahttp://pypi.hustunique.com\n\u5c71\u4e1c\u7406\u5de5\u5927\u5b66\uff1ahttp://pypi.sdutlinux.org\n\u8c46\u74e3\uff1ahttp://pypi.douban.com/simple\n\u4e34\u65f6\u4f7f\u7528\uff1a\n\u53ef\u4ee5\u5728\u4f7f\u7528pip\u7684\u65f6\u5019\u52a0\u53c2\u6570\npip install -i https://pypi.tuna.tsinghua.edu.cn/simple",
            "title": "pip \u4e0b\u8f7d\u63d0\u901f"
        },
        {
            "location": "/python/#pilimageopencv",
            "text": "import cv2\nfrom PIL import Image\nimport numpy\n\nimage = Image.open(\"plane.jpg\")\nimage.show()\nimg = cv2.cvtColor(numpy.asarray(image),cv2.COLOR_RGB2BGR)\ncv2.imshow(\"OpenCV\",img)\ncv2.waitKey()",
            "title": "PIL.Image\u8f6c\u6362\u6210OpenCV\u683c\u5f0f"
        },
        {
            "location": "/python/#opencvpilimage",
            "text": "import cv2\nfrom PIL import Image\nimport numpy\n\nimg = cv2.imread(\"plane.jpg\")\ncv2.imshow(\"OpenCV\",img)\nimage = Image.fromarray(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\nimage.show()\ncv2.waitKey()",
            "title": "OpenCV\u8f6c\u6362\u6210PIL.Image\u683c\u5f0f"
        },
        {
            "location": "/python/#python",
            "text": "xport PYTHONPATH=$PYTHONPATH:/home/dell/Deep/DeepSpeech/training",
            "title": "\u6dfb\u52a0python\u8def\u5f84"
        },
        {
            "location": "/python/#numpy",
            "text": "",
            "title": "Numpy"
        },
        {
            "location": "/python/#numpyamax",
            "text": "",
            "title": "numpy.amax()"
        },
        {
            "location": "/python/#numpyargmax",
            "text": "numpy.argmax(a, axis=None, out=None)[source]Returns the indices of the maximum values along an axis. parameters \\   a: array_like ,Input array.  axis:int, optional , By default, the index is into the flattened array, otherwise along the specified axis.  outarray, optional , If provided, the result will be inserted into this array. It should be of the appropriate shape and dtype.   Returns  index_array:ndarray of ints \nArray of indices into the array. It has the same shape as a.shape with the dimension along axis removed.",
            "title": "numpy.argmax"
        },
        {
            "location": "/python/#npsqueeze",
            "text": "Function  Remove dimensions of size 1 from ndarray\nYou can use  numpy.squeeze()  to remove all dimensions of size 1 from the NumPy array  ndarray. squeeze()  is also provided as a method of  ndarray .  import numpy as np\n\na = np.arange(6).reshape(1, 2, 1, 3, 1)\nprint(a)\n# [[[[[0]\n#     [1]\n#     [2]]]\n# \n# \n#   [[[3]\n#     [4]\n#     [5]]]]]\n\nprint(a.shape)\n# (1, 2, 1, 3, 1)\n\na_s = np.squeeze(a)\nprint(a_s)\n# [[0 1 2]\n#  [3 4 5]]\n\nprint(a_s.shape)\n# (2, 3)  By default, all dimensions with size  1  are removed, as in the example above.  You can specify the index of the dimension to be removed in the second argument  axis  of  numpy.squeeze() . Dimensions that are not the specified index are not removed.  print(a.shape)\n# (1, 2, 1, 3, 1)\nprint(np.squeeze(a, 0))\n# [[[[0]\n#    [1]\n#    [2]]]\n# \n# \n#  [[[3]\n#    [4]\n#    [5]]]]\n\nprint(np.squeeze(a, 0).shape)\n# (2, 1, 3, 1)  An error will occur if you specify a dimension whose size is not  1  or a dimension that does not exist.  axis can also be specified as a negative value.  -1 corresponds to the last dimension and can be specified by the position from the back.  print(np.squeeze(a, -1))\n# [[[[0 1 2]]\n# \n#   [[3 4 5]]]]\n\nprint(np.squeeze(a, -1).shape)\n# (1, 2, 1, 3)\n\nprint(np.squeeze(a, -3))\n# [[[[0]\n#    [1]\n#    [2]]\n# \n#   [[3]\n#    [4]\n#    [5]]]]\n\nprint(np.squeeze(a, -3).shape)\n# (1, 2, 3, 1)",
            "title": "np.squeeze"
        },
        {
            "location": "/python/#_1",
            "text": "# Get all filenames in the dataroot\n  filenames = os.listdir(dataset_root)\n  filenames = [_ for _ in filenames if _.endswith(\".h5\")]#\u63d0\u53d6h5\u6587\u4ef6",
            "title": "\u63d0\u53d6\u6587\u4ef6\u548c\u4e0d\u540c\u683c\u5f0f\u6587\u4ef6"
        },
        {
            "location": "/python/#_2",
            "text": "ver1_ori = [\"front\", \"back\"]\n  filenames = [_ for _ in filenames if os.path.splitext(_)[0].split(\"_\")[-1] in ver1_ori]",
            "title": "\u6839\u636e\u6587\u4ef6\u540d\u7ee7\u7eed\u63d0\u53d6\u6587\u4ef6"
        },
        {
            "location": "/python/#_3",
            "text": "assert len(filenames) > 0",
            "title": "\u786e\u4fdd\u6587\u4ef6\u540d\u4e0d\u4e3a \u7a7a"
        },
        {
            "location": "/python/#_4",
            "text": "# Add to full data path\n  filenames_original = [os.path.join(dataset_root, _) for _ in filenames]",
            "title": "\u8fd4\u56de\u5b8c\u6574\u8def\u5f84"
        },
        {
            "location": "/python/#_5",
            "text": "# Check modalities\n  avail_modality = [\"rgb\", \"rgbd\"]\n  if not modality in avail_modality:\n      raise ValueError(\"[Error] Unsupported modality. Consider \", avail_modality)",
            "title": "\u67e5\u770b\u662f\u5426\u5c5e\u4e8e\u5df2\u77e5\u7c7b"
        },
        {
            "location": "/python/#pkl-file",
            "text": "Your  pkl  file is, in fact, a serialized pickle file, which means it has been dumped using Python's  pickle  module.  import pickle\nwith open('serialized.pkl', 'rb') as f:\n    data = pickle.load(f)  Note  gzip  is only needed if the file is compressed:  import gzip\nimport pickle\nwith gzip.open('mnist.pkl.gz', 'rb') as f:\n    train_set, valid_set, test_set = pickle.load(f)  Where each set can be further divided  (i.e. for the training set) :  train_x, train_y = train_set  If you want to display the dataset  import matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nplt.imshow(train_x[0].reshape((28, 28)), cmap=cm.Greys_r)\nplt.show()",
            "title": ".pkl file"
        },
        {
            "location": "/python/#_6",
            "text": "if not os.path.exists(datapoint):\n  raise ValueError(\"[Error] File does not exist.\")",
            "title": "\u5224\u65ad\u6587\u4ef6\u662f\u5426\u5b58\u5728"
        },
        {
            "location": "/python/#h5-file-open",
            "text": "In order to open a HDF5 file with the  h5py  module you can use  h5py.File(filename) .  import h5py\nfilename = \"vstoxx_data_31032014.h5\"\nh5 = h5py.File(filename,'r')\nfutures_data = h5['futures_data']  # VSTOXX futures data\noptions_data = h5['options_data']  # VSTOXX call option data\nh5.close()",
            "title": "h5 file Open"
        },
        {
            "location": "/python/#numpyrandomuniform",
            "text": "we can get the random samples from uniform distribution and returns the random samples as numpy array by using this method.  # import numpy\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Using uniform() method\ngfg = np.random.uniform(2.1, 5.5, 1000)\nplt.hist(gfg, bins = 100, density = True)\nplt.show()",
            "title": "numpy.random.uniform"
        },
        {
            "location": "/python/#isinstance",
            "text": "The  isinstance()  function returns  True  if the specified object is of the specified type, otherwise  False .     Parameter  Description      object  Required, An Object    type  A type or class, or a tuple of types and/or classes",
            "title": "isinstance()"
        },
        {
            "location": "/python/#_7",
            "text": "def _is_pil_image(img):\n    if accimage is not None:\n        return isinstance(img, (Image.Image, accimage.Image))\n    else:\n        return isinstance(img, Image.Image)",
            "title": "\u5224\u65ad\u56fe\u50cf\u683c\u5f0f"
        },
        {
            "location": "/python/#pil",
            "text": "def adjust_brightness(img, brightness_factor):\n    \"\"\"Adjust brightness of an Image.\n    Args:\n        img (PIL Image): PIL Image to be adjusted.\n        brightness_factor (float):  How much to adjust the brightness. Can be\n            any non negative number. 0 gives a black image, 1 gives the\n            original image while 2 increases the brightness by a factor of 2.\n    Returns:\n        PIL Image: Brightness adjusted image.\n    \"\"\"\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    enhancer = ImageEnhance.Brightness(img)\n    img = enhancer.enhance(brightness_factor)\n    return img",
            "title": "PIL\u56fe\u50cf\u4eae\u5ea6\u8c03\u6574"
        },
        {
            "location": "/python/#contrast-adjusted-image",
            "text": "def adjust_contrast(img, contrast_factor):\n    \"\"\"Adjust contrast of an Image.\n    Args:\n        img (PIL Image): PIL Image to be adjusted.\n        contrast_factor (float): How much to adjust the contrast. Can be any\n            non negative number. 0 gives a solid gray image, 1 gives the\n            original image while 2 increases the contrast by a factor of 2.\n    Returns:\n        PIL Image: Contrast adjusted image.\n    \"\"\"\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    enhancer = ImageEnhance.Contrast(img)\n    img = enhancer.enhance(contrast_factor)\n    return img",
            "title": "Contrast adjusted image"
        },
        {
            "location": "/python/#saturation-adjusted-image",
            "text": "def adjust_saturation(img, saturation_factor):\n    \"\"\"Adjust color saturation of an image.\n    Args:\n        img (PIL Image): PIL Image to be adjusted.\n        saturation_factor (float):  How much to adjust the saturation. 0 will\n            give a black and white image, 1 will give the original image while\n            2 will enhance the saturation by a factor of 2.\n    Returns:\n        PIL Image: Saturation adjusted image.\n    \"\"\"\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    enhancer = ImageEnhance.Color(img)\n    img = enhancer.enhance(saturation_factor)\n    return img",
            "title": "Saturation adjusted image"
        },
        {
            "location": "/python/#hue-adjusted-image",
            "text": "def adjust_hue(img, hue_factor):\n  \"\"\"Adjust hue of an image.\n  The image hue is adjusted by converting the image to HSV and\n  cyclically shifting the intensities in the hue channel (H).\n  The image is then converted back to original image mode.\n  `hue_factor` is the amount of shift in H channel and must be in the\n  interval `[-0.5, 0.5]`.\n  See https://en.wikipedia.org/wiki/Hue for more details on Hue.\n  Args:\n      img (PIL Image): PIL Image to be adjusted.\n      hue_factor (float):  How much to shift the hue channel. Should be in\n          [-0.5, 0.5]. 0.5 and -0.5 give complete reversal of hue channel in\n          HSV space in positive and negative direction respectively.\n          0 means no shift. Therefore, both -0.5 and 0.5 will give an image\n          with complementary colors while 0 gives the original image.\n  Returns:\n      PIL Image: Hue adjusted image.\n  \"\"\"\n  if not(-0.5 <= hue_factor <= 0.5):\n      raise ValueError('hue_factor is not in [-0.5, 0.5].'.format(hue_factor))\n\n  if not _is_pil_image(img):\n      raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n  input_mode = img.mode\n  if input_mode in {'L', '1', 'I', 'F'}:\n      return img\n\n  h, s, v = img.convert('HSV').split()\n\n  np_h = np.array(h, dtype=np.uint8)\n  # uint8 addition take cares of rotation across boundaries\n  with np.errstate(over='ignore'):\n      np_h += np.uint8(hue_factor * 255)\n  h = Image.fromarray(np_h, 'L')\n\n  img = Image.merge('HSV', (h, s, v)).convert(input_mode)\n  return img",
            "title": "Hue adjusted image"
        },
        {
            "location": "/python/#adjust-gamma-on-img",
            "text": "def adjust_gamma(img, gamma, gain=1):\n  \"\"\"Perform gamma correction on an image.\n  Also known as Power Law Transform. Intensities in RGB mode are adjusted\n  based on the following equation:\n      I_out = 255 * gain * ((I_in / 255) ** gamma)\n  See https://en.wikipedia.org/wiki/Gamma_correction for more details.\n  Args:\n      img (PIL Image): PIL Image to be adjusted.\n      gamma (float): Non negative real number. gamma larger than 1 make the\n          shadows darker, while gamma smaller than 1 make dark regions\n          lighter.\n      gain (float): The constant multiplier.\n  \"\"\"\n  if not _is_pil_image(img):\n      raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n  if gamma < 0:\n      raise ValueError('Gamma should be a non-negative real number')\n\n  input_mode = img.mode\n  img = img.convert('RGB')\n\n  np_img = np.array(img, dtype=np.float32)\n  np_img = 255 * gain * ((np_img / 255) ** gamma)\n  np_img = np.uint8(np.clip(np_img, 0, 255))\n\n  img = Image.fromarray(np_img, 'RGB').convert(input_mode)\n  return img",
            "title": "Adjust gamma on IMG"
        },
        {
            "location": "/python/#npclipimg-0-255",
            "text": "Given an interval, values outside the interval are clipped to the interval edges. For example, if an interval of [0, 1] is specified, values smaller than 0 become 0, and values larger than 1 become 1.",
            "title": "np.clip(img, 0, 255)"
        },
        {
            "location": "/python/#convert-npndarray-img-to-tensor",
            "text": "class ToTensor(object):\n  \"\"\"Convert a ``numpy.ndarray`` to tensor.\n  Converts a numpy.ndarray (H x W x C) to a torch.FloatTensor of shape (C x H x W).\n  \"\"\"\n\n  def __call__(self, img):\n      \"\"\"Convert a ``numpy.ndarray`` to tensor.\n      Args:\n          img (numpy.ndarray): Image to be converted to tensor.\n      Returns:\n          Tensor: Converted image.\n      \"\"\"\n      if not(_is_numpy_image(img)):\n          raise TypeError('img should be ndarray. Got {}'.format(type(img)))\n\n      if isinstance(img, np.ndarray):\n          # handle numpy array\n          if img.ndim == 3:\n              img = torch.from_numpy(img.transpose((2, 0, 1)).copy())\n          elif img.ndim == 2:\n              img = torch.from_numpy(img.copy())\n          else:\n              raise RuntimeError('img should be ndarray with 2 or 3 dimensions. Got {}'.format(img.ndim))\n\n          # backward compatibility\n          # return img.float().div(255)\n          return img.float()",
            "title": "Convert np.ndarray img to tensor"
        },
        {
            "location": "/python/#mathfloor",
            "text": "Round numbers down to the nearest integer  #Import math library\nimport math\n# Round numbers down to the nearest integer\nprint(math.floor(0.6))\nprint(math.floor(1.4))\nprint(math.floor(5.3))\nprint(math.floor(-5.3))\nprint(math.floor(22.6))\nprint(math.floor(10.0))\n\n# 0\n# 1\n# 5\n# -6\n# 22\n# 10",
            "title": "math.floor"
        },
        {
            "location": "/python/#unsqueeze",
            "text": "If you look at the shape of the array before and after, you see that before it was (4,) and after it is (1, 4)  (when second parameter is 0)  and (4, 1)  (when second parameter is 1) . So a 1 was inserted in the shape of the array at axis 0 or 1, depending on the value of the second parameter.  That is opposite of  np.squeeze()  (nomenclature borrowed from MATLAB) which removes axes of size 1 (singletons).  >>> x = torch.tensor([1, 2, 3, 4])\n>>> torch.unsqueeze(x, 0)\ntensor([[ 1,  2,  3,  4]])\n>>> torch.unsqueeze(x, 1)\ntensor([[ 1],\n        [ 2],\n        [ 3],\n        [ 4]])",
            "title": "unsqueeze"
        },
        {
            "location": "/python/#nptranspose",
            "text": "The transpose() function is used to permute\u7f6e\u6362 the dimensions of an array.  import numpy as np\na = np.ones((2,3,4))\nprint(a.shape)\nprint(\"--------------------------------\")\nprint(np.transpose(a,(2,0,1)).shape)\nprint(\"--------------------------------\")\nprint(np.transpose(a,(2,1,0)).shape)\n\n# (2, 3, 4)\n# --------------------------------\n# (4, 2, 3)\n# --------------------------------\n# (4, 3, 2)\n# transpose\u91cc\u7684 0\u4ee3\u8868\u7b2c\u4e00\u7ef4\u6848\u4f8b\u4e2d\u662f2\uff0c 1\u4ee3\u8868\u7b2c\u4e8c\u7ef4\u6848\u4f8b\u4e2d\u662f3\uff0c 2\u4ee3\u8868\u7b2c\u4e09\u7ef4\u6848\u4f8b\u4e2d\u662f4.",
            "title": "np.transpose"
        },
        {
            "location": "/python/#what-does-axis0-do-in-numpys-sum-function",
            "text": "a = np.array([[1, 2, 3], [4, 5, 6]])\nnp.sum(a, axis = 0)\n# result array([5, 7, 9])\n\na = np.array([1, 2, 3])\nnp.sum(a, axis = 0)\n#result 6",
            "title": "What does axis=0 do in Numpy's sum function"
        },
        {
            "location": "/python/#pad-the-points-to-512",
            "text": "num_points = points.shape[0]\n  target_size = 512\n  output_points = np.repeat(points[-1, :][None, ...], target_size, axis=0)\n  output_points[:num_points, :] = points\n  output_labels = np.repeat(labels[-1, :][None, ...], target_size, axis=0)\n  output_labels[:num_points, :] = labels",
            "title": "Pad the points to 512"
        },
        {
            "location": "/python/#numpyrandomshuffle",
            "text": "With the help of  numpy.random.shuffle()  method, we can get the random positioning of different integer values in the numpy array or we can say that all the values in an array will be shuffled randomly.",
            "title": "numpy.random.shuffle()"
        },
        {
            "location": "/python/#ordereddict",
            "text": "An OrderedDict is a dictionary subclass that remembers the  order  that keys were first  inserted .",
            "title": "OrderedDict"
        },
        {
            "location": "/python/#image-to-array-array-to-image",
            "text": "from PIL import Image\nimport numpy as np\nim = Image.open('1.jpg')\nim2arr = np.array(im) # im2arr.shape: height x width x channel\narr2im = Image.fromarray(im2arr)",
            "title": "Image to Array &amp; Array to Image"
        },
        {
            "location": "/python/#pyton-thread",
            "text": "import threading\nclass SubscribeThread(threading.Thread):\n    def __init__(self):\n        threading.Thread.__init__(self)\n\n    def callback(self,data):\n        rospy.loginfo(rospy.get_caller_id() + \"I heard %s\", data.data)\n\n    def run(self):\n        rospy.Subscriber(\"image_button\", String, self.callback)\n        rospy.spin()\n\nsubscribe_thread = SubscribeThread()\nsubscribe_thread.setDaemon(True)\nsubscribe_thread.start()",
            "title": "pyton thread"
        },
        {
            "location": "/python/#python-wheel-arm",
            "text": "wheel",
            "title": "python wheel arm"
        },
        {
            "location": "/python\u6709\u7528\u7684\u51fd\u6570/",
            "text": "python \u6709\u7528\u7684\u51fd\u6570\uff08\u4e00\uff09\n\n\n\"\"\"\n Adapt from fangchangma/sparse_to_dense.pytorch on github\n\"\"\"\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport torch\nimport torchvision\nimport math\nimport random\n\nfrom PIL import Image, ImageOps, ImageEnhance\ntry:\n    import accimage\nexcept ImportError:\n    accimage = None\n\nimport numpy as np\nimport numbers\nimport types\nimport collections\n\nimport scipy.ndimage.interpolation as itpl\nimport scipy.misc as misc\n\n\n# Check whether the input is a numpy array\ndef _is_numpy_image(img):\n    return isinstance(img, np.ndarray) and (img.ndim in {2, 3})\n\n\n# Check whether the input is a PIL image\ndef _is_pil_image(img):\n    if accimage is not None:\n        return isinstance(img, (Image.Image, accimage.Image))\n    else:\n        return isinstance(img, Image.Image)\n\n\n# Check wheter the input is a tensor\ndef _is_tensor_image(img):\n    return torch.is_tensor(img) and img.ndimension() == 3\n\n\ndef adjust_brightness(img, brightness_factor):\n    \"\"\"Adjust brightness of an Image.\n    Args:\n        img (PIL Image): PIL Image to be adjusted.\n        brightness_factor (float):  How much to adjust the brightness. Can be\n            any non negative number. 0 gives a black image, 1 gives the\n            original image while 2 increases the brightness by a factor of 2.\n    Returns:\n        PIL Image: Brightness adjusted image.\n    \"\"\"\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    enhancer = ImageEnhance.Brightness(img)\n    img = enhancer.enhance(brightness_factor)\n    return img\n\n\ndef adjust_contrast(img, contrast_factor):\n    \"\"\"Adjust contrast of an Image.\n    Args:\n        img (PIL Image): PIL Image to be adjusted.\n        contrast_factor (float): How much to adjust the contrast. Can be any\n            non negative number. 0 gives a solid gray image, 1 gives the\n            original image while 2 increases the contrast by a factor of 2.\n    Returns:\n        PIL Image: Contrast adjusted image.\n    \"\"\"\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    enhancer = ImageEnhance.Contrast(img)\n    img = enhancer.enhance(contrast_factor)\n    return img\n\n\ndef adjust_saturation(img, saturation_factor):\n    \"\"\"Adjust color saturation of an image.\n    Args:\n        img (PIL Image): PIL Image to be adjusted.\n        saturation_factor (float):  How much to adjust the saturation. 0 will\n            give a black and white image, 1 will give the original image while\n            2 will enhance the saturation by a factor of 2.\n    Returns:\n        PIL Image: Saturation adjusted image.\n    \"\"\"\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    enhancer = ImageEnhance.Color(img)\n    img = enhancer.enhance(saturation_factor)\n    return img\n\n\ndef adjust_hue(img, hue_factor):\n    \"\"\"Adjust hue of an image.\n    The image hue is adjusted by converting the image to HSV and\n    cyclically shifting the intensities in the hue channel (H).\n    The image is then converted back to original image mode.\n    `hue_factor` is the amount of shift in H channel and must be in the\n    interval `[-0.5, 0.5]`.\n    See https://en.wikipedia.org/wiki/Hue for more details on Hue.\n    Args:\n        img (PIL Image): PIL Image to be adjusted.\n        hue_factor (float):  How much to shift the hue channel. Should be in\n            [-0.5, 0.5]. 0.5 and -0.5 give complete reversal of hue channel in\n            HSV space in positive and negative direction respectively.\n            0 means no shift. Therefore, both -0.5 and 0.5 will give an image\n            with complementary colors while 0 gives the original image.\n    Returns:\n        PIL Image: Hue adjusted image.\n    \"\"\"\n    if not(-0.5 <= hue_factor <= 0.5):\n        raise ValueError('hue_factor is not in [-0.5, 0.5].'.format(hue_factor))\n\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    input_mode = img.mode\n    if input_mode in {'L', '1', 'I', 'F'}:\n        return img\n\n    h, s, v = img.convert('HSV').split()\n\n    np_h = np.array(h, dtype=np.uint8)\n    # uint8 addition take cares of rotation across boundaries\n    with np.errstate(over='ignore'):\n        np_h += np.uint8(hue_factor * 255)\n    h = Image.fromarray(np_h, 'L')\n\n    img = Image.merge('HSV', (h, s, v)).convert(input_mode)\n    return img\n\n\ndef adjust_gamma(img, gamma, gain=1):\n    \"\"\"Perform gamma correction on an image.\n    Also known as Power Law Transform. Intensities in RGB mode are adjusted\n    based on the following equation:\n        I_out = 255 * gain * ((I_in / 255) ** gamma)\n    See https://en.wikipedia.org/wiki/Gamma_correction for more details.\n    Args:\n        img (PIL Image): PIL Image to be adjusted.\n        gamma (float): Non negative real number. gamma larger than 1 make the\n            shadows darker, while gamma smaller than 1 make dark regions\n            lighter.\n        gain (float): The constant multiplier.\n    \"\"\"\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    if gamma < 0:\n        raise ValueError('Gamma should be a non-negative real number')\n\n    input_mode = img.mode\n    img = img.convert('RGB')\n\n    np_img = np.array(img, dtype=np.float32)\n    np_img = 255 * gain * ((np_img / 255) ** gamma)\n    np_img = np.uint8(np.clip(np_img, 0, 255))\n\n    img = Image.fromarray(np_img, 'RGB').convert(input_mode)\n    return img\n\n\nclass Compose(object):\n    \"\"\"Composes several transforms together.\n    Args:\n        transforms (list of ``Transform`` objects): list of transforms to compose.\n    Example:\n        >>> transforms.Compose([\n        >>>     transforms.CenterCrop(10),\n        >>>     transforms.ToTensor(),\n        >>> ])\n    \"\"\"\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img):\n        for t in self.transforms:\n            img = t(img)\n        return img\n\n\nclass ToTensor(object):\n    \"\"\"Convert a ``numpy.ndarray`` to tensor.\n    Converts a numpy.ndarray (H x W x C) to a torch.FloatTensor of shape (C x H x W).\n    \"\"\"\n\n    def __call__(self, img):\n        \"\"\"Convert a ``numpy.ndarray`` to tensor.\n        Args:\n            img (numpy.ndarray): Image to be converted to tensor.\n        Returns:\n            Tensor: Converted image.\n        \"\"\"\n        if not(_is_numpy_image(img)):\n            raise TypeError('img should be ndarray. Got {}'.format(type(img)))\n\n        if isinstance(img, np.ndarray):\n            # handle numpy array\n            if img.ndim == 3:\n                img = torch.from_numpy(img.transpose((2, 0, 1)).copy())\n            elif img.ndim == 2:\n                img = torch.from_numpy(img.copy())\n            else:\n                raise RuntimeError('img should be ndarray with 2 or 3 dimensions. Got {}'.format(img.ndim))\n\n            # backward compatibility\n            # return img.float().div(255)\n            return img.float()\n\n\nclass NormalizeNumpyArray(object):\n    \"\"\"Normalize a ``numpy.ndarray`` with mean and standard deviation.\n    Given mean: ``(M1,...,Mn)`` and std: ``(M1,..,Mn)`` for ``n`` channels, this transform\n    will normalize each channel of the input ``numpy.ndarray`` i.e.\n    ``input[channel] = (input[channel] - mean[channel]) / std[channel]``\n    Args:\n        mean (sequence): Sequence of means for each channel.\n        std (sequence): Sequence of standard deviations for each channel.\n    \"\"\"\n\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (numpy.ndarray): Image of size (H, W, C) to be normalized.\n        Returns:\n            Tensor: Normalized image.\n        \"\"\"\n        if not(_is_numpy_image(img)):\n            raise TypeError('img should be ndarray. Got {}'.format(type(img)))\n        # TODO: make efficient\n        print(img.shape)\n        for i in range(3):\n            img[:,:,i] = (img[:,:,i] - self.mean[i]) / self.std[i]\n        return img\n\n\nclass NormalizeTensor(object):\n    \"\"\"Normalize an tensor image with mean and standard deviation.\n    Given mean: ``(M1,...,Mn)`` and std: ``(M1,..,Mn)`` for ``n`` channels, this transform\n    will normalize each channel of the input ``torch.*Tensor`` i.e.\n    ``input[channel] = (input[channel] - mean[channel]) / std[channel]``\n    Args:\n        mean (sequence): Sequence of means for each channel.\n        std (sequence): Sequence of standard deviations for each channel.\n    \"\"\"\n\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        \"\"\"\n        Args:\n            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n        Returns:\n            Tensor: Normalized Tensor image.\n        \"\"\"\n        if not _is_tensor_image(tensor):\n            raise TypeError('tensor is not a torch image.')\n        # TODO: make efficient\n        for t, m, s in zip(tensor, self.mean, self.std):\n            t.sub_(m).div_(s)\n        return tensor\n\n\nclass Rotate(object):\n    \"\"\"Rotates the given ``numpy.ndarray``.\n    Args:\n        angle (float): The rotation angle in degrees.\n    \"\"\"\n\n    def __init__(self, angle):\n        self.angle = angle\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (numpy.ndarray (C x H x W)): Image to be rotated.\n        Returns:\n            img (numpy.ndarray (C x H x W)): Rotated image.\n        \"\"\"\n\n        # order=0 means nearest-neighbor type interpolation\n        return itpl.rotate(img, self.angle, reshape=False, prefilter=False, order=0)\n\n\nclass Resize(object):\n    \"\"\"Resize the the given ``numpy.ndarray`` to the given size.\n    Args:\n        size (sequence or int): Desired output size. If size is a sequence like\n            (h, w), output size will be matched to this. If size is an int,\n            smaller edge of the image will be matched to this number.\n            i.e, if height > width, then image will be rescaled to\n            (size * height / width, size)\n        interpolation (int, optional): Desired interpolation. Default is\n            ``PIL.Image.BILINEAR``\n    \"\"\"\n\n    def __init__(self, size, interpolation='nearest'):\n        assert isinstance(size, int) or isinstance(size, float) or \\\n               (isinstance(size, collections.Iterable) and len(size) == 2)\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to be scaled.\n        Returns:\n            PIL Image: Rescaled image.\n        \"\"\"\n        if img.ndim == 3:\n            return misc.imresize(img, self.size, self.interpolation)\n        elif img.ndim == 2:\n            return misc.imresize(img, self.size, self.interpolation, 'F')\n        else:\n            RuntimeError('img should be ndarray with 2 or 3 dimensions. Got {}'.format(img.ndim))\n\n\nclass CenterCrop(object):\n    \"\"\"Crops the given ``numpy.ndarray`` at the center.\n    Args:\n        size (sequence or int): Desired output size of the crop. If size is an\n            int instead of sequence like (h, w), a square crop (size, size) is\n            made.\n    \"\"\"\n\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n    @staticmethod\n    def get_params(img, output_size):\n        \"\"\"Get parameters for ``crop`` for center crop.\n        Args:\n            img (numpy.ndarray (C x H x W)): Image to be cropped.\n            output_size (tuple): Expected output size of the crop.\n        Returns:\n            tuple: params (i, j, h, w) to be passed to ``crop`` for center crop.\n        \"\"\"\n        h = img.shape[0]\n        w = img.shape[1]\n        th, tw = output_size\n        i = int(round((h - th) / 2.))\n        j = int(round((w - tw) / 2.))\n\n        # # randomized cropping\n        # i = np.random.randint(i-3, i+4)\n        # j = np.random.randint(j-3, j+4)\n\n        return i, j, th, tw\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (numpy.ndarray (C x H x W)): Image to be cropped.\n        Returns:\n            img (numpy.ndarray (C x H x W)): Cropped image.\n        \"\"\"\n        i, j, h, w = self.get_params(img, self.size)\n\n        \"\"\"\n        i: Upper pixel coordinate.\n        j: Left pixel coordinate.\n        h: Height of the cropped image.\n        w: Width of the cropped image.\n        \"\"\"\n        if not(_is_numpy_image(img)):\n            raise TypeError('img should be ndarray. Got {}'.format(type(img)))\n        if img.ndim == 3:\n            return img[i:i+h, j:j+w, :]\n        elif img.ndim == 2:\n            return img[i:i + h, j:j + w]\n        else:\n            raise RuntimeError('img should be ndarray with 2 or 3 dimensions. Got {}'.format(img.ndim))\n\n\nclass Lambda(object):\n    \"\"\"Apply a user-defined lambda as a transform.\n    Args:\n        lambd (function): Lambda/function to be used for transform.\n    \"\"\"\n\n    def __init__(self, lambd):\n        assert isinstance(lambd, types.LambdaType)\n        self.lambd = lambd\n\n    def __call__(self, img):\n        return self.lambd(img)\n\n\nclass HorizontalFlip(object):\n    \"\"\"Horizontally flip the given ``numpy.ndarray``.\n    Args:\n        do_flip (boolean): whether or not do horizontal flip.\n    \"\"\"\n\n    def __init__(self, do_flip):\n        self.do_flip = do_flip\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (numpy.ndarray (C x H x W)): Image to be flipped.\n        Returns:\n            img (numpy.ndarray (C x H x W)): flipped image.\n        \"\"\"\n        if not(_is_numpy_image(img)):\n            raise TypeError('img should be ndarray. Got {}'.format(type(img)))\n\n        if self.do_flip:\n            return np.fliplr(img)\n        else:\n            return img\n\n\nclass ColorJitter(object):\n    \"\"\"Randomly change the brightness, contrast and saturation of an image.\n    Args:\n        brightness (float): How much to jitter brightness. brightness_factor\n            is chosen uniformly from [max(0, 1 - brightness), 1 + brightness].\n        contrast (float): How much to jitter contrast. contrast_factor\n            is chosen uniformly from [max(0, 1 - contrast), 1 + contrast].\n        saturation (float): How much to jitter saturation. saturation_factor\n            is chosen uniformly from [max(0, 1 - saturation), 1 + saturation].\n        hue(float): How much to jitter hue. hue_factor is chosen uniformly from\n            [-hue, hue]. Should be >=0 and <= 0.5.\n    \"\"\"\n    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n        self.brightness = brightness\n        self.contrast = contrast\n        self.saturation = saturation\n        self.hue = hue\n\n    @staticmethod\n    def get_params(brightness, contrast, saturation, hue):\n        \"\"\"Get a randomized transform to be applied on image.\n        Arguments are same as that of __init__.\n        Returns:\n            Transform which randomly adjusts brightness, contrast and\n            saturation in a random order.\n        \"\"\"\n        transforms = []\n        if brightness > 0:\n            brightness_factor = np.random.uniform(max(0, 1 - brightness), 1 + brightness)\n            transforms.append(Lambda(lambda img: adjust_brightness(img, brightness_factor)))\n\n        if contrast > 0:\n            contrast_factor = np.random.uniform(max(0, 1 - contrast), 1 + contrast)\n            transforms.append(Lambda(lambda img: adjust_contrast(img, contrast_factor)))\n\n        if saturation > 0:\n            saturation_factor = np.random.uniform(max(0, 1 - saturation), 1 + saturation)\n            transforms.append(Lambda(lambda img: adjust_saturation(img, saturation_factor)))\n\n        if hue > 0:\n            hue_factor = np.random.uniform(-hue, hue)\n            transforms.append(Lambda(lambda img: adjust_hue(img, hue_factor)))\n\n        np.random.shuffle(transforms)\n        transform = Compose(transforms)\n\n        return transform\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (numpy.ndarray (C x H x W)): Input image.\n        Returns:\n            img (numpy.ndarray (C x H x W)): Color jittered image.\n        \"\"\"\n        if not(_is_numpy_image(img)):\n            raise TypeError('img should be ndarray. Got {}'.format(type(img)))\n\n        pil = Image.fromarray(img)\n        transform = self.get_params(self.brightness, self.contrast,\n                                    self.saturation, self.hue)\n        return np.array(transform(pil))\n\n\n# Easier version of color jitter\ndef Colorjitter2(brightness=0, contrast=0, saturation=0):\n    return torchvision.transforms.ColorJitter(\n                brightness=brightness,\n                contrast=contrast,\n                saturation=saturation\n            )\n\n\n# Normalization using imagenet mean and variance\ndef normalization_imagenet(inputs):\n    # Construct the normalization\n    normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n    return normalize(inputs)\n\n# Denormalization using imagenet mean and variance\ndef denormalization_imagenet(inputs):\n    # Construct the denormalization\n    mean_r = 0.485\n    mean_g = 0.456\n    mean_b = 0.406\n    std_r = 0.229\n    std_g = 0.224\n    std_b = 0.225\n    denormalize = torchvision.transforms.Normalize(mean=[-mean_r/std_r, -mean_g/std_g, -mean_b/std_b],\n                                                   std=[1/std_r, 1/std_g, 1/std_b])\n    return denormalize(inputs)\n\n\n# Denormalize batch of tensors\ndef denormalization_batch(inputs):\n    # Get the batch size\n    batch_size = inputs.shape[0]\n    tensor_list = []\n    for i in range(batch_size):\n        tensor_list.append(torch.unsqueeze(denormalization_imagenet(inputs[i, :, :, :]), dim=0))\n\n    return torch.cat(tuple(tensor_list), dim=0)\n\n\nclass Crop(object):\n    \"\"\"Crops the given PIL Image to a rectangular region based on a given\n    4-tuple defining the left, upper pixel coordinated, hight and width size.\n    Args:\n        a tuple: (upper pixel coordinate, left pixel coordinate, hight, width)-tuple\n    \"\"\"\n\n    def __init__(self, i, j, h, w):\n        \"\"\"\n        i: Upper pixel coordinate.\n        j: Left pixel coordinate.\n        h: Height of the cropped image.\n        w: Width of the cropped image.\n        \"\"\"\n        self.i = i\n        self.j = j\n        self.h = h\n        self.w = w\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (numpy.ndarray (C x H x W)): Image to be cropped.\n        Returns:\n            img (numpy.ndarray (C x H x W)): Cropped image.\n        \"\"\"\n\n        i, j, h, w = self.i, self.j, self.h, self.w\n\n        if not(_is_numpy_image(img)):\n            raise TypeError('img should be ndarray. Got {}'.format(type(img)))\n        if img.ndim == 3:\n            return img[i:i + h, j:j + w, :]\n        elif img.ndim == 2:\n            return img[i:i + h, j:j + w]\n        else:\n            raise RuntimeError(\n                'img should be ndarray with 2 or 3 dimensions. Got {}'.format(img.ndim))\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(i={0},j={1},h={2},w={3})'.format(\n            self.i, self.j, self.h, self.w)",
            "title": "Python\u6709\u7528\u7684\u51fd\u6570"
        },
        {
            "location": "/python\u6709\u7528\u7684\u51fd\u6570/#python",
            "text": "\"\"\"\n Adapt from fangchangma/sparse_to_dense.pytorch on github\n\"\"\"\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport torch\nimport torchvision\nimport math\nimport random\n\nfrom PIL import Image, ImageOps, ImageEnhance\ntry:\n    import accimage\nexcept ImportError:\n    accimage = None\n\nimport numpy as np\nimport numbers\nimport types\nimport collections\n\nimport scipy.ndimage.interpolation as itpl\nimport scipy.misc as misc\n\n\n# Check whether the input is a numpy array\ndef _is_numpy_image(img):\n    return isinstance(img, np.ndarray) and (img.ndim in {2, 3})\n\n\n# Check whether the input is a PIL image\ndef _is_pil_image(img):\n    if accimage is not None:\n        return isinstance(img, (Image.Image, accimage.Image))\n    else:\n        return isinstance(img, Image.Image)\n\n\n# Check wheter the input is a tensor\ndef _is_tensor_image(img):\n    return torch.is_tensor(img) and img.ndimension() == 3\n\n\ndef adjust_brightness(img, brightness_factor):\n    \"\"\"Adjust brightness of an Image.\n    Args:\n        img (PIL Image): PIL Image to be adjusted.\n        brightness_factor (float):  How much to adjust the brightness. Can be\n            any non negative number. 0 gives a black image, 1 gives the\n            original image while 2 increases the brightness by a factor of 2.\n    Returns:\n        PIL Image: Brightness adjusted image.\n    \"\"\"\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    enhancer = ImageEnhance.Brightness(img)\n    img = enhancer.enhance(brightness_factor)\n    return img\n\n\ndef adjust_contrast(img, contrast_factor):\n    \"\"\"Adjust contrast of an Image.\n    Args:\n        img (PIL Image): PIL Image to be adjusted.\n        contrast_factor (float): How much to adjust the contrast. Can be any\n            non negative number. 0 gives a solid gray image, 1 gives the\n            original image while 2 increases the contrast by a factor of 2.\n    Returns:\n        PIL Image: Contrast adjusted image.\n    \"\"\"\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    enhancer = ImageEnhance.Contrast(img)\n    img = enhancer.enhance(contrast_factor)\n    return img\n\n\ndef adjust_saturation(img, saturation_factor):\n    \"\"\"Adjust color saturation of an image.\n    Args:\n        img (PIL Image): PIL Image to be adjusted.\n        saturation_factor (float):  How much to adjust the saturation. 0 will\n            give a black and white image, 1 will give the original image while\n            2 will enhance the saturation by a factor of 2.\n    Returns:\n        PIL Image: Saturation adjusted image.\n    \"\"\"\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    enhancer = ImageEnhance.Color(img)\n    img = enhancer.enhance(saturation_factor)\n    return img\n\n\ndef adjust_hue(img, hue_factor):\n    \"\"\"Adjust hue of an image.\n    The image hue is adjusted by converting the image to HSV and\n    cyclically shifting the intensities in the hue channel (H).\n    The image is then converted back to original image mode.\n    `hue_factor` is the amount of shift in H channel and must be in the\n    interval `[-0.5, 0.5]`.\n    See https://en.wikipedia.org/wiki/Hue for more details on Hue.\n    Args:\n        img (PIL Image): PIL Image to be adjusted.\n        hue_factor (float):  How much to shift the hue channel. Should be in\n            [-0.5, 0.5]. 0.5 and -0.5 give complete reversal of hue channel in\n            HSV space in positive and negative direction respectively.\n            0 means no shift. Therefore, both -0.5 and 0.5 will give an image\n            with complementary colors while 0 gives the original image.\n    Returns:\n        PIL Image: Hue adjusted image.\n    \"\"\"\n    if not(-0.5 <= hue_factor <= 0.5):\n        raise ValueError('hue_factor is not in [-0.5, 0.5].'.format(hue_factor))\n\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    input_mode = img.mode\n    if input_mode in {'L', '1', 'I', 'F'}:\n        return img\n\n    h, s, v = img.convert('HSV').split()\n\n    np_h = np.array(h, dtype=np.uint8)\n    # uint8 addition take cares of rotation across boundaries\n    with np.errstate(over='ignore'):\n        np_h += np.uint8(hue_factor * 255)\n    h = Image.fromarray(np_h, 'L')\n\n    img = Image.merge('HSV', (h, s, v)).convert(input_mode)\n    return img\n\n\ndef adjust_gamma(img, gamma, gain=1):\n    \"\"\"Perform gamma correction on an image.\n    Also known as Power Law Transform. Intensities in RGB mode are adjusted\n    based on the following equation:\n        I_out = 255 * gain * ((I_in / 255) ** gamma)\n    See https://en.wikipedia.org/wiki/Gamma_correction for more details.\n    Args:\n        img (PIL Image): PIL Image to be adjusted.\n        gamma (float): Non negative real number. gamma larger than 1 make the\n            shadows darker, while gamma smaller than 1 make dark regions\n            lighter.\n        gain (float): The constant multiplier.\n    \"\"\"\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    if gamma < 0:\n        raise ValueError('Gamma should be a non-negative real number')\n\n    input_mode = img.mode\n    img = img.convert('RGB')\n\n    np_img = np.array(img, dtype=np.float32)\n    np_img = 255 * gain * ((np_img / 255) ** gamma)\n    np_img = np.uint8(np.clip(np_img, 0, 255))\n\n    img = Image.fromarray(np_img, 'RGB').convert(input_mode)\n    return img\n\n\nclass Compose(object):\n    \"\"\"Composes several transforms together.\n    Args:\n        transforms (list of ``Transform`` objects): list of transforms to compose.\n    Example:\n        >>> transforms.Compose([\n        >>>     transforms.CenterCrop(10),\n        >>>     transforms.ToTensor(),\n        >>> ])\n    \"\"\"\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img):\n        for t in self.transforms:\n            img = t(img)\n        return img\n\n\nclass ToTensor(object):\n    \"\"\"Convert a ``numpy.ndarray`` to tensor.\n    Converts a numpy.ndarray (H x W x C) to a torch.FloatTensor of shape (C x H x W).\n    \"\"\"\n\n    def __call__(self, img):\n        \"\"\"Convert a ``numpy.ndarray`` to tensor.\n        Args:\n            img (numpy.ndarray): Image to be converted to tensor.\n        Returns:\n            Tensor: Converted image.\n        \"\"\"\n        if not(_is_numpy_image(img)):\n            raise TypeError('img should be ndarray. Got {}'.format(type(img)))\n\n        if isinstance(img, np.ndarray):\n            # handle numpy array\n            if img.ndim == 3:\n                img = torch.from_numpy(img.transpose((2, 0, 1)).copy())\n            elif img.ndim == 2:\n                img = torch.from_numpy(img.copy())\n            else:\n                raise RuntimeError('img should be ndarray with 2 or 3 dimensions. Got {}'.format(img.ndim))\n\n            # backward compatibility\n            # return img.float().div(255)\n            return img.float()\n\n\nclass NormalizeNumpyArray(object):\n    \"\"\"Normalize a ``numpy.ndarray`` with mean and standard deviation.\n    Given mean: ``(M1,...,Mn)`` and std: ``(M1,..,Mn)`` for ``n`` channels, this transform\n    will normalize each channel of the input ``numpy.ndarray`` i.e.\n    ``input[channel] = (input[channel] - mean[channel]) / std[channel]``\n    Args:\n        mean (sequence): Sequence of means for each channel.\n        std (sequence): Sequence of standard deviations for each channel.\n    \"\"\"\n\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (numpy.ndarray): Image of size (H, W, C) to be normalized.\n        Returns:\n            Tensor: Normalized image.\n        \"\"\"\n        if not(_is_numpy_image(img)):\n            raise TypeError('img should be ndarray. Got {}'.format(type(img)))\n        # TODO: make efficient\n        print(img.shape)\n        for i in range(3):\n            img[:,:,i] = (img[:,:,i] - self.mean[i]) / self.std[i]\n        return img\n\n\nclass NormalizeTensor(object):\n    \"\"\"Normalize an tensor image with mean and standard deviation.\n    Given mean: ``(M1,...,Mn)`` and std: ``(M1,..,Mn)`` for ``n`` channels, this transform\n    will normalize each channel of the input ``torch.*Tensor`` i.e.\n    ``input[channel] = (input[channel] - mean[channel]) / std[channel]``\n    Args:\n        mean (sequence): Sequence of means for each channel.\n        std (sequence): Sequence of standard deviations for each channel.\n    \"\"\"\n\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        \"\"\"\n        Args:\n            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n        Returns:\n            Tensor: Normalized Tensor image.\n        \"\"\"\n        if not _is_tensor_image(tensor):\n            raise TypeError('tensor is not a torch image.')\n        # TODO: make efficient\n        for t, m, s in zip(tensor, self.mean, self.std):\n            t.sub_(m).div_(s)\n        return tensor\n\n\nclass Rotate(object):\n    \"\"\"Rotates the given ``numpy.ndarray``.\n    Args:\n        angle (float): The rotation angle in degrees.\n    \"\"\"\n\n    def __init__(self, angle):\n        self.angle = angle\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (numpy.ndarray (C x H x W)): Image to be rotated.\n        Returns:\n            img (numpy.ndarray (C x H x W)): Rotated image.\n        \"\"\"\n\n        # order=0 means nearest-neighbor type interpolation\n        return itpl.rotate(img, self.angle, reshape=False, prefilter=False, order=0)\n\n\nclass Resize(object):\n    \"\"\"Resize the the given ``numpy.ndarray`` to the given size.\n    Args:\n        size (sequence or int): Desired output size. If size is a sequence like\n            (h, w), output size will be matched to this. If size is an int,\n            smaller edge of the image will be matched to this number.\n            i.e, if height > width, then image will be rescaled to\n            (size * height / width, size)\n        interpolation (int, optional): Desired interpolation. Default is\n            ``PIL.Image.BILINEAR``\n    \"\"\"\n\n    def __init__(self, size, interpolation='nearest'):\n        assert isinstance(size, int) or isinstance(size, float) or \\\n               (isinstance(size, collections.Iterable) and len(size) == 2)\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to be scaled.\n        Returns:\n            PIL Image: Rescaled image.\n        \"\"\"\n        if img.ndim == 3:\n            return misc.imresize(img, self.size, self.interpolation)\n        elif img.ndim == 2:\n            return misc.imresize(img, self.size, self.interpolation, 'F')\n        else:\n            RuntimeError('img should be ndarray with 2 or 3 dimensions. Got {}'.format(img.ndim))\n\n\nclass CenterCrop(object):\n    \"\"\"Crops the given ``numpy.ndarray`` at the center.\n    Args:\n        size (sequence or int): Desired output size of the crop. If size is an\n            int instead of sequence like (h, w), a square crop (size, size) is\n            made.\n    \"\"\"\n\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n    @staticmethod\n    def get_params(img, output_size):\n        \"\"\"Get parameters for ``crop`` for center crop.\n        Args:\n            img (numpy.ndarray (C x H x W)): Image to be cropped.\n            output_size (tuple): Expected output size of the crop.\n        Returns:\n            tuple: params (i, j, h, w) to be passed to ``crop`` for center crop.\n        \"\"\"\n        h = img.shape[0]\n        w = img.shape[1]\n        th, tw = output_size\n        i = int(round((h - th) / 2.))\n        j = int(round((w - tw) / 2.))\n\n        # # randomized cropping\n        # i = np.random.randint(i-3, i+4)\n        # j = np.random.randint(j-3, j+4)\n\n        return i, j, th, tw\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (numpy.ndarray (C x H x W)): Image to be cropped.\n        Returns:\n            img (numpy.ndarray (C x H x W)): Cropped image.\n        \"\"\"\n        i, j, h, w = self.get_params(img, self.size)\n\n        \"\"\"\n        i: Upper pixel coordinate.\n        j: Left pixel coordinate.\n        h: Height of the cropped image.\n        w: Width of the cropped image.\n        \"\"\"\n        if not(_is_numpy_image(img)):\n            raise TypeError('img should be ndarray. Got {}'.format(type(img)))\n        if img.ndim == 3:\n            return img[i:i+h, j:j+w, :]\n        elif img.ndim == 2:\n            return img[i:i + h, j:j + w]\n        else:\n            raise RuntimeError('img should be ndarray with 2 or 3 dimensions. Got {}'.format(img.ndim))\n\n\nclass Lambda(object):\n    \"\"\"Apply a user-defined lambda as a transform.\n    Args:\n        lambd (function): Lambda/function to be used for transform.\n    \"\"\"\n\n    def __init__(self, lambd):\n        assert isinstance(lambd, types.LambdaType)\n        self.lambd = lambd\n\n    def __call__(self, img):\n        return self.lambd(img)\n\n\nclass HorizontalFlip(object):\n    \"\"\"Horizontally flip the given ``numpy.ndarray``.\n    Args:\n        do_flip (boolean): whether or not do horizontal flip.\n    \"\"\"\n\n    def __init__(self, do_flip):\n        self.do_flip = do_flip\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (numpy.ndarray (C x H x W)): Image to be flipped.\n        Returns:\n            img (numpy.ndarray (C x H x W)): flipped image.\n        \"\"\"\n        if not(_is_numpy_image(img)):\n            raise TypeError('img should be ndarray. Got {}'.format(type(img)))\n\n        if self.do_flip:\n            return np.fliplr(img)\n        else:\n            return img\n\n\nclass ColorJitter(object):\n    \"\"\"Randomly change the brightness, contrast and saturation of an image.\n    Args:\n        brightness (float): How much to jitter brightness. brightness_factor\n            is chosen uniformly from [max(0, 1 - brightness), 1 + brightness].\n        contrast (float): How much to jitter contrast. contrast_factor\n            is chosen uniformly from [max(0, 1 - contrast), 1 + contrast].\n        saturation (float): How much to jitter saturation. saturation_factor\n            is chosen uniformly from [max(0, 1 - saturation), 1 + saturation].\n        hue(float): How much to jitter hue. hue_factor is chosen uniformly from\n            [-hue, hue]. Should be >=0 and <= 0.5.\n    \"\"\"\n    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n        self.brightness = brightness\n        self.contrast = contrast\n        self.saturation = saturation\n        self.hue = hue\n\n    @staticmethod\n    def get_params(brightness, contrast, saturation, hue):\n        \"\"\"Get a randomized transform to be applied on image.\n        Arguments are same as that of __init__.\n        Returns:\n            Transform which randomly adjusts brightness, contrast and\n            saturation in a random order.\n        \"\"\"\n        transforms = []\n        if brightness > 0:\n            brightness_factor = np.random.uniform(max(0, 1 - brightness), 1 + brightness)\n            transforms.append(Lambda(lambda img: adjust_brightness(img, brightness_factor)))\n\n        if contrast > 0:\n            contrast_factor = np.random.uniform(max(0, 1 - contrast), 1 + contrast)\n            transforms.append(Lambda(lambda img: adjust_contrast(img, contrast_factor)))\n\n        if saturation > 0:\n            saturation_factor = np.random.uniform(max(0, 1 - saturation), 1 + saturation)\n            transforms.append(Lambda(lambda img: adjust_saturation(img, saturation_factor)))\n\n        if hue > 0:\n            hue_factor = np.random.uniform(-hue, hue)\n            transforms.append(Lambda(lambda img: adjust_hue(img, hue_factor)))\n\n        np.random.shuffle(transforms)\n        transform = Compose(transforms)\n\n        return transform\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (numpy.ndarray (C x H x W)): Input image.\n        Returns:\n            img (numpy.ndarray (C x H x W)): Color jittered image.\n        \"\"\"\n        if not(_is_numpy_image(img)):\n            raise TypeError('img should be ndarray. Got {}'.format(type(img)))\n\n        pil = Image.fromarray(img)\n        transform = self.get_params(self.brightness, self.contrast,\n                                    self.saturation, self.hue)\n        return np.array(transform(pil))\n\n\n# Easier version of color jitter\ndef Colorjitter2(brightness=0, contrast=0, saturation=0):\n    return torchvision.transforms.ColorJitter(\n                brightness=brightness,\n                contrast=contrast,\n                saturation=saturation\n            )\n\n\n# Normalization using imagenet mean and variance\ndef normalization_imagenet(inputs):\n    # Construct the normalization\n    normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n    return normalize(inputs)\n\n# Denormalization using imagenet mean and variance\ndef denormalization_imagenet(inputs):\n    # Construct the denormalization\n    mean_r = 0.485\n    mean_g = 0.456\n    mean_b = 0.406\n    std_r = 0.229\n    std_g = 0.224\n    std_b = 0.225\n    denormalize = torchvision.transforms.Normalize(mean=[-mean_r/std_r, -mean_g/std_g, -mean_b/std_b],\n                                                   std=[1/std_r, 1/std_g, 1/std_b])\n    return denormalize(inputs)\n\n\n# Denormalize batch of tensors\ndef denormalization_batch(inputs):\n    # Get the batch size\n    batch_size = inputs.shape[0]\n    tensor_list = []\n    for i in range(batch_size):\n        tensor_list.append(torch.unsqueeze(denormalization_imagenet(inputs[i, :, :, :]), dim=0))\n\n    return torch.cat(tuple(tensor_list), dim=0)\n\n\nclass Crop(object):\n    \"\"\"Crops the given PIL Image to a rectangular region based on a given\n    4-tuple defining the left, upper pixel coordinated, hight and width size.\n    Args:\n        a tuple: (upper pixel coordinate, left pixel coordinate, hight, width)-tuple\n    \"\"\"\n\n    def __init__(self, i, j, h, w):\n        \"\"\"\n        i: Upper pixel coordinate.\n        j: Left pixel coordinate.\n        h: Height of the cropped image.\n        w: Width of the cropped image.\n        \"\"\"\n        self.i = i\n        self.j = j\n        self.h = h\n        self.w = w\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (numpy.ndarray (C x H x W)): Image to be cropped.\n        Returns:\n            img (numpy.ndarray (C x H x W)): Cropped image.\n        \"\"\"\n\n        i, j, h, w = self.i, self.j, self.h, self.w\n\n        if not(_is_numpy_image(img)):\n            raise TypeError('img should be ndarray. Got {}'.format(type(img)))\n        if img.ndim == 3:\n            return img[i:i + h, j:j + w, :]\n        elif img.ndim == 2:\n            return img[i:i + h, j:j + w]\n        else:\n            raise RuntimeError(\n                'img should be ndarray with 2 or 3 dimensions. Got {}'.format(img.ndim))\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(i={0},j={1},h={2},w={3})'.format(\n            self.i, self.j, self.h, self.w)",
            "title": "python \u6709\u7528\u7684\u51fd\u6570\uff08\u4e00\uff09"
        },
        {
            "location": "/pytorch/",
            "text": "Pytorch Exception in Thread: ValueError: signal number 32 out of range\n\n\n\u8bbe\u7f6e \nnum_workers=0\n\n\nUnable to create file (unable to lock file\n\n\n\u7531\u4e8e\u6709\u5e94\u7528\u7a0b\u5e8f\u6b63\u5728\u4f7f\u7528\u8be5\u6587\u4ef6\n\nps aux | grep main.py\n\n\nsudo kill\n\n\nHDF5\n\n\nHDF5 simplifies the file structure to include only two major types of object:\n* DataSets, which are multimensional arrays of homogeneous type\n* Groups, which are container structures which can hold datasets and other groups.\n\n\nThis results in a truly hierarchical\u7b49\u7ea7\u5236\uff0cfilesystem-like data format.In fact ,resources in an HDF5 file can be accessed using the POSIX-like syntax \n/path/to/resource\n. Metadata is stored in the form of user-defined, named attributes\u5c5e\u6027 attached to groups and datasets. More complex storage APIs representing images and tables can then be built up using datasets, groups and attributs.\n\n\nIn addition to these advances in the file format, HDF5 includes an improved type system, and dataspace objects which represent selections over dataset regions. The API is also object-oriented\u9762\u5411\u5bf9\u8c61\u7684 with respect to datasets, groups, attributes, types, dataspaces and property lists.\n\n\nBecause it uses B-trees to index table objects, HDF5 works well for time series data such as stock\u80a1\u7968 price series, network monitoring data, and 3D meteorological\u6c14\u8c61 data. The bulk of the data goes into straightforward arrays (the table objects) that can be accessed much more quickly than the rows of an SQL database, but B-tree access is available for non-array data. The HDF5 data storage mechanism can be simpler and faster than an SQL star schema.\n\n\njupyter-lab show IMG\n\n\nfrom matplotlib.pyplot import imshow\nimport numpy as np\nfrom PIL import Image\n\n%matplotlib inline\npil_im = Image.open('data/empire.jpg', 'r')\nimshow(np.asarray(pil_im))\n\n\n\nHow to create and Use a Pytorch DataLoader\n\n\nIn the early days of pytroch , you had to write completely custom code for data loading. Now however, the vast majority of PyTorch systems I've seen (and created myself) use the PyTorch DataSet and DataLoader interfaces to serve up training or test data.Briefly a Dataset object loads training or test data into memory , and a DataLoader object fetches\u83b7\u53d6 data from a Dataset and serves the data up in batches.\n\n\n\n\nYou must write code to create a DataSet that matches your data and problem scenario\u8bbe\u60f3\uff1bNo two Dataset implementations are exactly the same.On the other hand,a DataLoader object is used mostly the same no matter which Dataset object it's associated with . For example:\n\n\nclass MyDataSet(T.utils.data.Dataset):\n    #implent custom code to load data here\nmy_ds = MyDataSet(\"my_train_data.txt\")\nmy_ldr = torch.utils.data.DataLoader(my_ds, 10, True)\nfor (idx, batch) in enumerate(my_ldr):\n\n\n\nThe code fragment shows you must implement a Dataset class yourself. Then you create a Dataset instance and pass it to a DataLoader constructor. The DataLoader object serves up batches of data, in this case with batch size = 10 training items in a random (True) order.\n\n\nThis article explains how to create and use PyTorch Dataset and DataLoader objects. A good way to see where this article is headed is to take a look at the screenshot of a demo program in \nFigure 1\n . The source data is a tiny 8-item file. Each line represents a person: sex(male = 1 0, female = 0 1), normalized age, region (east= 1 0 0, west = 0 1 0, central = 0 0 1), normalized income, and political\u653f\u6cbb\u7684 learning (conservative = 0, moderate = 1, liberal = 2).The goal of the demo is to serve up data in batches where the dependent variable to predict is political learning and the other variables are the predictors.\n\n\nThe 8-item source data is stored in a tab-delimited file named people_train.txt and looks like:\n\n\n1 0 0.171429 1 0 0 0.966805 0\n0 1  0.085714  0 1 0  0.188797  1\n1 0  0.000000  0 0 1  0.690871  2\n1 0  0.057143  0 1 0  1.000000  1\n0 1  1.000000  0 0 1  0.016598  2\n1 0  0.171429  1 0 0  0.802905  0\n0 1  0.171429  1 0 0  0.966805  1\n1 0  0.257143  0 1 0  0.329876  0\n\n\n\nBehind the scenes, the demo loads data into memory using a custom Dataset object ,and then serves the data up in randomly selected batches of size 3 rows/items. Because the source data has 8 lines, the first two batches have 3 data items , but the last batch has 2 items. The demo processes the source data twice, in other words,two epochs.\n\n\nThis artical assumes you have intermediate or better skill with a C-family programming language. The demo program is coded using Python.which is used by PyTorch and which is essentially the primary language for deep neural networks.The complete source code for the demo program is presented in this article. The source code and source data are also available  in the file download that accompanies this article.\n\n\nThe demo program\n\n\nThe demo program , with a few minor edits to save space, is presented in \nListing 1\n . I indent my Python programs using two spaces, rather than the more common four spaces or a tab character,as a matter of personal preference.\n\n\nListing 1:\n DataLoader Demo Program\n\n\nimport numpy as np\nimport Torch as T\ndevice = T.device(\"cpu\")\n\n#predictors and label in same file \n#data has been normialized and encoded like:\n# sex   age        region   income      politic\n# [0]   [2]        [3]      [6]         [7]\n# 1 0   0.057143   0 1 0    0.690871    2\n\nclass PeopleDataset(T.utils.data.Dataset):\n    def __init__(self, src_file, num_rows= None):\n        x_tmp = np.loadtxt(src_file, max_rows=num_rows, usecols = rane(0, 7), delimiter=\"\\t\", skiprows=0, dtype=np.float32)\n        y_tmp = np.loadtxt(src_file, max_rows=num_rows, usecols =7, delimiter=\"\\t\", skiprows=0, dtype= np.long)\n\n        self.x_data = T.tensor(x_tmp, dtype=T.float32).to(device)\n        self.y_data = T.tensor(y_tmp, dtype = T.long).to(device)\n\n    def __len__ (self):\n        return len(self.x_data) #required\n\n    def __getitem__(self, idx):\n        if T.is_tensor(idx):\n            idx = idx.tolist()\n        preds = self.x_data[idx, 0:7]\n        pol = self.y_data[idx]\n        sample =\\\n            {'predictors': preds, 'political':pol }\n        return sample\n\ndef main():\n    print(\"\\nBegin PyTorch DataLoader demo\" )\n    # 0. miscellaneous prep\n    T.manual_seed(0)\n    np.random.seed(0)\n\n    print(\"\\nSource data looks like: \")\n    print(\"1 0  0.171429  1 0 0  0.966805  0\")\n    print(\"0 1  0.085714  0 1 0  0.188797  1\")\n    print(\" . . . \")\n\n    # 1. create Dataset and DataLoader object\n    print(\"\\nCreating Dataset and DataLoader \")\n\n    train_file = \"./people_train.txt\"\n    train_ds = PeopleDataset(train_file, num_rows = 8)\n\n    bat_size = 3 \n    train_dir = T.utils.data.DataLoader(train_ds, batch_size=bat_size, shuffle=True)\n\n    #2. iterate thru training data twice\n    for epoch in range(2):\n        print(\"\\n===========================\")\n        print(\"Epoch = \"  + str(epoch))\n        for (batch_idx, batch) in enumerate(train_ldr):\n            print(\"\\nBatch = \" + str(batch_idx))\n            X = batch['predictors']\n            Y = batch['political']\n            print(X)\n            print(Y)\n    print(\"\\n============================\")\n    print(\"\\nEnd demo\")\n\nif __name__ = \"__main__\":\n    main()\n\n\n\nThe excution of the demo program begins with:\n\n\ndef main():\n    print(\"\\nBegin PyTorch DataLoader demo\" )\n    # 0. miscellaneous prep\n    T.manual_seed(0)\n    np.random.seed(0)\n    . . .\n\n\n\nIn almost all PyTorch programs, it's a good idea to set the system random number generator seed values so that your results will be reproducible. Unfortunately,because of execution across multiple processes, sometimes your results are not reproducible even if you set the random generator seeds. But if you don't set the seeds, your results will almost certainly not be reproducible.\n\n\nNext a Dataset and a DataLoader object are created:\n\n\ntrain_file = \".\\\\people_train.txt\"\ntrain_ds = PeopleDataset(train_file, num_rows=8)\n\nbat_size = 3\ntrain_ldr = T.utils.data.DataLoader(train_ds,\nbatch_size=bat_size, shuffle=True)\n\n\n\nThe custom PeopleDataset object constructor accepts a path to the training data, and a num_rows parameter in case you want to load just part of a very large data file during system development.\n\n\nThe built-in DataLoader class definition is housed in the torch.utils.data module. The class constructor has one required parameter, the Dataset that holds the data. There are 10 optional parameters . The demo specifies values for just the batch_size and shuffle parameters, and therefore uses the default values for the other 8 optional parameters.\n\n\nThe demo concudes by using the DataLoader to iterate through the source data:\n\n\n  for epoch in range(2):\n    print(\"\\n==============================\\n\")\n    print(\"Epoch = \" + str(epoch))\n    for (batch_idx, batch) in enumerate(train_ldr):\n      print(\"\\nBatch = \" + str(batch_idx))\n      X = batch['predictors']  # [3,7]\n      Y = batch['political']   # [3]\n      print(X)\n      print(Y)\n\n\n\nIn neural network terminology\u672f\u8bed\uff0can epoch is one pass through all source data. The DataLoader class is designed so that it can be iterated using the enumerate() function, which returns a tuple with the current batch zero-based index value, and the actual batch of data.There is a tight coupling between a Dataset and its associated DataLoader, meaning you have to know the names of the keys used for the predictor values and the dependent variable values. In this case the two keys are \"predictors\" and \"political\".\n\n\nImplementing a Dataset Class\n\n\nYou have a lot of flexibility when implementing a Dataset class. You are required to implement three methods and you can optionally add other methods depending on your source data The required methods are \ninit\n(), \nlen\n(),and \ngetitem\n(). The demo PeopleDataset defines its \ninit\n() method as\n\n\ndef __init__(self, src_file, num_rows=None):\n    x_tmp = np.loadtxt(src_file, max_rows=num_rows, usecols= range(0,7), delimiter=\"\\t\", skiprows=0, dtype=np.float32)\n    y_tmp = np.loadtxt(src_file, max_rows= num_rows, usecols=7, delimiter=\"\\t\", skiprows = 0, dtype =np.long)\n\n    self.x_data = T.tensor(x_tmp, dtype=T.float32).to(device)\n    self.y_data = T.tensor(y_tmp, dtype=T.long).to(device)\n\n\n\n\nIn situations where your source data is too large to fit into memory, you will have to read data into a buffer and refill the buffer when the buffer has been emptied . This is a fairly difficult task.\n\n\nThe demo data stores both the predictor values and dependent values-to-predict in the same file. In situation where the predictor values and dependent variables values are in separate files, you'd have to pass in two source file names instead of just one. Another common alternative is to pass in just a single source directory and then use hard-coded file names for the training and test data.\n\n\nThe demo \ninit\n() method bulk-converts\u5927\u91cf\u8f6c\u6362 all Numpy array data to PyTorch tensors. An alternative is to leave the data in memory as Numpy arrays and then convert to batches of data to tensors in the \ngetitem\n() method.Conversion from Numpy array data to PyTorch tensor data is an expensive operation so it's usually better to convert just once rather than repeatedly converting batches of data.\n\n\nThe \nlen\n() method is defined as:\n\n\ndef __len__(self):\n    return len(self.x_data)\n\n\n\nA Dataset object has to know how much data there is so that an associated DataLoader object knows how to iterate through all data in batches.\n\n\nThe \ngetitem\n() method is defined as :\n\n\ndef __getitem__(self, idx):\n    if T.is_tensor(idx):\n        idx = idx.tolist()\n    preds = self.x_data[idx, 0:7]\n    pol = self.y_data[idx]\n    sample = {'prediction':preds, 'political': pol}\n    return sample\n\n\n\nIt's common practice to name the parameter which specifies which data to fetch as \"idx\" but this is somewhat misleading because the idx parameter is usually a Python list of several idexes.The \ngetitem\n() method checks to see if the idx parameter is a PyTorch tensor instead of a Python list, and if so, converts the tensor to a list. The method return value, sample, is a Python Dictionary object and so you must specify names for dictionary keys(\"predictors\" in the demo) and the dictionary values (\"political\" in the demo).\n\n\n Using a Dataset in DataLoader \n\nThe demo program creates a relatively simple DataLoader object using just the Dataset object plus the batch_size and shuffle parameters.\n\n\ntrain_file = \"./people_train.txt\"\ntrain_ds = PeopleDataset(train_file, num_rows=8)\n\nbat_size = 3\ntrain_ldr = T.utils.data.DataLoader(train_ds, batch_size=bat_size, shuffle = True)\n\n\n\n\nIn some situations, instead of using a DataLoader to consume the data in a Dataset, it's useful to iterate through a Dataset directly. For example:\n\n\ndef process_ds(model , ds):\n    #ds is an iterable Dataset of tensors\n    for i in range(len(ds)):\n        inpts = ds[i]['predictors']\n        trgt = ds[i]['target']\n        oupt = model(inpts)\n        #do something\n    return somevalue\n\n\n\nUsing other DataLoaders\n\nOnce you understand how to create a custom Dataset and use it in a DataLoader, many of the built-in PyTorch library Dataset objects make more sense than they might otherwise. For example, the TorchVision module has data and functions that are useful for image processing . One of the Dataset classes in TorchVision holds the MNIST image data. There are 70,000 MNIST images. Each image is a handwritten digit from '0' to '9'. Each image has size 28x28 pixels and pixels are grayscale values from 0 to 255.\n\n\nA Dataset class for the MNIST images is defined in the torchvison.datasets package and is named MNIST. You can create a Dataset for MNIST training images like so:\n\n\nimport torchvision as tv\ntform = tv.transforms.Compose([tv.transforms.ToTensor()])\nmnist_train_ds = tv.datasets.MNIST(root=\"./MNIST_Data\", train=True, transform=tform, target_transform=None, download=True)\n\n\n\nAfter an MNIST Dataset object has been created, it can be used in a DataLoader as normal, for example \n\n\nmnist_train_dataldr = T.utils.data.DataLoader(mnist_train_ds,batch_size =2, shuffle = True)\n\nfor (batch_idx, batch) in enumerate(mnist_train_dataldr):\n    print(\"\")\n    print(batch_idx)\n    print(batch)\n    input()\n\n\n\nTo recap\u56de\u987e\uff0c there are many built-in Dataset classes defined in various PyTorch packages. They have different calling signatures, but they all read in data from some source (often a hard-coded URL), and have a way to convert their data to PyTorch tensors.After a built-in Dataset has been created, it can be processed by a DataLoader object using the enumerate() function.",
            "title": "Pytorch"
        },
        {
            "location": "/pytorch/#pytorch-exception-in-thread-valueerror-signal-number-32-out-of-range",
            "text": "\u8bbe\u7f6e  num_workers=0",
            "title": "Pytorch Exception in Thread: ValueError: signal number 32 out of range"
        },
        {
            "location": "/pytorch/#unable-to-create-file-unable-to-lock-file",
            "text": "\u7531\u4e8e\u6709\u5e94\u7528\u7a0b\u5e8f\u6b63\u5728\u4f7f\u7528\u8be5\u6587\u4ef6 ps aux | grep main.py  sudo kill",
            "title": "Unable to create file (unable to lock file"
        },
        {
            "location": "/pytorch/#hdf5",
            "text": "HDF5 simplifies the file structure to include only two major types of object:\n* DataSets, which are multimensional arrays of homogeneous type\n* Groups, which are container structures which can hold datasets and other groups.  This results in a truly hierarchical\u7b49\u7ea7\u5236\uff0cfilesystem-like data format.In fact ,resources in an HDF5 file can be accessed using the POSIX-like syntax  /path/to/resource . Metadata is stored in the form of user-defined, named attributes\u5c5e\u6027 attached to groups and datasets. More complex storage APIs representing images and tables can then be built up using datasets, groups and attributs.  In addition to these advances in the file format, HDF5 includes an improved type system, and dataspace objects which represent selections over dataset regions. The API is also object-oriented\u9762\u5411\u5bf9\u8c61\u7684 with respect to datasets, groups, attributes, types, dataspaces and property lists.  Because it uses B-trees to index table objects, HDF5 works well for time series data such as stock\u80a1\u7968 price series, network monitoring data, and 3D meteorological\u6c14\u8c61 data. The bulk of the data goes into straightforward arrays (the table objects) that can be accessed much more quickly than the rows of an SQL database, but B-tree access is available for non-array data. The HDF5 data storage mechanism can be simpler and faster than an SQL star schema.",
            "title": "HDF5"
        },
        {
            "location": "/pytorch/#jupyter-lab-show-img",
            "text": "from matplotlib.pyplot import imshow\nimport numpy as np\nfrom PIL import Image\n\n%matplotlib inline\npil_im = Image.open('data/empire.jpg', 'r')\nimshow(np.asarray(pil_im))",
            "title": "jupyter-lab show IMG"
        },
        {
            "location": "/pytorch/#how-to-create-and-use-a-pytorch-dataloader",
            "text": "In the early days of pytroch , you had to write completely custom code for data loading. Now however, the vast majority of PyTorch systems I've seen (and created myself) use the PyTorch DataSet and DataLoader interfaces to serve up training or test data.Briefly a Dataset object loads training or test data into memory , and a DataLoader object fetches\u83b7\u53d6 data from a Dataset and serves the data up in batches.   You must write code to create a DataSet that matches your data and problem scenario\u8bbe\u60f3\uff1bNo two Dataset implementations are exactly the same.On the other hand,a DataLoader object is used mostly the same no matter which Dataset object it's associated with . For example:  class MyDataSet(T.utils.data.Dataset):\n    #implent custom code to load data here\nmy_ds = MyDataSet(\"my_train_data.txt\")\nmy_ldr = torch.utils.data.DataLoader(my_ds, 10, True)\nfor (idx, batch) in enumerate(my_ldr):  The code fragment shows you must implement a Dataset class yourself. Then you create a Dataset instance and pass it to a DataLoader constructor. The DataLoader object serves up batches of data, in this case with batch size = 10 training items in a random (True) order.  This article explains how to create and use PyTorch Dataset and DataLoader objects. A good way to see where this article is headed is to take a look at the screenshot of a demo program in  Figure 1  . The source data is a tiny 8-item file. Each line represents a person: sex(male = 1 0, female = 0 1), normalized age, region (east= 1 0 0, west = 0 1 0, central = 0 0 1), normalized income, and political\u653f\u6cbb\u7684 learning (conservative = 0, moderate = 1, liberal = 2).The goal of the demo is to serve up data in batches where the dependent variable to predict is political learning and the other variables are the predictors.  The 8-item source data is stored in a tab-delimited file named people_train.txt and looks like:  1 0 0.171429 1 0 0 0.966805 0\n0 1  0.085714  0 1 0  0.188797  1\n1 0  0.000000  0 0 1  0.690871  2\n1 0  0.057143  0 1 0  1.000000  1\n0 1  1.000000  0 0 1  0.016598  2\n1 0  0.171429  1 0 0  0.802905  0\n0 1  0.171429  1 0 0  0.966805  1\n1 0  0.257143  0 1 0  0.329876  0  Behind the scenes, the demo loads data into memory using a custom Dataset object ,and then serves the data up in randomly selected batches of size 3 rows/items. Because the source data has 8 lines, the first two batches have 3 data items , but the last batch has 2 items. The demo processes the source data twice, in other words,two epochs.  This artical assumes you have intermediate or better skill with a C-family programming language. The demo program is coded using Python.which is used by PyTorch and which is essentially the primary language for deep neural networks.The complete source code for the demo program is presented in this article. The source code and source data are also available  in the file download that accompanies this article.",
            "title": "How to create and Use a Pytorch DataLoader"
        },
        {
            "location": "/pytorch/#the-demo-program",
            "text": "The demo program , with a few minor edits to save space, is presented in  Listing 1  . I indent my Python programs using two spaces, rather than the more common four spaces or a tab character,as a matter of personal preference.  Listing 1:  DataLoader Demo Program  import numpy as np\nimport Torch as T\ndevice = T.device(\"cpu\")\n\n#predictors and label in same file \n#data has been normialized and encoded like:\n# sex   age        region   income      politic\n# [0]   [2]        [3]      [6]         [7]\n# 1 0   0.057143   0 1 0    0.690871    2\n\nclass PeopleDataset(T.utils.data.Dataset):\n    def __init__(self, src_file, num_rows= None):\n        x_tmp = np.loadtxt(src_file, max_rows=num_rows, usecols = rane(0, 7), delimiter=\"\\t\", skiprows=0, dtype=np.float32)\n        y_tmp = np.loadtxt(src_file, max_rows=num_rows, usecols =7, delimiter=\"\\t\", skiprows=0, dtype= np.long)\n\n        self.x_data = T.tensor(x_tmp, dtype=T.float32).to(device)\n        self.y_data = T.tensor(y_tmp, dtype = T.long).to(device)\n\n    def __len__ (self):\n        return len(self.x_data) #required\n\n    def __getitem__(self, idx):\n        if T.is_tensor(idx):\n            idx = idx.tolist()\n        preds = self.x_data[idx, 0:7]\n        pol = self.y_data[idx]\n        sample =\\\n            {'predictors': preds, 'political':pol }\n        return sample\n\ndef main():\n    print(\"\\nBegin PyTorch DataLoader demo\" )\n    # 0. miscellaneous prep\n    T.manual_seed(0)\n    np.random.seed(0)\n\n    print(\"\\nSource data looks like: \")\n    print(\"1 0  0.171429  1 0 0  0.966805  0\")\n    print(\"0 1  0.085714  0 1 0  0.188797  1\")\n    print(\" . . . \")\n\n    # 1. create Dataset and DataLoader object\n    print(\"\\nCreating Dataset and DataLoader \")\n\n    train_file = \"./people_train.txt\"\n    train_ds = PeopleDataset(train_file, num_rows = 8)\n\n    bat_size = 3 \n    train_dir = T.utils.data.DataLoader(train_ds, batch_size=bat_size, shuffle=True)\n\n    #2. iterate thru training data twice\n    for epoch in range(2):\n        print(\"\\n===========================\")\n        print(\"Epoch = \"  + str(epoch))\n        for (batch_idx, batch) in enumerate(train_ldr):\n            print(\"\\nBatch = \" + str(batch_idx))\n            X = batch['predictors']\n            Y = batch['political']\n            print(X)\n            print(Y)\n    print(\"\\n============================\")\n    print(\"\\nEnd demo\")\n\nif __name__ = \"__main__\":\n    main()  The excution of the demo program begins with:  def main():\n    print(\"\\nBegin PyTorch DataLoader demo\" )\n    # 0. miscellaneous prep\n    T.manual_seed(0)\n    np.random.seed(0)\n    . . .  In almost all PyTorch programs, it's a good idea to set the system random number generator seed values so that your results will be reproducible. Unfortunately,because of execution across multiple processes, sometimes your results are not reproducible even if you set the random generator seeds. But if you don't set the seeds, your results will almost certainly not be reproducible.  Next a Dataset and a DataLoader object are created:  train_file = \".\\\\people_train.txt\"\ntrain_ds = PeopleDataset(train_file, num_rows=8)\n\nbat_size = 3\ntrain_ldr = T.utils.data.DataLoader(train_ds,\nbatch_size=bat_size, shuffle=True)  The custom PeopleDataset object constructor accepts a path to the training data, and a num_rows parameter in case you want to load just part of a very large data file during system development.  The built-in DataLoader class definition is housed in the torch.utils.data module. The class constructor has one required parameter, the Dataset that holds the data. There are 10 optional parameters . The demo specifies values for just the batch_size and shuffle parameters, and therefore uses the default values for the other 8 optional parameters.  The demo concudes by using the DataLoader to iterate through the source data:    for epoch in range(2):\n    print(\"\\n==============================\\n\")\n    print(\"Epoch = \" + str(epoch))\n    for (batch_idx, batch) in enumerate(train_ldr):\n      print(\"\\nBatch = \" + str(batch_idx))\n      X = batch['predictors']  # [3,7]\n      Y = batch['political']   # [3]\n      print(X)\n      print(Y)  In neural network terminology\u672f\u8bed\uff0can epoch is one pass through all source data. The DataLoader class is designed so that it can be iterated using the enumerate() function, which returns a tuple with the current batch zero-based index value, and the actual batch of data.There is a tight coupling between a Dataset and its associated DataLoader, meaning you have to know the names of the keys used for the predictor values and the dependent variable values. In this case the two keys are \"predictors\" and \"political\".  Implementing a Dataset Class  You have a lot of flexibility when implementing a Dataset class. You are required to implement three methods and you can optionally add other methods depending on your source data The required methods are  init (),  len (),and  getitem (). The demo PeopleDataset defines its  init () method as  def __init__(self, src_file, num_rows=None):\n    x_tmp = np.loadtxt(src_file, max_rows=num_rows, usecols= range(0,7), delimiter=\"\\t\", skiprows=0, dtype=np.float32)\n    y_tmp = np.loadtxt(src_file, max_rows= num_rows, usecols=7, delimiter=\"\\t\", skiprows = 0, dtype =np.long)\n\n    self.x_data = T.tensor(x_tmp, dtype=T.float32).to(device)\n    self.y_data = T.tensor(y_tmp, dtype=T.long).to(device)  In situations where your source data is too large to fit into memory, you will have to read data into a buffer and refill the buffer when the buffer has been emptied . This is a fairly difficult task.  The demo data stores both the predictor values and dependent values-to-predict in the same file. In situation where the predictor values and dependent variables values are in separate files, you'd have to pass in two source file names instead of just one. Another common alternative is to pass in just a single source directory and then use hard-coded file names for the training and test data.  The demo  init () method bulk-converts\u5927\u91cf\u8f6c\u6362 all Numpy array data to PyTorch tensors. An alternative is to leave the data in memory as Numpy arrays and then convert to batches of data to tensors in the  getitem () method.Conversion from Numpy array data to PyTorch tensor data is an expensive operation so it's usually better to convert just once rather than repeatedly converting batches of data.  The  len () method is defined as:  def __len__(self):\n    return len(self.x_data)  A Dataset object has to know how much data there is so that an associated DataLoader object knows how to iterate through all data in batches.  The  getitem () method is defined as :  def __getitem__(self, idx):\n    if T.is_tensor(idx):\n        idx = idx.tolist()\n    preds = self.x_data[idx, 0:7]\n    pol = self.y_data[idx]\n    sample = {'prediction':preds, 'political': pol}\n    return sample  It's common practice to name the parameter which specifies which data to fetch as \"idx\" but this is somewhat misleading because the idx parameter is usually a Python list of several idexes.The  getitem () method checks to see if the idx parameter is a PyTorch tensor instead of a Python list, and if so, converts the tensor to a list. The method return value, sample, is a Python Dictionary object and so you must specify names for dictionary keys(\"predictors\" in the demo) and the dictionary values (\"political\" in the demo).   Using a Dataset in DataLoader  \nThe demo program creates a relatively simple DataLoader object using just the Dataset object plus the batch_size and shuffle parameters.  train_file = \"./people_train.txt\"\ntrain_ds = PeopleDataset(train_file, num_rows=8)\n\nbat_size = 3\ntrain_ldr = T.utils.data.DataLoader(train_ds, batch_size=bat_size, shuffle = True)  In some situations, instead of using a DataLoader to consume the data in a Dataset, it's useful to iterate through a Dataset directly. For example:  def process_ds(model , ds):\n    #ds is an iterable Dataset of tensors\n    for i in range(len(ds)):\n        inpts = ds[i]['predictors']\n        trgt = ds[i]['target']\n        oupt = model(inpts)\n        #do something\n    return somevalue  Using other DataLoaders \nOnce you understand how to create a custom Dataset and use it in a DataLoader, many of the built-in PyTorch library Dataset objects make more sense than they might otherwise. For example, the TorchVision module has data and functions that are useful for image processing . One of the Dataset classes in TorchVision holds the MNIST image data. There are 70,000 MNIST images. Each image is a handwritten digit from '0' to '9'. Each image has size 28x28 pixels and pixels are grayscale values from 0 to 255.  A Dataset class for the MNIST images is defined in the torchvison.datasets package and is named MNIST. You can create a Dataset for MNIST training images like so:  import torchvision as tv\ntform = tv.transforms.Compose([tv.transforms.ToTensor()])\nmnist_train_ds = tv.datasets.MNIST(root=\"./MNIST_Data\", train=True, transform=tform, target_transform=None, download=True)  After an MNIST Dataset object has been created, it can be used in a DataLoader as normal, for example   mnist_train_dataldr = T.utils.data.DataLoader(mnist_train_ds,batch_size =2, shuffle = True)\n\nfor (batch_idx, batch) in enumerate(mnist_train_dataldr):\n    print(\"\")\n    print(batch_idx)\n    print(batch)\n    input()  To recap\u56de\u987e\uff0c there are many built-in Dataset classes defined in various PyTorch packages. They have different calling signatures, but they all read in data from some source (often a hard-coded URL), and have a way to convert their data to PyTorch tensors.After a built-in Dataset has been created, it can be processed by a DataLoader object using the enumerate() function.",
            "title": "The demo program"
        },
        {
            "location": "/radar_camera_fusion/",
            "text": "radar camera fusion\n\n\n\u6211\u4eec\u7684\u65b9\u6cd5\u79f0\u4e3aCenterFusion\uff0c\u5b83\u9996\u5148\u4f7f\u7528\u4e2d\u5fc3\u70b9\u68c0\u6d4b\u7f51\u7edc\u901a\u8fc7\u5728\u56fe\u50cf\u4e0a\u8bc6\u522b\u5bf9\u8c61\u7684\u4e2d\u5fc3\u70b9\u6765\u68c0\u6d4b\u5bf9\u8c61\u3002\n\n\n\u7136\u540e\u5b83\u4f7f\u7528\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u89c6\u9525\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u5173\u952e\u6570\u636e\u5173\u8054\u95ee\u9898\uff0c\u4ece\u800c\u5c06\u96f7\u8fbe\u68c0\u6d4b\u7ed3\u679c\u4e0e\u76f8\u5bf9\u5e94\u5bf9\u8c61\u7684\u4e2d\u5fc3\u70b9\u76f8\u5173\u8054\u3002\n\n\n\u76f8\u5173\u7684radar\u68c0\u6d4b\u7528\u4e8e\u751f\u6210\u57fa\u4e8eradar\u7684\u7279\u5f81\u56fe\u4ee5\u8865\u5145\u56fe\u50cf\u7279\u5f81\uff0c\u5e76\u56de\u5f52\u5230\u8bf8\u5982\u6df1\u5ea6\uff0c\u65cb\u8f6c\u548c\u901f\u5ea6\u4e4b\u7c7b\u7684\u5bf9\u8c61\u5c5e\u6027\u3002\n\n\nradar\u4f7f\u7528\u591a\u666e\u52d2\u6548\u5e94\u5feb\u901f\u51c6\u786e\u7684\u786e\u5b9a\u7269\u4f53\u7684\u901f\u5ea6\u3002\n\n\n\u89c6\u9525\u4f53\u5173\u8054\u673a\u5236\n\n\n\u4f7f\u7528\u5bf9\u8c61\u76842d\u8fb9\u6846\u6781\u5176\u4f30\u8ba1\u7684\u6df1\u5ea6\u548c\u5927\u5c0f\uff0c\u4e3a\u8be5\u5bf9\u8c61\u521b\u5efa3d\u5174\u8da3\u533a\u57df\uff0c\u89c6\u9525\u4f53\u3002\n\n\n\u5728roi\u5185\u79bb\u56fe\u50cf\u4e2d\u5fc3\u70b9\u6700\u8fd1\u7684\u70b9\uff0c\u5173\u8054\u3002\n\n\n\n\n\u5176\u4e2d\n\u662f\u5c3a\u5bf8\u81ea\u9002\u5e94\u6807\u51c6\u504f\u5dee\u3002\n\n\nFor autonomous robots to navigate a complex environment , it is crucial \u81f3\u5173\u91cd\u8981\u7684 to understand the surrounding scene both geometrically and semantically.Modern autonomous robots employ multiple sets of sensors, including lidars, radars, and cameras. Managing the different reference frames and characteristics of the sensors, and merging their observations into a single representation complicates perception.\n\n\nOur approach consists of an ensemble of neural networks which take in sensor data from different modalities \u5f62\u5f0f and transform them into a single common top-down semantic grid representation.We find representation favourable as it is agnostic\u65e0\u5173 to sensor-specific reference frames and captures both the semantic and geometric information for the surrounding scene.\n\n\nBecause the modalities share a single output representation,they can be easily aggregated \u6c47\u603bto produce a fused output.In this work we predict short-term semantic grids but the framework can be extended to other tasks.This approach offers a simple, extensible , end to end approach for multi-modal perception and prediction.\n\n\nThe benifits of a shared top-down representation across modalities are threefold. First, it is an interpretable representation that better facilities\u4fc3\u8fdb debugging\u8c03\u8bd5 and reasoning\u63a8\u7406 about inherent\u56fa\u6709 failure modes\u6545\u969c\u6a21\u5f0f of each modality.Second it is independent of any particular sensors characteristics and so is easily extensible for adding new modalities.Finally , it simplifies the task of late fusion by sharing a spatial\u7a7a\u95f4\u7684 representation in a succinct manner.\n\n\nIn this work we present a novel end-to-end framework that predicts the top-down view of the current scene($t_0$) as well as multiple timesteps into the future.The pipleline consists of a convolutional neural network for each of three sensor modalities : lidar, radar , camera.Each sensor modality predicts a sequence of top-down semantic grids, then these outputs are fused to produce a single output grid.We explore fusing using two different aggregation mechanisms.\n\n\nEstimating 3D orientation and translation of objects is essential for infrastructure-less .In case of monocular vision, successful methods have been mainly based on two ingredients\u56e0\u7d20: \uff081\uff09a network generating 2D region proposals(2D\u533a\u57df\u63d0\u6848)\u3002(2) a R-CNN structure predicting 3D object pose by utilizing the acquired regions of interest. We argue that the 2D detection network is redundant\u591a\u4f59\u7684 and introduces non-negligible \u4e0d\u53ef\u5ffd\u7565\u7684 noise for 3D detection.Hence,we propose a novel 3D object detection method, named SMOKE, in this paper that predicts a 3D bounding box for each detected object by combining a single keypoint estimate with regressed 3D   variables.As a second contribution, we propose a multi-step disentangling approach for constructing the 3D bounding box ,which significantly improves both training convergence\u6536\u655b and detection accuracy. In constract to previous 3D detection techniques, our method does not require complicated pre/post-processing.extra data, and a refinement\u7ec6\u5316 stage. Despite of its structural\u7ed3\u6784 simplicity\uff0c our proposed SMOKE network outperforms all existing monocular 3D detection methods on the KITTI dataset , giving the best state-of-the-art result on both 3D objection dection and bird's eye view evaluation . The code will be made publicly available.\n\n\nVison based object detection is an essential ingredient\u6210\u5206 of autonomous vehicle perception \u6d1e\u5bdf\u529b of autonomous vehicle perception and infrastructure\u57fa\u7840\u8bbe\u65bd less robot navigation in general. This type of detection methods are used to perceive \u611f\u77e5 the surrounding environment by detecting and classifying object instances\u5b9e\u4f8b into categories \u7c7b\u522b and identifying their locations and orientations. Recent developments in 2D object detection have achieved promising performance\u6709\u524d\u9014\u7684 on both detection accuracy and speed. In constract, 3D object detection have proven to be a more challenging task as it aims to estimate pose and location for each object simulataneously.\n\n\nCurrently,the most successful 3D object detection methods heavily depend on Lidar point cloud.\n\n\nor LIDAR-Image fusion information.(features learned from the point cloud are key components of the detection network).However , LIDAR sensors are extremely expensive , have a short service life time and too heavy for autonomous robots. Hence LIdars are currently not considered to be econnomical\u6d41\u884c\u7684 to support autonomous vehicle operations . Alternatively , cameras are cost-effective, easily mountable and light-weight solutions for 3D object detection with long expected service time. Unlike lidar senors , a single camera in itself cannot obtain sufficient spatial information for the whole environment as single RGB images can not supply object location information or dimensional contour \u8f6e\u5ed3 in the real world. While binocular \u53cc\u76ee vision restores the missing spatial information. in many robot applications, especially UAVs , it is difficult to realize biocular vision. Hence , it is desirable to perform 3D detection on a monocular  image even if it is a more difficult and chanllenging task.\n\n\nTo enhance performance, geometry reasoning \u51e0\u4f55\u63a8\u7406 synthetic data \u7efc\u5408\u6570\u636e and post 3D-2D processing have also been used to improve 3D object detection on single image.By the knowledge of the authors , no reliable monocular 3D detection method has been introduced so far to learn 3D information directly from image plane avoiding the performance decrease that is inevitable\u4e0d\u53ef\u907f\u514d\u7684 with multi-stage method.\n\n\n\n\nIn this paper we propose an innovative\u521b\u65b0 single-stage 3D object detection method that pairs each object with a single keypoint. We argue and later show that a 2D detection, which introduces nonnegligible noise in 3D parameter estimation, is redundant\u591a\u4f59\u7684 to perform 3D object detection.Furthermore 2D information can be naturally obtained if 3D variables and camera instrinsic matrix are already known.Consequently, our designed network eliminate \u6392\u9664 the 2D detection branch and estimates the projected 3D points on the image plane instead.A 3d parameter regression branch is added in parallel.This design results in a simple network structure with two estimation threads.Rather than regressing variables  in a separate method by use multiple loss functions, we transform these variables together with projected keypoint to 8 corner representation of 3D boxes and regress them with a unified\u7edf\u4e00\u7684 loss function. As in most single state 2D object detection algorithms, our 3D detection approach only contains one classification and regression branch.Benefiting from the simple structure , the network exhibits \u5c55\u793a improved accuracy in learning 3D variables, has better convergence and less overall computional needs.\n\n\nSecond constribution of our work is a multi-step disentanglement \u7ea0\u7f20 approach for 3D bounding box regression. Since all the geometry information is grouped  into the parameter, it is difficult for the network to learn each variable accurately n a unified way.Our proposed method isolates\u5206\u79bb the contribution of each parameter in both the 3D bounding box ecoding phase and regression loss function , which significantly helps to train the whole network effectively.\n\n\nOur contribution is summarized as follows:\n\n\n\n\nWe propose a one-stage monocular 3D object detection with a simple architecture that can precisely learn 3D geometry in an end-to-end fashion.\n\n\nWe provide a multistep distanglement approach to improve the convergence\u6536\u655b of 3D parameters and detection accuracy.\n\n\nThe result method outperforms all existing state-of-art monocular 3D object detection algorithms on the chanllenging KITTI dataset at the submission date November 2019.\n\n\n\n\n\n\nWe formulate\u5b9a\u4e49 the monocular 3D object detection problem as follow:\n\n\ngiven a single RGB image \n ,with \n the width and \n the height of the image, find for each present object its category\u7c7b\u522b label C and its 3D bounding box B, where the latter is parameterized by 7 variables \n .Here, \n represent the height, width and length of each object in meters , and \n is the coordinates (in meters) of the object center in the camera coordinate frame. Variable \n is the yaw orientation of the corresponding cubic box. The roll and pitch angles are set to zero by following the Kitti annotation .Additionally , we take the mild\u6e29\u548c assumption that the camera instrinsic matrix K is known for both training and inference.\n\n\nSmoke approach\n\n\nIn this section, we describe the smoke network that directly estimates 3D bounding boxes for detected object instances  from monocular imagery. In constrast to previous techniques that leverage \u6760\u6746\u4f5c\u7528 2D proposals to prediect 3D bounding box, our method can detect 3D information with a simple single stage.The propose method can be divided into three parts:(1) backbone,(2) 3D detection (3) loss function.First , we briefly discuss the backbone  for feature extraction , followed by the instruction of the the 3D detection network consisting of two separated branches. Finallly,we discuss the loss function design and the multi-step disentanglement to compute the regression loss.The overview of the network structure is depicted in Fig.2\n\n\nbackbone\n\n\nWe use a hierarchical layer fusion network DLA-34 as the backbone to extract features since it can aggregate information across different layers.Following the same structure as in [], all the hierarchical aggregation connections are replaced by a Deformable\u53ef\u53d8\u5f62\u7684 convolution network (DCN) The output feature map is downsample 4 times with respect to the original image.Compared with the original implementation, we replace all BatchNorm(BN) operation with GroupNorm(GN) since it has been proven to be less sensitive to batch size and more robust to training noise.We also use this technique in the two prediction branches.\n\n\n\n\nwhich will be discussed later.This adjustment not only improve detection accuracy , but it also reduces considerably the training time. Later ,we provide performance comparison of BN and GN to demonstrate  these properties.\n\n\n3D detection network\n\n\nKeypoint branch: We define the keypoint estimation network similar to such that each object is represented by one specific keypoint. Instead of identifying the center of a 2D bounding box , the key point is defined as the projected 3D center of the object on the image plane. The comparison between 2D center points and 3D projected points is visualized in Fig3.The projected keypoints allow to fully recover 3D location for each object with camera parameters. Let \n  represent the 3D center of each object in the camera frame.The projection of 3D points of points \n on the image plane can be obtained with the camera instrinsic matrix K in a homogeneous form:\n\n\n\n\n\\begin{bmatrix}z\\cdot x_c\\\\z\\cdot y_c\\\\z\\end{bmatrix}=K_{3\\times 3}\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}\\quad\\quad\\quad(1)\n\n\n\n\nFor each ground truth keypoint, its corresponding downsampled location on the feature map is computed and distributed using a Gaussian Kernel following.The standard deviation is allocated based on the 3D bounding boxes of the ground truth projected to the image plane.Each 3D box on the image is represented by 8 2D points \n and the standard deviation ia computed by the smallest 2D box with \n that encircles the 3D box.\n\n\nRegression Branch\n\n\n\\logeach key point on the heatmap. Similar to other monocular 3D detection framework.The 3D information is encoded as an 8-tuple\u5143\u7ec4 \n\n\n\n\n\n\\tau=\\begin{bmatrix}\\delta_z&\\delta_{x_c}&\\delta_{y_c}&\\delta_h&\\delta_w&\\delta_l&sin(a)&cos(a)\\end{bmatrix}^T\n\n\n\n\n\nHere \n  denotes the depth offset , \n is the discretization\u79bb\u6563\u5316 offset due to downsampling.\n denotes the resdual \u5269\u4f59 dimension. \n is the vectorial representation of the rotational angle \n . We encode all variables to be learnt in residual\u6b8b\u5dee representation to reduce the learning interval and ease the training task . The size of feature map for regression results in \n . Inspired by the lifting transformation\u63d0\u5347\u8f6c\u578b described  in [],we introduce a similar operation F that converts projected 3D points to a 3D bounding box \n .For each object, its depth $z$ can be recovered by pre-defined scale and shift parameters \n and \n as \n\n\nGiven the object depth $z$ , the location for each object in the camera frame can be recovered by using its discretized\u79bb\u6563\u7684 projected centroid $[x_c, y_c]^T$ on the image plane and the downsampling offset \n\n\n\n\n\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}=K^{-1}_{3\\times 3}\\begin{bmatrix}z\\cdot(x_c+\\delta_{x_c})\\\\z\\cdot(y_c+\\delta_{y_c})\\\\z\\end{bmatrix}\n\n\n\n\nThis equation is inverse of Eq.(1) In order to retrieve\u53d6\u56de object dimensions \n , we use a pre-calculated category-wise\u6309\u7c7b\u522b average dimension \n computed over the whole dataset. Each object dimension can be recovered by using the residual dimension offset \n :\n\n\n\n\n\\begin{bmatrix}h\\\\w\\\\l\\end{bmatrix}=\\begin{bmatrix}\\overline{h}\\cdot e^{\\delta_h}\\\\\\overline{w}\\cdot e^{\\delta_w}\\\\\\overline{l}\\cdot e^{\\delta_l}\\end{bmatrix}\\quad\\quad\\quad(4)\n\n\n\n\nInspired by [],we choose to regress the observation angle \u03b1 instead of the yaw rotation \u03b8 for each object. We further change the observation angle with respect to the object head \n ,instead of the commonly used observation angle value \n, by simply adding \n. The difference between these two angles is shown in Figure4.Moreover,each \n is encoded as the vector $[sin(a)\\quad cos(a)]^T$ The yaw angle $\\theta$ can be obtained by utilizing $\\alpha_z$ and the object location:\n\n\n\\theta=\\alpha + arctan\\left(\\frac{x}{z}\\right)\\qquad(5)\n\n\nFinally,we can construct the 8 corners of the 3D bounding box in the camera frame by using the yaw rotation matrix $R_\\theta$, object dimensions $[h\\quad w\\quad l]^T$ and location $[x\\quad y\\quad z]^T$:\n\nB=R_\\theta\\begin{bmatrix}\n    \\pm h/2\\\\\\pm w/2\\\\\\pm l/x\n\\end{bmatrix}+\\begin{bmatrix}\n    x\\\\y\\\\z\n\\end{bmatrix}\\qquad\\qquad(6)\n\n\n\n\nLoss function\n\n\nWe employ the penalty-reduce focal loss\u60e9\u7f5a\u51cf\u5c11\u7126\u70b9\u635f\u5931  [] in a point-wise manner on the downsampled heatmap.Let $s_{i,j}$ be the predicted score at the heatmap location $(i,j)$ and $y_{i,j}$ be the ground-truth value of each point assigned by Gaussian Kernel. Define $\\breve{y}\n{i,j}$ and $\\breve{s}\n{i,j}$ as :\n\n\n\\breve{y}_{i,j}=\\left\\{\\begin{matrix}\n    0,\\qquad if\\quad y_{i,j} =1\\\\y_{i,j}\n,\\qquad otherwise\\end{matrix}\\right.\n\n\n\n\\breve{s}_{i,j}=\\left\\{\\begin{matrix}\n    s_{i,j},\\qquad if\\quad y_{i,j}=1\\\\1-s_{i,j},\\qquad otherwise\n\\end{matrix}\\right.\n\n\n\n\nFor simplicity,we only consider a single object class here.Then , the classification loss function is constructed as\n\nL_{cls}=-\\frac{1}{N}\\sum^{h,w}_{i,j=1}(1-\\breve{y}_{i,j})^\\beta(1-\\breve{s}_{i,j})^\\alpha log(\\breve{s}_{i,j})\\qquad(7)\n\nwhere $\\alpha$ and $\\beta$ are tunable hyper-parameters and N is the number of keypoints per image.The term $(1-y_{i,j})$ corresponds to penalty reduction for points around the groundtruth location.\n\n\nRegression Loss\n\n\nWe regardless the 8D tuple $\\tau$ to construct the 3D bounding box for each object. We also add channel-wise action to the regressed parameters of dimension and orientation at each feature map location to preserve\u4fdd\u5b58 consistency\u4e00\u81f4\u6027 . The activation functions for the dimension and the orientation are chosen to be the sigmoid function $\\sigma$ and the $l_2$ norm ,respectively:\n\n\\begin{bmatrix}\n    \\delta_h\\\\\\delta_w\\\\\\delta_l\n\\end{bmatrix}=\\sigma\\left(\\begin{bmatrix}\n    O_h\\\\O_w\\\\O_l\n\\end{bmatrix}\\right)-\\frac{1}{2},\\quad\\begin{bmatrix}\n    sin(a)\\\\cos(a)\n\\end{bmatrix}=\\begin{bmatrix}\n    O_{sin}/\\sqrt{O^2_{sin}+O^2_{cos}}\\\\O_{cos}/\\sqrt{O^2_{sin}+O^2_{cos}}\n\\end{bmatrix}\n\nHere $o$ stands for the specific output of network. By adopting the keypoint lifting transformation introduced in Sec.4.2.we define the 3D bounding box regression loss as the $l_1$ distance between the predicted transform $\\hat{B}$ and the ground truth B:\n\nL_{reg}=\\frac{\\lambda}{N}||\\hat{B}-B||_1\\qquad(8)\n\nwhere $\\lambda$ is the scaling factor.This is used to ensure that neither the classification, nor the regression dominates\u4e3b\u5bfc the other.The disentangling transformation of loss has been proven to be an effective dynamic method to optimize 3D regression loss functions in [31] . Following this design , we extend the concept of loss disentanglement into a multi-step form. in Eq(3) , we use the projected 3D groundtruth points on the image plane $[x_c\\quad y_c]^T$ with the network predicted discretization offset $[\\hat{\\delta_{x_c}}\\quad \\hat{\\delta_{y_c}}]^T$ and depth $\\hat{z}$ to retrieve the location $[\\hat{x}\\quad \\hat{y}\\quad \\hat{z}]^T$ of each object. In Eq(5), we use the groundtruth location $[x\\quad y\\quad z]^T$ and the predicted observation angle $\\hat{a}_z$ to construct the estimated yaw orientation $\\hat{\\theta}$. The 8 corners representation of the 3D bounding box is also isolated \u9694\u79bb into three different groups following the concept of disentanglement , namely orientation, dimension and location.The final loss function can be represented by:\n\nL = L_{cls} + \\sum^3_{i=1}L_{reg}(\\hat{B_i}),\\qquad(9)\n\nwhere $i$ represents the number of groups we define in the 3D regression branch.The multi-step disentangling transformation divides the contribution of each parameter group to the final loss. In Sec.5.2, we show that this method significantly improves detection accuracy.\n\n\nImplementation\n\n\nIn this section,we discuss the implementation of our proposed methodology in detail together with selection of the hyperparameters.\n\n\nPreprocessing\n : we avoid applying any complicated preprocessing method on the dataset.Instead, we only eliminate\u6392\u9664 objects whose 3D projected center point on the image plane is out of the image range. Note that the total number of projected center points outside the image boundary for the car instance is 1582.This accounts for only the $5.5$% of the entire set of 28742 labeled cars.\n\n\nData Augmentation\n : Data augmentation techniques we used are random horizontal flip\u6c34\u5e73\u7ffb\u8f6c, random scale and shift. The scale ratio is set to 9 steps from 0.6 to 1.4, and the shift ratio is set to 5 steps from -0.2 to 0.2. Note that the scale and shift augmentation methods are only used for heatmap classification since the 3D information becomes inconsistent\u4e0d\u4e00\u81f4 with data augmentation. \n\n\nHyperparameter Choice\n : In the backbone, the group number for GroupNorm is set to 32.For channels loss than 32, it is set to be 16. For Eq.7, we set $\\alpha=2$ and $\\beta=4$ in all experiments. Based on [31], the reference car size and depth statistic\u7edf\u8ba1\u6570\u636e we use are $[\\overline{h}\\quad \\overline{w}\\quad \\overline{l}]^T=[1.63\\quad 1.53\\quad 3.88]^T$ and $[\\mu_z\\quad \\sigma_z]^T=[28.01\\quad 16.32]^T$ (measured in meters). \n\n\nTrainning\n :Our optimization schedule\u4f18\u5316\u8868 is easy and straightforward. We use the original image resolution and pad it to $1280\\times 384$. We train the network with a batch size of 32 on 4 Geforce TITAN X GPUs for 60 epochs. The learning rate is set at $2.5\\times 10^{-4}$ and drops at 25 and 40 epochs by a factor of 10. During testing, we use the top 100 detected 3D projected points and filter it with a threshold of 0.25. No data augmentation method and NMS are used in the test precedure.Our implementation platform in Pytorch1.1, CUDA 10.0 and CUDNN 7.5.\n\n\nDepth Estimation from Monocular Images and Sparse Radar Data\n\n\nIn this page, we explore the possibilty of achieving a more accurate depth estimation by fusing monocular images and Radar points using a deep neural network.We give a comprehensive study of the fusion between RGB images and Radar measurements from different accepts and proposed a working solution based on obeservations. We find that the noise existing in Radar measurements is one of the main key reasons that prevents one from applying the existing fusion methods developed for\nfor LiDAR data and images to the new fusion problem between Radar data and images to the new fusion problem between Radar data and images.The experiments are conducted\u5b9e\u65bd on the nuScenes dataset, which is one of the first datasets which features Camera\uff0c Radar, and LiDAR recordings in diverse\u5404\u79cd\u5404\u6837\u7684 scenes and weather conditions. Extensive\u5e7f\u6cdb\u7684 experiments demonstrate that our method outperforms existing fusion methods. We also provide detailed ablation\u707c\u70e7 studies to show the effectiveness of each component in our method. \n\n\nDense and robust depth estimation is an important component in self-driving system and unmanned\u65e0\u4eba aerial vehicles. While existing structure-light-based depth sensor or stereo camera can provide dense depth in indoor environments, the reliability of these sensors degrade a lot in outdoor applications. As a result, lots of research works focus on obtaining dense depth from monocular RGB images only. Recently, convolutional neural network (CNN) based methods have demonstrated impressive improvement on monocular depth estimation for both indoor and outdoor scenarios.However , there is still a gap between the accuracy and reliability of these methods and what the real-world applications need.\n\n\nApart from estimating depth from monocular camera, to imporve the robustness of the system, some methods also take other sensor modalities\u5f62\u5f0f into consideration.While thses sensors,LiDARS is the most commonly used one. Many works have been conducted on dense depth estimation from RGB images and sparse LiDAR scans.In addition to depth estimation and completion tasks, different RGB + LiDAR fusion techniques are also extensively used in tasks such as 3D object detection. Although LiDAR provides more accurate depth measurements in outdoor scenario, high-end LiDAR sensors are still far from affordable for many applications.\n\n\nCompared with LiDAR, Radar is an automotive-grade sensor that has been used for decades on vehicles, but has not attracted lots of attention in self-driving research based on deep learning. One reason might be that Radar measurements are not included in most of the dominant self-driving datasets.Compared with LiDAR, Radar sensors offer longer sensing range(200m~300m), more attributes including velocities, dynamic states, and measurement uncertainties. Most importantly, the costs of these sensors are much lower than LiDAR.However, Radar measurements are much lower than LiDAR ,However, Radar measurement are typically sparser, noiser, and have a more limited vertical field of view.\n\n\nThis work is to study the chanllenges of using Radar data for dense depth estimation and to propse a novel method for that aim. Given recently released nuScenes dataset consisting of RGB, LiDAR and Radar measurements.we are able to conduct experiments on cross-modality sensor fusion between RGB camera and Radar.Through our experiments we demonstrated that:1)Existing RGB + LiDAR fusion methods can be applied directly to RGB + Radar fusion task; and 2) with proper fusion strategies and a novel denosing operation, our proposed network is able to improve the performance of depth estimation by a good margin by using Radar measurements. According to our survey, our work is the first one that brings Radar sensors into dense depth estimation tasks.\n\n\nThe contributions of this work include: 1) a detailed study on the challenges of using Radar data for dense depth estimation; and 2) a novel and carefully motivated network architecture for depth estimation with monocular images and sparse Radar data.\n\n\nRELATED WORKS\n\n\nRGB-based Depth Estimation. Depth estimation from monocular or stereo camera is a popular research topic in both computer vision and robotics.Early works used either geometry-based algorithms on stereo images, or handcrafted features on single images.Recent years, convolutional neural networks(CNN) have demonstrated their ability in image understanding, dense predictions ,etc. given large scale datasets. Therefore, lots of research works of monocular depth estimation are conducted.In general , most of them used the encoder-decoder architecturres. Xie . futher introduced skip connection strategy which is a frequently used technique to multi-level features in dense prediction tasks. On the other hand, Huang achieve state-of-the-art performance by introducing space increasing discretization (SID) and ordinal regression. In some semi-/self-supervised formu loss is used.\n\n\n\n\nand the smoothness constraint\u7ea6\u675f is further imposed to enhance local consistency\u4e00\u81f4\u6027.Patil proposed a recurrent\u53cd\u590d\u53d1\u4f5c network architecture to exploit the long-range spatiotemporal\u65f6\u7a7a structures across video frames to yeild more accurate depth maps.While good performance has been obtained with only RGB images, the methods still have difficulty in generalizing to new scenario and chanllenging weather and light conditions. This motivates the existing line of work that fuses camera data with Lidar data and our work that fuses camera data with radar data is cheaper to obtain.\n\n\nDepth Estimation with Camera and Lidar data\n\n\nwhile monocular depth estimation task attracts lots of attention, achieving more reliable and accurate predictions using multi-modality\u591a\u6a21\u6001 information is also a popular topic.Existing works either take the whole set of LiDAR points , (known as depth completion), or the downsampled set as model inputs.Ma first projected LiDAR points to 2D sparse depth map and then perform so called early fusion by direct concatenation with RGB images along channel, or concatenation feature maps after one shallow convolution block.J used a late fusion method to combine features from different modalities and improved the overall performance through multitask learning.Q proposed to predict dense depth map by combining predictions from RGB and surface normal pathways, where surface normal is treated as an intermediate representation. Moreover, confidence maps are predicted to down-weight mixed measurements from LiDAR caused by the displacement between camera and LiDAR. In this work our main focus is sensor fusion . Thus , we use the widely adopted encoder-decoder architecture and focus on the necessary extensions in order to effectively use Radar data instead.\n\n\nPost-processing and Refinement Methods\n\u540e\u5904\u7406\u548c\u4f18\u5316\u65b9\u6cd5. Apart from treating sparse point clouds as inputs to the model, some methods also tried to directly refine the dense predictions of the trained models.W proposed a simple add-on module that can improve the prediction of depth estiamtion model using similar methods used by white box adversarial attack. Since the refinement is done using iterative re-inference, no re-training is required.This method can be integrated into most deep learning based methods.Cheng learned an affinity \u4eb2\u548c\u529b matrix from data to refine the outputs of their CNN model.The recurrent refinement  operation can also be extended to depth completion tasks.\n\n\nFusion of images and radar Data.\n\n\nThere are already works that fuse RGB images and Radar.given the fact that they are very much complementary\u8865\u5145.The line of work mainly focus on object detection and tracking.For instance,C fused\u878d\u5408 Radar data and images to detect small objects at a large distance. In [] and [] the authors enhance current 2D object detection networks by fusing camera data and projected sparse radar data in the network layers\uff0cwhile [] also performs free space semantic segmentation jointly.Both methods learn at which level the fusion of the sensor data is more beneficial for the task. There are also other datasets proposed for object detection with Radar data such as [].Exemplary\u793a\u8303\u6027\u7684 works on semantic segmentation with Radar point cloud have been conducted as well. For instance, in [] the authors have studied how the challenging task can be performed and provide results on a dataset with manually labeled radar reflections.Similar to these works for object detection and semantic segmentation, our work aims to study how the challenging task of dense depth estimation with radar data can be addressed with the popular deep neural network architectures.\n\n\nIn the line of increasing the robustness of depth estimation, Vasudevan, et al. [] have proposed a novel method to estimate depth maps based on binaural\u53cc\u8033 sounds.\n\n\nOur Method\n\n\nOur whole method are divided into multiple main components. In the following section, we will go through each component in detail.\n\n\nRadar Data background\n\n\nDifferent from well established depth completion or depth estimation tasks.There is no prior research works on RGB + radar depth estimation task.Therefore, we provide a brief introduction to the task formulation and some key differences between Radar and LiDAR measurements,which will help readers to understand the motivations behind the components of our method.\n\n\nData format.\n similar to LiDAR data, Radar measurements are recorded as sparse point clouds. The main difference is that , in addition to $x,y,z$,and $reflectance$, Radar data consist of additional measurements including the velocity along x and y direction, the standard deviation of the location and velocity measurements.and information such as the dynamic states of the measured object (encoded as discrete\u79bb\u6563\u7684 numbers).\n\n\n\n\nLimitation\n . While it seems that the radar data provide more information, it also introduces the following limitations compared with LiDAR data:\n* sparseness:In nuScenes dataset , there are more than 3000 LiDAR points after projection to the camera.However, there are less than 100 Radar points after the projection.\n* Limited vertical field of view.Because of the limitation of the sensor,Radar measurements mainly concentrate in the central horizontal bin (similar heights) as shwon in Fig.2.\n* Noisy measurements:Due to multiple reflections (Radar multipath problem) or other reasons , we have many noisy measurements as shown in Fig.2.\n* Inconsistency\u524d\u540e\u4e0d\u4e00\u81f4 with LiDAR data:Apart from noisy measurements, which are considered as outliers, the 3D points of Radar and LiDAR representing the same object can also be different.Since we typically use LiDAR measurements as ground truth, even noise-free  Radar measurements are not perfect on the evaluation metrics.\n\n\nAs we will show in Section IV, using Radar depth maps directly as the input of off-the-shelf\u73b0\u6210\u7684 RGB+LiDAR depth completion /prediction models resulted in marginal\u8fb9\u7f18 improvements.\n\n\nProblem formulation\n. In our RGB+ Radar formulation,each data sample from the dataset contains\n\n\n(1) an RGB image $x_{RGB}$\n\n\n(2) a set of Radar measurements $R={r_n}^N_{n=1}$ from 3 nearest timestamps \n\n\n(3) a set of Lidar measurements $L={l_m}^M_{m=1}$ \n\n\nRadar measurements $R$ can be further projected to a single-channel 2D depth map $x_{Radar}$ using the perspective projection.Similarly,LiDAR measurements can be objected to 2D map y, which is treated as ground truth depth map in our experiments(section IV).Our model takes both $x_{RGB}$ and $x_{Radar}$ as inputs and predicts dense 2D depth map $\\tilde{y}$ which minimizes the metrics errors. Same as all the depth estimation /completion tasks, loss and metric error are computed over the pixels with ground truth measurements.\n\n\n\n\nCNN architecture\n\n\nBackbone\n. Since our goal is to perform a comprehensive\u7efc\u5408\u7684 study on different fusion methods, we need to choose an efficient and widely-used backbone. Thus, we fixed our backbone to Resnet18 , and explored different fusion methods based on it as a pilot\u98de\u884c\u5458 experiment. As illustrated in Fig.3, we apply different encoder fusion methods to the simple encoder-decoder architecture proposed by Ma, and compare their performance. According to the experiment (Section IV-C), late fusion and multi-layer fusion model have comparable performance.Therefore, we adopt\u91c7\u7eb3 late fusion as our main encoder backbone design in the following experiments for simplicity.\n\n\nTwo-Stage architecture\n As shown by the pilot experiments(section IV-C) , a simple encoder-decoder architecture have some improvements on RGB  + Radar depth prediction task if we can remove most of the noisy measurements. However, we don't have LiDAR ground truth to help us performing the filtering in real applications.and it's hard to perform outlier rejections\u5f02\u5e38\u5254\u9664 without information on the 3D structure or objects of the scene.Therefore, we come up with a 2-stage design to address the noisy measurement issue.\n\n\nAs shown in Fig.1, our whole method contains two stages. The stage 1 model $f_{stage1}$ takes both RGB image $x_{RGB}$ and the radar depth map $x_{Radar}$ as inputs and predicts a coarse\u7c97 depth map $\\tilde{y}_{stage1}$ ,which gives us a dense 3D structure of the scene:\n\n\n\n\n\\tilde{y}_{stage1}=f_{stage1}(x_{RGB},x_{Radar})\\qquad\\quad(1)\n\n\n\n\n\nThen we compare the Radar depth map with the coarse prediction $\\tilde{y}\n{stage1}$ to reject some outliers (most details in next subsection) and obtain the filtered Radar depth map $\\tilde{x}\n{Radar}$. The assumption\u5047\u8bbe here is that although the predictions from stage 1 is not perfect, they are smooth and locally consistent. Therefore, they are suitable to reject outliner noises produced by Radar Multipath, which typically have certain margins with the correct depth values.\n\n\nThe stage2 model $f_{stage2}$ takes $x_{RGB}$ , $\\tilde{x}\n{Radar}$ , and the prediction from stage1 $\\tilde{y}\n{stage1}$ to predict the final result $\\tilde{y}$:\n\n\n\n\n\n\\tilde{y} = f_{stage_2}(x_{RGB},\\tilde{x}_{Radar},\\tilde{y}_{stage1})\\qquad(2)\n\n\n\n\n\nNoise filtering module\n\n\nSince Radar measurements are not exactly consistent with the LiDAR measurements as we mentioned in Section III-A .we need to have some tolerance\u516c\u5dee in the filtering process.Otherwise ,  we will end up discarding\u4e22\u5f03 all the measurements in the set $R$.\n\n\nInstead of setting a fixed distance tolerance threshold $\\tau$,we empirically\u51ed\u7ecf\u9a8c found that an adaptive threshold gives us better results. We design the threshold to be a function of depth value $\\tau(d)$ : We have larger tolerance for larger depth values, which is similar to the space-increasing disrectization (SID) from Huan et al.[2]:\n\n\n\n\n\n\\tau(d)=exp\\left(\\frac{d*log(\\frac{\\beta}{\\alpha})}{K}+log(\\alpha)\\right)\\qquad(3)\n\n\n\n\n\nhere we heuristically\u542f\u53d1\u5f0f\u7684 set $\\alpha=5$ and $\\beta=18$.\n\n\nLet $P$ denote the set pixel coordinates $(u,v)$ of the Radar measurements projected by the perspective projection function $proj(.)$ : $P={p_n}^N_{n=1}={proj(r_n)^N_{n=1}}$. The noise filtering module will keep the point $p_n$ if it satisfies the following constraint\u7ea6\u675f:\n\n\n\n\n\n|x_{Radar}(p_n)-\\tilde{y}_{stage1}(p_n)|\\leq\\tau(p_n),for\\quad p_n\\quad in\\quad P\\qquad(4)\n\n\n\n\n\nLoss Function\n\n\nBy design, each component of our model is differentiable. Thus, our whole model is end-to-end trainable.Following the setting from [7] ,We apply $L1$ loss to both the predictions of stage1 ($\\tilde{y}\n{stage1}$) and stage2 ($\\tilde{y}$). Considering that the main purpose of $\\tilde{y}\n{stage1}$ is to filter outlier noises in $x_{Radar}$, we futher add edge-aware smoothness constraint to it . To effectively balance multiple loss terms, we follow the method proposed by Kendall\n\n\n\n\n\nL_{total} = e^{-w1}*(L1(\\tilde{y}_{stage1},y)+10^{-3}*L_{smooth})+e^{-w2}*L1(\\tilde{y},y) + \\sum_{i}w_i\\qquad(6)\n\n\n\n\n\nwhere $w1$ and $w2$ are optimized variables ,and $L_{smooth}$ is defined as :\n\n\nL_{smooth}=|\\triangledown_u(\\tilde{y}_{stage1})|e^{-|\\triangledown_u(x_{RGB})|} + |\\triangledown_v(\\tilde{y}_{stage1})|e^{-|\\triangledown_v(x_{RGB})|}\\qquad(8)\n\n\n\n\n\n$\\triangledown_u$ and $\\triangledown_v$ denote the gradient along 2D height and width directions sperately.\n\n\nImplementation details\n\n\nUnless stated otherwise, all the models are trained using a batch size of 16 and SGD optimizer with a learning rate of $0.001$ and a momentum\u52bf\u5934  of $0.9$ for 20 epochs. The learning rate is multiplied by 0.1 alter every 5 epochs.All the models we used in experiment action are implemented in PyTorch.The experiments are conducted on the desktop computers/clusters with Nvidia GTX1080Ti and TeslaV100 GPUs.\n\n\nAll of our encoder network architectures (section III-B) are modified from the standard Reset18. For early fusion, we simply modified the input channels to 4 and randomly initialized the weights (weights of other layers were initialized from pre-trained models on Imagenet dataset). For mid, late, and multi-layer fusion , the depth branch has a similar architecture as the RGB branch.The only difference is that we change the number of channels to 1/4 of the original one. Fusion operations happened only on feature maps with same spatial resolution (width and height) . Regarding the decoder part, we kept the section from [] by using UpProj module as our upsampling operation.\n\n\nExperiment\n\n\nIn an early fusion approach, the raw or pre-processed sensory data from different sensor modalities are fused together. With this approach, the network learns a joint representation from the sensing modalities. Early fusion methods are usually sensitive to spatial or temporal misalignment\u9519\u4f4d of the data. On the other hand, a late fusion approach combines the data from different modalities at the decision level, and provides more flexibility \u7075\u6d3b\u6027 for introducing new sensing modalities to the network.However a late fusion approach does not exploit\u5f00\u53d1\u5229\u7528 the full potential of the available sensing modalities , as it does not acquire the intermediate\u4e2d\u95f4\u7684 features obtained by learning a joint representation.A compromise between the early and late fusion approaches is referred to as middle fusion. It extracts features from different modalities individually and combines them at an intermediate stage, enabling the network to learn joint representations and creating a balance between sensitivity and flexibility.\n\n\nWe propose CenterFusion, a middle-fusion approach to exploit radar and camera data for 3D object detection. CenterFusion focuses on associating radar detections to preliminary\u521d\u6b65\u7684 detection results obtained from the image, then generates radar feature maps and uses it in addition to image features to accurately estimate 3D bounding boxes for objects.Particularly, we generate perliminary 3D detections using a key point detection network,and propose a novel frustum-based \u57fa\u4e8e\u89c6\u9525 radar association method to accurately associate radar detections to their corresponding objects in 3D space. These radar detections are then mapped to the image plane and used to create feature maps to complement the image-based features.Finally, the fused features are used to accurately estimate object's 3D properties such as depth, rotation and velocity. The network architecture for CenterFusion is shown in Fig.1\n\n\nWe evaluate CenterFusion on the challenging nuscenes dataset\n,where it outperforms all previous camera-based object detection methods in the 3D object detection benchmark. We also show that exploiting radar information significantly improves velocity estimation for object without using any temporal information.\n\n\nRadar Point Cloud\n\n\nRadars are active sensors that transmit radio waves to sense the environment and measure the reflected waves to determine the location and velocity of objects. Automotive radars usually report the detected objects as 2D points in BEV, providing the azimuth\u65b9\u4f4d\u89d2 and radial distance to the object. For every detection, the radar also reports the instantaneous velocity of the object in the radial direction.This radial velocity does not necessarily match the object's actual velocity vector in it's direction of movement. Fig.2 illustrates the difference between the radial as reported by the radar, and actual velocity of the object in the vehicle's coordinate system.\n\n\nWe represent each radar detection as a 3D point in the egocentric coordinate system, and parameterize it as $P=(x,y,z,v_x,v_y)$ where $(x,y,z)$ is the position and $(v_x,v_y)$ is the reported radial velocity of the object in the $x$ and $y$ directions. The radial velocity is compensated by the ego vehicle's motion.For every scene,we aggregate 3 sweeps of the radar point cloud (detections within the past 0.25 seconds). The nuscenes dataset provides the calibration parameters needed for mapping the radar point clouds from the radar coordinates system to the egocentric and camera coordinate systems.\n\n\nCenterNet\n\n\nCenterNet represents the state-of-the-art in 3D object detection using single camera. It takes an image $I \\in \\mathbb{R}^{W\\times H\\times 3}$ as input and generates a keypoint heatmap $\\hat{Y}\\in[0,1]^{\\frac{W}{R}\\times\\frac{H}{R}\\times C}$ as output where $W$ and $H$ are the image width and height.$R$ is the downsampling ratio and $C$ is the number of object categories\u7c7b\u522b. A prediction of $\\hat{Y}\n\n{x,y,c} = 1$ as the output indicates a detected object of class $c$ centered at position $(x,y)$ on the image. The ground-truth heatmap $Y\\in [0, 1]^{\\frac{W}{R}\\times\\frac{H}{R}\\times C}$ is generated from the ground-truth 2D bounding boxes using a Gaussian kernel.For each bounding box center point $p_i\\in R^2$ of class $c$ in the image, a Gaussian heatmap is generated on $Y\n{:,:,c}$ , the final value of Y for class $c$ at position $q\\in R^2$ is defined as \n\n\n\n\nY_{qc} = \\underset{i}{max}exp\\left(-\\frac{(p_i-q)^2}{2\\sigma^2_i}\\right)\n\n\n\n\n\n\nFigure 3,Frustum\u89c6\u9525 association. An object detected using the image features(left),generating the ROI frustum based on object's 3D bounding box(middle), and the BEV of the ROI frustum showing radar detections inside the frustum (right).$\\delta$ is used to increase the frustum size in the testing phase.$\\hat{d}$ is the ground_truth depth in the training phase and the estimated object depth in the testing phase\n\n\n\n\nExpanding radar points to 3D pillars( top image). Directly mapping the pillars to the image and replacing with radar depth information results in poor association with object's center and many overlapping depth values(middle image),Frustum association accurately maps the radar detections to the center of objects and minimizes overlapping(bottom image). Radar detections are only associated to objects with a valid ground truth or detection box, and only if all or part of the radar detection pillar\u652f\u67f1 is inside the box.Frustum association also prevents associating radar detections caused by background objects such as buildings to foreground objects, as seen in the case of pedestrains on the right hand side of the image\n\n\nwhere $\\sigma_i$ is a size-adaptive\u5c3a\u5bf8\u9002\u5e94 standard deviation\u504f\u5dee,controlling the size of the heatmap for every object based on its size. A fully convolutional encode-decoder network is used to predict $\\hat{Y}$.\n\n\nTo generate 3D bounding boxes,sperate network heads are used to regress object's depth, dimensions and orientation directly from the detected center points.Depth is calculated as an additional output channel $\\hat{D}\\in [0,1]^{\\frac{W}{R}\\times\\frac{H}{R}}$ after applying the inverse sigmoidal transformation used in Eigen et al. to the original depth domain. The object dimensions are directly regressed to their absoluate values in meter as three output channels $\\hat{\\Gamma}\\in[0,1]^{\\frac{W}{R}\\times\\frac{H}{R}\\times3}$ .Orientation is encoded as two bins with 4 scalars in each bin, following the orientation representation in M. For each center point, a local offset is also predicted to compensate\u8865\u507f for the discretization\u79bb\u6563\u5316 error caused by the output strides in the backbone network.\n\n\nGiven the annotated objects $p_0, p_1, ...$ in an image,  the training objective is defined as below based on the focal loss:\n\nL_k=\\frac{1}{N}\\sum_{xyc}\\left\\{\\begin{matrix}(1-\\hat{Y}_{xyc})^{\\alpha}log(\\hat{Y}_{xyc})\\qquad Y_{xyc}=1\\\\\\\\ (1-Y_{xyc})^{\\beta}(\\hat{Y}_{xyc})^\\alpha log(1-\\hat{Y}_{xyc})\\qquad otherwise\\end{matrix}\\right.\n\n\n\n\nwhere $N$ is the number of objects, $Y\\in [0,1]^{\\frac{W}{R}\\times\\frac{H}{R}\\times C}$ is the annotated object's ground-truth heatmap and $\\alpha$ and $\\beta$ are focal loss hyperparameters.\n\n\nDetection identifies\u8bc6\u522b objects as axis-aligned boxes in an image.Most successful object detectors enumerate\u5217\u4e3e a nearly exhaustive\u8be6\u5c3d\u7684 list of potential object locations and classify each.This is wasteful, inefficient,and requires additional post-processing. In this paper, we take a different approach.We model an object as a single point - the center point of its bounding box.Our detector uses keypoint estimation to find center points and regresses  to all other object properties, such as size, 3D location, orientation, and even pose. Our center point based approach,\nCenterNet, is end-to-end differnetiable, simpler,faster,and more accurate than corresponding bounding box based detectors.CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1% AP at 142 FPS. We use the same approach to estimate 3D bounding box in the KITTI benchmark and human pose on the COCO keypoint dataset. Our method performs competitively with sophisticated multi-stage methods and runs in real-time.\n\n\nObject detection powers many vision tasks like instance segmentation, pose estimation, tracking, and action recognition.It has down-stream applications in surveillance\u76d1\u89c6\uff0c autonomous driving, and visual question answering. Current object detectors represent each object through an axis-aligned bounding box, that tightly encompasses the object.They then reduce object detection to image classification of an extensive number of potential object bounding boxes.For each bounding box, the classifer determines if the image content is a specific object or background. One stage detectors, slide a complex arrangement of possible bounding boxes called anchors, over the image and classify them directly without specifying the box content.Two-stage detectors recompute image features for each potential box, then classify those features.Post-processing, namely non-maxima suppression, then removes duplicated detections for the same instance by computing bounding box IoU. This post-processing is hard to differentiate and train, hence most current detectors are not end-to-end trainable. Nonetheless, over the past five years, this idea has achieved good empirical success.Sliding window based object detectors are however a bit wasteful , as they need to enumerate all possible object locations and diemnsions.\n\n\nIn this paper, we provide a much simpler and more efficient alternative. We represent objects by a single point at their bounding box center (see Fig 2), Other properties such as object size, dimension, 3D extent, orientation, and pose are then regressed directly from image features at the center location. Object detection is then a standard keypoint estimation problem.We simply feed the input image to a fully convolutional network that generates a heatmap.Peaks in this heatmap correspond to object centers.Image features at each peak predict the objects bounding box height and weight. The model trains using standard dense supervised\u76d1\u7763 learning.Inference is a single network forward-pass, without non-maximal suppression for post-processing.\n\n\n\n\nWe model an object as the center point of its bounding box. The bounding box size and other object properties are inffered from the keypoint feature at the center.Best viewed in color\n\n\nOur method is general and can be extended to other tasks with minor effort. We provide experiments on 3D obect detection and multi-person human pose estimation, by predicting additional outputs at each center point (see Figure 4). For 3D bounding box estimation, we regress to the object absolute depth, 3D bounding box dimensions, and object orientation. For human pose estimation, we consider the 2D joint locations as offsets from the center and directly regress to them at the center point location.\n\n\n\n\n\nThe simplicity of our method, CenterNet, allows it to run at a very high speed.With a simple Resnet-18 and up-convolutional layers, our network runs at 142 FPS with 28.1% COCO bounding box AP. With a carefully designed keypoint detection network. DLA-34 , our network achieves 37.4%  COCO AP at 52 FPS.\n\n\nRelated work\n\n\nObject detection by region classfication.\n One of the first successful deep object detectors, RCNN, enumerates object location from a larget set of region candidates,crops them , and classifies each using a deep network. Fast-RCNN crops image features instead, to save computation. However, both methods rely on slow low-level region proposal methods.\n\n\nObject detection with implicit(\u9690\u542b\u7684) anchors.Faster RCNN generates region proposal within the detection network. It samples fixed-shape bounding boxes (anchors) around a low-resolution image grid and classifies each into \u201cforeground or not\u201d. An anchor is labeled foreground with a $>0.7$ overlap with any ground truth object, background with a $<0.3$ overlap, or ignored otherwise. Each generated region proposal is again classified. Changing the proposal classifier to a multi-class classification forms the basis of one-stage detectors. Several improvements to one-stage detectors include anchor shape points, different feature resolution, and loss re-weighting among different samples.\n\n\nOur approach is closely related to anchor-based one-stage approaches.A center point can be seen as a single shape-agnostic \u4e0d\u53ef\u77e5\u5f62\u72b6 anchor.(see Figure3) .However,there are a few important differences. First, our CenterNet assigns the \"anchor\" based solely\u809a\u5b50 on location, not box overlap. We have no manual thresholds for foreground and background classification. Second, we only have one positive \"anchor\" per object. and hence do not need NonMaximum Suppression(NMS). We simply extract local peaks in the keypoint heatmap. Third, CenterNet uses a larger output resolution (output stride of 4) compared to traditional object detectors(output stride of 16). This eliminates the need for multiple anchors.\n\n\nObject detection by keypoint estimation\n We are not the first to use keypoint estimation for object detection. CornerNet detects two bounding box corners as keypoints, while ExtremeNet detects the top-,left-,bottom-,right-most, and center points of all objects. Both these methods build on the same robust keypoint estimation network as our CenterNet.However, they require a combinatorial\u7ec4\u5408 grouping stage after keypoint detection, which significantly slows down each algorithm. Our CenterNet , on the other hand,simply extracts a single center point per object without the need for grouping or post-processing.\n\n\nMonocular 3D object detection\n 3D bounding box estimation powers autonomous driving . Deep3Dbox use a slow-RCNN style framework, by first detecting 2D objects and then feeding each object into a 3D estimation network. 3D RCNN adds an additional head to Faster-RCNN followed by a 3D projection. Deep Manta uses a coarse-to-fine\u4ece\u7c97\u5230\u7ec6 Faster-RCNN trained on many tasks. Our method is similar to one-stage version of Deep3Box or 3DRCNN. As such, CenterNet is much simpler and faster than competing methods.\n\n\n\n\nstandard anchor based detection. Anchors count as positive with an overlap IoU>0.7 to any object, negative with overlap IoU<0.3, or are ignored otherwise\n\n\n\n\nCenter point based detection.The center pixel is assigned to the object. Nearby points have a reduced negative loss. Object size is regressed\n\n\nPreliminary\n\n\nLet $I\\in R^{W\\times H\\times3}$ be an input image of width $W$ and height $H$. Our aim is to produce a keypoint heatmap $\\hat{Y}\\in[0,1]^{\\frac{W}{R}\\times\\frac{H}{R}\\times C}$, where $R$ is the output stride and $C$ is the number of keypoint types. Keypoint types include $C=17$ human joints in human pose estimation, or $C=80$ object categorical in object detection. We use the default output stride of $R = 4$ in literature. The output stride downsamples the output prediction by a factor R. A prediction $\\hat{Y}\n{x,y,c}=1$ corresponds to a detected keypoint, while $\\hat{Y}\n{x,y,c} =0$ is background. We use serveral different fully-convolutional encoder-decoder networks to predict $\\hat{Y}$ from an image $I$: A stacked hourglass network\u5806\u53e0\u7684\u6c99\u6f0f\u7f51\u7edc upconvolutional residual network (ResNet) and deep layer aggregation\u805a\u5408 (DLA). \n\n\nWe train the keypoint prediction network following Law and Deng. For each ground truth keypoint $p \\in R^2$ of class $c$ , we compute a low-resolution equivalent\u76f8\u7b49\u7684 $\\widetilde{p}= \\left\\lfloor\\frac{p}{R}\\right\\rfloor$. We then splat\u6454\u5f97\u75db all ground truth keypoints onto a heatmap $Y\\in [0,1]^{\\frac{W}{R}\\times\\frac{H}{R}\\times C}$ using a Gaussian kernel $Y_{xyc}=exp\\left(-\\frac{(x-\\widetilde{p}_x)^2+(y-\\widetilde{p}_y)^2}{2\\sigma^2_p}\\right)$, where $\\sigma_p$ is an object size-adaptive standard deviation\u5c3a\u5bf8\u9002\u5e94\u6807\u51c6\u504f\u5dee. If two Gaussians of the same class overlap, we take the element-wise maximum. The training objective is a penalty-reduced\u51cf\u5211 pixel-wise logistic regression with focal loss:\n\nL_k=\\frac{-1}{N}\\sum_{xyz}\\left\\{\\begin{matrix}\n    (1-\\hat{Y}_{xyc})^\\alpha log(\\hat{Y}_{xyc})\\qquad if\\quad Y_{xyc} = 1\\\\\\\\\n    (1-Y_{xyc})^\\beta(\\hat{Y}_{xyc})^\\alpha log(1-\\hat{Y}_{xyc}) \\qquad otherwise\n\\end{matrix}\\right.\n\n\n\n\nwhere $\\alpha$ and $\\beta$ are hyper-parameters of the focal loss, and $N$ is the number of keypoints in image $I$ . The normalization by $N$ is chosen as to normalize all positive focal loss instances to 1. We use $\\alpha=2$ and $\\beta=4$ in all our experiments,following Law and Debug.\n\n\nTo recover the discretization\u79bb\u6563\u5316 error caused by the output stride, we additionally predict a local offset $\\hat{O}\\in R^{\\frac{W}{R}\\times\\frac{H}{R}\\times2}$ for each center point. All classes $c$ share the same offset prediction.The offset is trained with an $L1$ loss\n\nL_{off} = \\frac{1}{N}\\sum_p\\left|\\hat{O}_{\\widetilde{p}}-\\left(\\frac{p}{R}-\\widetilde{p}\\right)\\right|\\qquad(2)\n\n\n\n\nThe supervision acts only at keypoints locations $\\widetilde{p}$, all other locations are ignored.\n\n\nIn the next section,we will show how to extend this keypoint estimator to a general purpose object detector.\n\n\nObjects as Points\n\n\nLet $(x_1^{(k)},y_1^{(k)},x_2^{(k)},y_2^{(k)})$ be the bounding box of object $k$ with category $c_k$. Its center point is lies at $p_k=(\\frac{x_1^{(k)}+x_2^{(k)}}{2},\\frac{y_1^{(k)}+y_2^{(k)}}{2})$. We use our keypoint estimator $\\hat{Y}$ to predict all center points. In addition, we regress to the object size $s_k=(x_2^{(k)}-x_1^{(k)},y_2^{(k)}-y_1^{(k)})$ for each object $k$.To limit the computational burden\u8ba1\u7b97\u8d1f\u62c5 , we use a single size prediction $\\hat{S} \\in R^{\\frac{W}{R}\\times\\frac{H}{R}\\times C}$ for all object categories. We use an $L1$ loss at the center point similar to Objective 2:\n\nL_{size}=\\frac{1}{N}\\sum^N_{k=1}\\left|\\hat{S}_{p_k}-s_k\\right|\\qquad(3)\n\n\n\n\nWe do not normalize\u5f52\u4e00\u5316 the scale and directly use the raw pixel coordinates. We instead scale the loss by a constant $\\lambda_{size}$.The overall training objective is \n\n\n\n\nL_{det} = L_k + \\lambda_{size}L_{size} + \\lambda_{off}L_{off}\\qquad(4)\n\n\n\n\nWe set $\\lambda_{size}=0.1$ and $\\lambda_{off}=1$ in all our experiments unless specified otherwise. We use a single network to predict the keypoints $\\hat{Y}$, offset $\\hat{O}$ , and size $\\hat{S}$ The network predicts a toatal of $C + 4$ outputs at each location. All outputs share a common fully-convolutional backbone network. For each modality, the features of the backbone are then passed through a separate 3x3 convolution, ReLU and another 1x1 convolution. Figure4 shows an overview of the network output. Section 5 and supplementary material contain additional architectural details.\n\n\nFrom points to bounding boxes\n\nAt inference time, we first extract the peaks in the heatmap for each category independently. We detect all responses whose value is greater or equal to its 8-connected neighbors and keep the top 100 peaks. Let $\\hat{P}\nc$ be the set of $n$ detected center points $\\hat{P} ={(\\hat{x}_i,\\hat{y}_i)}^n\n{i=1}$ of class $c$. Each keypoint location is given by an integer coordinates $(x_i, y_i)$. We use the keypoint values $\\hat{Y}_{x_iy_ic}$ as a measure of its detection confidence, and produce a bounding box at location.\n\n(\\hat{x}_i+ \\delta\\hat{x}_i-\\hat{w}_i/2, \\hat{y}_i+\\delta\\hat{y}_i-\\hat{h}_i/2,\\hat{x}_i+\\delta\\hat{x}_i+\\hat{w}_i/2,\\hat{y}_i+\\delta\\hat{y}_i+\\hat{h}_i/2)\n\n\n\n\n\n\nOutputs of our network for different tasks: top for object detection, middle for 3D object detection,bottom for pose estimation. All modalities are produced from a common backbone, with a different 3x3 and 1x1 output convolutions separated by a ReLU. The number in brackets indicates the output channels. See section 4 for details\n\n\nwhere $(\\delta\\hat{x}\ni,\\delta\\hat{y}_i)=\\hat{O}\n{\\hat{x}\ni,\\hat{y}_i}$ is the offset prediction and $(\\hat{w}_i,\\hat{h}_i) = \\hat{S}\n{\\hat{x}_i,\\hat{y}_i}$ is the size prediction.",
            "title": "Radar camera fusion"
        },
        {
            "location": "/radar_camera_fusion/#radar-camera-fusion",
            "text": "\u6211\u4eec\u7684\u65b9\u6cd5\u79f0\u4e3aCenterFusion\uff0c\u5b83\u9996\u5148\u4f7f\u7528\u4e2d\u5fc3\u70b9\u68c0\u6d4b\u7f51\u7edc\u901a\u8fc7\u5728\u56fe\u50cf\u4e0a\u8bc6\u522b\u5bf9\u8c61\u7684\u4e2d\u5fc3\u70b9\u6765\u68c0\u6d4b\u5bf9\u8c61\u3002  \u7136\u540e\u5b83\u4f7f\u7528\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u89c6\u9525\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u5173\u952e\u6570\u636e\u5173\u8054\u95ee\u9898\uff0c\u4ece\u800c\u5c06\u96f7\u8fbe\u68c0\u6d4b\u7ed3\u679c\u4e0e\u76f8\u5bf9\u5e94\u5bf9\u8c61\u7684\u4e2d\u5fc3\u70b9\u76f8\u5173\u8054\u3002  \u76f8\u5173\u7684radar\u68c0\u6d4b\u7528\u4e8e\u751f\u6210\u57fa\u4e8eradar\u7684\u7279\u5f81\u56fe\u4ee5\u8865\u5145\u56fe\u50cf\u7279\u5f81\uff0c\u5e76\u56de\u5f52\u5230\u8bf8\u5982\u6df1\u5ea6\uff0c\u65cb\u8f6c\u548c\u901f\u5ea6\u4e4b\u7c7b\u7684\u5bf9\u8c61\u5c5e\u6027\u3002  radar\u4f7f\u7528\u591a\u666e\u52d2\u6548\u5e94\u5feb\u901f\u51c6\u786e\u7684\u786e\u5b9a\u7269\u4f53\u7684\u901f\u5ea6\u3002",
            "title": "radar camera fusion"
        },
        {
            "location": "/radar_camera_fusion/#_1",
            "text": "\u4f7f\u7528\u5bf9\u8c61\u76842d\u8fb9\u6846\u6781\u5176\u4f30\u8ba1\u7684\u6df1\u5ea6\u548c\u5927\u5c0f\uff0c\u4e3a\u8be5\u5bf9\u8c61\u521b\u5efa3d\u5174\u8da3\u533a\u57df\uff0c\u89c6\u9525\u4f53\u3002  \u5728roi\u5185\u79bb\u56fe\u50cf\u4e2d\u5fc3\u70b9\u6700\u8fd1\u7684\u70b9\uff0c\u5173\u8054\u3002   \u5176\u4e2d \u662f\u5c3a\u5bf8\u81ea\u9002\u5e94\u6807\u51c6\u504f\u5dee\u3002  For autonomous robots to navigate a complex environment , it is crucial \u81f3\u5173\u91cd\u8981\u7684 to understand the surrounding scene both geometrically and semantically.Modern autonomous robots employ multiple sets of sensors, including lidars, radars, and cameras. Managing the different reference frames and characteristics of the sensors, and merging their observations into a single representation complicates perception.  Our approach consists of an ensemble of neural networks which take in sensor data from different modalities \u5f62\u5f0f and transform them into a single common top-down semantic grid representation.We find representation favourable as it is agnostic\u65e0\u5173 to sensor-specific reference frames and captures both the semantic and geometric information for the surrounding scene.  Because the modalities share a single output representation,they can be easily aggregated \u6c47\u603bto produce a fused output.In this work we predict short-term semantic grids but the framework can be extended to other tasks.This approach offers a simple, extensible , end to end approach for multi-modal perception and prediction.  The benifits of a shared top-down representation across modalities are threefold. First, it is an interpretable representation that better facilities\u4fc3\u8fdb debugging\u8c03\u8bd5 and reasoning\u63a8\u7406 about inherent\u56fa\u6709 failure modes\u6545\u969c\u6a21\u5f0f of each modality.Second it is independent of any particular sensors characteristics and so is easily extensible for adding new modalities.Finally , it simplifies the task of late fusion by sharing a spatial\u7a7a\u95f4\u7684 representation in a succinct manner.  In this work we present a novel end-to-end framework that predicts the top-down view of the current scene($t_0$) as well as multiple timesteps into the future.The pipleline consists of a convolutional neural network for each of three sensor modalities : lidar, radar , camera.Each sensor modality predicts a sequence of top-down semantic grids, then these outputs are fused to produce a single output grid.We explore fusing using two different aggregation mechanisms.  Estimating 3D orientation and translation of objects is essential for infrastructure-less .In case of monocular vision, successful methods have been mainly based on two ingredients\u56e0\u7d20: \uff081\uff09a network generating 2D region proposals(2D\u533a\u57df\u63d0\u6848)\u3002(2) a R-CNN structure predicting 3D object pose by utilizing the acquired regions of interest. We argue that the 2D detection network is redundant\u591a\u4f59\u7684 and introduces non-negligible \u4e0d\u53ef\u5ffd\u7565\u7684 noise for 3D detection.Hence,we propose a novel 3D object detection method, named SMOKE, in this paper that predicts a 3D bounding box for each detected object by combining a single keypoint estimate with regressed 3D   variables.As a second contribution, we propose a multi-step disentangling approach for constructing the 3D bounding box ,which significantly improves both training convergence\u6536\u655b and detection accuracy. In constract to previous 3D detection techniques, our method does not require complicated pre/post-processing.extra data, and a refinement\u7ec6\u5316 stage. Despite of its structural\u7ed3\u6784 simplicity\uff0c our proposed SMOKE network outperforms all existing monocular 3D detection methods on the KITTI dataset , giving the best state-of-the-art result on both 3D objection dection and bird's eye view evaluation . The code will be made publicly available.  Vison based object detection is an essential ingredient\u6210\u5206 of autonomous vehicle perception \u6d1e\u5bdf\u529b of autonomous vehicle perception and infrastructure\u57fa\u7840\u8bbe\u65bd less robot navigation in general. This type of detection methods are used to perceive \u611f\u77e5 the surrounding environment by detecting and classifying object instances\u5b9e\u4f8b into categories \u7c7b\u522b and identifying their locations and orientations. Recent developments in 2D object detection have achieved promising performance\u6709\u524d\u9014\u7684 on both detection accuracy and speed. In constract, 3D object detection have proven to be a more challenging task as it aims to estimate pose and location for each object simulataneously.  Currently,the most successful 3D object detection methods heavily depend on Lidar point cloud.  or LIDAR-Image fusion information.(features learned from the point cloud are key components of the detection network).However , LIDAR sensors are extremely expensive , have a short service life time and too heavy for autonomous robots. Hence LIdars are currently not considered to be econnomical\u6d41\u884c\u7684 to support autonomous vehicle operations . Alternatively , cameras are cost-effective, easily mountable and light-weight solutions for 3D object detection with long expected service time. Unlike lidar senors , a single camera in itself cannot obtain sufficient spatial information for the whole environment as single RGB images can not supply object location information or dimensional contour \u8f6e\u5ed3 in the real world. While binocular \u53cc\u76ee vision restores the missing spatial information. in many robot applications, especially UAVs , it is difficult to realize biocular vision. Hence , it is desirable to perform 3D detection on a monocular  image even if it is a more difficult and chanllenging task.  To enhance performance, geometry reasoning \u51e0\u4f55\u63a8\u7406 synthetic data \u7efc\u5408\u6570\u636e and post 3D-2D processing have also been used to improve 3D object detection on single image.By the knowledge of the authors , no reliable monocular 3D detection method has been introduced so far to learn 3D information directly from image plane avoiding the performance decrease that is inevitable\u4e0d\u53ef\u907f\u514d\u7684 with multi-stage method.   In this paper we propose an innovative\u521b\u65b0 single-stage 3D object detection method that pairs each object with a single keypoint. We argue and later show that a 2D detection, which introduces nonnegligible noise in 3D parameter estimation, is redundant\u591a\u4f59\u7684 to perform 3D object detection.Furthermore 2D information can be naturally obtained if 3D variables and camera instrinsic matrix are already known.Consequently, our designed network eliminate \u6392\u9664 the 2D detection branch and estimates the projected 3D points on the image plane instead.A 3d parameter regression branch is added in parallel.This design results in a simple network structure with two estimation threads.Rather than regressing variables  in a separate method by use multiple loss functions, we transform these variables together with projected keypoint to 8 corner representation of 3D boxes and regress them with a unified\u7edf\u4e00\u7684 loss function. As in most single state 2D object detection algorithms, our 3D detection approach only contains one classification and regression branch.Benefiting from the simple structure , the network exhibits \u5c55\u793a improved accuracy in learning 3D variables, has better convergence and less overall computional needs.  Second constribution of our work is a multi-step disentanglement \u7ea0\u7f20 approach for 3D bounding box regression. Since all the geometry information is grouped  into the parameter, it is difficult for the network to learn each variable accurately n a unified way.Our proposed method isolates\u5206\u79bb the contribution of each parameter in both the 3D bounding box ecoding phase and regression loss function , which significantly helps to train the whole network effectively.  Our contribution is summarized as follows:   We propose a one-stage monocular 3D object detection with a simple architecture that can precisely learn 3D geometry in an end-to-end fashion.  We provide a multistep distanglement approach to improve the convergence\u6536\u655b of 3D parameters and detection accuracy.  The result method outperforms all existing state-of-art monocular 3D object detection algorithms on the chanllenging KITTI dataset at the submission date November 2019.    We formulate\u5b9a\u4e49 the monocular 3D object detection problem as follow:  given a single RGB image   ,with   the width and   the height of the image, find for each present object its category\u7c7b\u522b label C and its 3D bounding box B, where the latter is parameterized by 7 variables   .Here,   represent the height, width and length of each object in meters , and   is the coordinates (in meters) of the object center in the camera coordinate frame. Variable   is the yaw orientation of the corresponding cubic box. The roll and pitch angles are set to zero by following the Kitti annotation .Additionally , we take the mild\u6e29\u548c assumption that the camera instrinsic matrix K is known for both training and inference.",
            "title": "\u89c6\u9525\u4f53\u5173\u8054\u673a\u5236"
        },
        {
            "location": "/radar_camera_fusion/#smoke-approach",
            "text": "In this section, we describe the smoke network that directly estimates 3D bounding boxes for detected object instances  from monocular imagery. In constrast to previous techniques that leverage \u6760\u6746\u4f5c\u7528 2D proposals to prediect 3D bounding box, our method can detect 3D information with a simple single stage.The propose method can be divided into three parts:(1) backbone,(2) 3D detection (3) loss function.First , we briefly discuss the backbone  for feature extraction , followed by the instruction of the the 3D detection network consisting of two separated branches. Finallly,we discuss the loss function design and the multi-step disentanglement to compute the regression loss.The overview of the network structure is depicted in Fig.2",
            "title": "Smoke approach"
        },
        {
            "location": "/radar_camera_fusion/#backbone",
            "text": "We use a hierarchical layer fusion network DLA-34 as the backbone to extract features since it can aggregate information across different layers.Following the same structure as in [], all the hierarchical aggregation connections are replaced by a Deformable\u53ef\u53d8\u5f62\u7684 convolution network (DCN) The output feature map is downsample 4 times with respect to the original image.Compared with the original implementation, we replace all BatchNorm(BN) operation with GroupNorm(GN) since it has been proven to be less sensitive to batch size and more robust to training noise.We also use this technique in the two prediction branches.   which will be discussed later.This adjustment not only improve detection accuracy , but it also reduces considerably the training time. Later ,we provide performance comparison of BN and GN to demonstrate  these properties.",
            "title": "backbone"
        },
        {
            "location": "/radar_camera_fusion/#3d-detection-network",
            "text": "Keypoint branch: We define the keypoint estimation network similar to such that each object is represented by one specific keypoint. Instead of identifying the center of a 2D bounding box , the key point is defined as the projected 3D center of the object on the image plane. The comparison between 2D center points and 3D projected points is visualized in Fig3.The projected keypoints allow to fully recover 3D location for each object with camera parameters. Let    represent the 3D center of each object in the camera frame.The projection of 3D points of points   on the image plane can be obtained with the camera instrinsic matrix K in a homogeneous form:   \\begin{bmatrix}z\\cdot x_c\\\\z\\cdot y_c\\\\z\\end{bmatrix}=K_{3\\times 3}\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}\\quad\\quad\\quad(1)   For each ground truth keypoint, its corresponding downsampled location on the feature map is computed and distributed using a Gaussian Kernel following.The standard deviation is allocated based on the 3D bounding boxes of the ground truth projected to the image plane.Each 3D box on the image is represented by 8 2D points   and the standard deviation ia computed by the smallest 2D box with   that encircles the 3D box.",
            "title": "3D detection network"
        },
        {
            "location": "/radar_camera_fusion/#regression-branch",
            "text": "\\logeach key point on the heatmap. Similar to other monocular 3D detection framework.The 3D information is encoded as an 8-tuple\u5143\u7ec4    \n\\tau=\\begin{bmatrix}\\delta_z&\\delta_{x_c}&\\delta_{y_c}&\\delta_h&\\delta_w&\\delta_l&sin(a)&cos(a)\\end{bmatrix}^T   Here    denotes the depth offset ,   is the discretization\u79bb\u6563\u5316 offset due to downsampling.  denotes the resdual \u5269\u4f59 dimension.   is the vectorial representation of the rotational angle   . We encode all variables to be learnt in residual\u6b8b\u5dee representation to reduce the learning interval and ease the training task . The size of feature map for regression results in   . Inspired by the lifting transformation\u63d0\u5347\u8f6c\u578b described  in [],we introduce a similar operation F that converts projected 3D points to a 3D bounding box   .For each object, its depth $z$ can be recovered by pre-defined scale and shift parameters   and   as   Given the object depth $z$ , the location for each object in the camera frame can be recovered by using its discretized\u79bb\u6563\u7684 projected centroid $[x_c, y_c]^T$ on the image plane and the downsampling offset    \\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}=K^{-1}_{3\\times 3}\\begin{bmatrix}z\\cdot(x_c+\\delta_{x_c})\\\\z\\cdot(y_c+\\delta_{y_c})\\\\z\\end{bmatrix}   This equation is inverse of Eq.(1) In order to retrieve\u53d6\u56de object dimensions   , we use a pre-calculated category-wise\u6309\u7c7b\u522b average dimension   computed over the whole dataset. Each object dimension can be recovered by using the residual dimension offset   :   \\begin{bmatrix}h\\\\w\\\\l\\end{bmatrix}=\\begin{bmatrix}\\overline{h}\\cdot e^{\\delta_h}\\\\\\overline{w}\\cdot e^{\\delta_w}\\\\\\overline{l}\\cdot e^{\\delta_l}\\end{bmatrix}\\quad\\quad\\quad(4)   Inspired by [],we choose to regress the observation angle \u03b1 instead of the yaw rotation \u03b8 for each object. We further change the observation angle with respect to the object head   ,instead of the commonly used observation angle value  , by simply adding  . The difference between these two angles is shown in Figure4.Moreover,each   is encoded as the vector $[sin(a)\\quad cos(a)]^T$ The yaw angle $\\theta$ can be obtained by utilizing $\\alpha_z$ and the object location: \n\\theta=\\alpha + arctan\\left(\\frac{x}{z}\\right)\\qquad(5) \nFinally,we can construct the 8 corners of the 3D bounding box in the camera frame by using the yaw rotation matrix $R_\\theta$, object dimensions $[h\\quad w\\quad l]^T$ and location $[x\\quad y\\quad z]^T$: B=R_\\theta\\begin{bmatrix}\n    \\pm h/2\\\\\\pm w/2\\\\\\pm l/x\n\\end{bmatrix}+\\begin{bmatrix}\n    x\\\\y\\\\z\n\\end{bmatrix}\\qquad\\qquad(6)",
            "title": "Regression Branch"
        },
        {
            "location": "/radar_camera_fusion/#loss-function",
            "text": "We employ the penalty-reduce focal loss\u60e9\u7f5a\u51cf\u5c11\u7126\u70b9\u635f\u5931  [] in a point-wise manner on the downsampled heatmap.Let $s_{i,j}$ be the predicted score at the heatmap location $(i,j)$ and $y_{i,j}$ be the ground-truth value of each point assigned by Gaussian Kernel. Define $\\breve{y} {i,j}$ and $\\breve{s} {i,j}$ as : \n\\breve{y}_{i,j}=\\left\\{\\begin{matrix}\n    0,\\qquad if\\quad y_{i,j} =1\\\\y_{i,j}\n,\\qquad otherwise\\end{matrix}\\right.  \\breve{s}_{i,j}=\\left\\{\\begin{matrix}\n    s_{i,j},\\qquad if\\quad y_{i,j}=1\\\\1-s_{i,j},\\qquad otherwise\n\\end{matrix}\\right.   For simplicity,we only consider a single object class here.Then , the classification loss function is constructed as L_{cls}=-\\frac{1}{N}\\sum^{h,w}_{i,j=1}(1-\\breve{y}_{i,j})^\\beta(1-\\breve{s}_{i,j})^\\alpha log(\\breve{s}_{i,j})\\qquad(7) \nwhere $\\alpha$ and $\\beta$ are tunable hyper-parameters and N is the number of keypoints per image.The term $(1-y_{i,j})$ corresponds to penalty reduction for points around the groundtruth location.",
            "title": "Loss function"
        },
        {
            "location": "/radar_camera_fusion/#regression-loss",
            "text": "We regardless the 8D tuple $\\tau$ to construct the 3D bounding box for each object. We also add channel-wise action to the regressed parameters of dimension and orientation at each feature map location to preserve\u4fdd\u5b58 consistency\u4e00\u81f4\u6027 . The activation functions for the dimension and the orientation are chosen to be the sigmoid function $\\sigma$ and the $l_2$ norm ,respectively: \\begin{bmatrix}\n    \\delta_h\\\\\\delta_w\\\\\\delta_l\n\\end{bmatrix}=\\sigma\\left(\\begin{bmatrix}\n    O_h\\\\O_w\\\\O_l\n\\end{bmatrix}\\right)-\\frac{1}{2},\\quad\\begin{bmatrix}\n    sin(a)\\\\cos(a)\n\\end{bmatrix}=\\begin{bmatrix}\n    O_{sin}/\\sqrt{O^2_{sin}+O^2_{cos}}\\\\O_{cos}/\\sqrt{O^2_{sin}+O^2_{cos}}\n\\end{bmatrix} \nHere $o$ stands for the specific output of network. By adopting the keypoint lifting transformation introduced in Sec.4.2.we define the 3D bounding box regression loss as the $l_1$ distance between the predicted transform $\\hat{B}$ and the ground truth B: L_{reg}=\\frac{\\lambda}{N}||\\hat{B}-B||_1\\qquad(8) \nwhere $\\lambda$ is the scaling factor.This is used to ensure that neither the classification, nor the regression dominates\u4e3b\u5bfc the other.The disentangling transformation of loss has been proven to be an effective dynamic method to optimize 3D regression loss functions in [31] . Following this design , we extend the concept of loss disentanglement into a multi-step form. in Eq(3) , we use the projected 3D groundtruth points on the image plane $[x_c\\quad y_c]^T$ with the network predicted discretization offset $[\\hat{\\delta_{x_c}}\\quad \\hat{\\delta_{y_c}}]^T$ and depth $\\hat{z}$ to retrieve the location $[\\hat{x}\\quad \\hat{y}\\quad \\hat{z}]^T$ of each object. In Eq(5), we use the groundtruth location $[x\\quad y\\quad z]^T$ and the predicted observation angle $\\hat{a}_z$ to construct the estimated yaw orientation $\\hat{\\theta}$. The 8 corners representation of the 3D bounding box is also isolated \u9694\u79bb into three different groups following the concept of disentanglement , namely orientation, dimension and location.The final loss function can be represented by: L = L_{cls} + \\sum^3_{i=1}L_{reg}(\\hat{B_i}),\\qquad(9) \nwhere $i$ represents the number of groups we define in the 3D regression branch.The multi-step disentangling transformation divides the contribution of each parameter group to the final loss. In Sec.5.2, we show that this method significantly improves detection accuracy.",
            "title": "Regression Loss"
        },
        {
            "location": "/radar_camera_fusion/#implementation",
            "text": "In this section,we discuss the implementation of our proposed methodology in detail together with selection of the hyperparameters.  Preprocessing  : we avoid applying any complicated preprocessing method on the dataset.Instead, we only eliminate\u6392\u9664 objects whose 3D projected center point on the image plane is out of the image range. Note that the total number of projected center points outside the image boundary for the car instance is 1582.This accounts for only the $5.5$% of the entire set of 28742 labeled cars.  Data Augmentation  : Data augmentation techniques we used are random horizontal flip\u6c34\u5e73\u7ffb\u8f6c, random scale and shift. The scale ratio is set to 9 steps from 0.6 to 1.4, and the shift ratio is set to 5 steps from -0.2 to 0.2. Note that the scale and shift augmentation methods are only used for heatmap classification since the 3D information becomes inconsistent\u4e0d\u4e00\u81f4 with data augmentation.   Hyperparameter Choice  : In the backbone, the group number for GroupNorm is set to 32.For channels loss than 32, it is set to be 16. For Eq.7, we set $\\alpha=2$ and $\\beta=4$ in all experiments. Based on [31], the reference car size and depth statistic\u7edf\u8ba1\u6570\u636e we use are $[\\overline{h}\\quad \\overline{w}\\quad \\overline{l}]^T=[1.63\\quad 1.53\\quad 3.88]^T$ and $[\\mu_z\\quad \\sigma_z]^T=[28.01\\quad 16.32]^T$ (measured in meters).   Trainning  :Our optimization schedule\u4f18\u5316\u8868 is easy and straightforward. We use the original image resolution and pad it to $1280\\times 384$. We train the network with a batch size of 32 on 4 Geforce TITAN X GPUs for 60 epochs. The learning rate is set at $2.5\\times 10^{-4}$ and drops at 25 and 40 epochs by a factor of 10. During testing, we use the top 100 detected 3D projected points and filter it with a threshold of 0.25. No data augmentation method and NMS are used in the test precedure.Our implementation platform in Pytorch1.1, CUDA 10.0 and CUDNN 7.5.",
            "title": "Implementation"
        },
        {
            "location": "/radar_camera_fusion/#depth-estimation-from-monocular-images-and-sparse-radar-data",
            "text": "In this page, we explore the possibilty of achieving a more accurate depth estimation by fusing monocular images and Radar points using a deep neural network.We give a comprehensive study of the fusion between RGB images and Radar measurements from different accepts and proposed a working solution based on obeservations. We find that the noise existing in Radar measurements is one of the main key reasons that prevents one from applying the existing fusion methods developed for\nfor LiDAR data and images to the new fusion problem between Radar data and images to the new fusion problem between Radar data and images.The experiments are conducted\u5b9e\u65bd on the nuScenes dataset, which is one of the first datasets which features Camera\uff0c Radar, and LiDAR recordings in diverse\u5404\u79cd\u5404\u6837\u7684 scenes and weather conditions. Extensive\u5e7f\u6cdb\u7684 experiments demonstrate that our method outperforms existing fusion methods. We also provide detailed ablation\u707c\u70e7 studies to show the effectiveness of each component in our method.   Dense and robust depth estimation is an important component in self-driving system and unmanned\u65e0\u4eba aerial vehicles. While existing structure-light-based depth sensor or stereo camera can provide dense depth in indoor environments, the reliability of these sensors degrade a lot in outdoor applications. As a result, lots of research works focus on obtaining dense depth from monocular RGB images only. Recently, convolutional neural network (CNN) based methods have demonstrated impressive improvement on monocular depth estimation for both indoor and outdoor scenarios.However , there is still a gap between the accuracy and reliability of these methods and what the real-world applications need.  Apart from estimating depth from monocular camera, to imporve the robustness of the system, some methods also take other sensor modalities\u5f62\u5f0f into consideration.While thses sensors,LiDARS is the most commonly used one. Many works have been conducted on dense depth estimation from RGB images and sparse LiDAR scans.In addition to depth estimation and completion tasks, different RGB + LiDAR fusion techniques are also extensively used in tasks such as 3D object detection. Although LiDAR provides more accurate depth measurements in outdoor scenario, high-end LiDAR sensors are still far from affordable for many applications.  Compared with LiDAR, Radar is an automotive-grade sensor that has been used for decades on vehicles, but has not attracted lots of attention in self-driving research based on deep learning. One reason might be that Radar measurements are not included in most of the dominant self-driving datasets.Compared with LiDAR, Radar sensors offer longer sensing range(200m~300m), more attributes including velocities, dynamic states, and measurement uncertainties. Most importantly, the costs of these sensors are much lower than LiDAR.However, Radar measurements are much lower than LiDAR ,However, Radar measurement are typically sparser, noiser, and have a more limited vertical field of view.  This work is to study the chanllenges of using Radar data for dense depth estimation and to propse a novel method for that aim. Given recently released nuScenes dataset consisting of RGB, LiDAR and Radar measurements.we are able to conduct experiments on cross-modality sensor fusion between RGB camera and Radar.Through our experiments we demonstrated that:1)Existing RGB + LiDAR fusion methods can be applied directly to RGB + Radar fusion task; and 2) with proper fusion strategies and a novel denosing operation, our proposed network is able to improve the performance of depth estimation by a good margin by using Radar measurements. According to our survey, our work is the first one that brings Radar sensors into dense depth estimation tasks.  The contributions of this work include: 1) a detailed study on the challenges of using Radar data for dense depth estimation; and 2) a novel and carefully motivated network architecture for depth estimation with monocular images and sparse Radar data.",
            "title": "Depth Estimation from Monocular Images and Sparse Radar Data"
        },
        {
            "location": "/radar_camera_fusion/#related-works",
            "text": "RGB-based Depth Estimation. Depth estimation from monocular or stereo camera is a popular research topic in both computer vision and robotics.Early works used either geometry-based algorithms on stereo images, or handcrafted features on single images.Recent years, convolutional neural networks(CNN) have demonstrated their ability in image understanding, dense predictions ,etc. given large scale datasets. Therefore, lots of research works of monocular depth estimation are conducted.In general , most of them used the encoder-decoder architecturres. Xie . futher introduced skip connection strategy which is a frequently used technique to multi-level features in dense prediction tasks. On the other hand, Huang achieve state-of-the-art performance by introducing space increasing discretization (SID) and ordinal regression. In some semi-/self-supervised formu loss is used.   and the smoothness constraint\u7ea6\u675f is further imposed to enhance local consistency\u4e00\u81f4\u6027.Patil proposed a recurrent\u53cd\u590d\u53d1\u4f5c network architecture to exploit the long-range spatiotemporal\u65f6\u7a7a structures across video frames to yeild more accurate depth maps.While good performance has been obtained with only RGB images, the methods still have difficulty in generalizing to new scenario and chanllenging weather and light conditions. This motivates the existing line of work that fuses camera data with Lidar data and our work that fuses camera data with radar data is cheaper to obtain.  Depth Estimation with Camera and Lidar data  while monocular depth estimation task attracts lots of attention, achieving more reliable and accurate predictions using multi-modality\u591a\u6a21\u6001 information is also a popular topic.Existing works either take the whole set of LiDAR points , (known as depth completion), or the downsampled set as model inputs.Ma first projected LiDAR points to 2D sparse depth map and then perform so called early fusion by direct concatenation with RGB images along channel, or concatenation feature maps after one shallow convolution block.J used a late fusion method to combine features from different modalities and improved the overall performance through multitask learning.Q proposed to predict dense depth map by combining predictions from RGB and surface normal pathways, where surface normal is treated as an intermediate representation. Moreover, confidence maps are predicted to down-weight mixed measurements from LiDAR caused by the displacement between camera and LiDAR. In this work our main focus is sensor fusion . Thus , we use the widely adopted encoder-decoder architecture and focus on the necessary extensions in order to effectively use Radar data instead.  Post-processing and Refinement Methods \u540e\u5904\u7406\u548c\u4f18\u5316\u65b9\u6cd5. Apart from treating sparse point clouds as inputs to the model, some methods also tried to directly refine the dense predictions of the trained models.W proposed a simple add-on module that can improve the prediction of depth estiamtion model using similar methods used by white box adversarial attack. Since the refinement is done using iterative re-inference, no re-training is required.This method can be integrated into most deep learning based methods.Cheng learned an affinity \u4eb2\u548c\u529b matrix from data to refine the outputs of their CNN model.The recurrent refinement  operation can also be extended to depth completion tasks.  Fusion of images and radar Data.  There are already works that fuse RGB images and Radar.given the fact that they are very much complementary\u8865\u5145.The line of work mainly focus on object detection and tracking.For instance,C fused\u878d\u5408 Radar data and images to detect small objects at a large distance. In [] and [] the authors enhance current 2D object detection networks by fusing camera data and projected sparse radar data in the network layers\uff0cwhile [] also performs free space semantic segmentation jointly.Both methods learn at which level the fusion of the sensor data is more beneficial for the task. There are also other datasets proposed for object detection with Radar data such as [].Exemplary\u793a\u8303\u6027\u7684 works on semantic segmentation with Radar point cloud have been conducted as well. For instance, in [] the authors have studied how the challenging task can be performed and provide results on a dataset with manually labeled radar reflections.Similar to these works for object detection and semantic segmentation, our work aims to study how the challenging task of dense depth estimation with radar data can be addressed with the popular deep neural network architectures.  In the line of increasing the robustness of depth estimation, Vasudevan, et al. [] have proposed a novel method to estimate depth maps based on binaural\u53cc\u8033 sounds.",
            "title": "RELATED WORKS"
        },
        {
            "location": "/radar_camera_fusion/#our-method",
            "text": "Our whole method are divided into multiple main components. In the following section, we will go through each component in detail.",
            "title": "Our Method"
        },
        {
            "location": "/radar_camera_fusion/#radar-data-background",
            "text": "Different from well established depth completion or depth estimation tasks.There is no prior research works on RGB + radar depth estimation task.Therefore, we provide a brief introduction to the task formulation and some key differences between Radar and LiDAR measurements,which will help readers to understand the motivations behind the components of our method.  Data format.  similar to LiDAR data, Radar measurements are recorded as sparse point clouds. The main difference is that , in addition to $x,y,z$,and $reflectance$, Radar data consist of additional measurements including the velocity along x and y direction, the standard deviation of the location and velocity measurements.and information such as the dynamic states of the measured object (encoded as discrete\u79bb\u6563\u7684 numbers).   Limitation  . While it seems that the radar data provide more information, it also introduces the following limitations compared with LiDAR data:\n* sparseness:In nuScenes dataset , there are more than 3000 LiDAR points after projection to the camera.However, there are less than 100 Radar points after the projection.\n* Limited vertical field of view.Because of the limitation of the sensor,Radar measurements mainly concentrate in the central horizontal bin (similar heights) as shwon in Fig.2.\n* Noisy measurements:Due to multiple reflections (Radar multipath problem) or other reasons , we have many noisy measurements as shown in Fig.2.\n* Inconsistency\u524d\u540e\u4e0d\u4e00\u81f4 with LiDAR data:Apart from noisy measurements, which are considered as outliers, the 3D points of Radar and LiDAR representing the same object can also be different.Since we typically use LiDAR measurements as ground truth, even noise-free  Radar measurements are not perfect on the evaluation metrics.  As we will show in Section IV, using Radar depth maps directly as the input of off-the-shelf\u73b0\u6210\u7684 RGB+LiDAR depth completion /prediction models resulted in marginal\u8fb9\u7f18 improvements.  Problem formulation . In our RGB+ Radar formulation,each data sample from the dataset contains  (1) an RGB image $x_{RGB}$  (2) a set of Radar measurements $R={r_n}^N_{n=1}$ from 3 nearest timestamps   (3) a set of Lidar measurements $L={l_m}^M_{m=1}$   Radar measurements $R$ can be further projected to a single-channel 2D depth map $x_{Radar}$ using the perspective projection.Similarly,LiDAR measurements can be objected to 2D map y, which is treated as ground truth depth map in our experiments(section IV).Our model takes both $x_{RGB}$ and $x_{Radar}$ as inputs and predicts dense 2D depth map $\\tilde{y}$ which minimizes the metrics errors. Same as all the depth estimation /completion tasks, loss and metric error are computed over the pixels with ground truth measurements.",
            "title": "Radar Data background"
        },
        {
            "location": "/radar_camera_fusion/#cnn-architecture",
            "text": "Backbone . Since our goal is to perform a comprehensive\u7efc\u5408\u7684 study on different fusion methods, we need to choose an efficient and widely-used backbone. Thus, we fixed our backbone to Resnet18 , and explored different fusion methods based on it as a pilot\u98de\u884c\u5458 experiment. As illustrated in Fig.3, we apply different encoder fusion methods to the simple encoder-decoder architecture proposed by Ma, and compare their performance. According to the experiment (Section IV-C), late fusion and multi-layer fusion model have comparable performance.Therefore, we adopt\u91c7\u7eb3 late fusion as our main encoder backbone design in the following experiments for simplicity.  Two-Stage architecture  As shown by the pilot experiments(section IV-C) , a simple encoder-decoder architecture have some improvements on RGB  + Radar depth prediction task if we can remove most of the noisy measurements. However, we don't have LiDAR ground truth to help us performing the filtering in real applications.and it's hard to perform outlier rejections\u5f02\u5e38\u5254\u9664 without information on the 3D structure or objects of the scene.Therefore, we come up with a 2-stage design to address the noisy measurement issue.  As shown in Fig.1, our whole method contains two stages. The stage 1 model $f_{stage1}$ takes both RGB image $x_{RGB}$ and the radar depth map $x_{Radar}$ as inputs and predicts a coarse\u7c97 depth map $\\tilde{y}_{stage1}$ ,which gives us a dense 3D structure of the scene:   \\tilde{y}_{stage1}=f_{stage1}(x_{RGB},x_{Radar})\\qquad\\quad(1)   Then we compare the Radar depth map with the coarse prediction $\\tilde{y} {stage1}$ to reject some outliers (most details in next subsection) and obtain the filtered Radar depth map $\\tilde{x} {Radar}$. The assumption\u5047\u8bbe here is that although the predictions from stage 1 is not perfect, they are smooth and locally consistent. Therefore, they are suitable to reject outliner noises produced by Radar Multipath, which typically have certain margins with the correct depth values.  The stage2 model $f_{stage2}$ takes $x_{RGB}$ , $\\tilde{x} {Radar}$ , and the prediction from stage1 $\\tilde{y} {stage1}$ to predict the final result $\\tilde{y}$:   \n\\tilde{y} = f_{stage_2}(x_{RGB},\\tilde{x}_{Radar},\\tilde{y}_{stage1})\\qquad(2)",
            "title": "CNN architecture"
        },
        {
            "location": "/radar_camera_fusion/#noise-filtering-module",
            "text": "Since Radar measurements are not exactly consistent with the LiDAR measurements as we mentioned in Section III-A .we need to have some tolerance\u516c\u5dee in the filtering process.Otherwise ,  we will end up discarding\u4e22\u5f03 all the measurements in the set $R$.  Instead of setting a fixed distance tolerance threshold $\\tau$,we empirically\u51ed\u7ecf\u9a8c found that an adaptive threshold gives us better results. We design the threshold to be a function of depth value $\\tau(d)$ : We have larger tolerance for larger depth values, which is similar to the space-increasing disrectization (SID) from Huan et al.[2]:   \n\\tau(d)=exp\\left(\\frac{d*log(\\frac{\\beta}{\\alpha})}{K}+log(\\alpha)\\right)\\qquad(3)   here we heuristically\u542f\u53d1\u5f0f\u7684 set $\\alpha=5$ and $\\beta=18$.  Let $P$ denote the set pixel coordinates $(u,v)$ of the Radar measurements projected by the perspective projection function $proj(.)$ : $P={p_n}^N_{n=1}={proj(r_n)^N_{n=1}}$. The noise filtering module will keep the point $p_n$ if it satisfies the following constraint\u7ea6\u675f:   \n|x_{Radar}(p_n)-\\tilde{y}_{stage1}(p_n)|\\leq\\tau(p_n),for\\quad p_n\\quad in\\quad P\\qquad(4)",
            "title": "Noise filtering module"
        },
        {
            "location": "/radar_camera_fusion/#loss-function_1",
            "text": "By design, each component of our model is differentiable. Thus, our whole model is end-to-end trainable.Following the setting from [7] ,We apply $L1$ loss to both the predictions of stage1 ($\\tilde{y} {stage1}$) and stage2 ($\\tilde{y}$). Considering that the main purpose of $\\tilde{y} {stage1}$ is to filter outlier noises in $x_{Radar}$, we futher add edge-aware smoothness constraint to it . To effectively balance multiple loss terms, we follow the method proposed by Kendall   \nL_{total} = e^{-w1}*(L1(\\tilde{y}_{stage1},y)+10^{-3}*L_{smooth})+e^{-w2}*L1(\\tilde{y},y) + \\sum_{i}w_i\\qquad(6)   where $w1$ and $w2$ are optimized variables ,and $L_{smooth}$ is defined as : \nL_{smooth}=|\\triangledown_u(\\tilde{y}_{stage1})|e^{-|\\triangledown_u(x_{RGB})|} + |\\triangledown_v(\\tilde{y}_{stage1})|e^{-|\\triangledown_v(x_{RGB})|}\\qquad(8)   $\\triangledown_u$ and $\\triangledown_v$ denote the gradient along 2D height and width directions sperately.",
            "title": "Loss Function"
        },
        {
            "location": "/radar_camera_fusion/#implementation-details",
            "text": "Unless stated otherwise, all the models are trained using a batch size of 16 and SGD optimizer with a learning rate of $0.001$ and a momentum\u52bf\u5934  of $0.9$ for 20 epochs. The learning rate is multiplied by 0.1 alter every 5 epochs.All the models we used in experiment action are implemented in PyTorch.The experiments are conducted on the desktop computers/clusters with Nvidia GTX1080Ti and TeslaV100 GPUs.  All of our encoder network architectures (section III-B) are modified from the standard Reset18. For early fusion, we simply modified the input channels to 4 and randomly initialized the weights (weights of other layers were initialized from pre-trained models on Imagenet dataset). For mid, late, and multi-layer fusion , the depth branch has a similar architecture as the RGB branch.The only difference is that we change the number of channels to 1/4 of the original one. Fusion operations happened only on feature maps with same spatial resolution (width and height) . Regarding the decoder part, we kept the section from [] by using UpProj module as our upsampling operation.",
            "title": "Implementation details"
        },
        {
            "location": "/radar_camera_fusion/#experiment",
            "text": "In an early fusion approach, the raw or pre-processed sensory data from different sensor modalities are fused together. With this approach, the network learns a joint representation from the sensing modalities. Early fusion methods are usually sensitive to spatial or temporal misalignment\u9519\u4f4d of the data. On the other hand, a late fusion approach combines the data from different modalities at the decision level, and provides more flexibility \u7075\u6d3b\u6027 for introducing new sensing modalities to the network.However a late fusion approach does not exploit\u5f00\u53d1\u5229\u7528 the full potential of the available sensing modalities , as it does not acquire the intermediate\u4e2d\u95f4\u7684 features obtained by learning a joint representation.A compromise between the early and late fusion approaches is referred to as middle fusion. It extracts features from different modalities individually and combines them at an intermediate stage, enabling the network to learn joint representations and creating a balance between sensitivity and flexibility.  We propose CenterFusion, a middle-fusion approach to exploit radar and camera data for 3D object detection. CenterFusion focuses on associating radar detections to preliminary\u521d\u6b65\u7684 detection results obtained from the image, then generates radar feature maps and uses it in addition to image features to accurately estimate 3D bounding boxes for objects.Particularly, we generate perliminary 3D detections using a key point detection network,and propose a novel frustum-based \u57fa\u4e8e\u89c6\u9525 radar association method to accurately associate radar detections to their corresponding objects in 3D space. These radar detections are then mapped to the image plane and used to create feature maps to complement the image-based features.Finally, the fused features are used to accurately estimate object's 3D properties such as depth, rotation and velocity. The network architecture for CenterFusion is shown in Fig.1  We evaluate CenterFusion on the challenging nuscenes dataset\n,where it outperforms all previous camera-based object detection methods in the 3D object detection benchmark. We also show that exploiting radar information significantly improves velocity estimation for object without using any temporal information.",
            "title": "Experiment"
        },
        {
            "location": "/radar_camera_fusion/#radar-point-cloud",
            "text": "Radars are active sensors that transmit radio waves to sense the environment and measure the reflected waves to determine the location and velocity of objects. Automotive radars usually report the detected objects as 2D points in BEV, providing the azimuth\u65b9\u4f4d\u89d2 and radial distance to the object. For every detection, the radar also reports the instantaneous velocity of the object in the radial direction.This radial velocity does not necessarily match the object's actual velocity vector in it's direction of movement. Fig.2 illustrates the difference between the radial as reported by the radar, and actual velocity of the object in the vehicle's coordinate system.  We represent each radar detection as a 3D point in the egocentric coordinate system, and parameterize it as $P=(x,y,z,v_x,v_y)$ where $(x,y,z)$ is the position and $(v_x,v_y)$ is the reported radial velocity of the object in the $x$ and $y$ directions. The radial velocity is compensated by the ego vehicle's motion.For every scene,we aggregate 3 sweeps of the radar point cloud (detections within the past 0.25 seconds). The nuscenes dataset provides the calibration parameters needed for mapping the radar point clouds from the radar coordinates system to the egocentric and camera coordinate systems.",
            "title": "Radar Point Cloud"
        },
        {
            "location": "/radar_camera_fusion/#centernet",
            "text": "CenterNet represents the state-of-the-art in 3D object detection using single camera. It takes an image $I \\in \\mathbb{R}^{W\\times H\\times 3}$ as input and generates a keypoint heatmap $\\hat{Y}\\in[0,1]^{\\frac{W}{R}\\times\\frac{H}{R}\\times C}$ as output where $W$ and $H$ are the image width and height.$R$ is the downsampling ratio and $C$ is the number of object categories\u7c7b\u522b. A prediction of $\\hat{Y} {x,y,c} = 1$ as the output indicates a detected object of class $c$ centered at position $(x,y)$ on the image. The ground-truth heatmap $Y\\in [0, 1]^{\\frac{W}{R}\\times\\frac{H}{R}\\times C}$ is generated from the ground-truth 2D bounding boxes using a Gaussian kernel.For each bounding box center point $p_i\\in R^2$ of class $c$ in the image, a Gaussian heatmap is generated on $Y {:,:,c}$ , the final value of Y for class $c$ at position $q\\in R^2$ is defined as    Y_{qc} = \\underset{i}{max}exp\\left(-\\frac{(p_i-q)^2}{2\\sigma^2_i}\\right)    Figure 3,Frustum\u89c6\u9525 association. An object detected using the image features(left),generating the ROI frustum based on object's 3D bounding box(middle), and the BEV of the ROI frustum showing radar detections inside the frustum (right).$\\delta$ is used to increase the frustum size in the testing phase.$\\hat{d}$ is the ground_truth depth in the training phase and the estimated object depth in the testing phase   Expanding radar points to 3D pillars( top image). Directly mapping the pillars to the image and replacing with radar depth information results in poor association with object's center and many overlapping depth values(middle image),Frustum association accurately maps the radar detections to the center of objects and minimizes overlapping(bottom image). Radar detections are only associated to objects with a valid ground truth or detection box, and only if all or part of the radar detection pillar\u652f\u67f1 is inside the box.Frustum association also prevents associating radar detections caused by background objects such as buildings to foreground objects, as seen in the case of pedestrains on the right hand side of the image  where $\\sigma_i$ is a size-adaptive\u5c3a\u5bf8\u9002\u5e94 standard deviation\u504f\u5dee,controlling the size of the heatmap for every object based on its size. A fully convolutional encode-decoder network is used to predict $\\hat{Y}$.  To generate 3D bounding boxes,sperate network heads are used to regress object's depth, dimensions and orientation directly from the detected center points.Depth is calculated as an additional output channel $\\hat{D}\\in [0,1]^{\\frac{W}{R}\\times\\frac{H}{R}}$ after applying the inverse sigmoidal transformation used in Eigen et al. to the original depth domain. The object dimensions are directly regressed to their absoluate values in meter as three output channels $\\hat{\\Gamma}\\in[0,1]^{\\frac{W}{R}\\times\\frac{H}{R}\\times3}$ .Orientation is encoded as two bins with 4 scalars in each bin, following the orientation representation in M. For each center point, a local offset is also predicted to compensate\u8865\u507f for the discretization\u79bb\u6563\u5316 error caused by the output strides in the backbone network.  Given the annotated objects $p_0, p_1, ...$ in an image,  the training objective is defined as below based on the focal loss: L_k=\\frac{1}{N}\\sum_{xyc}\\left\\{\\begin{matrix}(1-\\hat{Y}_{xyc})^{\\alpha}log(\\hat{Y}_{xyc})\\qquad Y_{xyc}=1\\\\\\\\ (1-Y_{xyc})^{\\beta}(\\hat{Y}_{xyc})^\\alpha log(1-\\hat{Y}_{xyc})\\qquad otherwise\\end{matrix}\\right.   where $N$ is the number of objects, $Y\\in [0,1]^{\\frac{W}{R}\\times\\frac{H}{R}\\times C}$ is the annotated object's ground-truth heatmap and $\\alpha$ and $\\beta$ are focal loss hyperparameters.  Detection identifies\u8bc6\u522b objects as axis-aligned boxes in an image.Most successful object detectors enumerate\u5217\u4e3e a nearly exhaustive\u8be6\u5c3d\u7684 list of potential object locations and classify each.This is wasteful, inefficient,and requires additional post-processing. In this paper, we take a different approach.We model an object as a single point - the center point of its bounding box.Our detector uses keypoint estimation to find center points and regresses  to all other object properties, such as size, 3D location, orientation, and even pose. Our center point based approach,\nCenterNet, is end-to-end differnetiable, simpler,faster,and more accurate than corresponding bounding box based detectors.CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1% AP at 142 FPS. We use the same approach to estimate 3D bounding box in the KITTI benchmark and human pose on the COCO keypoint dataset. Our method performs competitively with sophisticated multi-stage methods and runs in real-time.  Object detection powers many vision tasks like instance segmentation, pose estimation, tracking, and action recognition.It has down-stream applications in surveillance\u76d1\u89c6\uff0c autonomous driving, and visual question answering. Current object detectors represent each object through an axis-aligned bounding box, that tightly encompasses the object.They then reduce object detection to image classification of an extensive number of potential object bounding boxes.For each bounding box, the classifer determines if the image content is a specific object or background. One stage detectors, slide a complex arrangement of possible bounding boxes called anchors, over the image and classify them directly without specifying the box content.Two-stage detectors recompute image features for each potential box, then classify those features.Post-processing, namely non-maxima suppression, then removes duplicated detections for the same instance by computing bounding box IoU. This post-processing is hard to differentiate and train, hence most current detectors are not end-to-end trainable. Nonetheless, over the past five years, this idea has achieved good empirical success.Sliding window based object detectors are however a bit wasteful , as they need to enumerate all possible object locations and diemnsions.  In this paper, we provide a much simpler and more efficient alternative. We represent objects by a single point at their bounding box center (see Fig 2), Other properties such as object size, dimension, 3D extent, orientation, and pose are then regressed directly from image features at the center location. Object detection is then a standard keypoint estimation problem.We simply feed the input image to a fully convolutional network that generates a heatmap.Peaks in this heatmap correspond to object centers.Image features at each peak predict the objects bounding box height and weight. The model trains using standard dense supervised\u76d1\u7763 learning.Inference is a single network forward-pass, without non-maximal suppression for post-processing.   We model an object as the center point of its bounding box. The bounding box size and other object properties are inffered from the keypoint feature at the center.Best viewed in color  Our method is general and can be extended to other tasks with minor effort. We provide experiments on 3D obect detection and multi-person human pose estimation, by predicting additional outputs at each center point (see Figure 4). For 3D bounding box estimation, we regress to the object absolute depth, 3D bounding box dimensions, and object orientation. For human pose estimation, we consider the 2D joint locations as offsets from the center and directly regress to them at the center point location.   The simplicity of our method, CenterNet, allows it to run at a very high speed.With a simple Resnet-18 and up-convolutional layers, our network runs at 142 FPS with 28.1% COCO bounding box AP. With a carefully designed keypoint detection network. DLA-34 , our network achieves 37.4%  COCO AP at 52 FPS.",
            "title": "CenterNet"
        },
        {
            "location": "/radar_camera_fusion/#related-work",
            "text": "Object detection by region classfication.  One of the first successful deep object detectors, RCNN, enumerates object location from a larget set of region candidates,crops them , and classifies each using a deep network. Fast-RCNN crops image features instead, to save computation. However, both methods rely on slow low-level region proposal methods.  Object detection with implicit(\u9690\u542b\u7684) anchors.Faster RCNN generates region proposal within the detection network. It samples fixed-shape bounding boxes (anchors) around a low-resolution image grid and classifies each into \u201cforeground or not\u201d. An anchor is labeled foreground with a $>0.7$ overlap with any ground truth object, background with a $<0.3$ overlap, or ignored otherwise. Each generated region proposal is again classified. Changing the proposal classifier to a multi-class classification forms the basis of one-stage detectors. Several improvements to one-stage detectors include anchor shape points, different feature resolution, and loss re-weighting among different samples.  Our approach is closely related to anchor-based one-stage approaches.A center point can be seen as a single shape-agnostic \u4e0d\u53ef\u77e5\u5f62\u72b6 anchor.(see Figure3) .However,there are a few important differences. First, our CenterNet assigns the \"anchor\" based solely\u809a\u5b50 on location, not box overlap. We have no manual thresholds for foreground and background classification. Second, we only have one positive \"anchor\" per object. and hence do not need NonMaximum Suppression(NMS). We simply extract local peaks in the keypoint heatmap. Third, CenterNet uses a larger output resolution (output stride of 4) compared to traditional object detectors(output stride of 16). This eliminates the need for multiple anchors.  Object detection by keypoint estimation  We are not the first to use keypoint estimation for object detection. CornerNet detects two bounding box corners as keypoints, while ExtremeNet detects the top-,left-,bottom-,right-most, and center points of all objects. Both these methods build on the same robust keypoint estimation network as our CenterNet.However, they require a combinatorial\u7ec4\u5408 grouping stage after keypoint detection, which significantly slows down each algorithm. Our CenterNet , on the other hand,simply extracts a single center point per object without the need for grouping or post-processing.  Monocular 3D object detection  3D bounding box estimation powers autonomous driving . Deep3Dbox use a slow-RCNN style framework, by first detecting 2D objects and then feeding each object into a 3D estimation network. 3D RCNN adds an additional head to Faster-RCNN followed by a 3D projection. Deep Manta uses a coarse-to-fine\u4ece\u7c97\u5230\u7ec6 Faster-RCNN trained on many tasks. Our method is similar to one-stage version of Deep3Box or 3DRCNN. As such, CenterNet is much simpler and faster than competing methods.   standard anchor based detection. Anchors count as positive with an overlap IoU>0.7 to any object, negative with overlap IoU<0.3, or are ignored otherwise   Center point based detection.The center pixel is assigned to the object. Nearby points have a reduced negative loss. Object size is regressed",
            "title": "Related work"
        },
        {
            "location": "/radar_camera_fusion/#preliminary",
            "text": "Let $I\\in R^{W\\times H\\times3}$ be an input image of width $W$ and height $H$. Our aim is to produce a keypoint heatmap $\\hat{Y}\\in[0,1]^{\\frac{W}{R}\\times\\frac{H}{R}\\times C}$, where $R$ is the output stride and $C$ is the number of keypoint types. Keypoint types include $C=17$ human joints in human pose estimation, or $C=80$ object categorical in object detection. We use the default output stride of $R = 4$ in literature. The output stride downsamples the output prediction by a factor R. A prediction $\\hat{Y} {x,y,c}=1$ corresponds to a detected keypoint, while $\\hat{Y} {x,y,c} =0$ is background. We use serveral different fully-convolutional encoder-decoder networks to predict $\\hat{Y}$ from an image $I$: A stacked hourglass network\u5806\u53e0\u7684\u6c99\u6f0f\u7f51\u7edc upconvolutional residual network (ResNet) and deep layer aggregation\u805a\u5408 (DLA).   We train the keypoint prediction network following Law and Deng. For each ground truth keypoint $p \\in R^2$ of class $c$ , we compute a low-resolution equivalent\u76f8\u7b49\u7684 $\\widetilde{p}= \\left\\lfloor\\frac{p}{R}\\right\\rfloor$. We then splat\u6454\u5f97\u75db all ground truth keypoints onto a heatmap $Y\\in [0,1]^{\\frac{W}{R}\\times\\frac{H}{R}\\times C}$ using a Gaussian kernel $Y_{xyc}=exp\\left(-\\frac{(x-\\widetilde{p}_x)^2+(y-\\widetilde{p}_y)^2}{2\\sigma^2_p}\\right)$, where $\\sigma_p$ is an object size-adaptive standard deviation\u5c3a\u5bf8\u9002\u5e94\u6807\u51c6\u504f\u5dee. If two Gaussians of the same class overlap, we take the element-wise maximum. The training objective is a penalty-reduced\u51cf\u5211 pixel-wise logistic regression with focal loss: L_k=\\frac{-1}{N}\\sum_{xyz}\\left\\{\\begin{matrix}\n    (1-\\hat{Y}_{xyc})^\\alpha log(\\hat{Y}_{xyc})\\qquad if\\quad Y_{xyc} = 1\\\\\\\\\n    (1-Y_{xyc})^\\beta(\\hat{Y}_{xyc})^\\alpha log(1-\\hat{Y}_{xyc}) \\qquad otherwise\n\\end{matrix}\\right.   where $\\alpha$ and $\\beta$ are hyper-parameters of the focal loss, and $N$ is the number of keypoints in image $I$ . The normalization by $N$ is chosen as to normalize all positive focal loss instances to 1. We use $\\alpha=2$ and $\\beta=4$ in all our experiments,following Law and Debug.  To recover the discretization\u79bb\u6563\u5316 error caused by the output stride, we additionally predict a local offset $\\hat{O}\\in R^{\\frac{W}{R}\\times\\frac{H}{R}\\times2}$ for each center point. All classes $c$ share the same offset prediction.The offset is trained with an $L1$ loss L_{off} = \\frac{1}{N}\\sum_p\\left|\\hat{O}_{\\widetilde{p}}-\\left(\\frac{p}{R}-\\widetilde{p}\\right)\\right|\\qquad(2)   The supervision acts only at keypoints locations $\\widetilde{p}$, all other locations are ignored.  In the next section,we will show how to extend this keypoint estimator to a general purpose object detector.",
            "title": "Preliminary"
        },
        {
            "location": "/radar_camera_fusion/#objects-as-points",
            "text": "Let $(x_1^{(k)},y_1^{(k)},x_2^{(k)},y_2^{(k)})$ be the bounding box of object $k$ with category $c_k$. Its center point is lies at $p_k=(\\frac{x_1^{(k)}+x_2^{(k)}}{2},\\frac{y_1^{(k)}+y_2^{(k)}}{2})$. We use our keypoint estimator $\\hat{Y}$ to predict all center points. In addition, we regress to the object size $s_k=(x_2^{(k)}-x_1^{(k)},y_2^{(k)}-y_1^{(k)})$ for each object $k$.To limit the computational burden\u8ba1\u7b97\u8d1f\u62c5 , we use a single size prediction $\\hat{S} \\in R^{\\frac{W}{R}\\times\\frac{H}{R}\\times C}$ for all object categories. We use an $L1$ loss at the center point similar to Objective 2: L_{size}=\\frac{1}{N}\\sum^N_{k=1}\\left|\\hat{S}_{p_k}-s_k\\right|\\qquad(3)   We do not normalize\u5f52\u4e00\u5316 the scale and directly use the raw pixel coordinates. We instead scale the loss by a constant $\\lambda_{size}$.The overall training objective is    L_{det} = L_k + \\lambda_{size}L_{size} + \\lambda_{off}L_{off}\\qquad(4)   We set $\\lambda_{size}=0.1$ and $\\lambda_{off}=1$ in all our experiments unless specified otherwise. We use a single network to predict the keypoints $\\hat{Y}$, offset $\\hat{O}$ , and size $\\hat{S}$ The network predicts a toatal of $C + 4$ outputs at each location. All outputs share a common fully-convolutional backbone network. For each modality, the features of the backbone are then passed through a separate 3x3 convolution, ReLU and another 1x1 convolution. Figure4 shows an overview of the network output. Section 5 and supplementary material contain additional architectural details.  From points to bounding boxes \nAt inference time, we first extract the peaks in the heatmap for each category independently. We detect all responses whose value is greater or equal to its 8-connected neighbors and keep the top 100 peaks. Let $\\hat{P} c$ be the set of $n$ detected center points $\\hat{P} ={(\\hat{x}_i,\\hat{y}_i)}^n {i=1}$ of class $c$. Each keypoint location is given by an integer coordinates $(x_i, y_i)$. We use the keypoint values $\\hat{Y}_{x_iy_ic}$ as a measure of its detection confidence, and produce a bounding box at location. (\\hat{x}_i+ \\delta\\hat{x}_i-\\hat{w}_i/2, \\hat{y}_i+\\delta\\hat{y}_i-\\hat{h}_i/2,\\hat{x}_i+\\delta\\hat{x}_i+\\hat{w}_i/2,\\hat{y}_i+\\delta\\hat{y}_i+\\hat{h}_i/2)    Outputs of our network for different tasks: top for object detection, middle for 3D object detection,bottom for pose estimation. All modalities are produced from a common backbone, with a different 3x3 and 1x1 output convolutions separated by a ReLU. The number in brackets indicates the output channels. See section 4 for details  where $(\\delta\\hat{x} i,\\delta\\hat{y}_i)=\\hat{O} {\\hat{x} i,\\hat{y}_i}$ is the offset prediction and $(\\hat{w}_i,\\hat{h}_i) = \\hat{S} {\\hat{x}_i,\\hat{y}_i}$ is the size prediction.",
            "title": "Objects as Points"
        },
        {
            "location": "/ros openai/",
            "text": "ubuntu18 melodic\n\n\n\n\nsudo apt-get install\npython-pip python3-vcstool python3-pyqt4\npyqt5-dev-tools\nlibbluetooth-dev libspnav-dev\npyqt4-dev-tools libcwiid-dev\ncmake gcc g++ qt4-qmake libqt4-dev\nlibusb-dev libftdi-dev\npython3-defusedxml python3-vcstool\nros-melodic-octomap-msgs\nros-melodic-joy\nros-melodic-geodesy\nros-melodic-octomap-ros\nros-melodic-control-toolbox\nros-melodic-pluginlib\nros-melodic-trajectory-msgs\nros-melodic-control-msgs\nros-melodic-std-srvs\nros-melodic-nodelet\nros-melodic-urdf\nros-melodic-rviz\nros-melodic-kdl-conversions\nros-melodic-eigen-conversions\nros-melodic-tf2-sensor-msgs\nros-melodic-pcl-ros\nros-melodic-navigation\nros-melodic-sophus\n\n\nsudo pip install gym\nsudo apt-get install python-skimage\nsudo pip install h5py\npip install tensorflow-gpu (if you have a gpu if not then just pip install tensorflow)\nsudo pip install keras\n\n\ncd ~\ngit clone https://github.com/erlerobot/gym-gazebo\ncd gym-gazebo\nsudo pip install -e .\n\n\ncd gym-gazebo/gym_gazebo/envs/installation\nbash setup_melodic.bash\n\n\n\n\nTensorflow \u7b97\u6cd5\u529f\u80fd\n\n\n\n\n\u8bb0\u5f55\u4eea\u8bbe\u7f6e\n\n\n\u968f\u673a\u79cd\u5b50\u8bbe\u7f6e\n\n\n\u73af\u5883\u5b9e\u4f8b\u5316\n\n\n\u4e3a\u8ba1\u7b97\u56fe\u5236\u4f5c\u5360\u4f4d\u7b26\n\n\nactor_critic \u901a\u8fc7\u4f5c\u4e3a\u53c2\u6570\u4f20\u9012\u7ed9\u7b97\u6cd5\u51fd\u6570\u6765\u6784\u5efa\u53c2\u4e0e\u8005\u8bc4\u8bba\u8ba1\u7b97\u56fe\n\n\n\u5b9e\u4f8b\u5316\u4f53\u9a8c\u7f13\u51b2\u533a\n\n\n\u5efa\u7acb\u7279\u5b9a\u4e8e\u7b97\u6cd5\u7684\u635f\u5931\u51fd\u6570\u548c\u8bca\u65ad\u7684\u8ba1\u7b97\u56fe\n\n\n\u8fdb\u884c\u57f9\u8bad\n\n\n\u8fdb\u884ctf\u4f1a\u8bdd\u5e76\u521d\u59cb\u5316\u53c2\u6570\n\n\n\u901a\u8fc7\u8bb0\u5f55\u5668\u8bbe\u7f6e\u6a21\u578b\u4fdd\u5b58\n\n\n\u5b9a\u4e49\u8fd0\u884c\u7b97\u6cd5\u4e3b\u5faa\u73af\u6240\u9700\u7684\u529f\u80fd\uff08\u4f8b\u5982\uff0c\u53d6\u51b3\u4e8e\u7b97\u6cd5\uff0c\u6838\u5fc3\u66f4\u65b0\u529f\u80fd\uff0c\u83b7\u53d6\u64cd\u4f5c\u529f\u80fd\u548c\u6d4b\u8bd5\u4ee3\u7406\u529f\u80fd\uff09\n\n\n\u8fd0\u884c\u7b97\u6cd5\u7684\u4e3b\u5faa\u73af\uff1a\n\n\n\u5728\u73af\u5883\u4e2d\u8fd0\u884c\u4ee3\u7406\n\n\n\u6839\u636e\u7b97\u6cd5\u7684\u4e3b\u8981\u65b9\u7a0b\u5f0f\u5b9a\u65f6\u66f4\u65b0\u4ee3\u7406\u53c2\u6570\n\n\n\u8bb0\u5f55\u5173\u952e\u6027\u80fd\u6307\u6807\u5e76\u4fdd\u5b58\u4ee3\u7406\n\n\n\n\n\n\n\n\n\u6838\u5fc3\u6587\u4ef6\n\n\n\u6838\u5fc3\u6587\u4ef6\u4e0d\u50cf\u7b97\u6cd5\u6587\u4ef6\u90a3\u6837\u7d27\u5bc6\u5730\u9644\u7740\u5728\u6a21\u7248\u4e0a\uff0c\u4f46\u5177\u6709\u4e00\u4e9b\u8fd1\u4f3c\u7684\u7ed3\u6784\uff1a\n\n\n\n\n\u4ec5tensorflow:\u4e0e\u5236\u4f5c\u548c\u7ba1\u7406\u5360\u4f4d\u7b26\u6709\u5173\u7684\u529f\u80fd\n\n\n\u7528\u4e8e\u5efa\u7acbactor_critic \u7279\u5b9a\u7b97\u6cd5\u7684\u65b9\u6cd5\u76f8\u5173\u7684\u8ba1\u7b97\u56fe\u90e8\u5206\u7684\u529f\u80fd\n\n\n\u4efb\u4f55\u5176\u4ed6\u6709\u7528\u7684\u529f\u80fd\n\n\n\u4e8e\u7b97\u6cd5\u517c\u5bb9\u7684MLP actor_critic \u7684\u5b9e\u73b0\uff0c\u5176\u4e2d\u7b56\u7565\u548c\u503c\u51fd\u6570\u90fd\u7531\u7b80\u5355\u7684MLP\u8868\u793a\u3002\n\n\n\n\n\u4f53\u9a8c\u6df1\u5ea6RL\u7684\u6700\u4f73\u65b9\u6cd5\u4e4b\u4e00\u662f\u8fd0\u884c\u7b97\u6cd5\uff0c\u5e76\u67e5\u770b\u5b83\u4eec\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u7684\u6267\u884c\u60c5\u51b5\u3002Spinning Up\u968f\u9644spinup/run.py,\u8fd9\u662f\u4e00\u4e2a\u65b9\u4fbf\u7684\u5de5\u5177\uff0c\u53ef\u4ee5\u8ba9\u4f60\u4ece\u547d\u4ee4\u884c\u8f7b\u677e\u7684\u542f\u52a8\u5404\u79cd\u597d\u7b97\u6cd5\uff08\u53ef\u4ee5\u9009\u62e9\u8d85\u53c2\u6570\uff09\n\n\n\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\n\n\nPPO\u53d7\u5230\u4e0eTRPO\u76f8\u540c\u95ee\u9898\u7684\u6fc0\u52b1\uff1a\u6211\u4eec\u5982\u4f55\u624d\u80fd\u4f7f\u7528\u5f53\u524d\u62e5\u6709\u7684\u6570\u636e\u5728\u7b56\u7565\u4e0a\u91c7\u53d6\u6700\u5927\u53ef\u80fd\u7684\u6539\u8fdb\u6b65\u9aa4\uff0c\u800c\u53c8\u4e0d\u4f1a\u8d70\u7684\u592a\u8fdc\u800c\u610f\u5916\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002TRPO\u5c1d\u8bd5\u4f7f\u7528\u590d\u6742\u7684\u4e8c\u9636\u65b9\u6cd5\u89e3\u51b3\u6b64\u95ee\u9898\u3002PPO\u662f\u4e00\u9636\u65b9\u6cd5\u7cfb\u5217\uff0c\u5b83\u4f7f\u7528\u5176\u4ed6\u4e00\u4e9b\u6280\u5de7\u6765\u4f7f\u65b0\u7b56\u7565\u63a5\u8fd1\u4e8e\u65e7\u7b56\u7565\u3002PPO\u65b9\u6cd5\u660e\u663e\u66f4\u6613\u4e8e\u5b9e\u73b0\uff0c\u5e76\u4e14\u4ece\u7ecf\u9a8c\u4e0a\u770b\uff0c\u5176\u6027\u80fd\u81f3\u5c11\u4e0eTRPO\u76f8\u540c\u3002PPO\u6709\u4e24\u79cd\u4e3b\u8981\u53d8\u4f53\uff1aPPO-Penalty\uff0c PPO-Clip\u3002\n\n\nPPO-Penalty \u5927\u7ea6\u89e3\u51b3\u4e86\u50cfTRPO\u8fd9\u6837\u53d7KL\u7ea6\u675f\u7684\u66f4\u65b0\u3002\u4f46\u662f\u4f1a\u60e9\u7f5a\u76ee\u6807\u51fd\u6570\u4e2d\u7684KL\u6563\u5ea6\uff0c\u800c\u4e0d\u4f7f\u5176\u6210\u4e3a\u786c\u7ea6\u675f\uff0c\u5e76\u4f1a\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u81ea\u52a8\u8c03\u6574\u60e9\u7f5a\u7cfb\u6570\uff0c\u4ee5\u4f7f\u5176\u9002\u5f53\u7f29\u653e\u3002\n\n\nPPO-Clip \u5728\u76ee\u6807\u4e2d\u6ca1\u6709KL\u6563\u5ea6\u9879\uff0c \u4e5f\u6ca1\u6709\u4efb\u4f55\u7ea6\u675f\u3002\u53d6\u800c\u4ee3\u4e4b\u7684\u662f\u4f9d\u9760\u5bf9\u76ee\u6807\u51fd\u6570\u7684\u4e13\u95e8\u88c1\u526a\u6765\u6d88\u9664\u65b0\u653f\u7b56\u8fdc\u79bb\u65e7\u653f\u7b56\u7684\u52a8\u673a\u3002\n\n\n\u8981\u95fb\u901f\u89c8\n\n\n\n\nPPO\u662f\u4e00\u79cd\u57fa\u4e8e\u7b56\u7565\u7684\u7b97\u6cd5\n\n\nPPO\u53ef\u4ee5\u5e94\u7528\u4e8e\u5177\u6709\u79bb\u6563\u7684\u6216\u8fde\u7eed\u7684\u52a8\u4f5c\u7a7a\u95f4\u7684\u73af\u5883\n\n\nPPO\u7684Spinningup\u5b9e\u73b0\u652f\u6301\u4e0eMPI\u5e76\u884c\u5316\n\n\n\n\nPPO\u66f4\u65b0\u7b56\u7565\u516c\u5f0f\n\n\n$$\n\\theta_{k+1}=arg\\underset{\\theta}{max}\\underset{s,\\alpha\\sim\\pi\\theta_k}{E}[L(s,\\alpha,\\theta_k,\\theta)]\n\n\n$$\n\n\n\u901a\u5e38\u91c7\u53d6\u591a\u4e2a\u6b65\u9aa4\uff08\u901a\u5e38\u662f\u5c0f\u6279\u91cf\uff09SGD\u6765\u6700\u5927\u5316\u76ee\u6807\n\n\n$$\nL(s,\\alpha,\\theta_k,\\theta)=min(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)}A^{\\pi\\theta_k}(s,\\alpha),clip(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)},1-\\epsilon,1+\\epsilon)A^{\\pi\\theta_k}(s,\\alpha))\n\n\n$$\n\n\n$\\epsilon$\u662f\u4e00\u4e2a\u8d85\u53c2\u6570\u7c97\u7565\u5730\u8bf4\u51fa\u65b0\u653f\u7b56\u88ab\u5141\u8bb8\u4e0e\u65e7\u653f\u7b56\u76f8\u8ddd\u591a\u8fdc\n\n\n\u8fd9\u662f\u4e00\u4e2a\u5f88\u590d\u6742\u7684\u65b9\u7a0b\u5f0f\uff0c\u7b2c\u4e00\u773c\u5f88\u96be\u8bf4\u660e\u767d\u5b83\u5728\u505a\u4ec0\u4e48\uff0c\u6216\u8005\u5b83\u5982\u4f55\u5e2e\u52a9\u65b0\u7b56\u7565\u9760\u8fd1\u65e7\u7b56\u7565\u3002\u4e8b\u5b9e\u8bc1\u660e\u6709\u4e00\u4e2a\u76f8\u5f53\u7b80\u5316\u7684\u7248\u672c\uff0c\u8f83\u5bb9\u6613\u7406\u89e3\uff0c\u4e5f\u662f\u6211\u4eec\u5728\u4ee3\u7801\u4e2d\u5b9e\u73b0\u7684\u7248\u672c\n\n\n$$\nL(s,\\alpha,\\theta_k,\\theta) = min\\left(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)}A^{\\pi\\theta_k}(s,\\alpha), g\\left(\\epsilon,A^{\\pi\\theta_k}(s,\\alpha)\\right)\\right)\n\n\n$$\n\n\n$$\ng(\\epsilon,A)=\\left{\n\\begin{matrix}(1+\\epsilon)A\\quad A\\geq0&\\\\(1-\\epsilon)A\\quad A<0\\end{matrix}\n\\right.\n\n\n$$\n\n\n\u4e3a\u4e86\u5f04\u6e05\u695a\u4ece\u4e2d\u5f97\u5230\u7684\u76f4\u89c9\uff0c\u8ba9\u6211\u770b\u4e00\u4e0b\u5355\u4e2a\u7684 \u72b6\u6001\u2014\u52a8\u4f5c \u5bf9\uff0c\u5e76\u8003\u8651\u6848\u4f8b\n\n\n\u4f18\u52bf\u662f\u79ef\u6781\n\u7684\uff0c\u5047\u8bbe\u8be5\u72b6\u6001\u2014\u52a8\u4f5c \u5bf9\u7684\u4f18\u52bf\u4e3a\u6b63\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b83\u5bf9\u76ee\u6807\u7684\u8d21\u732e\u51cf\u5c0f\u4e3a\uff1a\n\n\n$$\nL(s,\\alpha,\\theta,\\theta_k)=min\\left(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)},(1+\\epsilon)\\right)A^{\\pi\\theta_k}(s,\\alpha)\n\n\n$$\n\n\n\u56e0\u4e3a\u4f18\u52bf\u662f\u79ef\u6781\u7684\uff0c\u6240\u4ee5\u5982\u679c\u91c7\u53d6\u884c\u52a8\u7684\u53ef\u80fd\u6027\u66f4\u5927\uff08\u4e5f\u5c31\u662f$\\pi_{\\theta}(\\alpha|s)$\u589e\u52a0\uff09\uff0c\u76ee\u6807\u5c31\u4f1a\u589e\u52a0\u3002\u4f46\u662fterm\u4e2d\u7684min\u9650\u5236\u4e86\u53ef\u4ee5\u589e\u52a0\u591a\u5c11\u76ee\u6807\u3002\u4e00\u65e6$\\pi_\\theta(\\alpha|s)>(1+\\epsilon)\\pi_{\\theta_k}(\\alpha|s)$,\u8fd9\u65f6min\u8e22\u8fdb\uff0cterm\u5230\u8fbe$(1+\\epsilon)A^{\\pi\\theta_k}(s,\\alpha)$,\u56e0\u6b64\u7528\u8fc7\u8fdc\u79bb\u65e7\u653f\u7b56\uff0c\u65b0\u653f\u7b56\u4e0d\u4f1a\u83b7\u76ca\u3002\n\n\n\u4f18\u52bf\u662f\u8d1f\u9762\n\u7684\uff0c\u5047\u8bbe\u8be5\u72b6\u6001\u2014\u52a8\u4f5c \u5bf9\u7684\u4f18\u52bf\u662f\u8d1f\u7684\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b83\u5bf9\u76ee\u6807\u7684\u8d21\u732e\u51cf\u5c0f\u4e3a\uff1a\n\n\n$$\nL(s,\\alpha,\\theta,\\theta_k)=max\\left(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)},(1-\\epsilon)\\right)A^{\\pi\\theta_k}(s,\\alpha)\n\n\n$$\n\n\n\u56e0\u4e3a\u4f18\u52bf\u662f\u8d1f\u7684\uff0c\u6240\u4ee5\u5982\u679c \u91c7\u53d6\u884c\u52a8\u7684\u53ef\u80fd\u6027\u66f4\u5c0f\uff08\u4e5f\u5c31\u662f$\\pi_\\theta(\\alpha|s)$\u51cf\u5c0f\uff09\uff0c\u76ee\u6807\u5c31\u4f1a\u589e\u52a0\u3002\u4f46\u662fterm\u4e2d\u7684max\u9650\u5236\u4e86\u53ef\u4ee5\u589e\u52a0\u591a\u5c11\u76ee\u6807\u3002\u4e00\u65e6$\\pi_\\theta(\\alpha|s)<(1-\\epsilon)\\pi_{\\theta_k}(\\alpha|s)$,\u8fd9\u65f6max\u8e22\u8fdb\uff0cterm\u5230\u8fbe$(1-\\epsilon)A^{\\pi\\theta_k}(s,\\alpha)$,\u56e0\u6b64\uff0c\u518d\u6b21\uff0c\u8fdc\u79bb\u65e7\u653f\u7b56\uff0c\u4e0d\u4f1a\u4f7f\u65b0\u653f\u7b56\u53d7\u76ca\u3002\n\n\n\u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u901a\u8fc7\u53bb\u9664\u4f7f\u653f\u7b56\u53d1\u751f\u91cd\u5927\u53d8\u5316\u7684\u6fc0\u52b1\uff0c\u88c1\u526a\u5145\u5f53\u7740\u6b63\u5219\u5316\u5668\u3002\u800c\u8d85\u53c2\u6570$\\epsilon$\u5219\u5bf9\u5e94\u4e8e\u65b0\u653f\u7b56\u4e0e\u65e7\u653f\u7b56\u53ef\u4ee5\u6709\u591a\u8fdc\uff0c\u540c\u65f6\u4ecd\u4f7f\u76ee\u6807\u53d7\u76ca\u3002\n\n\n\u63a2\u7d22\u4e0e\u53d1\u73b0\n\n\nPPO\u6309\u57fa\u4e8e\u7b56\u7565\u65b9\u5f0f\u8bad\u7ec3\u968f\u673a\u7b56\u7565\uff0c\u8fd9\u610f\u5473\u7740\u5b83\u4f1a\u6839\u636e\u5176\u968f\u673a\u7b56\u7565\u7684\u6700\u65b0\u7248\u672c\u901a\u8fc7\u91c7\u6837\u64cd\u4f5c\u6765\u8fdb\u884c\u63a2\u7d22\u3002\u52a8\u4f5c\u9009\u62e9\u7684\u968f\u673a\u6027\u53d6\u51b3\u4e8e\u521d\u59cb\u6761\u4ef6\u548c\u8bad\u7ec3\u6b65\u9aa4\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u4ee3\u7406\u9010\u6b65\u51cf\u5c11\u968f\u673a\u6027\uff0c\u56e0\u4e3a\u66f4\u65b0\u89c4\u5219\u9f13\u52b1\u5b83\u5229\u7528\u5df2\u7ecf\u53d1\u73b0\u7684\u5956\u52b1\u3002\u8fd9\u53ef\u80fd\u5bfc\u81f4\u7b56\u7565\u9677\u5165\u5c40\u90e8\u6700\u4f18\u72b6\u6001\u3002\n\n\n\u4f2a\u4ee3\u7801\n\n\n\n\nPPO-CLIP\n\n\n\n\n\u8f93\u5165\uff1a\u521d\u59cb\u7b56\u7565\u53c2\u6570$\\theta_0$,\u521d\u59cb\u503c\u529f\u80fd\u53c2\u6570$\\phi_0$\n\n\nfor k = 0, 1, 2, ... do\n\n\n\u5728\u73af\u5883\u4e2d\u901a\u8fc7\u6267\u884c\u7b56\u7565$\\pi_k=\\pi(\\theta_k)$\uff0c\u6536\u96c6\u4e00\u7ec4\u8f68\u8ff9$D_k={\\tau_i}$\n\n\n\u7b97\u5956\u52b1$\\hat{R}_i$\n\n\n\u57fa\u4e8e\u5f53\u524d\u7684value function $V_{\\phi k}$,\u8ba1\u7b97advantage estimates $\\hat{A}_t$\n\n\n\u901a\u8fc7\u6700\u5927\u5316PPO-Clip\u76ee\u6807\uff0c\u66f4\u65b0\u7b56\u7565\n\n\n$$\n\\theta_{k+1}=arg\\ \\underset{\\theta}{max}\\frac{1}{|D_k|T}\\sum_{\\tau\\in D_k}\\sum^T_{t=0}min\\left(\\frac{\\pi_\\theta(\\alpha_t|s_t)}{\\pi_{\\theta_k}(\\alpha_t|s_t)}A^{\\pi\\theta_k}(s_t,\\alpha_t),g(\\epsilon,A^{\\pi\\theta_k}(s_t,\\alpha_t))\\right)\n\n\n$$\n\n\n\u901a\u5e38\u901a\u8fc7\u4e0eAdam\u4e00\u8d77\u7684\u968f\u673a\u68af\u5ea6\u4e0a\u5347\n\n\n\u901a\u8fc7\u5747\u65b9\u8bef\u5dee\u56de\u5f52\u62df\u5408value function\n\n\n$$\n\\phi_{k+1}=arg\\ \\underset{\\phi}{min}\\frac{1}{|D_k|T}\\sum_{\\tau\\in D_k}\\sum^T_{t=0}(V_\\phi(s_t)-\\hat{R}_t)^2\n\n\n$$\n\n\n\u7279\u522b\u662f\u901a\u8fc7\u4e00\u4e9b\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\n\n\nend for",
            "title": "Ros openai"
        },
        {
            "location": "/ros openai/#ubuntu18-melodic",
            "text": "sudo apt-get install\npython-pip python3-vcstool python3-pyqt4\npyqt5-dev-tools\nlibbluetooth-dev libspnav-dev\npyqt4-dev-tools libcwiid-dev\ncmake gcc g++ qt4-qmake libqt4-dev\nlibusb-dev libftdi-dev\npython3-defusedxml python3-vcstool\nros-melodic-octomap-msgs\nros-melodic-joy\nros-melodic-geodesy\nros-melodic-octomap-ros\nros-melodic-control-toolbox\nros-melodic-pluginlib\nros-melodic-trajectory-msgs\nros-melodic-control-msgs\nros-melodic-std-srvs\nros-melodic-nodelet\nros-melodic-urdf\nros-melodic-rviz\nros-melodic-kdl-conversions\nros-melodic-eigen-conversions\nros-melodic-tf2-sensor-msgs\nros-melodic-pcl-ros\nros-melodic-navigation\nros-melodic-sophus  sudo pip install gym\nsudo apt-get install python-skimage\nsudo pip install h5py\npip install tensorflow-gpu (if you have a gpu if not then just pip install tensorflow)\nsudo pip install keras  cd ~\ngit clone https://github.com/erlerobot/gym-gazebo\ncd gym-gazebo\nsudo pip install -e .  cd gym-gazebo/gym_gazebo/envs/installation\nbash setup_melodic.bash",
            "title": "ubuntu18 melodic"
        },
        {
            "location": "/ros openai/#tensorflow",
            "text": "\u8bb0\u5f55\u4eea\u8bbe\u7f6e  \u968f\u673a\u79cd\u5b50\u8bbe\u7f6e  \u73af\u5883\u5b9e\u4f8b\u5316  \u4e3a\u8ba1\u7b97\u56fe\u5236\u4f5c\u5360\u4f4d\u7b26  actor_critic \u901a\u8fc7\u4f5c\u4e3a\u53c2\u6570\u4f20\u9012\u7ed9\u7b97\u6cd5\u51fd\u6570\u6765\u6784\u5efa\u53c2\u4e0e\u8005\u8bc4\u8bba\u8ba1\u7b97\u56fe  \u5b9e\u4f8b\u5316\u4f53\u9a8c\u7f13\u51b2\u533a  \u5efa\u7acb\u7279\u5b9a\u4e8e\u7b97\u6cd5\u7684\u635f\u5931\u51fd\u6570\u548c\u8bca\u65ad\u7684\u8ba1\u7b97\u56fe  \u8fdb\u884c\u57f9\u8bad  \u8fdb\u884ctf\u4f1a\u8bdd\u5e76\u521d\u59cb\u5316\u53c2\u6570  \u901a\u8fc7\u8bb0\u5f55\u5668\u8bbe\u7f6e\u6a21\u578b\u4fdd\u5b58  \u5b9a\u4e49\u8fd0\u884c\u7b97\u6cd5\u4e3b\u5faa\u73af\u6240\u9700\u7684\u529f\u80fd\uff08\u4f8b\u5982\uff0c\u53d6\u51b3\u4e8e\u7b97\u6cd5\uff0c\u6838\u5fc3\u66f4\u65b0\u529f\u80fd\uff0c\u83b7\u53d6\u64cd\u4f5c\u529f\u80fd\u548c\u6d4b\u8bd5\u4ee3\u7406\u529f\u80fd\uff09  \u8fd0\u884c\u7b97\u6cd5\u7684\u4e3b\u5faa\u73af\uff1a  \u5728\u73af\u5883\u4e2d\u8fd0\u884c\u4ee3\u7406  \u6839\u636e\u7b97\u6cd5\u7684\u4e3b\u8981\u65b9\u7a0b\u5f0f\u5b9a\u65f6\u66f4\u65b0\u4ee3\u7406\u53c2\u6570  \u8bb0\u5f55\u5173\u952e\u6027\u80fd\u6307\u6807\u5e76\u4fdd\u5b58\u4ee3\u7406     \u6838\u5fc3\u6587\u4ef6  \u6838\u5fc3\u6587\u4ef6\u4e0d\u50cf\u7b97\u6cd5\u6587\u4ef6\u90a3\u6837\u7d27\u5bc6\u5730\u9644\u7740\u5728\u6a21\u7248\u4e0a\uff0c\u4f46\u5177\u6709\u4e00\u4e9b\u8fd1\u4f3c\u7684\u7ed3\u6784\uff1a   \u4ec5tensorflow:\u4e0e\u5236\u4f5c\u548c\u7ba1\u7406\u5360\u4f4d\u7b26\u6709\u5173\u7684\u529f\u80fd  \u7528\u4e8e\u5efa\u7acbactor_critic \u7279\u5b9a\u7b97\u6cd5\u7684\u65b9\u6cd5\u76f8\u5173\u7684\u8ba1\u7b97\u56fe\u90e8\u5206\u7684\u529f\u80fd  \u4efb\u4f55\u5176\u4ed6\u6709\u7528\u7684\u529f\u80fd  \u4e8e\u7b97\u6cd5\u517c\u5bb9\u7684MLP actor_critic \u7684\u5b9e\u73b0\uff0c\u5176\u4e2d\u7b56\u7565\u548c\u503c\u51fd\u6570\u90fd\u7531\u7b80\u5355\u7684MLP\u8868\u793a\u3002   \u4f53\u9a8c\u6df1\u5ea6RL\u7684\u6700\u4f73\u65b9\u6cd5\u4e4b\u4e00\u662f\u8fd0\u884c\u7b97\u6cd5\uff0c\u5e76\u67e5\u770b\u5b83\u4eec\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u7684\u6267\u884c\u60c5\u51b5\u3002Spinning Up\u968f\u9644spinup/run.py,\u8fd9\u662f\u4e00\u4e2a\u65b9\u4fbf\u7684\u5de5\u5177\uff0c\u53ef\u4ee5\u8ba9\u4f60\u4ece\u547d\u4ee4\u884c\u8f7b\u677e\u7684\u542f\u52a8\u5404\u79cd\u597d\u7b97\u6cd5\uff08\u53ef\u4ee5\u9009\u62e9\u8d85\u53c2\u6570\uff09",
            "title": "Tensorflow \u7b97\u6cd5\u529f\u80fd"
        },
        {
            "location": "/ros openai/#_1",
            "text": "PPO\u53d7\u5230\u4e0eTRPO\u76f8\u540c\u95ee\u9898\u7684\u6fc0\u52b1\uff1a\u6211\u4eec\u5982\u4f55\u624d\u80fd\u4f7f\u7528\u5f53\u524d\u62e5\u6709\u7684\u6570\u636e\u5728\u7b56\u7565\u4e0a\u91c7\u53d6\u6700\u5927\u53ef\u80fd\u7684\u6539\u8fdb\u6b65\u9aa4\uff0c\u800c\u53c8\u4e0d\u4f1a\u8d70\u7684\u592a\u8fdc\u800c\u610f\u5916\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002TRPO\u5c1d\u8bd5\u4f7f\u7528\u590d\u6742\u7684\u4e8c\u9636\u65b9\u6cd5\u89e3\u51b3\u6b64\u95ee\u9898\u3002PPO\u662f\u4e00\u9636\u65b9\u6cd5\u7cfb\u5217\uff0c\u5b83\u4f7f\u7528\u5176\u4ed6\u4e00\u4e9b\u6280\u5de7\u6765\u4f7f\u65b0\u7b56\u7565\u63a5\u8fd1\u4e8e\u65e7\u7b56\u7565\u3002PPO\u65b9\u6cd5\u660e\u663e\u66f4\u6613\u4e8e\u5b9e\u73b0\uff0c\u5e76\u4e14\u4ece\u7ecf\u9a8c\u4e0a\u770b\uff0c\u5176\u6027\u80fd\u81f3\u5c11\u4e0eTRPO\u76f8\u540c\u3002PPO\u6709\u4e24\u79cd\u4e3b\u8981\u53d8\u4f53\uff1aPPO-Penalty\uff0c PPO-Clip\u3002  PPO-Penalty \u5927\u7ea6\u89e3\u51b3\u4e86\u50cfTRPO\u8fd9\u6837\u53d7KL\u7ea6\u675f\u7684\u66f4\u65b0\u3002\u4f46\u662f\u4f1a\u60e9\u7f5a\u76ee\u6807\u51fd\u6570\u4e2d\u7684KL\u6563\u5ea6\uff0c\u800c\u4e0d\u4f7f\u5176\u6210\u4e3a\u786c\u7ea6\u675f\uff0c\u5e76\u4f1a\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u81ea\u52a8\u8c03\u6574\u60e9\u7f5a\u7cfb\u6570\uff0c\u4ee5\u4f7f\u5176\u9002\u5f53\u7f29\u653e\u3002  PPO-Clip \u5728\u76ee\u6807\u4e2d\u6ca1\u6709KL\u6563\u5ea6\u9879\uff0c \u4e5f\u6ca1\u6709\u4efb\u4f55\u7ea6\u675f\u3002\u53d6\u800c\u4ee3\u4e4b\u7684\u662f\u4f9d\u9760\u5bf9\u76ee\u6807\u51fd\u6570\u7684\u4e13\u95e8\u88c1\u526a\u6765\u6d88\u9664\u65b0\u653f\u7b56\u8fdc\u79bb\u65e7\u653f\u7b56\u7684\u52a8\u673a\u3002",
            "title": "\u8fd1\u7aef\u7b56\u7565\u4f18\u5316"
        },
        {
            "location": "/ros openai/#_2",
            "text": "PPO\u662f\u4e00\u79cd\u57fa\u4e8e\u7b56\u7565\u7684\u7b97\u6cd5  PPO\u53ef\u4ee5\u5e94\u7528\u4e8e\u5177\u6709\u79bb\u6563\u7684\u6216\u8fde\u7eed\u7684\u52a8\u4f5c\u7a7a\u95f4\u7684\u73af\u5883  PPO\u7684Spinningup\u5b9e\u73b0\u652f\u6301\u4e0eMPI\u5e76\u884c\u5316   PPO\u66f4\u65b0\u7b56\u7565\u516c\u5f0f  $$\n\\theta_{k+1}=arg\\underset{\\theta}{max}\\underset{s,\\alpha\\sim\\pi\\theta_k}{E}[L(s,\\alpha,\\theta_k,\\theta)]  $$  \u901a\u5e38\u91c7\u53d6\u591a\u4e2a\u6b65\u9aa4\uff08\u901a\u5e38\u662f\u5c0f\u6279\u91cf\uff09SGD\u6765\u6700\u5927\u5316\u76ee\u6807  $$\nL(s,\\alpha,\\theta_k,\\theta)=min(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)}A^{\\pi\\theta_k}(s,\\alpha),clip(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)},1-\\epsilon,1+\\epsilon)A^{\\pi\\theta_k}(s,\\alpha))  $$  $\\epsilon$\u662f\u4e00\u4e2a\u8d85\u53c2\u6570\u7c97\u7565\u5730\u8bf4\u51fa\u65b0\u653f\u7b56\u88ab\u5141\u8bb8\u4e0e\u65e7\u653f\u7b56\u76f8\u8ddd\u591a\u8fdc  \u8fd9\u662f\u4e00\u4e2a\u5f88\u590d\u6742\u7684\u65b9\u7a0b\u5f0f\uff0c\u7b2c\u4e00\u773c\u5f88\u96be\u8bf4\u660e\u767d\u5b83\u5728\u505a\u4ec0\u4e48\uff0c\u6216\u8005\u5b83\u5982\u4f55\u5e2e\u52a9\u65b0\u7b56\u7565\u9760\u8fd1\u65e7\u7b56\u7565\u3002\u4e8b\u5b9e\u8bc1\u660e\u6709\u4e00\u4e2a\u76f8\u5f53\u7b80\u5316\u7684\u7248\u672c\uff0c\u8f83\u5bb9\u6613\u7406\u89e3\uff0c\u4e5f\u662f\u6211\u4eec\u5728\u4ee3\u7801\u4e2d\u5b9e\u73b0\u7684\u7248\u672c  $$\nL(s,\\alpha,\\theta_k,\\theta) = min\\left(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)}A^{\\pi\\theta_k}(s,\\alpha), g\\left(\\epsilon,A^{\\pi\\theta_k}(s,\\alpha)\\right)\\right)  $$  $$\ng(\\epsilon,A)=\\left{ \\begin{matrix}(1+\\epsilon)A\\quad A\\geq0&\\\\(1-\\epsilon)A\\quad A<0\\end{matrix} \\right.  $$  \u4e3a\u4e86\u5f04\u6e05\u695a\u4ece\u4e2d\u5f97\u5230\u7684\u76f4\u89c9\uff0c\u8ba9\u6211\u770b\u4e00\u4e0b\u5355\u4e2a\u7684 \u72b6\u6001\u2014\u52a8\u4f5c \u5bf9\uff0c\u5e76\u8003\u8651\u6848\u4f8b  \u4f18\u52bf\u662f\u79ef\u6781 \u7684\uff0c\u5047\u8bbe\u8be5\u72b6\u6001\u2014\u52a8\u4f5c \u5bf9\u7684\u4f18\u52bf\u4e3a\u6b63\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b83\u5bf9\u76ee\u6807\u7684\u8d21\u732e\u51cf\u5c0f\u4e3a\uff1a  $$\nL(s,\\alpha,\\theta,\\theta_k)=min\\left(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)},(1+\\epsilon)\\right)A^{\\pi\\theta_k}(s,\\alpha)  $$  \u56e0\u4e3a\u4f18\u52bf\u662f\u79ef\u6781\u7684\uff0c\u6240\u4ee5\u5982\u679c\u91c7\u53d6\u884c\u52a8\u7684\u53ef\u80fd\u6027\u66f4\u5927\uff08\u4e5f\u5c31\u662f$\\pi_{\\theta}(\\alpha|s)$\u589e\u52a0\uff09\uff0c\u76ee\u6807\u5c31\u4f1a\u589e\u52a0\u3002\u4f46\u662fterm\u4e2d\u7684min\u9650\u5236\u4e86\u53ef\u4ee5\u589e\u52a0\u591a\u5c11\u76ee\u6807\u3002\u4e00\u65e6$\\pi_\\theta(\\alpha|s)>(1+\\epsilon)\\pi_{\\theta_k}(\\alpha|s)$,\u8fd9\u65f6min\u8e22\u8fdb\uff0cterm\u5230\u8fbe$(1+\\epsilon)A^{\\pi\\theta_k}(s,\\alpha)$,\u56e0\u6b64\u7528\u8fc7\u8fdc\u79bb\u65e7\u653f\u7b56\uff0c\u65b0\u653f\u7b56\u4e0d\u4f1a\u83b7\u76ca\u3002  \u4f18\u52bf\u662f\u8d1f\u9762 \u7684\uff0c\u5047\u8bbe\u8be5\u72b6\u6001\u2014\u52a8\u4f5c \u5bf9\u7684\u4f18\u52bf\u662f\u8d1f\u7684\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b83\u5bf9\u76ee\u6807\u7684\u8d21\u732e\u51cf\u5c0f\u4e3a\uff1a  $$\nL(s,\\alpha,\\theta,\\theta_k)=max\\left(\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(\\alpha|s)},(1-\\epsilon)\\right)A^{\\pi\\theta_k}(s,\\alpha)  $$  \u56e0\u4e3a\u4f18\u52bf\u662f\u8d1f\u7684\uff0c\u6240\u4ee5\u5982\u679c \u91c7\u53d6\u884c\u52a8\u7684\u53ef\u80fd\u6027\u66f4\u5c0f\uff08\u4e5f\u5c31\u662f$\\pi_\\theta(\\alpha|s)$\u51cf\u5c0f\uff09\uff0c\u76ee\u6807\u5c31\u4f1a\u589e\u52a0\u3002\u4f46\u662fterm\u4e2d\u7684max\u9650\u5236\u4e86\u53ef\u4ee5\u589e\u52a0\u591a\u5c11\u76ee\u6807\u3002\u4e00\u65e6$\\pi_\\theta(\\alpha|s)<(1-\\epsilon)\\pi_{\\theta_k}(\\alpha|s)$,\u8fd9\u65f6max\u8e22\u8fdb\uff0cterm\u5230\u8fbe$(1-\\epsilon)A^{\\pi\\theta_k}(s,\\alpha)$,\u56e0\u6b64\uff0c\u518d\u6b21\uff0c\u8fdc\u79bb\u65e7\u653f\u7b56\uff0c\u4e0d\u4f1a\u4f7f\u65b0\u653f\u7b56\u53d7\u76ca\u3002  \u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u901a\u8fc7\u53bb\u9664\u4f7f\u653f\u7b56\u53d1\u751f\u91cd\u5927\u53d8\u5316\u7684\u6fc0\u52b1\uff0c\u88c1\u526a\u5145\u5f53\u7740\u6b63\u5219\u5316\u5668\u3002\u800c\u8d85\u53c2\u6570$\\epsilon$\u5219\u5bf9\u5e94\u4e8e\u65b0\u653f\u7b56\u4e0e\u65e7\u653f\u7b56\u53ef\u4ee5\u6709\u591a\u8fdc\uff0c\u540c\u65f6\u4ecd\u4f7f\u76ee\u6807\u53d7\u76ca\u3002",
            "title": "\u8981\u95fb\u901f\u89c8"
        },
        {
            "location": "/ros openai/#_3",
            "text": "PPO\u6309\u57fa\u4e8e\u7b56\u7565\u65b9\u5f0f\u8bad\u7ec3\u968f\u673a\u7b56\u7565\uff0c\u8fd9\u610f\u5473\u7740\u5b83\u4f1a\u6839\u636e\u5176\u968f\u673a\u7b56\u7565\u7684\u6700\u65b0\u7248\u672c\u901a\u8fc7\u91c7\u6837\u64cd\u4f5c\u6765\u8fdb\u884c\u63a2\u7d22\u3002\u52a8\u4f5c\u9009\u62e9\u7684\u968f\u673a\u6027\u53d6\u51b3\u4e8e\u521d\u59cb\u6761\u4ef6\u548c\u8bad\u7ec3\u6b65\u9aa4\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u4ee3\u7406\u9010\u6b65\u51cf\u5c11\u968f\u673a\u6027\uff0c\u56e0\u4e3a\u66f4\u65b0\u89c4\u5219\u9f13\u52b1\u5b83\u5229\u7528\u5df2\u7ecf\u53d1\u73b0\u7684\u5956\u52b1\u3002\u8fd9\u53ef\u80fd\u5bfc\u81f4\u7b56\u7565\u9677\u5165\u5c40\u90e8\u6700\u4f18\u72b6\u6001\u3002",
            "title": "\u63a2\u7d22\u4e0e\u53d1\u73b0"
        },
        {
            "location": "/ros openai/#_4",
            "text": "PPO-CLIP   \u8f93\u5165\uff1a\u521d\u59cb\u7b56\u7565\u53c2\u6570$\\theta_0$,\u521d\u59cb\u503c\u529f\u80fd\u53c2\u6570$\\phi_0$  for k = 0, 1, 2, ... do  \u5728\u73af\u5883\u4e2d\u901a\u8fc7\u6267\u884c\u7b56\u7565$\\pi_k=\\pi(\\theta_k)$\uff0c\u6536\u96c6\u4e00\u7ec4\u8f68\u8ff9$D_k={\\tau_i}$  \u7b97\u5956\u52b1$\\hat{R}_i$  \u57fa\u4e8e\u5f53\u524d\u7684value function $V_{\\phi k}$,\u8ba1\u7b97advantage estimates $\\hat{A}_t$  \u901a\u8fc7\u6700\u5927\u5316PPO-Clip\u76ee\u6807\uff0c\u66f4\u65b0\u7b56\u7565  $$\n\\theta_{k+1}=arg\\ \\underset{\\theta}{max}\\frac{1}{|D_k|T}\\sum_{\\tau\\in D_k}\\sum^T_{t=0}min\\left(\\frac{\\pi_\\theta(\\alpha_t|s_t)}{\\pi_{\\theta_k}(\\alpha_t|s_t)}A^{\\pi\\theta_k}(s_t,\\alpha_t),g(\\epsilon,A^{\\pi\\theta_k}(s_t,\\alpha_t))\\right)  $$  \u901a\u5e38\u901a\u8fc7\u4e0eAdam\u4e00\u8d77\u7684\u968f\u673a\u68af\u5ea6\u4e0a\u5347  \u901a\u8fc7\u5747\u65b9\u8bef\u5dee\u56de\u5f52\u62df\u5408value function  $$\n\\phi_{k+1}=arg\\ \\underset{\\phi}{min}\\frac{1}{|D_k|T}\\sum_{\\tau\\in D_k}\\sum^T_{t=0}(V_\\phi(s_t)-\\hat{R}_t)^2  $$  \u7279\u522b\u662f\u901a\u8fc7\u4e00\u4e9b\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5  end for",
            "title": "\u4f2a\u4ee3\u7801"
        },
        {
            "location": "/tf_document/",
            "text": "tf.compat.v1.train.Saver\n\n\n\n\ntf.compat.v1.train.Saver\n\\\nSaves and restores\u8fd8\u539f variables\n\n\n\n\ntf.compat.v1.train.Saver(\n    var_list=None, \n    reshape=False, \n    sharded=False, \n    max_to_keep=5,\n    keep_checkpoint_every_n_hours=10000.0, \n    name=None, \n    restore_sequentially=False,\n    saver_def=None, \n    builder=None, \n    defer_build=False, \n    allow_empty=False,\n    write_version=tf.train.SaverDef.V2, pad_step_number=False,\n    save_relative_paths=False, \n    filename=None\n)\n\n\n\n\n\nThe \nSaver\n class adds ops\u884c\u52a8 to save and restore variables to and form checkpoints.It also provides convenience method to run these ops.\n\n\nCheckpoints are binary files in a proprietary\u6240\u6709\u6743 format which map variable names to tensor values.Checkpoints \u662f\u4e13\u6709\u683c\u5f0f\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u8be5\u6587\u4ef6\u5c06\u53d8\u91cf\u540d\u6620\u5c04\u5230\u5f20\u91cf\u503c.The best way to examine\u5ba1\u67e5 the contents of a checkpoints is to load it using a \nSaver\n\n\nvar_list\n A list of Variable/SaveableObject, or a dictionary mapping names to SaveableObjects.If None,defaults to the list of all saveable objects.\n\n\nreshape\n If True,allows restoring\u6062\u590d parameters from a checkpoint where the variables have a different shape.\n\n\nshareded\n If True,shard the checkpoints,one per device\u6bcf\u4e2a\u8bbe\u5907\u4e00\u4e2a\n\n\nmax_to_keep\n Maximum number of recent checkpoints to keep,Default to 5\n\n\nkeep_checkpoint_every_n_hours\n How often to keep checkpoints,Defaults to 10,000 hours.\n\n\nname\n String, Optional name to use as a prefix when adding operations.\n\n\nrestore_sequentially\n A Bool,which if true,causes restore of different variables to happen sequentially within each device.This can lower memory usage when restoring large models.\n\n\nsaver_def\n Optional SaverDef proto\u539f\u578b to use instead of running the builder.This is only useful for specialty code that wants to recreate a Saver object for a previously built Graph that had a Saver.The saver_def proto should be the one returned by the as_saver_def() call of the Saver that was created for that Graph.\n\n\nbuilder\n Optional SaverBuilder to use if a saver_def was not provided. Default to BuikSaverBuilder().\n\n\ndefer_build\n If True,defer adding the save and restore ops to the build() call. In that case build() should be called before finalizing\u5b9a\u6848 the graph or using the saver.\n\n\nallow_empty\n If False(defalut) rasie an error if there are no variables in the graph.Otherwise, construct the saver anyway and make it a no-op.\n\n\nwrite_version\n controls what format to use when saving checkpoints. It also affects certain filepath matching logic. The V2 format is the recommended choice: it is much more optimized than V1 in terms of memory required and latency incurred during restore. Regardless of this flag, the Saver is able to restore from both V2 and V1 checkpoints.\n\n\npad_step_number\n if True, pads the global step number in the checkpoint filepaths to some fixed width (8 by default). This is turned off by default.\n\n\nsave_relative_paths\n If True, will write relative paths to the checkpoint state file. This is needed if the user wants to copy the checkpoint directory and reload from the copied directory.\n\n\nfilename\n  If known at graph construction time, filename used for variable loading/saving.\n\n\n\n\ntf.compat.v1.variable_scope\n\n\n\n\ntf.compat.v1.variable_scope\n\\\nA \ncontext manager\n for define ops that create variables(layers)\u7528\u4e8e\u5b9a\u4e49\u521b\u5efa\u53d8\u91cf\uff08\u5c42\uff09\u7684\u64cd\u4f5c\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\n\n\n\n\ntf.compat.v1.variable_scope(\n    name_or_scope, \n    default_name=None, \n    values=None, \n    initializer=None,\n    regularizer=None, \n    caching_device=None, \n    partitioner=None, \n    custom_getter=None,\n    reuse=None, \n    dtype=None, \n    use_resource=None, \n    constraint=None,\n    auxiliary_name_scope=True\n)\n\n\n\n\n\nThis context manager validates\u9a8c\u8bc1 that the values are from the same graph,ensures that graph is the default graph,and pushes a name scope\u8303\u56f4 and a variable scope.\n\n\nVariable scope allows you to create new variables and to share already created ones while providing checks to not create or share by accident.\u53d8\u91cf\u4f5c\u7528\u57df\u5141\u8bb8\u60a8\u521b\u5efa\u65b0\u53d8\u91cf\u5e76\u5171\u4eab\u5df2\u521b\u5efa\u7684\u53d8\u91cf\uff0c\u540c\u65f6\u63d0\u4f9b\u68c0\u67e5\u4ee5\u9632\u6b62\u610f\u5916\u521b\u5efa\u6216\u5171\u4eab.\n\n\nKeep in mind that the counters for default_name are discarded\u4e22\u5f03 once the parent scope is exited. Therefore when the code re-enters the scope (for instance by saving it), all nested\u5d4c\u5957\u7684 default_name counters will be restarted.\n\n\nNote that \nreuse\n flag is inherited: if we open a resuing scope,then all its sub-scope become reusing as well.\n\n\n\n\nA note about name scoping:Setting \nreuse\n does not impact the naming of other ops such as mult.\n\n\n\n\nreuse\n True,None,or tf.compat.v1.AUTO_REUSE;if True, we go into reuse mode for this scope as well as all sub-scopes; if tf.compat.v1.AUTO_REUSE, we create variables if they do not exist, and return them otherwise; if None, we inherit the parent scope's reuse flag. When eager execution is enabled, new variables are always created unless an EagerVariableStore or template is currently active.\n\n\n\n\ntf.compat.v1.layers.dense\n\n\n\n\ntf.compat.v1.layers.dense\n\\\nFunctional interface for the densely-connected layer.\n\n\n\n\ntf.compat.v1.layers.dense(\n    inputs, units, activation=None, use_bias=True, kernel_initializer=None,\n    bias_initializer=tf.zeros_initializer(), kernel_regularizer=None,\n    bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,\n    bias_constraint=None, trainable=True, name=None, reuse=None\n)\n\n\n\n\n\nThe layer implement the operation:\noutput=activation(inputs * kernel + bias)\n where activation is the activation function passed as the activation argument(if not \n\uff2eone\n),kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer(only if use_bias is True).\n\n\ninputs\n Tensor input.\n\n\nunits\n Integer or Long, dimensionality of the output space.\n\n\nactivation\n Activation function(callable), Set it to None to maintain a linear activation.\n\n\nuse_bias\n Boolean, whether the layer uses a bias\n\n\nkernel_initializer\n Initializer function for the weight matrix. If None (default), weights are initialized using the default initializer used by tf.compat.v1.get_variable.\n\n\nbias_initializer\n Initializer function for the bias\n\n\nkernel_regularizer\n Regularizer\u6b63\u5219\u5316\u5668 function for the weight matrix\n\n\nbias_regularizer\n Regularizer function for the bias.\n\n\nactivity_regularizer\n Regularizer function for the output.\n\n\nkernel_constraint\n An optional projection function to be applied to the kernel after being updated by an Optimizer (e.g. used to implement norm constraints or value constraints for layer weights). The function must take as input the unprojected variable and must return the projected variable (which must have the same shape). Constraints\u7ea6\u675f\u6761\u4ef6 are not safe to use when doing asynchronous distributed training.\n\n\nbias_constraint\n An optional projection function to be applied to the bias after being updated by an Optimizer.\n\n\ntrainable\n Boolean, if True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable).\n\n\nname\n  String, the name of the layer.\n\n\nreuse\n Boolean, whether to reuse the weights of a previous layer by the same name.\n\n\n\n\ntf.compat.v1.layers.BatchNormalization\n\n\n\n\ntf.compat.v1.layers.BatchNormalization\n\\\nBatch Normalization layer\n\n\n\n\ntf.compat.v1.layers.BatchNormalization(\n    axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,\n    beta_initializer=tf.zeros_initializer(),\n    gamma_initializer=tf.ones_initializer(),\n    moving_mean_initializer=tf.zeros_initializer(),\n    moving_variance_initializer=tf.ones_initializer(), beta_regularizer=None,\n    gamma_regularizer=None, beta_constraint=None, gamma_constraint=None,\n    renorm=False, renorm_clipping=None, renorm_momentum=0.99, fused=None,\n    trainable=True, virtual_batch_size=None, adjustment=None, name=None, **kwargs\n)\n\n\n\n\n\nfused\n if None or True, use a faster, fused implementation if possible. If False, use the system recommended implementation.\n\n\n\n\ntf.math.reduce_max\n\n\n\n\ntf.math.reduce_max\n\\\nComputes the maximum of elements across dimensions of a tensor\n\n\n\n\ntf.math.reduce_max(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n\n\n\n\n\nReduces input_tensor along the dimensions given in axis.Unless keepdims is true, the rank of the tensor is reduced by 1 for each of the entires in axis, which must be unique. If keepdims is true, the reduced dimensions are retained with length 1.\n\n\nIf axis is None,all dimensions are reduced, and a tensor with a single element is returned.\n\n\ninput_tensor\n The tensor to reduce.Should have real numeric type\n\n\naxis\n The dimensions to reduce.If None (the default),reduces all dimensions.Must be in range [-rank(input_tensor),rank(input_tensor)]\n\n\nkeepdims\n If true,retains reduced dimensions with length 1.\n\n\nname\n A name for the operation(optional).\n\n\n\n\ntf.tile\n\n\n\n\ntf.tile\n\\\nConstructs\u6784\u9020 a tensor by tiling\u5e73\u94fa a given tensor\n\n\n\n\ntf.tile(\n    input, multiples, name=None\n)\n\n\n\n\n\nThis operation creates a new tensor by replicating\u590d\u5236 \ninput\n \nmultiples\n times.The output tensor's i'th dimension has \ninput.dims(i)*multiples[i]\n elements, and the values of \ninput\n are replicated \nmultiples[i]\n times along the i'th dimension.\n\n\ninput\n A tensor,1-D or higher\n\n\nmultiples\n A tensor. Must be one of the following types: int32, int64, 1-D Length must be the same as the number of dimensions in input.\n\n\nname\n A name for the operation(optional).\n\n\n\n\ntf.concat\n\n\n\n\ntf.concat\n\\\nConcatenates tensors along one dimension.\n\n\n\n\ntf.concat(\n    values, axis, name='concat'\n)\n\n\n\ntf.cast\n\n\n\n\ntf.cast\n\\\nCast a tensor to a new type.\n\n\n\n\ntf.cast(\n    x, dtype, name=None\n)\n\n\n\n\n\nThe operation supports data types (for x and dtype) of uint8, uint16, uint32, uint64, int8, int16, int32, int64, float16, float32, float64, complex64, complex128, bfloat16. In case of casting from complex types (complex64, complex128) to real types, only the real part of x is returned. In case of casting from real types to complex types (complex64, complex128), the imaginary part of the returned value is set to 0. The handling of complex types here matches the behavior of numpy.\n\n\n\n\ntf.where\n\n\n\n\nReturn the elements where condition is True (multiplexing x and y).\n\n\n\n\ntf.where(\n    condition, x=None, y=None, name=None\n)\n\n\n\n\\",
            "title": "Tf document"
        },
        {
            "location": "/tf_document/#tfcompatv1trainsaver",
            "text": "tf.compat.v1.train.Saver \\\nSaves and restores\u8fd8\u539f variables   tf.compat.v1.train.Saver(\n    var_list=None, \n    reshape=False, \n    sharded=False, \n    max_to_keep=5,\n    keep_checkpoint_every_n_hours=10000.0, \n    name=None, \n    restore_sequentially=False,\n    saver_def=None, \n    builder=None, \n    defer_build=False, \n    allow_empty=False,\n    write_version=tf.train.SaverDef.V2, pad_step_number=False,\n    save_relative_paths=False, \n    filename=None\n)   The  Saver  class adds ops\u884c\u52a8 to save and restore variables to and form checkpoints.It also provides convenience method to run these ops.  Checkpoints are binary files in a proprietary\u6240\u6709\u6743 format which map variable names to tensor values.Checkpoints \u662f\u4e13\u6709\u683c\u5f0f\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u8be5\u6587\u4ef6\u5c06\u53d8\u91cf\u540d\u6620\u5c04\u5230\u5f20\u91cf\u503c.The best way to examine\u5ba1\u67e5 the contents of a checkpoints is to load it using a  Saver  var_list  A list of Variable/SaveableObject, or a dictionary mapping names to SaveableObjects.If None,defaults to the list of all saveable objects.  reshape  If True,allows restoring\u6062\u590d parameters from a checkpoint where the variables have a different shape.  shareded  If True,shard the checkpoints,one per device\u6bcf\u4e2a\u8bbe\u5907\u4e00\u4e2a  max_to_keep  Maximum number of recent checkpoints to keep,Default to 5  keep_checkpoint_every_n_hours  How often to keep checkpoints,Defaults to 10,000 hours.  name  String, Optional name to use as a prefix when adding operations.  restore_sequentially  A Bool,which if true,causes restore of different variables to happen sequentially within each device.This can lower memory usage when restoring large models.  saver_def  Optional SaverDef proto\u539f\u578b to use instead of running the builder.This is only useful for specialty code that wants to recreate a Saver object for a previously built Graph that had a Saver.The saver_def proto should be the one returned by the as_saver_def() call of the Saver that was created for that Graph.  builder  Optional SaverBuilder to use if a saver_def was not provided. Default to BuikSaverBuilder().  defer_build  If True,defer adding the save and restore ops to the build() call. In that case build() should be called before finalizing\u5b9a\u6848 the graph or using the saver.  allow_empty  If False(defalut) rasie an error if there are no variables in the graph.Otherwise, construct the saver anyway and make it a no-op.  write_version  controls what format to use when saving checkpoints. It also affects certain filepath matching logic. The V2 format is the recommended choice: it is much more optimized than V1 in terms of memory required and latency incurred during restore. Regardless of this flag, the Saver is able to restore from both V2 and V1 checkpoints.  pad_step_number  if True, pads the global step number in the checkpoint filepaths to some fixed width (8 by default). This is turned off by default.  save_relative_paths  If True, will write relative paths to the checkpoint state file. This is needed if the user wants to copy the checkpoint directory and reload from the copied directory.  filename   If known at graph construction time, filename used for variable loading/saving.",
            "title": "tf.compat.v1.train.Saver"
        },
        {
            "location": "/tf_document/#tfcompatv1variable_scope",
            "text": "tf.compat.v1.variable_scope \\\nA  context manager  for define ops that create variables(layers)\u7528\u4e8e\u5b9a\u4e49\u521b\u5efa\u53d8\u91cf\uff08\u5c42\uff09\u7684\u64cd\u4f5c\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668   tf.compat.v1.variable_scope(\n    name_or_scope, \n    default_name=None, \n    values=None, \n    initializer=None,\n    regularizer=None, \n    caching_device=None, \n    partitioner=None, \n    custom_getter=None,\n    reuse=None, \n    dtype=None, \n    use_resource=None, \n    constraint=None,\n    auxiliary_name_scope=True\n)   This context manager validates\u9a8c\u8bc1 that the values are from the same graph,ensures that graph is the default graph,and pushes a name scope\u8303\u56f4 and a variable scope.  Variable scope allows you to create new variables and to share already created ones while providing checks to not create or share by accident.\u53d8\u91cf\u4f5c\u7528\u57df\u5141\u8bb8\u60a8\u521b\u5efa\u65b0\u53d8\u91cf\u5e76\u5171\u4eab\u5df2\u521b\u5efa\u7684\u53d8\u91cf\uff0c\u540c\u65f6\u63d0\u4f9b\u68c0\u67e5\u4ee5\u9632\u6b62\u610f\u5916\u521b\u5efa\u6216\u5171\u4eab.  Keep in mind that the counters for default_name are discarded\u4e22\u5f03 once the parent scope is exited. Therefore when the code re-enters the scope (for instance by saving it), all nested\u5d4c\u5957\u7684 default_name counters will be restarted.  Note that  reuse  flag is inherited: if we open a resuing scope,then all its sub-scope become reusing as well.   A note about name scoping:Setting  reuse  does not impact the naming of other ops such as mult.   reuse  True,None,or tf.compat.v1.AUTO_REUSE;if True, we go into reuse mode for this scope as well as all sub-scopes; if tf.compat.v1.AUTO_REUSE, we create variables if they do not exist, and return them otherwise; if None, we inherit the parent scope's reuse flag. When eager execution is enabled, new variables are always created unless an EagerVariableStore or template is currently active.",
            "title": "tf.compat.v1.variable_scope"
        },
        {
            "location": "/tf_document/#tfcompatv1layersdense",
            "text": "tf.compat.v1.layers.dense \\\nFunctional interface for the densely-connected layer.   tf.compat.v1.layers.dense(\n    inputs, units, activation=None, use_bias=True, kernel_initializer=None,\n    bias_initializer=tf.zeros_initializer(), kernel_regularizer=None,\n    bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,\n    bias_constraint=None, trainable=True, name=None, reuse=None\n)   The layer implement the operation: output=activation(inputs * kernel + bias)  where activation is the activation function passed as the activation argument(if not  \uff2eone ),kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer(only if use_bias is True).  inputs  Tensor input.  units  Integer or Long, dimensionality of the output space.  activation  Activation function(callable), Set it to None to maintain a linear activation.  use_bias  Boolean, whether the layer uses a bias  kernel_initializer  Initializer function for the weight matrix. If None (default), weights are initialized using the default initializer used by tf.compat.v1.get_variable.  bias_initializer  Initializer function for the bias  kernel_regularizer  Regularizer\u6b63\u5219\u5316\u5668 function for the weight matrix  bias_regularizer  Regularizer function for the bias.  activity_regularizer  Regularizer function for the output.  kernel_constraint  An optional projection function to be applied to the kernel after being updated by an Optimizer (e.g. used to implement norm constraints or value constraints for layer weights). The function must take as input the unprojected variable and must return the projected variable (which must have the same shape). Constraints\u7ea6\u675f\u6761\u4ef6 are not safe to use when doing asynchronous distributed training.  bias_constraint  An optional projection function to be applied to the bias after being updated by an Optimizer.  trainable  Boolean, if True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable).  name   String, the name of the layer.  reuse  Boolean, whether to reuse the weights of a previous layer by the same name.",
            "title": "tf.compat.v1.layers.dense"
        },
        {
            "location": "/tf_document/#tfcompatv1layersbatchnormalization",
            "text": "tf.compat.v1.layers.BatchNormalization \\\nBatch Normalization layer   tf.compat.v1.layers.BatchNormalization(\n    axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,\n    beta_initializer=tf.zeros_initializer(),\n    gamma_initializer=tf.ones_initializer(),\n    moving_mean_initializer=tf.zeros_initializer(),\n    moving_variance_initializer=tf.ones_initializer(), beta_regularizer=None,\n    gamma_regularizer=None, beta_constraint=None, gamma_constraint=None,\n    renorm=False, renorm_clipping=None, renorm_momentum=0.99, fused=None,\n    trainable=True, virtual_batch_size=None, adjustment=None, name=None, **kwargs\n)   fused  if None or True, use a faster, fused implementation if possible. If False, use the system recommended implementation.",
            "title": "tf.compat.v1.layers.BatchNormalization"
        },
        {
            "location": "/tf_document/#tfmathreduce_max",
            "text": "tf.math.reduce_max \\\nComputes the maximum of elements across dimensions of a tensor   tf.math.reduce_max(\n    input_tensor, axis=None, keepdims=False, name=None\n)   Reduces input_tensor along the dimensions given in axis.Unless keepdims is true, the rank of the tensor is reduced by 1 for each of the entires in axis, which must be unique. If keepdims is true, the reduced dimensions are retained with length 1.  If axis is None,all dimensions are reduced, and a tensor with a single element is returned.  input_tensor  The tensor to reduce.Should have real numeric type  axis  The dimensions to reduce.If None (the default),reduces all dimensions.Must be in range [-rank(input_tensor),rank(input_tensor)]  keepdims  If true,retains reduced dimensions with length 1.  name  A name for the operation(optional).",
            "title": "tf.math.reduce_max"
        },
        {
            "location": "/tf_document/#tftile",
            "text": "tf.tile \\\nConstructs\u6784\u9020 a tensor by tiling\u5e73\u94fa a given tensor   tf.tile(\n    input, multiples, name=None\n)   This operation creates a new tensor by replicating\u590d\u5236  input   multiples  times.The output tensor's i'th dimension has  input.dims(i)*multiples[i]  elements, and the values of  input  are replicated  multiples[i]  times along the i'th dimension.  input  A tensor,1-D or higher  multiples  A tensor. Must be one of the following types: int32, int64, 1-D Length must be the same as the number of dimensions in input.  name  A name for the operation(optional).",
            "title": "tf.tile"
        },
        {
            "location": "/tf_document/#tfconcat",
            "text": "tf.concat \\\nConcatenates tensors along one dimension.   tf.concat(\n    values, axis, name='concat'\n)",
            "title": "tf.concat"
        },
        {
            "location": "/tf_document/#tfcast",
            "text": "tf.cast \\\nCast a tensor to a new type.   tf.cast(\n    x, dtype, name=None\n)   The operation supports data types (for x and dtype) of uint8, uint16, uint32, uint64, int8, int16, int32, int64, float16, float32, float64, complex64, complex128, bfloat16. In case of casting from complex types (complex64, complex128) to real types, only the real part of x is returned. In case of casting from real types to complex types (complex64, complex128), the imaginary part of the returned value is set to 0. The handling of complex types here matches the behavior of numpy.",
            "title": "tf.cast"
        },
        {
            "location": "/tf_document/#tfwhere",
            "text": "Return the elements where condition is True (multiplexing x and y).   tf.where(\n    condition, x=None, y=None, name=None\n)  \\",
            "title": "tf.where"
        },
        {
            "location": "/ubuntu18/",
            "text": "\u66f4\u6539mac\u5730\u5740\n\n\n\n\n\u4f7f\u7528ifconfig\u3000\u67e5\u770b\u7f51\u53e3\n\n\n\u5b89\u88c5macmanager \nsudo apt install macchanger\n\n\n\u968f\u673a\u751f\u6210\u4e00\u4e2amac\u5730\u5740 \nsudo macchanger -r enp60s0enp60s0\n\u662f\u7f51\u53e3\uff0c\n-r\n\u4ee3\u8868\u7684\u662f\u968f\u673a \nrandom\n\u7684\u610f\u601d\uff0c\nmacchanger\n\u4f1a\u5e2e\u6211\u4eec\u4fee\u6539\u6210\u4e00\u4e2a\u968f\u673a\u4ea7\u751f\u7684 \nMAC\n\u53f7\n\n\n\u4fee\u6539\u4e3a\u6307\u5b9a\u7684mac\u5730\u5740 \nsudo macchanger -m AA:BB:CC:DD:EE:FF enp60s0\n\\\n\n\n\n\nubuntu18.04 \u7f51\u53e3\u521b\u5efa\u7f51\u7edc\u5171\u4eab\n\n\n\n\n\u7ec8\u7aef\u8f93\u5165nm-connection-editor\u6253\u5f00\u7f51\u7edc\u8fde\u63a5\n\n\n\u521b\u5efa\u4ee5\u592a\u7f51\u94fe\u63a5\n\n\n\u914d\u7f6e\u7f51\u7edc\u94fe\u63a5\n\n\n\u5c06\u5176\u4ed6\u9700\u8981\u4e0a\u7f51\u7684\u8bbe\u5907\u901a\u8fc7\u7f51\u7ebf\u94fe\u63a5\u5230\u5171\u4eab\u7f51\u7edc\u5373\u53ef\n\n\n\n\nxvfb\u3000ssh\n\n\nxvfb-run -s \"-screen 0 1400x900x24\" python <your_script.py>\n\n\nx forward\n\n\nssh -X username@ip\n\n\nvscode \u63d2\u4ef6\u3000Remote-ssh\n\n\nconda install jupyter\n\n\ngithub\u4e0b\u8f7d\u63d0\u901f\n\n\ngit clone https://github.com/Amritpal-001/Reinforcement-learning-projects.git\n\n\u6539\u4e3a\n\ngit clone http://hub.fastgit.org/Amritpal-001/Reinforcement-learning-projects.git\n\n\ngithub \u4e0b\u8f7d\u5de5\u5177\n\n\nhttps://d.serctl.com/\n\n\nhttps://www.python.org/ftp \u4e0b\u8f7d\u6162\n\n\nwget https://www.python.org/ftp/python/3.7.5/Python-3.7.5.tgz\n\n\u6539\u4e3a\n\nwget https://npm.taobao.org/mirrors/python/3.7.5/Python-3.7.5.tgz\n\n\npip \u4e0b\u8f7d\u4e1c\u897f\u6162\n\n\npip3.7.5 install -i http://mirrors.aliyun.com/pypi/simple/ psutil decorator numpy protobuf==3.11^C scipy sympy cffi grpcio grpcio-tools requests --user --trusted-host mirrors.aliyun.com\n\n\nmkdocs\n\n\nmkdocs build\n\n\nmkdocs serve\n\n\npush sites folders\n\n\nvscode python ros debug\n\n\n\u9996\u5148\n\n\ncatkin_make -DCMAKE_BUILD_TYPE=DEBUG\n\n\n\n\u5176\u6b21\u70b9\u51fbdebug\u6309\u94ae\uff0c\u9009\u62e9\u751f\u6210\u65b0\u7684launch\u6587\u4ef6\u3002\n\u7136\u540e\u70b9\u51fb \nshow\n,\n\n\n\uff44ebug\u65f6\u9047\u5230\u8def\u5f84\u95ee\u9898,\u4f8b\u5982rosmsg\u8def\u5f84\uff0c\u6700\u597d\u5728\u6587\u4ef6\u5c5e\u6027\u91cc\u590d\u5236\u8def\u5f84\uff0c\u4e0d\u7136\u5bb9\u6613\u51fa\u9519\n\n\nimport sys\nsys.path.append(\"/home/pmjd/Downloads/catkin_ws/devel/lib/python2.7/dist-packages\")\n\n\n\nTilix doesnt open in the folder from where it is split\n\n\nUpdate \n~.bashrc\n (or \n~.zshrc\n if you are using zsh) to execute vte.sh directly, this involves adding the following line at the end of the file.\n\n\nif [[ $TILIX_ID ]]; then\n        source /etc/profile.d/vte.sh\nfi\n\n\n\nOn Ubuntu (18.04), a symlink is probably missing. You can create it with:\n\n\nln -s /etc/profile.d/vte-2.91.sh /etc/profile.d/vte.sh\n\n\n\nsudo dist-upgrade \u662f\u6bc1\u706d\u6027\u7684\uff0c\u4f1a\u5347\u7ea7cuda\n\n\ngithub init\n\n\ngit config --global user.email \"you@example.com\"\ngit config --global user.name \"Your Name\"\ngit add .\ngit commit -m \"1\"\ngit push\n\n\n\n\u547d\u4ee4\u884c\u6253\u5f00\u6587\u4ef6\u7ba1\u7406\u5668\n\n\nnautilus --browser ~/\u6587\u6863\n\n\n\u5b89\u88c5\u6c49\u8bed\u8f93\u5165\u6cd5\n\n\n\u5148\u5728\u8bbe\u7f6e\u91cc\u8bbe\u7f6elanguage\u91cc\u6dfb\u52a0Chinese\n\n\nsudo apt-get install ibus-libpinyin ibus-clutter\n\n\n\nDocker\u521b\u5efa\u5bb9\u5668\n\n\n\u6784\u5efaubuntu18.04\u6620\u50cf\n\n\n\u5b89\u88c5\u4f9d\u8d56\n\n\nsudo apt-get install debootstrap\nsudo apt install docker.io\nsudo chmod 666 /var/run/docker.sock\n\n\n\n\u521b\u5efaubuntu 18.04 \u955c\u50cf\n\n\nsudo debootstrap bionic bionic > /dev/null\nsudo tar -C bionic -c . | docker import - bionic/smart_eye\n\n\n\n\u6d4b\u8bd5\n\n\ndocker run bionic cat /etc/lsb-release\n\n\n\n\u67e5\u770b\u955c\u50cf\n\n\ndocker images\n\n\n\n\u5220\u9664\u955c\u50cf\n\n\ndocker rmi [IMAGE ID]\n\n\n\n\u67e5\u770b\u5bb9\u5668\u8fd0\u884c\u60c5\u51b5\n\n\nsudo docker ps -a\n\n\n\n\u9000\u51fa\u5bb9\u5668\n\n\nsudo docker stop \u5bb9\u5668id\n\n\n\n\u5220\u9664\u5bb9\u5668\n\n\nsudo docker rm \u5bb9\u5668id\n\n\n\n\u542f\u52a8\u955c\u50cf\n\n\ndocker run -it bionic/smart_eye  /bin/bash\n\n\n\n\u9000\u51fa\u955c\u50cf\n\n\nexit\n\n\n\n\u6587\u4ef6\u4f20\u9012\n\n\n\u4ece\u672c\u5730\u81f3docker\n\n\ndocker cp FILE_PATH \u5bb9\u5668ID:/root\n\n\n\n\u4ecedocker \u81f3\u672c\u5730\n\n\ndocker cp \u5bb9\u5668ID:/root/data.tar /home/user\n\n\n\n\u66f4\u6539docker image\u5b58\u653e\u8def\u5f84\n\n\nsudo service docker stop\nsudo touch /etc/docker/daemon.json\n\n\n\ndaemon.json\n\n\n{\n    \"data-root\":\"/home/pmjd/docker\"\n}\n\n\n\nsudo service docker start\n\n\n\nDocker\u66f4\u65b0apt source.list\n\n\nsudo nano /etc/apt/sources.list\n\n\n\n\u6dfb\u52a0\n\n\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\n\n\n\n\nDocker \u542f\u52a8\u5bb9\u5668\n\n\ndocker start \u5bb9\u5668ID\ndocker attach \u5bb9\u5668ID\n\n\n\n\u5bfc\u51fa\u5bb9\u5668\u5feb\u7167\n\n\ndocker export c91c33f28594 > smart_eye_docker.tar\n\n\n\n\u5bb9\u5668\u5feb\u7167\u5bfc\u5165\u4e3a\u955c\u50cf\n\n\ncat smart_eye_docker.tar | docker import - test/smart_eye:v1\n\n\n\n\u518d\u4fdd\u5b58\u6b64\u955c\u50cf\n\n\n docker save -o smart_eye_image_docker.tar test/smart_eye\n\n\n\n\u518d\u52a0\u8f7d\u955c\u50cf\n\n\ndocker load --input samrt_eye_image_docker.tar\n\n\n\n\u542f\u52a8\n\n\ndocker run -it test/smart_eye:v1 /bin/bash\n\n\n\ndocker \u542f\u52a8bash\n\n\ndocker start c91c33f28594\ndocker exec -it c91c33f28594 /home/run.sh\ndocker exec mycontainer /bin/sh -c \"cmd1;cmd2;...;cmdn\"\n\n\n\ndocker bash file example\n\n\n#!/bin/sh\ndocker run -it --net host --add-host in_release_docker:127.0.0.1 --add-host localhost:127.0.0.1 --hostname in_release_docker --rm promote/smart_eye:v1 /bin/bash -c \"/home/run.sh\"\n\n\n\nsounddevice invalid sample rate\n\n\nnano /etc/asound.conf\n\n\n\n\n\u542f\u52a8\u5feb\u6377\u65b9\u5f0f\n\n\n1.desktop\n\n\n[Desktop Entry]\nName=Smart_eye\nGenericName=3D modeler\nKeywords=3d;cg;modeling;animation;painting;sculpting;texturing;video editing;video tracking;rendering;render engine;cycles;game engine;python;\nExec=/bin/bash -c '/home/promote/run.sh'\n#Icon=/home/pmjd/Disk/blender-2.90.1-linux64/blender.svg\nTerminal=true\nType=Application\nCategories=Graphics;3DGraphics;\nMimeType=application/x-blender;\n\n\n\n\u754c\u9762\u663e\u793a\n\n\nxhost +\n-e DISPLAY=${DISPLAY} -v /tmp/.X11-unix:/tmp/.X11-unix\n\n\n\n\u5b89\u88c5NVIDIA Container Toolkit\n\n\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\\n   && curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \\\n   && curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n\n\n\ncurl -s -L https://nvidia.github.io/nvidia-container-runtime/experimental/$distribution/nvidia-container-runtime.list | sudo tee /etc/apt/sources.list.d/nvidia-container-runtime.list\n\n\n\nsudo apt-get update\nsudo apt-get install -y nvidia-docker2\n\n\n\n\u8fd9\u4e2a\u65f6\u5019 \n/etc/docker/daemon.json\n \u5185\u5bb9\u4fee\u6539\u4e3a\n\n\n{\n    \"runtimes\": {\n        \"nvidia\": {\n            \"path\": \"nvidia-container-runtime\",\n            \"runtimeArgs\": []\n        }\n    },\n    \"data-root\":\"/home/pmjd/docker\"\n}\n\n\n\n\u91cd\u542fdocker\n\n\nsudo systemctl restart docker\n\n\n\n\u4e0b\u8f7d\u8fd0\u884c\uff1a\n\n\nsudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi\nsudo docker run --rm --gpus all nvidia/cuda:10.1-base nvidia-smi\n\n\n\n\u4e0b\u8f7d\u8fd0\u884ctensorflow:\n\n\ndocker run --gpus all --runtime=nvidia -it tensorflow/tensorflow:2.3.0-gpu bash\n\n\n\ndocker\u914d\u7f6e\u7f51\u7edc\n\n\n--net=host \n\n\n\ndocker \u652f\u6301\u9ea6\u514b,\u6c49\u8bed\n\n\ndocker run -it --volume=/run/user/1000/pulse:/run/user/1000/pulse --user promote --gpus all --runtime=nvidia --name=xiaomeng -e LANG=C.UTF-8 --device /dev/snd promote/xiaomeng:v1.6 /bin/bash\n\n\n\ndocker \u8fd0\u884c\u9700\u8981sudo\n\n\n\u901a\u8fc7\u5c06\u7528\u6237\u6dfb\u52a0\u5230docker\u7528\u6237\u7ec4\u53ef\u4ee5\u5c06sudo\u53bb\u6389\uff0c\u547d\u4ee4\u5982\u4e0b\n\nsudo groupadd docker #\u6dfb\u52a0docker\u7528\u6237\u7ec4\n\nsudo gpasswd -a $USER docker #\u5c06\u767b\u9646\u7528\u6237\u52a0\u5165\u5230docker\u7528\u6237\u7ec4\u4e2d\n\nnewgrp docker #\u66f4\u65b0\u7528\u6237\u7ec4\n\n\n\nbash \u767b\u5165ssh\n\n\n#!/usr/bin/expect   \n\nset timeout 3             \nspawn ssh -X promote@192.168.1.2 \nexpect \"*password*\" {send \"123456\\r\"}\nexpect \"$ \" { send \"bash /home/username/1.sh\\r\" }\ninteract   \n\n\n\nNano \u5220\u9664\u884c\n\n\nctrl+k\n\n\n\nNano\u663e\u793a\u884c\u53f7\n\n\nalt+shift+3\n\n\n\n\u4fee\u6539\u9ed8\u8ba4python\u7248\u672c\n\n\n\u5220\u9664/usr/bin \u4e0b\u7684Python\u94fe\u63a5\n\n\nsudo rm /usr/bin/python\n\n\n\n\u7528\u4e0b\u9762\u547d\u4ee4\u5efa\u7acb\u65b0\u7684\u94fe\u63a5\n\n\nsudo ln -s /usr/bin/python3.6 /usr/bin/python\n\n\n\n\u7528\u4e0b\u9762\u7684\u547d\u4ee4\u8fd8\u539f2\u7248\u672c\n\n\nsudo ln -s /usr/bin/python2.7 /usr/bin/python\n\n\n\n\u66f4\u79d1\u5b66\u7684\u505a\u6cd5\u662f\uff1a\n\n\nsudo update-alternatives --install /usr/bin/python python /usr/bin/python2.7 1\n\nsudo update-alternatives --install /usr/bin/python python /usr/bin/python3.6 2\n\nsudo update-alternatives --config python \u9009\u62e9\u8981\u4f7f\u7528\u7684\u7248\u672c\uff0c\u56de\u8f66\uff0c\u641e\u5b9a\n\n\n\n\u8bbe\u7f6e\u9ed8\u8ba4pip\u7248\u672c\n\n\nsudo update-alternatives --install /usr/bin/pip pip /usr/bin/pip2 1\n\nsudo update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 2\n\nsudo update-alternatives --config pip\n\n\n\npip\u5347\u7ea7\n\n\nsudo pip install --upgrade pip\n\n\n\n\u684c\u9762\u5feb\u6377\u65b9\u5f0f\n\n\n[Desktop Entry]\nName=smart_view\nGenericName=3D modeler\nKeywords=python;\nExec=/bin/bash -c 'source /opt/ros/melodic/setup.bash;rosrun image_view image_view image:=/smart_eye_view'\n#Icon=/home/pmjd/Disk/blender-2.90.1-linux64/blender.svg\nTerminal=false\nType=Application\nCategories=Graphics;3DGraphics;\nMimeType=application/x-blender;\nName[en_US]=smart_view\n\n\n\nmd\u516c\u5f0f\u91cc\u6dfb\u52a0\u7a7a\u683c\n\n\n\\quad\n\u6216\u8005 \n\\+\u7a7a\u683c\n \u6216\u8005 \n&nbsp\n\n\nmd\u6dfb\u52a0\u591a\u884c\u516c\u5f0f\n\n\n![](https://latex.codecogs.com/svg.latex?\\Large&space;\\theta_{k+1}=arg\\quad\\underset{\\theta}{max})\n\n\n\n\n<img src=\"https://latex.codecogs.com/svg.image?\\&space;\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}=K^{-1}_{3\\times 3}\\begin{bmatrix}z\\cdot(x_c+\\delta_{x_c})\\\\z\\cdot(y_c+\\delta_{y_c})\\\\z\\end{bmatrix}\"/>\n\n\n<img src=\"https://latex.codecogs.com/svg.image?\\&space;\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}=K^{-1}_{3\\times 3}\\begin{bmatrix}z\\cdot(x_c+\\delta_{x_c})\\\\z\\cdot(y_c+\\delta_{y_c})\\\\z\\end{bmatrix}\"/>\n\n\nmd\u6dfb\u52a0\u884c\u5185\u516c\u5f0f\n\n\n$\\theta$\n $\\theta$\n\n\nmkdocs \u663e\u793a\u516c\u5f0f\n\n\npip install https://github.com/mitya57/python-markdown-math/archive/master.zip\n\n\n\n\u5728 \nconfig.yaml\n\u4e2d\u6dfb\u52a0\n\n\nextra_javascript: \n    - https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML\n    - mathjaxhelper.js\nmarkdown_extensions:\n    - mdx_math\n\n\n\n\u5728docs\u6587\u4ef6\u5939\u4e0b\u65b0\u5efa \nmathjaxhelper.js\n\n\nMathJax.Hub.Config({\n  \"tex2jax\": { inlineMath: [ [ '$', '$' ] ] }\n\n});\nMathJax.Hub.Config({\n  config: [\"MMLorHTML.js\"],\n  jax: [\"input/TeX\", \"output/HTML-CSS\", \"output/NativeMML\"],\n  extensions: [\"MathMenu.js\", \"MathZoom.js\"]\n});\n\n\n\n\u8fd9\u6837\u5c31\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 \n$\u516c\u5f0f$\n \u548c \n$$\u516c\u5f0f$$\n\n\nwget \u65ad\u7f51\u91cd\u8fde\n\n\n  wget -t 0 -c < \u6587\u4ef6\u5730\u5740>\n  -c \u8868\u793a\u65ad\u70b9\u7eed\u8fde\n  -t \u8868\u793a \u65ad\u4e86\u4f1a\u6bcf\u7ecf\u8fc7\u51e0\u79d2\u91cd\u65b0\u8fde\u63a5\u8fde\u63a5\u4e00\u6b21\uff0c0\u8868\u793a\u4e00\u76f4\u4e0d\u65ad\u91cd\u8fde\uff0c\u6709\u6700\u5927\u8fde\u63a5\u6b21\u6570\u7684\u3002\n  \u6ce8\u610f\uff1a \u8bb0\u5f97\u5728\u539f\u6765\u7684\u76ee\u5f55\u4e0b\u6267\u884c\u8fd9\u4e2a\u547d\u4ee4\uff0c\u624d\u4f1a\u63a5\u7740\u4e0a\u6587\u4e0b\u8f7d\uff0c\u627e\u4e0d\u5230\u6587\u4ef6\u76f4\u63a5\u91cd\u65b0\u4e0b\u8f7d\n\n\n\n\u6307\u4ee4\u67e5\u770b\u786c\u76d8\u5b58\u50a8\u60c5\u51b5\n\n\ndf -h\n\n\n\u67e5\u770b\u6240\u6709\u786c\u76d8\n\n\nlsblk\n\n\n\u6302\u8f7d\u786c\u76d8\n\n\nsudo fdisk -l #\u67e5\u770b\u78c1\u76d8\u4fe1\u606f\nsudo blkid #\u67e5\u770b\u5206\u533a\nmkdir NeDisk # \u521b\u5efa\u6302\u8f7d\u70b9\ndf -kh #\u67e5\u770b\u5df2\u6709\u6302\u8f7d\uff0c\u786e\u5b9a\u662f\u5426\u5df2\u6302\u8f7d\nsudo umount /dev/sda5 #\u5378\u8f7d\u5df2\u6302\u8f7d\nsudo nano /etc/fstab\nsudo blkid /dev/sda5 # \u627e\u5230\u5176UUID\n#\u7136\u540e,\u6211\u4eec\u6309\u7167/etc/fstab\u6587\u4ef6\u4e2d\u7684\u683c\u5f0f\u6dfb\u52a0\u4e00\u884c\u5982\u4e0b\u5185\u5bb9:\nUUID=0001D3CE0001E53B /home/ubuntu/NewDisk ntfs defaults 0 2\n#\u5176\u4e2d\u7b2c\u4e00\u5217\u4e3aUUID, \u7b2c\u4e8c\u5217\u4e3a\u6302\u8f7d\u76ee\u5f55\uff08\u8be5\u76ee\u5f55\u5fc5\u987b\u4e3a\u7a7a\u76ee\u5f55\uff09\uff0c\u7b2c\u4e09\u5217\u4e3a\u6587\u4ef6\u7cfb\u7edf\u7c7b\u578b\uff0c\u7b2c\u56db\u5217\u4e3a\u53c2\u6570\uff0c\u7b2c\u4e94\u52170\u8868\u793a\u4e0d\u5907\u4efd\uff0c\u6700\u540e\u4e00\u5217\u5fc5\u987b\u4e3a\uff12\u62160(\u9664\u975e\u5f15\u5bfc\u5206\u533a\u4e3a1)\nsudo mount -a\n\n\n\n\u67e5\u770b\u6587\u4ef6\u5939\u7684\u5927\u5c0f\n\n\ndu * -sh \n\n\n\nubuntu vscode \u7ec8\u7aef\u7a7a\u767d\n\n\nFile -> Preferences -> Setting -> Features -> Terminal -> Inherit Env\n\n\n\nubuntu \u67e5\u770b\u663e\u5361\n\n\nlspci -vnn |grep VGA -A 12\n\n\nvscode python debug \u5e26\u53c2\n\n\n{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Python: Current File\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\": \"${file}\",\n            \"console\": \"integratedTerminal\",\n            \"args\": [\n                \"--evaluate\",\"/home/promote/NeDisk/radar_depth/pretrained/resnet18_multistage.pth.tar\",\n                \"--data\",\"nuscenes\"\n            ]\n            \"env\":\n            {\n        \"PYTHONPATH\":\"/home/promote/ChineseSpeech\"\n            }\n        }\n    ]\n}\n\n\n\nVSCode Ubuntu\u4e0b\u8c03\u8bd5\u5931\u8d25 \u65e0\u6cd5\u6253\u5f00 libc-start.c raise.c\u7b49\n\n\ncatkin_make -DCMAKE_BUILD_TYPE=debug\n\n\n\n\u6dfb\u52a0\"open in code\" \u53f3\u952e\u5feb\u6377\u65b9\u5f0f\n\n\nwget -qO- https://raw.githubusercontent.com/cra0zy/code-nautilus/master/install.sh | bash\n\n\n\nremote ssh \u6dfb\u52a0\u6c49\u8bed\u8f93\u5165\u6cd5\n\n\nibus-daemon -drx\n\n\n\n\u5b89\u88c5\u9489\u9489\n\n\nsudo apt-add-repository 'deb https://dl.winehq.org/wine-builds/ubuntu/ bionic main' \n\nsudo apt-key adv --recv-keys --keyserver keyserver.Ubuntu.com F987672F\n\nsudo apt-get update\n\nsudo apt-get install wine-development \n\n\n\n\n\u4e0b\u8f7d\u9489\u9489\n\n\n\u5386\u53f2\u7248\u672c\uff0c\u9489\u9489\u57285.0\u7248\u672c\u5f00\u59cb\u53ef\u4ee5\u5728\u7ebf\u7f16\u8f91\n\n\nhttps://download.alicdn.com/dingtalk-desktop/Release/install/Dingtalk_Release_v1.10.0.msi\nhttps://download.alicdn.com/dingtalk-desktop/Release/install/Dingtalk_Release_v1.11.0.msi\nhttps://download.alicdn.com/dingtalk-desktop/Release/install/Dingtalk_Release_v1.11.1.msi\nhttps://download.alicdn.com/dingtalk-desktop/Release/install/Dingtalk_Release_v1.12.0.msi\nhttps://download.alicdn.com/dingtalk-desktop/Release/install/Dingtalk_Release_v1.13.1.msi\nhttps://download.alicdn.com/dingtalk-desktop/Release/install/Dingtalk_Release_v1.13.0.msi\nhttps://download.alicdn.com/dingtalk-desktop/Release/install/Dingtalk_Release_v1.15.5.msi\nhttps://download.alicdn.com/dingtalk-desktop/Release/install/Dingtalk_Release_v1.15.6.msi\n\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.5.3.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.5.5.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.5.8.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.5.10.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.6.2.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.6.7.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.6.8.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.6.9.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.6.11.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.6.12.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.8.1.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.8.2.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.8.4.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.8.5.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.8.6.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.8.8.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.8.9.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.8.10.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.8.12.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.8.15.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.8.18.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.0.1.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.0.2.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.0.3.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.0.13.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.0.14.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.0.15.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.0.16.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.0.18.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.0.21.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.1.2.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.1.5.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.1.6.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.1.9.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.1.10.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.1.11.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.1.13.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.2.2.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.2.3.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.2.4.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.2.6.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.2.7.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.2.8.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.2.9.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.2.12.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.2.13.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.3.1.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.3.2.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.3.3.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.3.4.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.3.6.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.3.7.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.6.1.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.6.21.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.6.22.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.6.25.exe\n\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.1.0.21.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.2.0.20.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.2.0.23.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.2.0.25.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.2.0.26.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.2.0.27.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.2.0.28.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.2.6.17.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.2.6.25.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.2.8.29.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.3.7.19.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.3.7.20.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.3.7.27.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.0.13.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.0.16.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.0.17.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.0.18.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.0.20.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.0.21.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.0.22.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.3.10.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.3.22.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.5.15.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.5.16.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.5.18.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.5.19.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.6.1.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.8.12.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.8.15.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.8.16.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.8.18.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.8.19.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.8.20.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.8.21.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.8.22.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.8.23.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.8.29.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.6.3.13.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.6.3.15.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.6.3.21.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.6.5.11.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.6.5.12.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.6.13.9.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.5.8.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.5.18.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.5.19.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.5.22.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.7.5.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.7.8.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.7.9.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.7.10.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.7.18.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.8.8.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.8.10.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.8.12.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.8.18.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.9.10.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.9.14.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.9.18.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.9.25.exe\n\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.9.10.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.9.11.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.9.12.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.1.11.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.1.17.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.1.18.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.1.20.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.1.24.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.1.25.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.2.9.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.2.12.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.2.16.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.2.21.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.5.1.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.5.2.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.7.5.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.7.7.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.7.8.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.8.2.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.8.5.exe\n\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.0.24.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.0.28.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.0.32.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.0.33.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.0.37.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.5.8.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.5.9.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.5.20.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.6.1.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.7.26.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.7.29.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.8.21.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.8.24.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.9.3.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.9.5.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.9.9.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.9.10.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.9.11.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.9.12.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.11.2.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.12.1.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.15.8.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.15.14.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.15.16.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.15.23.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.15.26.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.16.26.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.16.29.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.16.31.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.16.32.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.16.33.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.1.11.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.1.17.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.1.18.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.1.20.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.1.24.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.1.25.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.1.30.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.2.9.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.2.12.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.2.16.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.2.21.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.5.1.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.5.2.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.7.5.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.7.7.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.7.8.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.8.2.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.8.5.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.9.1.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.9.4.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.9.9.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.9.11.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.10.27.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.10.28.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.11.1.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.11.6.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.11.15.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.11.16.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.11.17.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.11.19.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.11.22.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.15.1.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.15.3.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.15.5.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.15.6.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.15.7.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.15.8.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.15.15.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.15.16.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.15.24.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.21.7.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.21.9.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.21.11.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.21.12.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.21.19.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.22.8.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.22.12.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.22.18.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.22.20.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.26.1.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.26.2.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.26.3.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.27.8.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.27.10.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.27.13.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.27.15.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.27.18.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.28.2.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.28.12.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.33.2.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.33.5.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.33.7.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.33.8.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.33.10.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.33.11.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.33.12.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.33.13.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.36.21.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.36.25.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.36.29.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.36.30.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.36.31.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.39.1.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.39.5.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.39.6.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.39.9.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.39.10.exe\n\n\n\n\u5b89\u88c5\u9489\u9489\n\n\nwine DingTalk_v5.0.9.10.exe\n\n\n\n\u89e3\u51b3\u65b9\u5757\u5b57\u95ee\u9898\n\n\n\u62f7\u8d1d c:/windows/Fonts \u76ee\u5f55\u4e0b\u5b57\u4f53\u81f3 /usr/share/wine/fonts\u76ee\u5f55\u4e0b\n\u4e0b\u8f7dsimsun.ttf \u5b58\u81f3 /usr/share/wine/fonts\u76ee\u5f55\u4e0b\u548c.wine/drive_c/windows/Fonts\u76ee\u5f55\u4e0b\n\n\n\nwinecfg\n\n\n\n\n\n\u8fd9\u4e2a\u5e93\u53ef\u4ee5\u5728win7 \u6216 xp (32\u4f4d\u7684) \u7684c:/windows/system32/riched20.dll\u627e\u5230\uff0criched20.dll\n\n\n\u518d\u70b9\u51fb\u51fd\u6570\u5e93\u9009\u9879\u5361\uff0c\u5728\u65b0\u589e\u51fd\u6570\u5e93\u9876\u66ff\u4e2d\u5206\u522b\u8f93\u5165msvcp60\u3001riched20\u3001riched32\n\n\n\n\n\u542f\u52a8\u9489\u9489\n\n\nwine DingtalkLauncher.exe\n\n\n\nubuntu \u5b89\u88c5\u5206\u533a\n\n\n37G \u201c/\u201d ,16G \"swap\", 1G \"/boot\" , \u5176\u4f59\u7684\u662f \u201c/home\u201d\n\n\nubuntu \u5361\u5c4f\n\n\n\u7b2c\u4e00\u6b65\uff1a\n\n\nUbuntu\u9009\u62e9\u5b89\u88c5\u754c\u9762\uff0c\u5728\u6309e\u952e\u8fdb\u5165\u7f16\u8f91\u754c\u9762\u3002\n\n\n\u627e\u5230 \nBoot Options ed boot=\u2026 initrd=/casper/initrd.lz quiet splash \u2014\n\n\n\u4fee\u6539\u7ea2\u8272\u90e8\u5206\uff08\u5220\u53bb \n\u2014\n\u5e76\u6dfb\u52a0 \nnomodeset\n\uff09\u5982\u4e0b\n\n\nBoot Options ed boot=\u2026 initrd=/casper/initrd.lz nomodeset quiet splash\n\n\n\u63a5\u7740\u6309 '\u2018F10\u2019'\u542f\u52a8\u7cfb\u7edf.\n\n\n\u7b2c\u4e8c\u6b65\uff1a\n\n\n\u5b89\u88c5\u7ed3\u675f\u540e\uff0c\u8fdb\u5165\u9009\u62e9\u7cfb\u7edf\u754c\u9762\u3002\n\n\n\u6309\u2019\u2019\u2018e\u2019\u2019\u2019 \u8fdb\u5165\u7f16\u8f91\u5f00\u673a\u6307\u4ee4\u7684\u6a21\u5f0f, \u540c\u6837\u627e\u5230\u2019\u2019\u2018quite splash\u2019\u2019\u2019 \u5e76\u5728\u540e\u9762\u52a0\u4e0a\u5bf9\u5e94\u7684\u201cnomodeset\u201d,\u5373\u3002\n\n\n\u2018\u2019\u2018quite splash nomodeset\u2019\u2019\u2019\n\n\n\u63a5\u7740\u6309 '\u2018F10\u2019'\u542f\u52a8\u7cfb\u7edf.\n\n\n1\u3001\u8fdb\u53bb\u7cfb\u7edf\u4e4b\u540e\u7f16\u8f91\u2019\u2019\u2019/etc/default/grub\u2019\u2019\u2019 \uff1aUbuntu>\u6253\u5f00\u7ec8\u7aef\u673a\uff0c\u8f93\u5165\n\n\n$sudo vi /etc/default/grub \n\n\n\n2.\u627e\u5230\u8fd9\u4e00\u884c:\n\n\n  GRUB_CMDLINE_LINUX_DEFAULT=\u201cquiet splash\u201d\n  \u4fee\u6539\u4e3a\uff1a\n  GRUB_CMDLINE_LINUX_DEFAULT=\u201cquiet splash nomodeset\u201d\n\n\n\n\u66f4\u65b0GRUB\uff1a\n\n\n$sudo update-grub\n\n\n\n\u91cd\u65b0\u5f00\u673a\u3002\n\n\nubuntu \u5b89\u88c5 rar\n\n\nsudo apt-get install rar unrar\n\nrar x \u6587\u4ef6 #\u89e3\u538b \nrar a FileName.rar DirName # \u538b\u7f29\n\n\n\nfind \u67e5\u627e\u6587\u4ef6\n\n\nsudo find / -name \u6587\u4ef6\u540d",
            "title": "Ubuntu18"
        },
        {
            "location": "/ubuntu18/#mac",
            "text": "\u4f7f\u7528ifconfig\u3000\u67e5\u770b\u7f51\u53e3  \u5b89\u88c5macmanager  sudo apt install macchanger  \u968f\u673a\u751f\u6210\u4e00\u4e2amac\u5730\u5740  sudo macchanger -r enp60s0enp60s0 \u662f\u7f51\u53e3\uff0c -r \u4ee3\u8868\u7684\u662f\u968f\u673a  random \u7684\u610f\u601d\uff0c macchanger \u4f1a\u5e2e\u6211\u4eec\u4fee\u6539\u6210\u4e00\u4e2a\u968f\u673a\u4ea7\u751f\u7684  MAC \u53f7  \u4fee\u6539\u4e3a\u6307\u5b9a\u7684mac\u5730\u5740  sudo macchanger -m AA:BB:CC:DD:EE:FF enp60s0 \\",
            "title": "\u66f4\u6539mac\u5730\u5740"
        },
        {
            "location": "/ubuntu18/#ubuntu1804",
            "text": "\u7ec8\u7aef\u8f93\u5165nm-connection-editor\u6253\u5f00\u7f51\u7edc\u8fde\u63a5  \u521b\u5efa\u4ee5\u592a\u7f51\u94fe\u63a5  \u914d\u7f6e\u7f51\u7edc\u94fe\u63a5  \u5c06\u5176\u4ed6\u9700\u8981\u4e0a\u7f51\u7684\u8bbe\u5907\u901a\u8fc7\u7f51\u7ebf\u94fe\u63a5\u5230\u5171\u4eab\u7f51\u7edc\u5373\u53ef",
            "title": "ubuntu18.04 \u7f51\u53e3\u521b\u5efa\u7f51\u7edc\u5171\u4eab"
        },
        {
            "location": "/ubuntu18/#xvfb-ssh",
            "text": "xvfb-run -s \"-screen 0 1400x900x24\" python <your_script.py>",
            "title": "xvfb\u3000ssh"
        },
        {
            "location": "/ubuntu18/#x-forward",
            "text": "ssh -X username@ip",
            "title": "x forward"
        },
        {
            "location": "/ubuntu18/#vscode-remote-ssh",
            "text": "conda install jupyter",
            "title": "vscode \u63d2\u4ef6\u3000Remote-ssh"
        },
        {
            "location": "/ubuntu18/#github",
            "text": "git clone https://github.com/Amritpal-001/Reinforcement-learning-projects.git \n\u6539\u4e3a git clone http://hub.fastgit.org/Amritpal-001/Reinforcement-learning-projects.git",
            "title": "github\u4e0b\u8f7d\u63d0\u901f"
        },
        {
            "location": "/ubuntu18/#github_1",
            "text": "https://d.serctl.com/",
            "title": "github \u4e0b\u8f7d\u5de5\u5177"
        },
        {
            "location": "/ubuntu18/#httpswwwpythonorgftp",
            "text": "wget https://www.python.org/ftp/python/3.7.5/Python-3.7.5.tgz \n\u6539\u4e3a wget https://npm.taobao.org/mirrors/python/3.7.5/Python-3.7.5.tgz",
            "title": "https://www.python.org/ftp \u4e0b\u8f7d\u6162"
        },
        {
            "location": "/ubuntu18/#pip",
            "text": "pip3.7.5 install -i http://mirrors.aliyun.com/pypi/simple/ psutil decorator numpy protobuf==3.11^C scipy sympy cffi grpcio grpcio-tools requests --user --trusted-host mirrors.aliyun.com",
            "title": "pip \u4e0b\u8f7d\u4e1c\u897f\u6162"
        },
        {
            "location": "/ubuntu18/#mkdocs",
            "text": "mkdocs build  mkdocs serve  push sites folders",
            "title": "mkdocs"
        },
        {
            "location": "/ubuntu18/#vscode-python-ros-debug",
            "text": "\u9996\u5148  catkin_make -DCMAKE_BUILD_TYPE=DEBUG  \u5176\u6b21\u70b9\u51fbdebug\u6309\u94ae\uff0c\u9009\u62e9\u751f\u6210\u65b0\u7684launch\u6587\u4ef6\u3002\n\u7136\u540e\u70b9\u51fb  show ,  \uff44ebug\u65f6\u9047\u5230\u8def\u5f84\u95ee\u9898,\u4f8b\u5982rosmsg\u8def\u5f84\uff0c\u6700\u597d\u5728\u6587\u4ef6\u5c5e\u6027\u91cc\u590d\u5236\u8def\u5f84\uff0c\u4e0d\u7136\u5bb9\u6613\u51fa\u9519  import sys\nsys.path.append(\"/home/pmjd/Downloads/catkin_ws/devel/lib/python2.7/dist-packages\")",
            "title": "vscode python ros debug"
        },
        {
            "location": "/ubuntu18/#tilix-doesnt-open-in-the-folder-from-where-it-is-split",
            "text": "Update  ~.bashrc  (or  ~.zshrc  if you are using zsh) to execute vte.sh directly, this involves adding the following line at the end of the file.  if [[ $TILIX_ID ]]; then\n        source /etc/profile.d/vte.sh\nfi  On Ubuntu (18.04), a symlink is probably missing. You can create it with:  ln -s /etc/profile.d/vte-2.91.sh /etc/profile.d/vte.sh",
            "title": "Tilix doesnt open in the folder from where it is split"
        },
        {
            "location": "/ubuntu18/#sudo-dist-upgrade-cuda",
            "text": "",
            "title": "sudo dist-upgrade \u662f\u6bc1\u706d\u6027\u7684\uff0c\u4f1a\u5347\u7ea7cuda"
        },
        {
            "location": "/ubuntu18/#github-init",
            "text": "git config --global user.email \"you@example.com\"\ngit config --global user.name \"Your Name\"\ngit add .\ngit commit -m \"1\"\ngit push",
            "title": "github init"
        },
        {
            "location": "/ubuntu18/#_1",
            "text": "nautilus --browser ~/\u6587\u6863",
            "title": "\u547d\u4ee4\u884c\u6253\u5f00\u6587\u4ef6\u7ba1\u7406\u5668"
        },
        {
            "location": "/ubuntu18/#_2",
            "text": "\u5148\u5728\u8bbe\u7f6e\u91cc\u8bbe\u7f6elanguage\u91cc\u6dfb\u52a0Chinese  sudo apt-get install ibus-libpinyin ibus-clutter",
            "title": "\u5b89\u88c5\u6c49\u8bed\u8f93\u5165\u6cd5"
        },
        {
            "location": "/ubuntu18/#docker",
            "text": "",
            "title": "Docker\u521b\u5efa\u5bb9\u5668"
        },
        {
            "location": "/ubuntu18/#ubuntu1804_1",
            "text": "",
            "title": "\u6784\u5efaubuntu18.04\u6620\u50cf"
        },
        {
            "location": "/ubuntu18/#_3",
            "text": "sudo apt-get install debootstrap\nsudo apt install docker.io\nsudo chmod 666 /var/run/docker.sock",
            "title": "\u5b89\u88c5\u4f9d\u8d56"
        },
        {
            "location": "/ubuntu18/#ubuntu-1804",
            "text": "sudo debootstrap bionic bionic > /dev/null\nsudo tar -C bionic -c . | docker import - bionic/smart_eye",
            "title": "\u521b\u5efaubuntu 18.04 \u955c\u50cf"
        },
        {
            "location": "/ubuntu18/#_4",
            "text": "docker run bionic cat /etc/lsb-release  \u67e5\u770b\u955c\u50cf  docker images  \u5220\u9664\u955c\u50cf  docker rmi [IMAGE ID]  \u67e5\u770b\u5bb9\u5668\u8fd0\u884c\u60c5\u51b5  sudo docker ps -a  \u9000\u51fa\u5bb9\u5668  sudo docker stop \u5bb9\u5668id  \u5220\u9664\u5bb9\u5668  sudo docker rm \u5bb9\u5668id  \u542f\u52a8\u955c\u50cf  docker run -it bionic/smart_eye  /bin/bash  \u9000\u51fa\u955c\u50cf  exit  \u6587\u4ef6\u4f20\u9012  \u4ece\u672c\u5730\u81f3docker  docker cp FILE_PATH \u5bb9\u5668ID:/root  \u4ecedocker \u81f3\u672c\u5730  docker cp \u5bb9\u5668ID:/root/data.tar /home/user",
            "title": "\u6d4b\u8bd5"
        },
        {
            "location": "/ubuntu18/#docker-image",
            "text": "sudo service docker stop\nsudo touch /etc/docker/daemon.json  daemon.json  {\n    \"data-root\":\"/home/pmjd/docker\"\n}  sudo service docker start",
            "title": "\u66f4\u6539docker image\u5b58\u653e\u8def\u5f84"
        },
        {
            "location": "/ubuntu18/#dockerapt-sourcelist",
            "text": "sudo nano /etc/apt/sources.list  \u6dfb\u52a0  deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse",
            "title": "Docker\u66f4\u65b0apt source.list"
        },
        {
            "location": "/ubuntu18/#docker_1",
            "text": "docker start \u5bb9\u5668ID\ndocker attach \u5bb9\u5668ID  \u5bfc\u51fa\u5bb9\u5668\u5feb\u7167  docker export c91c33f28594 > smart_eye_docker.tar  \u5bb9\u5668\u5feb\u7167\u5bfc\u5165\u4e3a\u955c\u50cf  cat smart_eye_docker.tar | docker import - test/smart_eye:v1  \u518d\u4fdd\u5b58\u6b64\u955c\u50cf   docker save -o smart_eye_image_docker.tar test/smart_eye  \u518d\u52a0\u8f7d\u955c\u50cf  docker load --input samrt_eye_image_docker.tar  \u542f\u52a8  docker run -it test/smart_eye:v1 /bin/bash  docker \u542f\u52a8bash  docker start c91c33f28594\ndocker exec -it c91c33f28594 /home/run.sh\ndocker exec mycontainer /bin/sh -c \"cmd1;cmd2;...;cmdn\"  docker bash file example  #!/bin/sh\ndocker run -it --net host --add-host in_release_docker:127.0.0.1 --add-host localhost:127.0.0.1 --hostname in_release_docker --rm promote/smart_eye:v1 /bin/bash -c \"/home/run.sh\"",
            "title": "Docker \u542f\u52a8\u5bb9\u5668"
        },
        {
            "location": "/ubuntu18/#sounddevice-invalid-sample-rate",
            "text": "nano /etc/asound.conf",
            "title": "sounddevice invalid sample rate"
        },
        {
            "location": "/ubuntu18/#_5",
            "text": "1.desktop  [Desktop Entry]\nName=Smart_eye\nGenericName=3D modeler\nKeywords=3d;cg;modeling;animation;painting;sculpting;texturing;video editing;video tracking;rendering;render engine;cycles;game engine;python;\nExec=/bin/bash -c '/home/promote/run.sh'\n#Icon=/home/pmjd/Disk/blender-2.90.1-linux64/blender.svg\nTerminal=true\nType=Application\nCategories=Graphics;3DGraphics;\nMimeType=application/x-blender;  \u754c\u9762\u663e\u793a  xhost +\n-e DISPLAY=${DISPLAY} -v /tmp/.X11-unix:/tmp/.X11-unix",
            "title": "\u542f\u52a8\u5feb\u6377\u65b9\u5f0f"
        },
        {
            "location": "/ubuntu18/#nvidia-container-toolkit",
            "text": "distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\\n   && curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \\\n   && curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list  curl -s -L https://nvidia.github.io/nvidia-container-runtime/experimental/$distribution/nvidia-container-runtime.list | sudo tee /etc/apt/sources.list.d/nvidia-container-runtime.list  sudo apt-get update\nsudo apt-get install -y nvidia-docker2  \u8fd9\u4e2a\u65f6\u5019  /etc/docker/daemon.json  \u5185\u5bb9\u4fee\u6539\u4e3a  {\n    \"runtimes\": {\n        \"nvidia\": {\n            \"path\": \"nvidia-container-runtime\",\n            \"runtimeArgs\": []\n        }\n    },\n    \"data-root\":\"/home/pmjd/docker\"\n}  \u91cd\u542fdocker  sudo systemctl restart docker  \u4e0b\u8f7d\u8fd0\u884c\uff1a  sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi\nsudo docker run --rm --gpus all nvidia/cuda:10.1-base nvidia-smi  \u4e0b\u8f7d\u8fd0\u884ctensorflow:  docker run --gpus all --runtime=nvidia -it tensorflow/tensorflow:2.3.0-gpu bash  docker\u914d\u7f6e\u7f51\u7edc  --net=host   docker \u652f\u6301\u9ea6\u514b,\u6c49\u8bed  docker run -it --volume=/run/user/1000/pulse:/run/user/1000/pulse --user promote --gpus all --runtime=nvidia --name=xiaomeng -e LANG=C.UTF-8 --device /dev/snd promote/xiaomeng:v1.6 /bin/bash",
            "title": "\u5b89\u88c5NVIDIA Container Toolkit"
        },
        {
            "location": "/ubuntu18/#docker-sudo",
            "text": "\u901a\u8fc7\u5c06\u7528\u6237\u6dfb\u52a0\u5230docker\u7528\u6237\u7ec4\u53ef\u4ee5\u5c06sudo\u53bb\u6389\uff0c\u547d\u4ee4\u5982\u4e0b\n\nsudo groupadd docker #\u6dfb\u52a0docker\u7528\u6237\u7ec4\n\nsudo gpasswd -a $USER docker #\u5c06\u767b\u9646\u7528\u6237\u52a0\u5165\u5230docker\u7528\u6237\u7ec4\u4e2d\n\nnewgrp docker #\u66f4\u65b0\u7528\u6237\u7ec4",
            "title": "docker \u8fd0\u884c\u9700\u8981sudo"
        },
        {
            "location": "/ubuntu18/#bash-ssh",
            "text": "#!/usr/bin/expect   \n\nset timeout 3             \nspawn ssh -X promote@192.168.1.2 \nexpect \"*password*\" {send \"123456\\r\"}\nexpect \"$ \" { send \"bash /home/username/1.sh\\r\" }\ninteract",
            "title": "bash \u767b\u5165ssh"
        },
        {
            "location": "/ubuntu18/#nano",
            "text": "ctrl+k  Nano\u663e\u793a\u884c\u53f7  alt+shift+3",
            "title": "Nano \u5220\u9664\u884c"
        },
        {
            "location": "/ubuntu18/#python",
            "text": "\u5220\u9664/usr/bin \u4e0b\u7684Python\u94fe\u63a5  sudo rm /usr/bin/python  \u7528\u4e0b\u9762\u547d\u4ee4\u5efa\u7acb\u65b0\u7684\u94fe\u63a5  sudo ln -s /usr/bin/python3.6 /usr/bin/python  \u7528\u4e0b\u9762\u7684\u547d\u4ee4\u8fd8\u539f2\u7248\u672c  sudo ln -s /usr/bin/python2.7 /usr/bin/python  \u66f4\u79d1\u5b66\u7684\u505a\u6cd5\u662f\uff1a  sudo update-alternatives --install /usr/bin/python python /usr/bin/python2.7 1\n\nsudo update-alternatives --install /usr/bin/python python /usr/bin/python3.6 2\n\nsudo update-alternatives --config python \u9009\u62e9\u8981\u4f7f\u7528\u7684\u7248\u672c\uff0c\u56de\u8f66\uff0c\u641e\u5b9a",
            "title": "\u4fee\u6539\u9ed8\u8ba4python\u7248\u672c"
        },
        {
            "location": "/ubuntu18/#pip_1",
            "text": "sudo update-alternatives --install /usr/bin/pip pip /usr/bin/pip2 1\n\nsudo update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 2\n\nsudo update-alternatives --config pip",
            "title": "\u8bbe\u7f6e\u9ed8\u8ba4pip\u7248\u672c"
        },
        {
            "location": "/ubuntu18/#pip_2",
            "text": "sudo pip install --upgrade pip",
            "title": "pip\u5347\u7ea7"
        },
        {
            "location": "/ubuntu18/#_6",
            "text": "[Desktop Entry]\nName=smart_view\nGenericName=3D modeler\nKeywords=python;\nExec=/bin/bash -c 'source /opt/ros/melodic/setup.bash;rosrun image_view image_view image:=/smart_eye_view'\n#Icon=/home/pmjd/Disk/blender-2.90.1-linux64/blender.svg\nTerminal=false\nType=Application\nCategories=Graphics;3DGraphics;\nMimeType=application/x-blender;\nName[en_US]=smart_view",
            "title": "\u684c\u9762\u5feb\u6377\u65b9\u5f0f"
        },
        {
            "location": "/ubuntu18/#md",
            "text": "\\quad \u6216\u8005  \\+\u7a7a\u683c  \u6216\u8005  &nbsp",
            "title": "md\u516c\u5f0f\u91cc\u6dfb\u52a0\u7a7a\u683c"
        },
        {
            "location": "/ubuntu18/#md_1",
            "text": "![](https://latex.codecogs.com/svg.latex?\\Large&space;\\theta_{k+1}=arg\\quad\\underset{\\theta}{max})   <img src=\"https://latex.codecogs.com/svg.image?\\&space;\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}=K^{-1}_{3\\times 3}\\begin{bmatrix}z\\cdot(x_c+\\delta_{x_c})\\\\z\\cdot(y_c+\\delta_{y_c})\\\\z\\end{bmatrix}\"/>  <img src=\"https://latex.codecogs.com/svg.image?\\&space;\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}=K^{-1}_{3\\times 3}\\begin{bmatrix}z\\cdot(x_c+\\delta_{x_c})\\\\z\\cdot(y_c+\\delta_{y_c})\\\\z\\end{bmatrix}\"/>",
            "title": "md\u6dfb\u52a0\u591a\u884c\u516c\u5f0f"
        },
        {
            "location": "/ubuntu18/#md_2",
            "text": "$\\theta$  $\\theta$",
            "title": "md\u6dfb\u52a0\u884c\u5185\u516c\u5f0f"
        },
        {
            "location": "/ubuntu18/#mkdocs_1",
            "text": "pip install https://github.com/mitya57/python-markdown-math/archive/master.zip  \u5728  config.yaml \u4e2d\u6dfb\u52a0  extra_javascript: \n    - https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML\n    - mathjaxhelper.js\nmarkdown_extensions:\n    - mdx_math  \u5728docs\u6587\u4ef6\u5939\u4e0b\u65b0\u5efa  mathjaxhelper.js  MathJax.Hub.Config({\n  \"tex2jax\": { inlineMath: [ [ '$', '$' ] ] }\n\n});\nMathJax.Hub.Config({\n  config: [\"MMLorHTML.js\"],\n  jax: [\"input/TeX\", \"output/HTML-CSS\", \"output/NativeMML\"],\n  extensions: [\"MathMenu.js\", \"MathZoom.js\"]\n});  \u8fd9\u6837\u5c31\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528  $\u516c\u5f0f$  \u548c  $$\u516c\u5f0f$$",
            "title": "mkdocs \u663e\u793a\u516c\u5f0f"
        },
        {
            "location": "/ubuntu18/#wget",
            "text": "wget -t 0 -c < \u6587\u4ef6\u5730\u5740>\n  -c \u8868\u793a\u65ad\u70b9\u7eed\u8fde\n  -t \u8868\u793a \u65ad\u4e86\u4f1a\u6bcf\u7ecf\u8fc7\u51e0\u79d2\u91cd\u65b0\u8fde\u63a5\u8fde\u63a5\u4e00\u6b21\uff0c0\u8868\u793a\u4e00\u76f4\u4e0d\u65ad\u91cd\u8fde\uff0c\u6709\u6700\u5927\u8fde\u63a5\u6b21\u6570\u7684\u3002\n  \u6ce8\u610f\uff1a \u8bb0\u5f97\u5728\u539f\u6765\u7684\u76ee\u5f55\u4e0b\u6267\u884c\u8fd9\u4e2a\u547d\u4ee4\uff0c\u624d\u4f1a\u63a5\u7740\u4e0a\u6587\u4e0b\u8f7d\uff0c\u627e\u4e0d\u5230\u6587\u4ef6\u76f4\u63a5\u91cd\u65b0\u4e0b\u8f7d",
            "title": "wget \u65ad\u7f51\u91cd\u8fde"
        },
        {
            "location": "/ubuntu18/#_7",
            "text": "df -h",
            "title": "\u6307\u4ee4\u67e5\u770b\u786c\u76d8\u5b58\u50a8\u60c5\u51b5"
        },
        {
            "location": "/ubuntu18/#_8",
            "text": "lsblk",
            "title": "\u67e5\u770b\u6240\u6709\u786c\u76d8"
        },
        {
            "location": "/ubuntu18/#_9",
            "text": "sudo fdisk -l #\u67e5\u770b\u78c1\u76d8\u4fe1\u606f\nsudo blkid #\u67e5\u770b\u5206\u533a\nmkdir NeDisk # \u521b\u5efa\u6302\u8f7d\u70b9\ndf -kh #\u67e5\u770b\u5df2\u6709\u6302\u8f7d\uff0c\u786e\u5b9a\u662f\u5426\u5df2\u6302\u8f7d\nsudo umount /dev/sda5 #\u5378\u8f7d\u5df2\u6302\u8f7d\nsudo nano /etc/fstab\nsudo blkid /dev/sda5 # \u627e\u5230\u5176UUID\n#\u7136\u540e,\u6211\u4eec\u6309\u7167/etc/fstab\u6587\u4ef6\u4e2d\u7684\u683c\u5f0f\u6dfb\u52a0\u4e00\u884c\u5982\u4e0b\u5185\u5bb9:\nUUID=0001D3CE0001E53B /home/ubuntu/NewDisk ntfs defaults 0 2\n#\u5176\u4e2d\u7b2c\u4e00\u5217\u4e3aUUID, \u7b2c\u4e8c\u5217\u4e3a\u6302\u8f7d\u76ee\u5f55\uff08\u8be5\u76ee\u5f55\u5fc5\u987b\u4e3a\u7a7a\u76ee\u5f55\uff09\uff0c\u7b2c\u4e09\u5217\u4e3a\u6587\u4ef6\u7cfb\u7edf\u7c7b\u578b\uff0c\u7b2c\u56db\u5217\u4e3a\u53c2\u6570\uff0c\u7b2c\u4e94\u52170\u8868\u793a\u4e0d\u5907\u4efd\uff0c\u6700\u540e\u4e00\u5217\u5fc5\u987b\u4e3a\uff12\u62160(\u9664\u975e\u5f15\u5bfc\u5206\u533a\u4e3a1)\nsudo mount -a",
            "title": "\u6302\u8f7d\u786c\u76d8"
        },
        {
            "location": "/ubuntu18/#_10",
            "text": "du * -sh",
            "title": "\u67e5\u770b\u6587\u4ef6\u5939\u7684\u5927\u5c0f"
        },
        {
            "location": "/ubuntu18/#ubuntu-vscode",
            "text": "File -> Preferences -> Setting -> Features -> Terminal -> Inherit Env",
            "title": "ubuntu vscode \u7ec8\u7aef\u7a7a\u767d"
        },
        {
            "location": "/ubuntu18/#ubuntu",
            "text": "lspci -vnn |grep VGA -A 12",
            "title": "ubuntu \u67e5\u770b\u663e\u5361"
        },
        {
            "location": "/ubuntu18/#vscode-python-debug",
            "text": "{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Python: Current File\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\": \"${file}\",\n            \"console\": \"integratedTerminal\",\n            \"args\": [\n                \"--evaluate\",\"/home/promote/NeDisk/radar_depth/pretrained/resnet18_multistage.pth.tar\",\n                \"--data\",\"nuscenes\"\n            ]\n            \"env\":\n            {\n        \"PYTHONPATH\":\"/home/promote/ChineseSpeech\"\n            }\n        }\n    ]\n}",
            "title": "vscode python debug \u5e26\u53c2"
        },
        {
            "location": "/ubuntu18/#vscode-ubuntu-libc-startc-raisec",
            "text": "catkin_make -DCMAKE_BUILD_TYPE=debug",
            "title": "VSCode Ubuntu\u4e0b\u8c03\u8bd5\u5931\u8d25 \u65e0\u6cd5\u6253\u5f00 libc-start.c raise.c\u7b49"
        },
        {
            "location": "/ubuntu18/#open-in-code",
            "text": "wget -qO- https://raw.githubusercontent.com/cra0zy/code-nautilus/master/install.sh | bash",
            "title": "\u6dfb\u52a0\"open in code\" \u53f3\u952e\u5feb\u6377\u65b9\u5f0f"
        },
        {
            "location": "/ubuntu18/#remote-ssh",
            "text": "ibus-daemon -drx",
            "title": "remote ssh \u6dfb\u52a0\u6c49\u8bed\u8f93\u5165\u6cd5"
        },
        {
            "location": "/ubuntu18/#_11",
            "text": "sudo apt-add-repository 'deb https://dl.winehq.org/wine-builds/ubuntu/ bionic main' \n\nsudo apt-key adv --recv-keys --keyserver keyserver.Ubuntu.com F987672F\n\nsudo apt-get update\n\nsudo apt-get install wine-development   \u4e0b\u8f7d\u9489\u9489  \u5386\u53f2\u7248\u672c\uff0c\u9489\u9489\u57285.0\u7248\u672c\u5f00\u59cb\u53ef\u4ee5\u5728\u7ebf\u7f16\u8f91  https://download.alicdn.com/dingtalk-desktop/Release/install/Dingtalk_Release_v1.10.0.msi\nhttps://download.alicdn.com/dingtalk-desktop/Release/install/Dingtalk_Release_v1.11.0.msi\nhttps://download.alicdn.com/dingtalk-desktop/Release/install/Dingtalk_Release_v1.11.1.msi\nhttps://download.alicdn.com/dingtalk-desktop/Release/install/Dingtalk_Release_v1.12.0.msi\nhttps://download.alicdn.com/dingtalk-desktop/Release/install/Dingtalk_Release_v1.13.1.msi\nhttps://download.alicdn.com/dingtalk-desktop/Release/install/Dingtalk_Release_v1.13.0.msi\nhttps://download.alicdn.com/dingtalk-desktop/Release/install/Dingtalk_Release_v1.15.5.msi\nhttps://download.alicdn.com/dingtalk-desktop/Release/install/Dingtalk_Release_v1.15.6.msi\n\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.5.3.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.5.5.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.5.8.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.5.10.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.6.2.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.6.7.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.6.8.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.6.9.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.6.11.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.6.12.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.8.1.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.8.2.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.8.4.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.8.5.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.8.6.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.8.8.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.8.9.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.8.10.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.8.12.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.8.15.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.4.8.18.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.0.1.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.0.2.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.0.3.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.0.13.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.0.14.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.0.15.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.0.16.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.0.18.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.0.21.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.1.2.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.1.5.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.1.6.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.1.9.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.1.10.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.1.11.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.1.13.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.2.2.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.2.3.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.2.4.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.2.6.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.2.7.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.2.8.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.2.9.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.2.12.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.2.13.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.3.1.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.3.2.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.3.3.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.3.4.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.3.6.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.3.7.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.6.1.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.6.21.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.6.22.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v3.5.6.25.exe\n\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.1.0.21.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.2.0.20.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.2.0.23.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.2.0.25.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.2.0.26.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.2.0.27.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.2.0.28.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.2.6.17.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.2.6.25.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.2.8.29.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.3.7.19.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.3.7.20.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.3.7.27.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.0.13.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.0.16.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.0.17.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.0.18.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.0.20.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.0.21.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.0.22.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.3.10.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.3.22.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.5.15.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.5.16.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.5.18.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.5.19.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.6.1.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.8.12.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.8.15.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.8.16.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.8.18.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.8.19.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.8.20.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.8.21.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.8.22.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.8.23.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.5.8.29.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.6.3.13.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.6.3.15.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.6.3.21.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.6.5.11.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.6.5.12.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.6.13.9.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.5.8.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.5.18.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.5.19.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.5.22.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.7.5.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.7.8.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.7.9.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.7.10.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.7.18.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.8.8.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.8.10.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.8.12.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.8.18.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.9.10.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.9.14.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.9.18.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v4.7.9.25.exe\n\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.9.10.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.9.11.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.9.12.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.1.11.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.1.17.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.1.18.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.1.20.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.1.24.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.1.25.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.2.9.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.2.12.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.2.16.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.2.21.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.5.1.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.5.2.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.7.5.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.7.7.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.7.8.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.8.2.exe\nhttps://download.alicdn.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.8.5.exe\n\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.0.24.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.0.28.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.0.32.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.0.33.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.0.37.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.5.8.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.5.9.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.5.20.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.6.1.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.7.26.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.7.29.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.8.21.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.8.24.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.9.3.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.9.5.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.9.9.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.9.10.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.9.11.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.9.12.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.11.2.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.12.1.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.15.8.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.15.14.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.15.16.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.15.23.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.15.26.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.16.26.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.16.29.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.16.31.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.16.32.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.0.16.33.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.1.11.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.1.17.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.1.18.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.1.20.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.1.24.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.1.25.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.1.30.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.2.9.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.2.12.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.2.16.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.2.21.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.5.1.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.5.2.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.7.5.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.7.7.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.7.8.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.8.2.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.8.5.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.9.1.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.9.4.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.9.9.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.9.11.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.10.27.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.10.28.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.11.1.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.11.6.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.11.15.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.11.16.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.11.17.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.11.19.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.11.22.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.15.1.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.15.3.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.15.5.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.15.6.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.15.7.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.15.8.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.15.15.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.15.16.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.15.24.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.21.7.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.21.9.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.21.11.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.21.12.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.21.19.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.22.8.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.22.12.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.22.18.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.22.20.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.26.1.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.26.2.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.26.3.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.27.8.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.27.10.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.27.13.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.27.15.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.27.18.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.28.2.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.28.12.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.33.2.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.33.5.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.33.7.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.33.8.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.33.10.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.33.11.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.33.12.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.33.13.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.36.21.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.36.25.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.36.29.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.36.30.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.36.31.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.39.1.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.39.5.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.39.6.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.39.9.exe\nhttps://dtapp-pub.dingtalk.com/dingtalk-desktop/win_installer/Release/DingTalk_v5.1.39.10.exe  \u5b89\u88c5\u9489\u9489  wine DingTalk_v5.0.9.10.exe  \u89e3\u51b3\u65b9\u5757\u5b57\u95ee\u9898  \u62f7\u8d1d c:/windows/Fonts \u76ee\u5f55\u4e0b\u5b57\u4f53\u81f3 /usr/share/wine/fonts\u76ee\u5f55\u4e0b\n\u4e0b\u8f7dsimsun.ttf \u5b58\u81f3 /usr/share/wine/fonts\u76ee\u5f55\u4e0b\u548c.wine/drive_c/windows/Fonts\u76ee\u5f55\u4e0b  winecfg   \u8fd9\u4e2a\u5e93\u53ef\u4ee5\u5728win7 \u6216 xp (32\u4f4d\u7684) \u7684c:/windows/system32/riched20.dll\u627e\u5230\uff0criched20.dll  \u518d\u70b9\u51fb\u51fd\u6570\u5e93\u9009\u9879\u5361\uff0c\u5728\u65b0\u589e\u51fd\u6570\u5e93\u9876\u66ff\u4e2d\u5206\u522b\u8f93\u5165msvcp60\u3001riched20\u3001riched32  \n\u542f\u52a8\u9489\u9489  wine DingtalkLauncher.exe",
            "title": "\u5b89\u88c5\u9489\u9489"
        },
        {
            "location": "/ubuntu18/#ubuntu_1",
            "text": "37G \u201c/\u201d ,16G \"swap\", 1G \"/boot\" , \u5176\u4f59\u7684\u662f \u201c/home\u201d",
            "title": "ubuntu \u5b89\u88c5\u5206\u533a"
        },
        {
            "location": "/ubuntu18/#ubuntu_2",
            "text": "\u7b2c\u4e00\u6b65\uff1a  Ubuntu\u9009\u62e9\u5b89\u88c5\u754c\u9762\uff0c\u5728\u6309e\u952e\u8fdb\u5165\u7f16\u8f91\u754c\u9762\u3002  \u627e\u5230  Boot Options ed boot=\u2026 initrd=/casper/initrd.lz quiet splash \u2014  \u4fee\u6539\u7ea2\u8272\u90e8\u5206\uff08\u5220\u53bb  \u2014 \u5e76\u6dfb\u52a0  nomodeset \uff09\u5982\u4e0b  Boot Options ed boot=\u2026 initrd=/casper/initrd.lz nomodeset quiet splash  \u63a5\u7740\u6309 '\u2018F10\u2019'\u542f\u52a8\u7cfb\u7edf.  \u7b2c\u4e8c\u6b65\uff1a  \u5b89\u88c5\u7ed3\u675f\u540e\uff0c\u8fdb\u5165\u9009\u62e9\u7cfb\u7edf\u754c\u9762\u3002  \u6309\u2019\u2019\u2018e\u2019\u2019\u2019 \u8fdb\u5165\u7f16\u8f91\u5f00\u673a\u6307\u4ee4\u7684\u6a21\u5f0f, \u540c\u6837\u627e\u5230\u2019\u2019\u2018quite splash\u2019\u2019\u2019 \u5e76\u5728\u540e\u9762\u52a0\u4e0a\u5bf9\u5e94\u7684\u201cnomodeset\u201d,\u5373\u3002  \u2018\u2019\u2018quite splash nomodeset\u2019\u2019\u2019  \u63a5\u7740\u6309 '\u2018F10\u2019'\u542f\u52a8\u7cfb\u7edf.  1\u3001\u8fdb\u53bb\u7cfb\u7edf\u4e4b\u540e\u7f16\u8f91\u2019\u2019\u2019/etc/default/grub\u2019\u2019\u2019 \uff1aUbuntu>\u6253\u5f00\u7ec8\u7aef\u673a\uff0c\u8f93\u5165  $sudo vi /etc/default/grub   2.\u627e\u5230\u8fd9\u4e00\u884c:    GRUB_CMDLINE_LINUX_DEFAULT=\u201cquiet splash\u201d\n  \u4fee\u6539\u4e3a\uff1a\n  GRUB_CMDLINE_LINUX_DEFAULT=\u201cquiet splash nomodeset\u201d  \u66f4\u65b0GRUB\uff1a  $sudo update-grub  \u91cd\u65b0\u5f00\u673a\u3002",
            "title": "ubuntu \u5361\u5c4f"
        },
        {
            "location": "/ubuntu18/#ubuntu-rar",
            "text": "sudo apt-get install rar unrar\n\nrar x \u6587\u4ef6 #\u89e3\u538b \nrar a FileName.rar DirName # \u538b\u7f29",
            "title": "ubuntu \u5b89\u88c5 rar"
        },
        {
            "location": "/ubuntu18/#find",
            "text": "sudo find / -name \u6587\u4ef6\u540d",
            "title": "find \u67e5\u627e\u6587\u4ef6"
        },
        {
            "location": "/ubuntu20/",
            "text": "vscode Debug cpp Ros node\n\n\n\n\ncatkin_make -DCMAKE_BUILD_TYPE=RelWithDebInfo\n\n\n\u5bf9\u4e8eintelliSenseEngine\u81ea\u52a8\u5b8c\u6210\u529f\u80fd\u6b63\u786e\u8bbe\u7f6e \"C_Cpp.intelliSenseEngine\": \"Tag Parser\"\n\n\nDebuge->add_Configuration->{} ROS:Launch\n\n\n\n\nlaunch.json\n\n\n{\n    // Use IntelliSense to learn about possible attributes.\n    // Hover to view descriptions of existing attributes.\n    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n\n        {\n            \"name\": \"ROS: Launch\",\n            \"type\": \"ros\",\n            \"request\": \"launch\",\n            \"target\": \"/home/pmjd/Disk/radar/src/ars_40X/launch/ars_40X.launch\"\n        }\n    ]\n}\n\n\n\n\n\n\u8d77\u4e00\u4e2aroscore\n\n\n\u5c31\u53ef\u4ee5debug\u4e86\n\n\n\u4e5f\u53ef\u4ee5 debug ros \u8282\u70b9\uff0c Debuge->add_Configuration->{} ROS:Attach\n\n\n\u542f\u52a8terminal \uff0c rosrun ars_40 deal_img\n\n\n\u5f00\u59cbdebug, \u9009\u62e9c++ \u548c\u8282\u70b9\u540d\u5b57\n\n\n\n\n\u66f4\u6539apt\u955c\u50cf\u6e90\n\n\nsudo nano /etc/apt/sources.list\n\n\n\n# \u9ed8\u8ba4\u6ce8\u91ca\u4e86\u6e90\u7801\u955c\u50cf\u4ee5\u63d0\u9ad8 apt update \u901f\u5ea6\uff0c\u5982\u6709\u9700\u8981\u53ef\u81ea\u884c\u53d6\u6d88\u6ce8\u91ca\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-updates main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-updates main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-backports main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-backports main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-security main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-security main restricted universe multiverse\n\n# \u9884\u53d1\u5e03\u8f6f\u4ef6\u6e90\uff0c\u4e0d\u5efa\u8bae\u542f\u7528\n# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-proposed main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-proposed main restricted universe multiverse\n\n\n\n\n\u6e90\u7f51\u5740\n\n\nubuntu20 gcc \u7248\u672c\n\n\nubuntu20 \u9ed8\u8ba4gcc\u7248\u672c\u4e3a9.3.0 \n\n\ncuda 10.0\u4ec5\u652f\u6301 gcc <8,\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u5b89\u88c5gcc7\n\u9700\u8981\u5728\u955c\u50cf\u6e90\u91cc\u6dfb\u52a0\n\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse\n\n\n\u7136\u540e\uff0c \nsudo apt-get update\n ,\u518d \nsudo apt-get install gcc-7\n\n\ngcc \u7248\u672c \u63a7\u5236\n\n\nsudo update-alternatives --config gcc\n\n\n\nubuntu20\u5b89\u88c5 cuda10.0\n\n\n\n\n\u5207\u6362gcc\u5230\u7248\u672c7\n\n\n\u5728Cuda\u5b98\u7f51\uff0c\u4e0b\u8f7dcuda10.0 Linux -> x86_64 -> ubuntu -> 18.04 -> runfile(local)\n\n\nsudo sh cuda***.run\n\n\n\u6309\nq\n\u952e\uff0c\u8f93\u5165\naccept\n\uff0c \u5728\u9009\u62e9\u662f\u5426\u5b89\u88c5\u9a71\u52a8\u5904\uff0c\u9009\u62e9\nno\n\n\n\u5b89\u88c5\u5b8c\u6210\uff0c\u4fee\u6539\nnano ~/.bashrc\n \u5728\u672b\u5c3e\u6dfb\u52a0\n\n\n\n\nexport CUDA_HOME=/usr/local/cuda-10.0\nexport PATH=$PATH:$CUDA_HOME/bin\nexport LD_LIBRARY_PATH=${CUDA_HOME}/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\n\n\n\n\u5b89\u88c5cudnn\n\u4e0b\u8f7d cuDNN v7.6.5 for Cuda10 runtime library for ubuntu18.04 deb\n\nsudo dpkg -i libcudnnxxxxxxxxxxxxxxxxxx.deb\n\n\n\u5b89\u88c5pytorch 1.3.1 cuda 10.0\n\n\npip install torch==1.3.1+cu100 torchvision==0.4.2+cu100 -f https://download.pytorch.org/whl/torch_stable.html\n\n\n\n\u91cd\u88c5nvidia \u9a71\u52a8\n\n\n\u5173\u95edgpu\n\n\nsudo -i\nsystemctl isolate multi-user.target\nmodprobe -r nvidia-drm\n\n\n\nkeep X11 display after su or sudo\n\n\nchown user:group ~/.Xauthority\nchmod 0600 ~/.Xauthority\nsudo xauth add $(xauth -f ~promote/.Xauthority list|tail -1)\n\n\n\nvscode remote \u6c49\u5b57\u8f93\u5165\n\n\nibus-daemon -drx\n\n\n\nwps\u6dfb\u52a0ibus\n\n\n\u4fee\u6539/usr/bin/wps\u3002\n\u5982\u679c\u4f60\u7528\u7684\u8f93\u5165\u6cd5\uff08input method\uff09\u662f\u57fa\u4e8eibus\u7684\uff0c\u90a3\u4e48\u52a0\u5165\u5982\u4e0b\u4ee3\u7801\uff1a\n\n\nexport XMODIFIERS=\"@im=ibus\"\nexport QT_IM_MODULE=\u201cibus\u201d\u3002\n\n\n\nfcitx\u5e94\u8be5\u540c\u7406\u3002",
            "title": "Ubuntu20"
        },
        {
            "location": "/ubuntu20/#vscode-debug-cpp-ros-node",
            "text": "catkin_make -DCMAKE_BUILD_TYPE=RelWithDebInfo  \u5bf9\u4e8eintelliSenseEngine\u81ea\u52a8\u5b8c\u6210\u529f\u80fd\u6b63\u786e\u8bbe\u7f6e \"C_Cpp.intelliSenseEngine\": \"Tag Parser\"  Debuge->add_Configuration->{} ROS:Launch   launch.json  {\n    // Use IntelliSense to learn about possible attributes.\n    // Hover to view descriptions of existing attributes.\n    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n\n        {\n            \"name\": \"ROS: Launch\",\n            \"type\": \"ros\",\n            \"request\": \"launch\",\n            \"target\": \"/home/pmjd/Disk/radar/src/ars_40X/launch/ars_40X.launch\"\n        }\n    ]\n}   \u8d77\u4e00\u4e2aroscore  \u5c31\u53ef\u4ee5debug\u4e86  \u4e5f\u53ef\u4ee5 debug ros \u8282\u70b9\uff0c Debuge->add_Configuration->{} ROS:Attach  \u542f\u52a8terminal \uff0c rosrun ars_40 deal_img  \u5f00\u59cbdebug, \u9009\u62e9c++ \u548c\u8282\u70b9\u540d\u5b57",
            "title": "vscode Debug cpp Ros node"
        },
        {
            "location": "/ubuntu20/#apt",
            "text": "sudo nano /etc/apt/sources.list  # \u9ed8\u8ba4\u6ce8\u91ca\u4e86\u6e90\u7801\u955c\u50cf\u4ee5\u63d0\u9ad8 apt update \u901f\u5ea6\uff0c\u5982\u6709\u9700\u8981\u53ef\u81ea\u884c\u53d6\u6d88\u6ce8\u91ca\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-updates main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-updates main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-backports main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-backports main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-security main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-security main restricted universe multiverse\n\n# \u9884\u53d1\u5e03\u8f6f\u4ef6\u6e90\uff0c\u4e0d\u5efa\u8bae\u542f\u7528\n# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-proposed main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-proposed main restricted universe multiverse  \u6e90\u7f51\u5740",
            "title": "\u66f4\u6539apt\u955c\u50cf\u6e90"
        },
        {
            "location": "/ubuntu20/#ubuntu20-gcc",
            "text": "ubuntu20 \u9ed8\u8ba4gcc\u7248\u672c\u4e3a9.3.0   cuda 10.0\u4ec5\u652f\u6301 gcc <8,\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u5b89\u88c5gcc7\n\u9700\u8981\u5728\u955c\u50cf\u6e90\u91cc\u6dfb\u52a0 deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse  \u7136\u540e\uff0c  sudo apt-get update  ,\u518d  sudo apt-get install gcc-7  gcc \u7248\u672c \u63a7\u5236  sudo update-alternatives --config gcc",
            "title": "ubuntu20 gcc \u7248\u672c"
        },
        {
            "location": "/ubuntu20/#ubuntu20-cuda100",
            "text": "\u5207\u6362gcc\u5230\u7248\u672c7  \u5728Cuda\u5b98\u7f51\uff0c\u4e0b\u8f7dcuda10.0 Linux -> x86_64 -> ubuntu -> 18.04 -> runfile(local)  sudo sh cuda***.run  \u6309 q \u952e\uff0c\u8f93\u5165 accept \uff0c \u5728\u9009\u62e9\u662f\u5426\u5b89\u88c5\u9a71\u52a8\u5904\uff0c\u9009\u62e9 no  \u5b89\u88c5\u5b8c\u6210\uff0c\u4fee\u6539 nano ~/.bashrc  \u5728\u672b\u5c3e\u6dfb\u52a0   export CUDA_HOME=/usr/local/cuda-10.0\nexport PATH=$PATH:$CUDA_HOME/bin\nexport LD_LIBRARY_PATH=${CUDA_HOME}/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}  \u5b89\u88c5cudnn\n\u4e0b\u8f7d cuDNN v7.6.5 for Cuda10 runtime library for ubuntu18.04 deb sudo dpkg -i libcudnnxxxxxxxxxxxxxxxxxx.deb",
            "title": "ubuntu20\u5b89\u88c5 cuda10.0"
        },
        {
            "location": "/ubuntu20/#pytorch-131-cuda-100",
            "text": "pip install torch==1.3.1+cu100 torchvision==0.4.2+cu100 -f https://download.pytorch.org/whl/torch_stable.html",
            "title": "\u5b89\u88c5pytorch 1.3.1 cuda 10.0"
        },
        {
            "location": "/ubuntu20/#nvidia",
            "text": "\u5173\u95edgpu  sudo -i\nsystemctl isolate multi-user.target\nmodprobe -r nvidia-drm",
            "title": "\u91cd\u88c5nvidia \u9a71\u52a8"
        },
        {
            "location": "/ubuntu20/#keep-x11-display-after-su-or-sudo",
            "text": "chown user:group ~/.Xauthority\nchmod 0600 ~/.Xauthority\nsudo xauth add $(xauth -f ~promote/.Xauthority list|tail -1)",
            "title": "keep X11 display after su or sudo"
        },
        {
            "location": "/ubuntu20/#vscode-remote",
            "text": "ibus-daemon -drx",
            "title": "vscode remote \u6c49\u5b57\u8f93\u5165"
        },
        {
            "location": "/ubuntu20/#wpsibus",
            "text": "\u4fee\u6539/usr/bin/wps\u3002\n\u5982\u679c\u4f60\u7528\u7684\u8f93\u5165\u6cd5\uff08input method\uff09\u662f\u57fa\u4e8eibus\u7684\uff0c\u90a3\u4e48\u52a0\u5165\u5982\u4e0b\u4ee3\u7801\uff1a  export XMODIFIERS=\"@im=ibus\"\nexport QT_IM_MODULE=\u201cibus\u201d\u3002  fcitx\u5e94\u8be5\u540c\u7406\u3002",
            "title": "wps\u6dfb\u52a0ibus"
        },
        {
            "location": "/util_wiki/",
            "text": "rqt_image_view \u63d0\u793a ImageView.callback_image() could not convert image from '8UC3' to 'rgb8' ([8UC3] is not a color format. but [rgb8] is. The conversion does not make sense)\n\n\n\u89e3\u51b3\u529e\u6cd5:\\\n- \u9996\u5148\u5728\uff4f\uff50\uff45\uff4e\uff43\uff56\u5904\u628a\uff18\uff35\uff23\uff13\u683c\u5f0f\u7684\uff4d\uff41\uff54\u8f6c\u5316\u4e3a\uff52\uff47\uff42\u683c\u5f0f\n\n\ncv::cvtColor(compoundFrame, compoundFrame, cv::COLOR_RGB2BGR);\n\n- \u7136\u540ecv_bridge\u7f16\u7801\u683c\u5f0f\u6539\u4e3asensor_msgs::image_encodings::RGB8\n\n\nimg_bridge = cv_bridge::CvImage(headers, sensor_msgs::image_encodings::RGB8, compoundFrame);\n\n\nRos\u81ea\u5b9a\u4e49\uff4d\uff53\uff47\u7f16\u8bd1\u65f6\u62a5\u9519\uff1a\u7f3a\u5c11\u5934\u6587\u4ef6\n\n\n\u53ea\u9700\u5728CMakeLists.txt\u91cc\u6dfb\u52a0\\\n\nadd_dependencies(GetLaneExtDemo smart_eye_gencpp)\n\\\nsmart_eye \u53ef\u6362\u6210\u4efb\u610fpkg \u540d\u79f0\\\nGetLaneExtDemo\u3000\u4e3a\u8282\u70b9\u540d\u79f0\n\n\nModuleNotFoundError: No module named \u2018rospkg\u2019\n\n\npip install rospkg                       //\u66f4\u65b0\u65b9\u5f0f1\nsudo apt-get install python-rospkg           //\u66f4\u65b0\u65b9\u5f0f2\n//\u7f51\u4e0a\u8bf4\u6709\u7684\u65b9\u5f0f1\u80fd\u89e3\u51b3\uff0c\u6709\u7684\u65b9\u5f0f2\u53ef\u4ee5\u89e3\u51b3\uff0c\u7528pip\u66f4\u65b0\u7684\u524d\u63d0\u662f\u5b89\u88c5\u4e86pip\n\n\n\nImportError: No module named genmsg\n\n\n\u547d\u4ee4\uff1a\u628asudo make \u6539\u4e3a\u3000make,\u5c31\u53ef\u4ee5\u627e\u5230\u5e93\uff0c\u76ee\u524d\u539f\u56e0\u4e0d\u660e\n\n\nui_mainwindow.h: No such file or directory\n\n\nset(CMAKE_AUTOUIC ON)\nset(CMAKE_INCLUDE_CURRENT_DIR ON)\n\n\ngit\u7684\u547d\u4ee4\u884c\u5e94\u7528\n\n\n\n\n\n\ngit status\n\n\ngit add .\n\n\ngit commit -m \"q\"\n\n\ngit push\n\n\n\n\n\n\n\u867d\u7136\u6709\u4e0d\u540c\u7684\uff52\uff4f\uff53\u5de5\u4f5c\u7a7a\u95f4\n\n\n\u4f46\u662f\u6700\u597d\u4e0d\u8981\u6709\u76f8\u540c\u7684\u8282\u70b9\u540d\u5b57\u3002\u56e0\u4e3a\u5bb9\u6613\u5f15\u8d77\u6df7\u4e71\uff0c\u8fd0\u884c\u6df7\u4e71\u7b49\u3002\u3002\u3002",
            "title": "Util wiki"
        },
        {
            "location": "/util_wiki/#rqt_image_view-imageviewcallback_image-could-not-convert-image-from-8uc3-to-rgb8-8uc3-is-not-a-color-format-but-rgb8-is-the-conversion-does-not-make-sense",
            "text": "\u89e3\u51b3\u529e\u6cd5:\\\n- \u9996\u5148\u5728\uff4f\uff50\uff45\uff4e\uff43\uff56\u5904\u628a\uff18\uff35\uff23\uff13\u683c\u5f0f\u7684\uff4d\uff41\uff54\u8f6c\u5316\u4e3a\uff52\uff47\uff42\u683c\u5f0f  cv::cvtColor(compoundFrame, compoundFrame, cv::COLOR_RGB2BGR); \n- \u7136\u540ecv_bridge\u7f16\u7801\u683c\u5f0f\u6539\u4e3asensor_msgs::image_encodings::RGB8  img_bridge = cv_bridge::CvImage(headers, sensor_msgs::image_encodings::RGB8, compoundFrame);",
            "title": "rqt_image_view \u63d0\u793a ImageView.callback_image() could not convert image from '8UC3' to 'rgb8' ([8UC3] is not a color format. but [rgb8] is. The conversion does not make sense)"
        },
        {
            "location": "/util_wiki/#rosmsg",
            "text": "\u53ea\u9700\u5728CMakeLists.txt\u91cc\u6dfb\u52a0\\ add_dependencies(GetLaneExtDemo smart_eye_gencpp) \\\nsmart_eye \u53ef\u6362\u6210\u4efb\u610fpkg \u540d\u79f0\\\nGetLaneExtDemo\u3000\u4e3a\u8282\u70b9\u540d\u79f0",
            "title": "Ros\u81ea\u5b9a\u4e49\uff4d\uff53\uff47\u7f16\u8bd1\u65f6\u62a5\u9519\uff1a\u7f3a\u5c11\u5934\u6587\u4ef6"
        },
        {
            "location": "/util_wiki/#modulenotfounderror-no-module-named-rospkg",
            "text": "pip install rospkg                       //\u66f4\u65b0\u65b9\u5f0f1\nsudo apt-get install python-rospkg           //\u66f4\u65b0\u65b9\u5f0f2\n//\u7f51\u4e0a\u8bf4\u6709\u7684\u65b9\u5f0f1\u80fd\u89e3\u51b3\uff0c\u6709\u7684\u65b9\u5f0f2\u53ef\u4ee5\u89e3\u51b3\uff0c\u7528pip\u66f4\u65b0\u7684\u524d\u63d0\u662f\u5b89\u88c5\u4e86pip",
            "title": "ModuleNotFoundError: No module named \u2018rospkg\u2019"
        },
        {
            "location": "/util_wiki/#importerror-no-module-named-genmsg",
            "text": "\u547d\u4ee4\uff1a\u628asudo make \u6539\u4e3a\u3000make,\u5c31\u53ef\u4ee5\u627e\u5230\u5e93\uff0c\u76ee\u524d\u539f\u56e0\u4e0d\u660e",
            "title": "ImportError: No module named genmsg"
        },
        {
            "location": "/util_wiki/#ui_mainwindowh-no-such-file-or-directory",
            "text": "set(CMAKE_AUTOUIC ON)\nset(CMAKE_INCLUDE_CURRENT_DIR ON)",
            "title": "ui_mainwindow.h: No such file or directory"
        },
        {
            "location": "/util_wiki/#git",
            "text": "git status  git add .  git commit -m \"q\"  git push",
            "title": "git\u7684\u547d\u4ee4\u884c\u5e94\u7528"
        },
        {
            "location": "/util_wiki/#ros",
            "text": "\u4f46\u662f\u6700\u597d\u4e0d\u8981\u6709\u76f8\u540c\u7684\u8282\u70b9\u540d\u5b57\u3002\u56e0\u4e3a\u5bb9\u6613\u5f15\u8d77\u6df7\u4e71\uff0c\u8fd0\u884c\u6df7\u4e71\u7b49\u3002\u3002\u3002",
            "title": "\u867d\u7136\u6709\u4e0d\u540c\u7684\uff52\uff4f\uff53\u5de5\u4f5c\u7a7a\u95f4"
        },
        {
            "location": "/vision360/",
            "text": "\u786c\u4ef6\u5b89\u88c5\n\n\n\n\n\u5b89\u88c5\u9a71\u52a8\n\n\nAHD_8CH.rar\n\n\nrar x AHD_8CH.rar\ncd AHD_8CH/driver\nmake\nsudo insmod dvrs_hw.ko\n\n\n\n\u5b89\u88c5\u8fc7\u7a0b\u4e2d\u9047\u5230\u7684\u95ee\u9898\n\n\n\n\n\u6444\u50cf\u5934\u8bfb\u4e0d\u51fa\u6570\u636e\n\n\n\n\n\u56e0\u4e3a\u6444\u50cf\u5934\u751f\u4ea7\u5546\u63d0\u4f9b\u7684\u7ebf\u5e8f\u6709\u95ee\u9898\n\n\n\n\n\npcie\u8bfb\u5230\u7684\u56fe\u50cf\u662f\u7eff\u8272\u7684\n\n\n\n\n\u56e0\u4e3apcie\u677f\u5361\u751f\u4ea7\u5546\u63d0\u4f9b\u7684\u677f\u5361\u6709\u95ee\u9898\uff0c\u56fe\u50cf\u89e3\u6790\u683c\u5f0f\u95ee\u9898",
            "title": "Vision360"
        },
        {
            "location": "/vision360/#_1",
            "text": "",
            "title": "\u786c\u4ef6\u5b89\u88c5"
        },
        {
            "location": "/vision360/#_2",
            "text": "AHD_8CH.rar  rar x AHD_8CH.rar\ncd AHD_8CH/driver\nmake\nsudo insmod dvrs_hw.ko  \u5b89\u88c5\u8fc7\u7a0b\u4e2d\u9047\u5230\u7684\u95ee\u9898   \u6444\u50cf\u5934\u8bfb\u4e0d\u51fa\u6570\u636e   \u56e0\u4e3a\u6444\u50cf\u5934\u751f\u4ea7\u5546\u63d0\u4f9b\u7684\u7ebf\u5e8f\u6709\u95ee\u9898   pcie\u8bfb\u5230\u7684\u56fe\u50cf\u662f\u7eff\u8272\u7684   \u56e0\u4e3apcie\u677f\u5361\u751f\u4ea7\u5546\u63d0\u4f9b\u7684\u677f\u5361\u6709\u95ee\u9898\uff0c\u56fe\u50cf\u89e3\u6790\u683c\u5f0f\u95ee\u9898",
            "title": "\u5b89\u88c5\u9a71\u52a8"
        },
        {
            "location": "/voxelnet/",
            "text": "VoxelNet\n\n\nTo interface a highly sparse\u7a00\u758f LIDAR point cloud with a region proposal network(RPN\u533a\u57df\u5efa\u8bae\u7f51\u7edc),most existing efforts have focused on hand-crafted\u624b\u5de5\u5236\u4f5c feature representations\u7279\u5f81\u8868\u793a\uff0cfor example, a bird's eye view projectiong\u9e1f\u77b0\u56fe.\n\n\nIn this work,we remove the need of manual feature engineering for 3D point clouds and purpose VoxelNet, a generic 3D detection network that unifies feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network.\n\n\nSpecially,VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a unified feature representation through the newly introduced voxel feature encoding(VFE) layer.In this way,the point cloud is encoded as a descriptive volumetric representation, which is then connected to a RPN to generate detections.\n\n\nOur network learns an effective discriminative representation\u533a\u5206\u6027\u8868\u793a of objects with various geometrics, leading to encouraging results in 3D detection of pedestrains and cyclists, based on only Lidar.\n\n\n\\\nVoxelNet architecture. The feature learning network takes a raw point cloud as input, partitions the space into voxels, and transforms points within each voxel to a vector representation characterizing the shape information.The space is represented as a sparse 4D tensor.The convolutional middle layers processes the 4D tensor to aggregate spatial context \u805a\u5408\u7a7a\u95f4\u8bed\u5883.Finally,a RPN generates the 3D detection.\n\n\nScalling up 3D feature learning networks to orders of magnitude more points and to 3D detection tasks are the main challenges that we address in this paper.\n\n\nWe present VoxelNet, a generic 3D detection framework that simultaneously\u540c\u65f6 learns a discriminative\u5224\u522b\u6027 feature representation from point clouds and predicts accurate 3D bounding boxes, in an end-to-end fashion\u65b9\u5f0f.\n\n\nWe design a novel\u65b0\u9896\u7684 voxel feature encoding(VFE) layer,which enables inter-point interaction\u70b9\u95f4\u4ea4\u4e92\u3000within a voxel,by combining point-wise features\u9010\u70b9\u7279\u5f81 with a locally aggregated feature\u805a\u5408\u7279\u5f81.\n\n\nStacking \u5806\u53e0 multiple VFE layers allows learning complex features for characterizing\u8868\u5f81 local 3D shape information.\n\n\nSpecially, VoxelNet divides the point cloud into equally spaced 3D voxel\u7b49\u8ddd\u7684\uff13\uff24\u4f53\u7d20 ,encodes each voxel via stacked VFE layers, and then 3D convolution furture\u8fdb\u4e00\u6b65 aggregate\u805a\u5408 local voxel features, transforming the pointcloud into a high-dimensional volumetric\u4f53\u79ef representation.\n\n\nFinally,a RPN consumes the volumetric represetation and yields\u4ea7\u751f the detection result.\n\n\nThis efficient algorithm benefits both from the sparse\u7a00\u758f\u7684 point structure and efficient parallel processing on the voxel grid\u4f53\u7d20\u7f51\u683c\u7684\u9ad8\u6548\u5e76\u884c\u5904\u7406.\n\n\nVoxelNet Architecture\n\n\nThe proposed VoxelNet consists of three functional blocks:(1) Feature learning network,(2) Convolutional middle layers, and (3) Region proposal network\n\n\nFeature Learning Network\n\n\nVoxel Partition\n Given a point cloud, we subdivide the 3D space into equally spaced voxels as shown in Figure 2.Suppose the point cloud encompasses\u5305\u542b 3D space with range D,H,W\uff08\u70b9\u4e91\u5c3a\u5bf8\uff09 along the Z,Y,X axes respectively.We define each voxel of size vD,vH,and vW\uff08\u4f53\u7d20\u5c3a\u5bf8\uff09 accordingly.The resulting\u6240\u751f\u6210\u7684 3D voxel grid is of size D'=D/vD, H'=H/vH, W'=W/vW\uff08\u70b9\u4e91\u5305\u542b\u4f53\u7d20\u4e2a\u6570\uff09.Here, for simplicity,we assume D,H,W are a multiple of vD,vH,vW.\u6211\u4eec\u5047\u8bbeD,H,W\u662fvD, vH, vW\u7684\u500d\u6570\u3002\n\n\nGrouping\n We group the points according to the voxel they reside in\u6211\u4eec\u6839\u636e\u5b83\u4eec\u6240\u5728\u7684\u4f53\u7d20\u5bf9\u70b9\u8fdb\u884c\u5206\u7ec4.Due to factors such as distance,occlusion\u906e\u6321,object's relative pose\u7269\u4f53\u7684\u76f8\u5bf9\u59ff\u52bf,and non-uniform\u4e0d\u5747\u5300 sampling,the LiDAR point cloud is sparse and highly variable\u9ad8\u5ea6\u53ef\u53d8\u7684 point density\u70b9\u5bc6\u5ea6 throughout the space.Therefore, after gouping, a voxel will contain a variable number of points.\n\n\nRandom Sampling\n Typically a high-definition LiDAR point cloud is composed of ~100k points. Directly processing all the points not only imposes\u52a0\u5f3a increased memory/efficiency burdens\u8d1f\u62c5\u3000on the computing platform, but also highly variable point density  throughout the space might bias the detection.To this end\u56e0\u6b64,we randomly sample a fixed number, T,of points from those voxels containing more than T points.This sampling strategy\u6218\u7565 has two purpose,(1)computational saving;and (2)decrease the imbalance of points between the voxels which reduces the sampling bias, and adds more variation\u53d8\u5316 to training.\n\n\nStacked Voxel Feature Encoding\n\\\n\n\\\nThe key innovation\u9769\u65b0 is the chain\u94fe of VFE layers.For simplicity, Figure 2 illustrates the hierachical\u9636\u7ea7\u5f0f feature encoding process for one voxel.Without loss of generality\u6982\u8981,(\u5728\u4e0d\u5931\u4e00\u822c\u6027\u7684\u524d\u63d0\u4e0b)we use VFE Layer-1 to describe the details in the following paragraph\u6bb5\u843d.Figure 3 shows the architecture for VFE Layer-1.\\\nDenote\u8868\u793a \n as a non-empty voxel containing t\u2264\uff34 LiDAR points,where \n contains XYZ coordinates for the i-th point and \n is the received reflectance\u53cd\u5c04\u7387.We first compute the local mean\u5c40\u90e8\u5747\u503c as the centroid\u8d28\u5fc3 of all the points in V, denoted as \n.Then we augment\u589e\u52a0 each point \n with the relative offset w.r.t. the centriod and obtain the input feature set \n, Next, each \n is transformed through the fully connected network(FCN) into a feature space,where we can aggregate\u6c47\u603b information from the point features \n to ecode the shape of the surface contained within the voxel.The FCN is composed of a linear layer, a batch normalization(BN\u6279\u91cf\u6807\u51c6\u5316) layer and a rectified\u7ea0\u6b63\u7684 linear unit(ReLU) layer.After obtaining point-wise feature representations, we use element-wise MaxPooling across all \n associated to V to get the locally aggregated feature \n \nFinally, we augment each \n with \n to form the point-wise concatenated\u7ea7\u8054\u7684 feature as \n Thus we obtain the output feature set \n.All non-empty voxels are ecoded in the same way and they share the same set of parameters in FCN.\n\n\nWe use \n to represent the i-th VFE layer that transforms input features of dimension \n into output features of dimension \n. The linear layer learns a matrix of size \n, and the point-wise concatenation yields the output of dimension \n.\n\n\nBecause the output feature combines both point-wise features and locally aggregated feature, stacking VFE layers encodes point interactions\u4e92\u52a8\u3000within a voxel and enables the final feature representation to learn descriptive shape information. The voxel-wise feature is obtained by transforming the output of VFE-n into \n via FCN and applying element-wise Maxpool where C is the dimension of the voxel-wise feature, as shown in Figure2.\n\n\nSaparse Tensor Representation\n\u7a00\u758f\u5f20\u91cf\u8868\u793a\u3000By processing only the non-empty voxels, we obtain a list of voxel features,each uniquely\u72ec\u7279\u7684 associated to the spatial\u7a7a\u95f4 coordinates of a pictular non-empty voxel.The obtained list of voxel-wise features can be represented as a sparse 4D tensor, of size C x D' x H' x W' as shown in Figure2.Although the point cloud contains ~100k points, more than 90% of voxels typically are empty.Representing non-empty voxel features as a sparse tensor greatly reduce the memory usage and computation cost during backpropagation\u53cd\u5411\u4f20\u64ad, and it is a critical step in our efficient implementation.\n\n\nConvolutional Middle Layers\n\n\nWe use ConvMD\n to represent an M-dimensional  convolution operator where \n and \n are the number of input and output channels, k,s, and p are the M-dimensional vectors correspoinding to kernel size, stride size and padding\u586b\u5145 size respectively.When the size across the M-dimensions are the same, we use a scalar to represent the size e.g. k for k=(k,k,k).\n\n\nEach convolutional middle layer applies 3D convolution,BN layer, and ReLU layer sequentially\u4f9d\u6b21.The convolutional middle layers aggregate voxel-wise features within a progressively\u9010\u6b65 expanding\u6269\u5927\u7684 receptive\u63a5\u6536 field\uff0cadding more context to the shape description.The detialed sizes of the filters in the convolutional middle layers are explained in Section 3.\n\n\nRegion Proposal Network\n\n\n\\\nRecently, region proposal\u63d0\u6848 networks have become an important building block of top-performing object detection frameworks. In this work,we make servel key modifications to the RPN architecture proposed in [34], and combine it with the feature learning network and convolutional middle layers to form an end-to-end trainable pipeline\u7ba1\u9053.\n\n\nThe input to our RPN is the feature map provided by the convolutional middle layers.The architecture of this network is illustrate in Figure 4.The network has three blocks of fully convolutional layers.The first layer of each block downsamples the feature map by half via a convolution with a stride size of 2,followed by a sequence of convolutions of stride 1(xq means q applications of the filter).After each convolution layer,BN and ReLU operations are applied.We then upsample the output of every block to a fixed size and concatanate to construct the high resolution feature map. Finally, this feature map is mapped to the desired learning targets:(1) a probability score map and (2) a regression map.\uff32\uff30\uff2e\u7684\u8f93\u5165\u662f\u5377\u79ef\u4e2d\u95f4\u5c42\u63d0\u4f9b\u7684\u7279\u5f81\u56fe\u3002\u8be5\u7f51\u7edc\u7684\u4f53\u7cfb\u7ed3\u6784\u5982\u56fe\uff14\u6240\u793a\u3002\u8be5\u7f51\u7edc\u5177\u6709\u4e09\u4e2a\u5b8c\u5168\u5377\u57fa\u5c42\u7684\u5757\u3002\u6bcf\u4e2a\u5757\u7684\u7b2c\u4e00\u5c42\u901a\u8fc7\u6b65\u5e45\u4e3a\uff12\u7684\u5377\u79ef\u5bf9\u7279\u5f81\u56fe\u8fdb\u884c\u4e00\u534a\u4e0b\u91c7\u6837\uff0c\u7136\u540e\u662f\u6b65\u5e45\uff11\u7684\u5377\u79ef\u5e8f\u5217\uff08xq \u8868\u793a\u6ee4\u6ce2\u5668\u7684\uff51\u4e2a\u5e94\u7528\uff09\u3002\u5728\u6bcf\u4e2a\u5377\u57fa\u5c42\u4e4b\u540e\uff0cBN \u548cReLU \u64cd\u4f5c\u88ab\u5e94\u7528\u3002\u7136\u540e\uff0c\u6211\u4eec\u5c06\u6bcf\u4e2a\u5757\u7684\u8f93\u51fa\u4e0a\u91c7\u6837\u5230\u56fa\u5b9a\u5927\u5c0f\uff0c\u5e76\u6c47\u603b\u4ee5\u6784\u5efa\u9ad8\u5206\u8fa8\u7387\u7279\u5f81\u56fe\u3002\u6700\u540e\uff0c\u5c06\u6b64\u7279\u5f81\u56fe\u6620\u5c04\u5230\u6240\u9700\u7684\u5b66\u4e60\u76ee\u6807\uff1a\uff08\uff11\uff09\u6982\u7387\u5206\u6570\u56fe\u548c\uff08\uff12\uff09\u56de\u5f52\u56fe\u3002\n\n\nLoss Function\n\n\nLet \n be the set of \n positive anchors\u951a\u70b9 and \n be the set of \n negative anchors.\nWe parameterize a 3D ground truth box as \n,where \n represent the center location,\n are length ,width,height of the box, and \n is the yaw rotation around Z-axis. To retrieve\u627e\u56de the ground truth box from a matching positive anchorparameterized as \n,we define the residual vector\u6b8b\u5dee\u5411\u91cf \n containing the 7 regression targets corresponding to center location \u25b3x,\u25b3y,\u25b3z,three dimensions \u25b3l, \u25b3w, \u25b3h, and the rotation \u25b3\u0398, which are computed as :\\\n\n\\\nwhere \n is the diagonal\u5bf9\u89d2\u7ebf of the base of the anchor box.Here, we aim to directly estimate the oriented\u5b9a\u5411\u7684 3D box and normallize \u0394x and \u0394y homogeneously\u5747\u5300\u7684 with the diagonal\u5bf9\u89d2\u7ebf \n,We define the loss function as follows:\\\n\n\\\nwhere \n and \n represent the softmax output for positive anchor \n and negative anchor \n respectively,while \n and \n are the regression\u56de\u5f52 output and ground truth for positive anchor \n.The first two terms are the normalized classification loss for \n and \n, where the \n stands for binary cross entropy loss\u4e8c\u5143\u4ea4\u53c9\u71b5\u635f\u5931 and \u03b1\uff0c\u03b2 are postive constants balancing the relative importance. The last term \n is the regression loss, where we use the SmoothL1 function.\n\n\nEfficient Implementation\n\n\nGpus are optimized\u4f18\u5316 for processing dense\u7a20\u5bc6 tensor structures\uff0eThe problem with working directly with the point cloud is that the points are sparsely distributed across sapce and each voxel has a variable number of points.We devised\u8bbe\u8ba1\u7684 \na method that converts the point cloud into a dense tensor structure where stacked VFE operations can be processed in parallel across points and voxels.\\\n\n\\\nThe method is summarized in Figure 5. We initialize a KxTx7 dimensional tensor structure to store the voxel input feature where K is the maximum number of non-empty voxels, T is the maximum number of points per voxel, and 7 is the input encoding dimension for each point.The points are randomized\u968f\u673a\u7684 before processing.For each point in pointcloud, we check if the corresponding voxel already exists. This lookup operation is done efficiently in O(1) using a hash table where the voxel coordinate is used as the hash key.If the voxel is already initialized we insert the point to voxel location if there are less than T points, otherwise the point is ignored.If the voxel is not initialized, we initialize a new voxel, store its coordinate in the voxel coordinate buffer, and insert the point to this voxel location.The voxel input feature and coordinate buffers can be constructed\u5efa via a signle pass over the point list, therefore its complexity is O(n).\u4f53\u7d20\u8f93\u5165\u7279\u5f81\u548c\u5750\u6807\u7f13\u51b2\u533a\u53ef\u4ee5\u901a\u8fc7\u5bf9\u70b9\u5217\u8868\u7684\u4e00\u6b21\u904d\u5386\u6765\u6784\u9020\uff0c\u56e0\u6b64\u5176\u590d\u6742\u5ea6\u4e3aO\uff08n\uff09\u3002To further improve the memory/compute efficiency it is possible to only store a limited number of voxels(K) and ignore points coming from voxels with few points.\n\n\nAfter the voxel input buffer is constructed,the stacked VFE only involves\u6d89\u53ca point level and voxel level dense operations which can be computed on a GPU in parallel.Note that, after concatenation\u7ea7\u8054 operations in VFE, we reset the features corresponding to empty points to zero such that they do not affect the computed voxel features.Finally, using the stored coordinate buffer we reorganize\u6539\u7ec4 the computed sparse voxel-wise structures to the dense voxel grid.\u6700\u540e\uff0c\u4f7f\u7528\u5b58\u50a8\u7684\u5750\u6807\u7f13\u51b2\u533a\uff0c\u6211\u4eec\u5c06\u8ba1\u7b97\u7684\u7a00\u758f\u4f53\u7d20\u7ed3\u6784\u91cd\u7ec4\u4e3a\u5bc6\u96c6\u4f53\u7d20\u7f51\u683c.The following convolutional middle layers and RPN operations work on a dense voxel grid which can be efficiently implemented on a GPU.\n\n\nTraining Details\n\n\nNetwork Details\n\n\nOur experimental setup\u5b9e\u9a8c\u8bbe\u7f6e is based on the LiDAR specifications of the KITTI dataset.\\\n\nCar Detection\n For this task,we consider point clouds within the range of [-3,1]x[-40,40]x[0,70.4]meters along Z,Y,X axis respectively.Points taht are Points that are projected outside of image boundaries are removed.We choose a voxel size of \n meters,which leads to D'=10,H'=400,W'=352. We set T = 35 as the maximum number of randomly sampled points in each non-empty voxel.We use two VFE layers VFE-1(7,32) and VFE-2(32,128).The final FCN maps VFE-2 output to \n.Thus our feature learning net generate a sparse tensor of shape 128x10x400x352.To aggregate voxel-wise features, we employ three convolution middle layers sequentially as Conv3D(128,64,3,(2,1,1),(1,1,1)),Conv3D(64,64,3,(1,1,1),(0,1,1)),and Conv3D(64,64,3,(2,1,1),(1,1,1)), which yields a 4D tensor of size 64x2x400x352.After reshaping, the input to RPN is a feature map of size 128x400x352,where the dimensions correspond to channel,height, and width of the 3D tensor.Figure 4 illustrates the detailed network architecture for this task.Unlike, we use only one anchor size,\n meters, centered at\u96c6\u4e2d\u4e8e \n meters with two rotations, 0 and 90 degrees.Our anchor matching criteria\u5339\u914d\u6807\u51c6 as follows: An anchor is considered as positive if it has the highest Intersection over Union(IoU) with a ground truth or its IoU with ground truth is above 0.6(in bird's eye view).An anchor is considered as negative if the IoU between it and all ground true boxes is less than 0.45. We treat anchors as don't care if they have 0.45\u2264IoU\u22640.6 with any ground truth.We set \u03b1\uff1d1.5 and \u03b2=1 in Eqn.2.\n\n\nPedestrain and Cyclist Detection\n The input range is [-3,1]x[-20,20]x[0,48] meters along Z,Y,X axis respectively.We use the same voxel size as for car detection,which yields D=10, H=200, W=240.We set T=45 in order to obtain more LiDAR points for better capturing shape information.The feature learning network and convolutional middle layers ate identical\u76f8\u540c to the networks used in car detection task.For the RPN, we make one modification to block 1 in Figure 4 by changing the stride size in the first 2D convolution from 2 to 1. This allows finer resolution in anchor matching, which is necessary for detecting pedestrains and cyclists.We use anchor size \n \n meters centered at \n with 0 and 90 degrees rotation for pedestrain detection and use anchor size \n meters centered at \n with 0 and 90 degrees rotation for cyclist detecction.The specific anchor matching criteria is as follows:We assign an anchor as positive if it has the highest IoU with a ground truth, or its IoU with ground truth is above 0.5. An anchor is considered as negative if its IoU with every ground truth is less than 0.35.For anchors having 0.35\u2264IoU\u22640.5 with any ground truth,we treat them as don't care.\n\n\nDuring training, we use stochastic\u968f\u673a gradient descent(SGD) with learning rate 0.01 for the first 150 epochs and decrease the learning rate to 0.001 for the last 10 epochs.We use a batchsize of 16 point clouds.\n\n\nData Augmentation \u6570\u636e\u6269\u5c55\n\n\n\uff37ith less than 4000 training point clouds, training our network from scratch will inevitably \u4e0d\u53ef\u907f\u514d suffer from overfitting. To reduce this issue, we introduce three different forms of data augmentation. The augmented training data are generated on-the-fly\u5373\u65f6\u3000without the need to be stored on disk.\n\n\nDefine set \n as the whole point cloud, consisting of N points.We parameterize a 3D bounding box \n,where \n are center locations, l,w,h are length, width, height, and \u03b8 is the yaw rotation around Z-axis.We define \n \n as the set containing all LiDAR points within \n, where p=[x,y,z,r] denotes a particular LiDAR point in the whole set M.\n\n\nThe first form of data augmentation applies perturbation\u6444\u52a8 independently to each ground truth 3D bounding box together with those LiDAR points within the box.Specifically,around Z-axis we rotate \n and the associated \n with respect to (\n) by a uniformally\u7edf\u4e00\u7684 distributed random variable \n. Then we add a translation (\u0394x,\u0394y,\u0394z) to the XYZ components of \n and to each point in \n,where \u0394x,\u0394y,\u0394z are drawn independently from a Gaussian distribution with mean zero and standard deviation 1.0. To avoid physically impossible outcomes, we perform a collision\u78b0\u649e test between any two boxes after the perturbation and revert\u8fd8\u539f to orignal if a collision is detected.Since the perturbation is applied to each ground truth box and the associated LiDAR points independently, the network is able to learn from substantially\u5b9e\u8d28\u4e0a more variations than from the orignal training data.\n\n\nSecondly,we apply global scaling to all ground truth boxes \n and to whole point cloud M. Specifically, we multiply the XYZ coordinates and the three dimensions of each \n, and the XYZ coordinates of all points in M with a random variable drawn from uniform distribution [0.95,1.05]. Introducing global scale augementation improves robustness of the network for detecting objects with various sizes and distances as shown in image-based classification and detection tasks.\n\n\nFinally, we apply global rotation to all ground truth boxes \n and to the whole point cloud M. The rotation is applied along Z-axis and around(0,0,0). The global rotation offset is determined by sampling from uniform distribution [-\u03c0/4,\u03c0/4].By rotating the entire point cloud, we simulate the vehicle making a turn.",
            "title": "Voxelnet"
        },
        {
            "location": "/voxelnet/#voxelnet",
            "text": "To interface a highly sparse\u7a00\u758f LIDAR point cloud with a region proposal network(RPN\u533a\u57df\u5efa\u8bae\u7f51\u7edc),most existing efforts have focused on hand-crafted\u624b\u5de5\u5236\u4f5c feature representations\u7279\u5f81\u8868\u793a\uff0cfor example, a bird's eye view projectiong\u9e1f\u77b0\u56fe.  In this work,we remove the need of manual feature engineering for 3D point clouds and purpose VoxelNet, a generic 3D detection network that unifies feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network.  Specially,VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a unified feature representation through the newly introduced voxel feature encoding(VFE) layer.In this way,the point cloud is encoded as a descriptive volumetric representation, which is then connected to a RPN to generate detections.  Our network learns an effective discriminative representation\u533a\u5206\u6027\u8868\u793a of objects with various geometrics, leading to encouraging results in 3D detection of pedestrains and cyclists, based on only Lidar.  \\\nVoxelNet architecture. The feature learning network takes a raw point cloud as input, partitions the space into voxels, and transforms points within each voxel to a vector representation characterizing the shape information.The space is represented as a sparse 4D tensor.The convolutional middle layers processes the 4D tensor to aggregate spatial context \u805a\u5408\u7a7a\u95f4\u8bed\u5883.Finally,a RPN generates the 3D detection.  Scalling up 3D feature learning networks to orders of magnitude more points and to 3D detection tasks are the main challenges that we address in this paper.  We present VoxelNet, a generic 3D detection framework that simultaneously\u540c\u65f6 learns a discriminative\u5224\u522b\u6027 feature representation from point clouds and predicts accurate 3D bounding boxes, in an end-to-end fashion\u65b9\u5f0f.  We design a novel\u65b0\u9896\u7684 voxel feature encoding(VFE) layer,which enables inter-point interaction\u70b9\u95f4\u4ea4\u4e92\u3000within a voxel,by combining point-wise features\u9010\u70b9\u7279\u5f81 with a locally aggregated feature\u805a\u5408\u7279\u5f81.  Stacking \u5806\u53e0 multiple VFE layers allows learning complex features for characterizing\u8868\u5f81 local 3D shape information.  Specially, VoxelNet divides the point cloud into equally spaced 3D voxel\u7b49\u8ddd\u7684\uff13\uff24\u4f53\u7d20 ,encodes each voxel via stacked VFE layers, and then 3D convolution furture\u8fdb\u4e00\u6b65 aggregate\u805a\u5408 local voxel features, transforming the pointcloud into a high-dimensional volumetric\u4f53\u79ef representation.  Finally,a RPN consumes the volumetric represetation and yields\u4ea7\u751f the detection result.  This efficient algorithm benefits both from the sparse\u7a00\u758f\u7684 point structure and efficient parallel processing on the voxel grid\u4f53\u7d20\u7f51\u683c\u7684\u9ad8\u6548\u5e76\u884c\u5904\u7406.",
            "title": "VoxelNet"
        },
        {
            "location": "/voxelnet/#voxelnet-architecture",
            "text": "The proposed VoxelNet consists of three functional blocks:(1) Feature learning network,(2) Convolutional middle layers, and (3) Region proposal network",
            "title": "VoxelNet Architecture"
        },
        {
            "location": "/voxelnet/#feature-learning-network",
            "text": "Voxel Partition  Given a point cloud, we subdivide the 3D space into equally spaced voxels as shown in Figure 2.Suppose the point cloud encompasses\u5305\u542b 3D space with range D,H,W\uff08\u70b9\u4e91\u5c3a\u5bf8\uff09 along the Z,Y,X axes respectively.We define each voxel of size vD,vH,and vW\uff08\u4f53\u7d20\u5c3a\u5bf8\uff09 accordingly.The resulting\u6240\u751f\u6210\u7684 3D voxel grid is of size D'=D/vD, H'=H/vH, W'=W/vW\uff08\u70b9\u4e91\u5305\u542b\u4f53\u7d20\u4e2a\u6570\uff09.Here, for simplicity,we assume D,H,W are a multiple of vD,vH,vW.\u6211\u4eec\u5047\u8bbeD,H,W\u662fvD, vH, vW\u7684\u500d\u6570\u3002  Grouping  We group the points according to the voxel they reside in\u6211\u4eec\u6839\u636e\u5b83\u4eec\u6240\u5728\u7684\u4f53\u7d20\u5bf9\u70b9\u8fdb\u884c\u5206\u7ec4.Due to factors such as distance,occlusion\u906e\u6321,object's relative pose\u7269\u4f53\u7684\u76f8\u5bf9\u59ff\u52bf,and non-uniform\u4e0d\u5747\u5300 sampling,the LiDAR point cloud is sparse and highly variable\u9ad8\u5ea6\u53ef\u53d8\u7684 point density\u70b9\u5bc6\u5ea6 throughout the space.Therefore, after gouping, a voxel will contain a variable number of points.  Random Sampling  Typically a high-definition LiDAR point cloud is composed of ~100k points. Directly processing all the points not only imposes\u52a0\u5f3a increased memory/efficiency burdens\u8d1f\u62c5\u3000on the computing platform, but also highly variable point density  throughout the space might bias the detection.To this end\u56e0\u6b64,we randomly sample a fixed number, T,of points from those voxels containing more than T points.This sampling strategy\u6218\u7565 has two purpose,(1)computational saving;and (2)decrease the imbalance of points between the voxels which reduces the sampling bias, and adds more variation\u53d8\u5316 to training.  Stacked Voxel Feature Encoding \\ \\\nThe key innovation\u9769\u65b0 is the chain\u94fe of VFE layers.For simplicity, Figure 2 illustrates the hierachical\u9636\u7ea7\u5f0f feature encoding process for one voxel.Without loss of generality\u6982\u8981,(\u5728\u4e0d\u5931\u4e00\u822c\u6027\u7684\u524d\u63d0\u4e0b)we use VFE Layer-1 to describe the details in the following paragraph\u6bb5\u843d.Figure 3 shows the architecture for VFE Layer-1.\\\nDenote\u8868\u793a   as a non-empty voxel containing t\u2264\uff34 LiDAR points,where   contains XYZ coordinates for the i-th point and   is the received reflectance\u53cd\u5c04\u7387.We first compute the local mean\u5c40\u90e8\u5747\u503c as the centroid\u8d28\u5fc3 of all the points in V, denoted as  .Then we augment\u589e\u52a0 each point   with the relative offset w.r.t. the centriod and obtain the input feature set  , Next, each   is transformed through the fully connected network(FCN) into a feature space,where we can aggregate\u6c47\u603b information from the point features   to ecode the shape of the surface contained within the voxel.The FCN is composed of a linear layer, a batch normalization(BN\u6279\u91cf\u6807\u51c6\u5316) layer and a rectified\u7ea0\u6b63\u7684 linear unit(ReLU) layer.After obtaining point-wise feature representations, we use element-wise MaxPooling across all   associated to V to get the locally aggregated feature   \nFinally, we augment each   with   to form the point-wise concatenated\u7ea7\u8054\u7684 feature as   Thus we obtain the output feature set  .All non-empty voxels are ecoded in the same way and they share the same set of parameters in FCN.  We use   to represent the i-th VFE layer that transforms input features of dimension   into output features of dimension  . The linear layer learns a matrix of size  , and the point-wise concatenation yields the output of dimension  .  Because the output feature combines both point-wise features and locally aggregated feature, stacking VFE layers encodes point interactions\u4e92\u52a8\u3000within a voxel and enables the final feature representation to learn descriptive shape information. The voxel-wise feature is obtained by transforming the output of VFE-n into   via FCN and applying element-wise Maxpool where C is the dimension of the voxel-wise feature, as shown in Figure2.  Saparse Tensor Representation \u7a00\u758f\u5f20\u91cf\u8868\u793a\u3000By processing only the non-empty voxels, we obtain a list of voxel features,each uniquely\u72ec\u7279\u7684 associated to the spatial\u7a7a\u95f4 coordinates of a pictular non-empty voxel.The obtained list of voxel-wise features can be represented as a sparse 4D tensor, of size C x D' x H' x W' as shown in Figure2.Although the point cloud contains ~100k points, more than 90% of voxels typically are empty.Representing non-empty voxel features as a sparse tensor greatly reduce the memory usage and computation cost during backpropagation\u53cd\u5411\u4f20\u64ad, and it is a critical step in our efficient implementation.",
            "title": "Feature Learning Network"
        },
        {
            "location": "/voxelnet/#convolutional-middle-layers",
            "text": "We use ConvMD  to represent an M-dimensional  convolution operator where   and   are the number of input and output channels, k,s, and p are the M-dimensional vectors correspoinding to kernel size, stride size and padding\u586b\u5145 size respectively.When the size across the M-dimensions are the same, we use a scalar to represent the size e.g. k for k=(k,k,k).  Each convolutional middle layer applies 3D convolution,BN layer, and ReLU layer sequentially\u4f9d\u6b21.The convolutional middle layers aggregate voxel-wise features within a progressively\u9010\u6b65 expanding\u6269\u5927\u7684 receptive\u63a5\u6536 field\uff0cadding more context to the shape description.The detialed sizes of the filters in the convolutional middle layers are explained in Section 3.",
            "title": "Convolutional Middle Layers"
        },
        {
            "location": "/voxelnet/#region-proposal-network",
            "text": "\\\nRecently, region proposal\u63d0\u6848 networks have become an important building block of top-performing object detection frameworks. In this work,we make servel key modifications to the RPN architecture proposed in [34], and combine it with the feature learning network and convolutional middle layers to form an end-to-end trainable pipeline\u7ba1\u9053.  The input to our RPN is the feature map provided by the convolutional middle layers.The architecture of this network is illustrate in Figure 4.The network has three blocks of fully convolutional layers.The first layer of each block downsamples the feature map by half via a convolution with a stride size of 2,followed by a sequence of convolutions of stride 1(xq means q applications of the filter).After each convolution layer,BN and ReLU operations are applied.We then upsample the output of every block to a fixed size and concatanate to construct the high resolution feature map. Finally, this feature map is mapped to the desired learning targets:(1) a probability score map and (2) a regression map.\uff32\uff30\uff2e\u7684\u8f93\u5165\u662f\u5377\u79ef\u4e2d\u95f4\u5c42\u63d0\u4f9b\u7684\u7279\u5f81\u56fe\u3002\u8be5\u7f51\u7edc\u7684\u4f53\u7cfb\u7ed3\u6784\u5982\u56fe\uff14\u6240\u793a\u3002\u8be5\u7f51\u7edc\u5177\u6709\u4e09\u4e2a\u5b8c\u5168\u5377\u57fa\u5c42\u7684\u5757\u3002\u6bcf\u4e2a\u5757\u7684\u7b2c\u4e00\u5c42\u901a\u8fc7\u6b65\u5e45\u4e3a\uff12\u7684\u5377\u79ef\u5bf9\u7279\u5f81\u56fe\u8fdb\u884c\u4e00\u534a\u4e0b\u91c7\u6837\uff0c\u7136\u540e\u662f\u6b65\u5e45\uff11\u7684\u5377\u79ef\u5e8f\u5217\uff08xq \u8868\u793a\u6ee4\u6ce2\u5668\u7684\uff51\u4e2a\u5e94\u7528\uff09\u3002\u5728\u6bcf\u4e2a\u5377\u57fa\u5c42\u4e4b\u540e\uff0cBN \u548cReLU \u64cd\u4f5c\u88ab\u5e94\u7528\u3002\u7136\u540e\uff0c\u6211\u4eec\u5c06\u6bcf\u4e2a\u5757\u7684\u8f93\u51fa\u4e0a\u91c7\u6837\u5230\u56fa\u5b9a\u5927\u5c0f\uff0c\u5e76\u6c47\u603b\u4ee5\u6784\u5efa\u9ad8\u5206\u8fa8\u7387\u7279\u5f81\u56fe\u3002\u6700\u540e\uff0c\u5c06\u6b64\u7279\u5f81\u56fe\u6620\u5c04\u5230\u6240\u9700\u7684\u5b66\u4e60\u76ee\u6807\uff1a\uff08\uff11\uff09\u6982\u7387\u5206\u6570\u56fe\u548c\uff08\uff12\uff09\u56de\u5f52\u56fe\u3002",
            "title": "Region Proposal Network"
        },
        {
            "location": "/voxelnet/#loss-function",
            "text": "Let   be the set of   positive anchors\u951a\u70b9 and   be the set of   negative anchors.\nWe parameterize a 3D ground truth box as  ,where   represent the center location,  are length ,width,height of the box, and   is the yaw rotation around Z-axis. To retrieve\u627e\u56de the ground truth box from a matching positive anchorparameterized as  ,we define the residual vector\u6b8b\u5dee\u5411\u91cf   containing the 7 regression targets corresponding to center location \u25b3x,\u25b3y,\u25b3z,three dimensions \u25b3l, \u25b3w, \u25b3h, and the rotation \u25b3\u0398, which are computed as :\\ \\\nwhere   is the diagonal\u5bf9\u89d2\u7ebf of the base of the anchor box.Here, we aim to directly estimate the oriented\u5b9a\u5411\u7684 3D box and normallize \u0394x and \u0394y homogeneously\u5747\u5300\u7684 with the diagonal\u5bf9\u89d2\u7ebf  ,We define the loss function as follows:\\ \\\nwhere   and   represent the softmax output for positive anchor   and negative anchor   respectively,while   and   are the regression\u56de\u5f52 output and ground truth for positive anchor  .The first two terms are the normalized classification loss for   and  , where the   stands for binary cross entropy loss\u4e8c\u5143\u4ea4\u53c9\u71b5\u635f\u5931 and \u03b1\uff0c\u03b2 are postive constants balancing the relative importance. The last term   is the regression loss, where we use the SmoothL1 function.",
            "title": "Loss Function"
        },
        {
            "location": "/voxelnet/#efficient-implementation",
            "text": "Gpus are optimized\u4f18\u5316 for processing dense\u7a20\u5bc6 tensor structures\uff0eThe problem with working directly with the point cloud is that the points are sparsely distributed across sapce and each voxel has a variable number of points.We devised\u8bbe\u8ba1\u7684 \na method that converts the point cloud into a dense tensor structure where stacked VFE operations can be processed in parallel across points and voxels.\\ \\\nThe method is summarized in Figure 5. We initialize a KxTx7 dimensional tensor structure to store the voxel input feature where K is the maximum number of non-empty voxels, T is the maximum number of points per voxel, and 7 is the input encoding dimension for each point.The points are randomized\u968f\u673a\u7684 before processing.For each point in pointcloud, we check if the corresponding voxel already exists. This lookup operation is done efficiently in O(1) using a hash table where the voxel coordinate is used as the hash key.If the voxel is already initialized we insert the point to voxel location if there are less than T points, otherwise the point is ignored.If the voxel is not initialized, we initialize a new voxel, store its coordinate in the voxel coordinate buffer, and insert the point to this voxel location.The voxel input feature and coordinate buffers can be constructed\u5efa via a signle pass over the point list, therefore its complexity is O(n).\u4f53\u7d20\u8f93\u5165\u7279\u5f81\u548c\u5750\u6807\u7f13\u51b2\u533a\u53ef\u4ee5\u901a\u8fc7\u5bf9\u70b9\u5217\u8868\u7684\u4e00\u6b21\u904d\u5386\u6765\u6784\u9020\uff0c\u56e0\u6b64\u5176\u590d\u6742\u5ea6\u4e3aO\uff08n\uff09\u3002To further improve the memory/compute efficiency it is possible to only store a limited number of voxels(K) and ignore points coming from voxels with few points.  After the voxel input buffer is constructed,the stacked VFE only involves\u6d89\u53ca point level and voxel level dense operations which can be computed on a GPU in parallel.Note that, after concatenation\u7ea7\u8054 operations in VFE, we reset the features corresponding to empty points to zero such that they do not affect the computed voxel features.Finally, using the stored coordinate buffer we reorganize\u6539\u7ec4 the computed sparse voxel-wise structures to the dense voxel grid.\u6700\u540e\uff0c\u4f7f\u7528\u5b58\u50a8\u7684\u5750\u6807\u7f13\u51b2\u533a\uff0c\u6211\u4eec\u5c06\u8ba1\u7b97\u7684\u7a00\u758f\u4f53\u7d20\u7ed3\u6784\u91cd\u7ec4\u4e3a\u5bc6\u96c6\u4f53\u7d20\u7f51\u683c.The following convolutional middle layers and RPN operations work on a dense voxel grid which can be efficiently implemented on a GPU.",
            "title": "Efficient Implementation"
        },
        {
            "location": "/voxelnet/#training-details",
            "text": "",
            "title": "Training Details"
        },
        {
            "location": "/voxelnet/#network-details",
            "text": "Our experimental setup\u5b9e\u9a8c\u8bbe\u7f6e is based on the LiDAR specifications of the KITTI dataset.\\ Car Detection  For this task,we consider point clouds within the range of [-3,1]x[-40,40]x[0,70.4]meters along Z,Y,X axis respectively.Points taht are Points that are projected outside of image boundaries are removed.We choose a voxel size of   meters,which leads to D'=10,H'=400,W'=352. We set T = 35 as the maximum number of randomly sampled points in each non-empty voxel.We use two VFE layers VFE-1(7,32) and VFE-2(32,128).The final FCN maps VFE-2 output to  .Thus our feature learning net generate a sparse tensor of shape 128x10x400x352.To aggregate voxel-wise features, we employ three convolution middle layers sequentially as Conv3D(128,64,3,(2,1,1),(1,1,1)),Conv3D(64,64,3,(1,1,1),(0,1,1)),and Conv3D(64,64,3,(2,1,1),(1,1,1)), which yields a 4D tensor of size 64x2x400x352.After reshaping, the input to RPN is a feature map of size 128x400x352,where the dimensions correspond to channel,height, and width of the 3D tensor.Figure 4 illustrates the detailed network architecture for this task.Unlike, we use only one anchor size,  meters, centered at\u96c6\u4e2d\u4e8e   meters with two rotations, 0 and 90 degrees.Our anchor matching criteria\u5339\u914d\u6807\u51c6 as follows: An anchor is considered as positive if it has the highest Intersection over Union(IoU) with a ground truth or its IoU with ground truth is above 0.6(in bird's eye view).An anchor is considered as negative if the IoU between it and all ground true boxes is less than 0.45. We treat anchors as don't care if they have 0.45\u2264IoU\u22640.6 with any ground truth.We set \u03b1\uff1d1.5 and \u03b2=1 in Eqn.2.  Pedestrain and Cyclist Detection  The input range is [-3,1]x[-20,20]x[0,48] meters along Z,Y,X axis respectively.We use the same voxel size as for car detection,which yields D=10, H=200, W=240.We set T=45 in order to obtain more LiDAR points for better capturing shape information.The feature learning network and convolutional middle layers ate identical\u76f8\u540c to the networks used in car detection task.For the RPN, we make one modification to block 1 in Figure 4 by changing the stride size in the first 2D convolution from 2 to 1. This allows finer resolution in anchor matching, which is necessary for detecting pedestrains and cyclists.We use anchor size     meters centered at   with 0 and 90 degrees rotation for pedestrain detection and use anchor size   meters centered at   with 0 and 90 degrees rotation for cyclist detecction.The specific anchor matching criteria is as follows:We assign an anchor as positive if it has the highest IoU with a ground truth, or its IoU with ground truth is above 0.5. An anchor is considered as negative if its IoU with every ground truth is less than 0.35.For anchors having 0.35\u2264IoU\u22640.5 with any ground truth,we treat them as don't care.  During training, we use stochastic\u968f\u673a gradient descent(SGD) with learning rate 0.01 for the first 150 epochs and decrease the learning rate to 0.001 for the last 10 epochs.We use a batchsize of 16 point clouds.",
            "title": "Network Details"
        },
        {
            "location": "/voxelnet/#data-augmentation",
            "text": "\uff37ith less than 4000 training point clouds, training our network from scratch will inevitably \u4e0d\u53ef\u907f\u514d suffer from overfitting. To reduce this issue, we introduce three different forms of data augmentation. The augmented training data are generated on-the-fly\u5373\u65f6\u3000without the need to be stored on disk.  Define set   as the whole point cloud, consisting of N points.We parameterize a 3D bounding box  ,where   are center locations, l,w,h are length, width, height, and \u03b8 is the yaw rotation around Z-axis.We define     as the set containing all LiDAR points within  , where p=[x,y,z,r] denotes a particular LiDAR point in the whole set M.  The first form of data augmentation applies perturbation\u6444\u52a8 independently to each ground truth 3D bounding box together with those LiDAR points within the box.Specifically,around Z-axis we rotate   and the associated   with respect to ( ) by a uniformally\u7edf\u4e00\u7684 distributed random variable  . Then we add a translation (\u0394x,\u0394y,\u0394z) to the XYZ components of   and to each point in  ,where \u0394x,\u0394y,\u0394z are drawn independently from a Gaussian distribution with mean zero and standard deviation 1.0. To avoid physically impossible outcomes, we perform a collision\u78b0\u649e test between any two boxes after the perturbation and revert\u8fd8\u539f to orignal if a collision is detected.Since the perturbation is applied to each ground truth box and the associated LiDAR points independently, the network is able to learn from substantially\u5b9e\u8d28\u4e0a more variations than from the orignal training data.  Secondly,we apply global scaling to all ground truth boxes   and to whole point cloud M. Specifically, we multiply the XYZ coordinates and the three dimensions of each  , and the XYZ coordinates of all points in M with a random variable drawn from uniform distribution [0.95,1.05]. Introducing global scale augementation improves robustness of the network for detecting objects with various sizes and distances as shown in image-based classification and detection tasks.  Finally, we apply global rotation to all ground truth boxes   and to the whole point cloud M. The rotation is applied along Z-axis and around(0,0,0). The global rotation offset is determined by sampling from uniform distribution [-\u03c0/4,\u03c0/4].By rotating the entire point cloud, we simulate the vehicle making a turn.",
            "title": "Data Augmentation \u6570\u636e\u6269\u5c55"
        },
        {
            "location": "/\u6444\u50cf\u673a\u548c\u6beb\u7c73\u6ce2\u5750\u6807\u7cfb\u5173\u7cfb/",
            "text": "\u6444\u50cf\u673a\u5750\u6807\u7cfb\n\n\n\n\u6444\u50cf\u673a\u9ed8\u8ba4\u7684\u662f\u4e0e\u4e16\u754c\u5750\u6807\u7cfb\u5bf9\u9f50\u7684\u4f46\u662f\u76f8\u673a\u7684z\u8f74\u65b9\u5411\u6307\u5411\u76f8\u673a\u5185\u90e8\u3002\n\n\n\u6beb\u7c73\u6ce2\u5750\u6807\u7cfb\n\n\n\n\n\u7ea2\u8272\u4e3ax\u8f74\uff0c\u7eff\u8272\u4e3ay\u8f74\uff0c\u84dd\u8272\u4e3az\u8f74\n\n\n\u7ed5x\u8f74\u7684\u65cb\u8f6c\u77e9\u9635\n\n\n\n\n\nR_x(\\theta)=\\begin{bmatrix}\n    1&0&0\\\\\n    0&cos\\theta&-sin\\theta\\\\\n    0&sin\\theta&cos\\theta\n\\end{bmatrix}\n\n\n\n\n\n\u7ed5y\u8f74\u7684\u65cb\u8f6c\u77e9\u9635\n\n\n\n\n\nR_y(\\theta)=\\begin{bmatrix}\n    cos\\theta&0&sin\\theta\\\\\n    0&1&0\\\\\n    -sin\\theta&0&cos\\theta\n\\end{bmatrix}\n\n\n\n\n\n\u7ed5z\u8f74\u65cb\u8f6c\u77e9\u9635\n\n\n\n\n\nR_z(\\theta)=\\begin{bmatrix}\n    cos\\theta&-sin\\theta&0\\\\\n    sin\\theta&cos\\theta&0\\\\\n    0&0&1\n\\end{bmatrix}\n\n\n\n\n\n\u901a\u7528\u65cb\u8f6c\u77e9\u9635\n\n\n\n\n\nR=\nR_z(\\alpha)R_y(\\beta)R_z(\\gamma)\n=\n\\begin{bmatrix}\n    cos\\alpha cos\\beta&cos\\alpha sin\\beta sin\\gamma -sin\\alpha cos\\gamma&cos\\alpha sin\\beta cos\\gamma+sin\\alpha sin\\gamma\\\\\n    sin\\alpha cos\\beta&sin\\alpha sin\\beta sin\\gamma + cos\\alpha cos\\gamma&sin\\alpha sin\\beta cos\\gamma -cos\\alpha sin\\gamma\\\\\n    -sin\\beta&cos\\beta sin\\gamma&cos\\beta cos\\gamma\n    \\end{bmatrix}\n\n\n\n\n\n\u53d8\u6362\u77e9\u9635\n\n\n\n\n\nT=\\begin{bmatrix}\n    cos\\alpha cos\\beta&cos\\alpha sin\\beta sin\\gamma -sin\\alpha cos\\gamma&cos\\alpha sin\\beta cos\\gamma+sin\\alpha sin\\gamma&x\\\\\n    sin\\alpha cos\\beta&sin\\alpha sin\\beta sin\\gamma + cos\\alpha cos\\gamma&sin\\alpha sin\\beta cos\\gamma -cos\\alpha sin\\gamma&y\\\\\n    -sin\\beta&cos\\beta sin\\gamma&cos\\beta cos\\gamma&z\\\\\n    0&0&0&1\n    \\end{bmatrix}\n\n\n\n\n\n/**\n * x :\u6beb\u7c73\u6ce2\u6570\u636e\u7684x\u5750\u6807\u503c\n * y :\u6beb\u7c73\u6ce2\u6570\u636e\u7684y\u5750\u6807\u503c\n * radar_x :\u6beb\u7c73\u6ce2\u76f8\u5bf9\u4e8e\u76f8\u673a\u5750\u6807\u7cfbx\u8f74\u7684\u5750\u6807\u4f4d\u7f6e\n * radar_y \uff1a\u6beb\u7c73\u6ce2\u76f8\u5bf9\u4e8e\u76f8\u673a\u5750\u6807\u7cfby\u8f74\u7684\u5750\u6807\u4f4d\u7f6e\n * radar_z \uff1a\u6beb\u7c73\u6ce2\u76f8\u5bf9\u4e8e\u76f8\u673a\u5750\u6807\u7cfbz\u8f74\u7684\u5750\u6807\u4f4d\u7f6e\n * a : \u6beb\u7c73\u6ce2\u5bf9\u4e8e\u76f8\u673a\u7ed5x\u8f74\u65cb\u8f6c\u89d2\u5ea6,\u9006\u65f6\u9488\u4e3a\u6b63\n * b : \u6beb\u7c73\u6ce2\u5bf9\u4e8e\u76f8\u673a\u7ed5y\u8f74\u65cb\u8f6c\u89d2\u5ea6,\u9006\u65f6\u9488\u4e3a\u6b63\n * c : \u6beb\u7c73\u6ce2\u76f8\u5bf9\u4e8e\u76f8\u673a\u7ed5z\u8f74\u65cb\u8f6c\u89d2\u5ea6,\u9006\u65f6\u9488\u4e3a\u6b63\n\n**/\nEigen::Vector3d radar2camera(double x, double y, double radar_x=0.,double radar_y=0., double radar_z=0.,double a=0.,double b=0., double c=0.)\n{\n    Eigen::Isometry3d T=Eigen::Isometry3d::Identity();\n    T.pretranslate ( Eigen::Vector3d (radar_x,radar_y,radar_z ) );\n    Eigen::AngleAxisd rotation_vector_x (a, Eigen::Vector3d ( 1,0,0 ));  \n    Eigen::AngleAxisd rotation_vector_y (b, Eigen::Vector3d ( 0,1,0 ));\n    Eigen::AngleAxisd rotation_vector_z (c, Eigen::Vector3d ( 0,0,1 ));\n    T.rotate(rotation_vector_x);\n    T.rotate(rotation_vector_y);\n    T.rotate(rotation_vector_z);\n    Eigen::Vector3d vtmp(x,y,0.0);\n    vtmp = T*vtmp;\n    return vtmp;\n}",
            "title": "\u6444\u50cf\u673a\u548c\u6beb\u7c73\u6ce2\u5750\u6807\u7cfb\u5173\u7cfb"
        },
        {
            "location": "/\u6444\u50cf\u673a\u548c\u6beb\u7c73\u6ce2\u5750\u6807\u7cfb\u5173\u7cfb/#_1",
            "text": "\u6444\u50cf\u673a\u9ed8\u8ba4\u7684\u662f\u4e0e\u4e16\u754c\u5750\u6807\u7cfb\u5bf9\u9f50\u7684\u4f46\u662f\u76f8\u673a\u7684z\u8f74\u65b9\u5411\u6307\u5411\u76f8\u673a\u5185\u90e8\u3002",
            "title": "\u6444\u50cf\u673a\u5750\u6807\u7cfb"
        },
        {
            "location": "/\u6444\u50cf\u673a\u548c\u6beb\u7c73\u6ce2\u5750\u6807\u7cfb\u5173\u7cfb/#_2",
            "text": "\u7ea2\u8272\u4e3ax\u8f74\uff0c\u7eff\u8272\u4e3ay\u8f74\uff0c\u84dd\u8272\u4e3az\u8f74  \u7ed5x\u8f74\u7684\u65cb\u8f6c\u77e9\u9635   \nR_x(\\theta)=\\begin{bmatrix}\n    1&0&0\\\\\n    0&cos\\theta&-sin\\theta\\\\\n    0&sin\\theta&cos\\theta\n\\end{bmatrix}   \u7ed5y\u8f74\u7684\u65cb\u8f6c\u77e9\u9635   \nR_y(\\theta)=\\begin{bmatrix}\n    cos\\theta&0&sin\\theta\\\\\n    0&1&0\\\\\n    -sin\\theta&0&cos\\theta\n\\end{bmatrix}   \u7ed5z\u8f74\u65cb\u8f6c\u77e9\u9635   \nR_z(\\theta)=\\begin{bmatrix}\n    cos\\theta&-sin\\theta&0\\\\\n    sin\\theta&cos\\theta&0\\\\\n    0&0&1\n\\end{bmatrix}   \u901a\u7528\u65cb\u8f6c\u77e9\u9635   \nR=\nR_z(\\alpha)R_y(\\beta)R_z(\\gamma)\n=\n\\begin{bmatrix}\n    cos\\alpha cos\\beta&cos\\alpha sin\\beta sin\\gamma -sin\\alpha cos\\gamma&cos\\alpha sin\\beta cos\\gamma+sin\\alpha sin\\gamma\\\\\n    sin\\alpha cos\\beta&sin\\alpha sin\\beta sin\\gamma + cos\\alpha cos\\gamma&sin\\alpha sin\\beta cos\\gamma -cos\\alpha sin\\gamma\\\\\n    -sin\\beta&cos\\beta sin\\gamma&cos\\beta cos\\gamma\n    \\end{bmatrix}   \u53d8\u6362\u77e9\u9635   \nT=\\begin{bmatrix}\n    cos\\alpha cos\\beta&cos\\alpha sin\\beta sin\\gamma -sin\\alpha cos\\gamma&cos\\alpha sin\\beta cos\\gamma+sin\\alpha sin\\gamma&x\\\\\n    sin\\alpha cos\\beta&sin\\alpha sin\\beta sin\\gamma + cos\\alpha cos\\gamma&sin\\alpha sin\\beta cos\\gamma -cos\\alpha sin\\gamma&y\\\\\n    -sin\\beta&cos\\beta sin\\gamma&cos\\beta cos\\gamma&z\\\\\n    0&0&0&1\n    \\end{bmatrix}   /**\n * x :\u6beb\u7c73\u6ce2\u6570\u636e\u7684x\u5750\u6807\u503c\n * y :\u6beb\u7c73\u6ce2\u6570\u636e\u7684y\u5750\u6807\u503c\n * radar_x :\u6beb\u7c73\u6ce2\u76f8\u5bf9\u4e8e\u76f8\u673a\u5750\u6807\u7cfbx\u8f74\u7684\u5750\u6807\u4f4d\u7f6e\n * radar_y \uff1a\u6beb\u7c73\u6ce2\u76f8\u5bf9\u4e8e\u76f8\u673a\u5750\u6807\u7cfby\u8f74\u7684\u5750\u6807\u4f4d\u7f6e\n * radar_z \uff1a\u6beb\u7c73\u6ce2\u76f8\u5bf9\u4e8e\u76f8\u673a\u5750\u6807\u7cfbz\u8f74\u7684\u5750\u6807\u4f4d\u7f6e\n * a : \u6beb\u7c73\u6ce2\u5bf9\u4e8e\u76f8\u673a\u7ed5x\u8f74\u65cb\u8f6c\u89d2\u5ea6,\u9006\u65f6\u9488\u4e3a\u6b63\n * b : \u6beb\u7c73\u6ce2\u5bf9\u4e8e\u76f8\u673a\u7ed5y\u8f74\u65cb\u8f6c\u89d2\u5ea6,\u9006\u65f6\u9488\u4e3a\u6b63\n * c : \u6beb\u7c73\u6ce2\u76f8\u5bf9\u4e8e\u76f8\u673a\u7ed5z\u8f74\u65cb\u8f6c\u89d2\u5ea6,\u9006\u65f6\u9488\u4e3a\u6b63\n\n**/\nEigen::Vector3d radar2camera(double x, double y, double radar_x=0.,double radar_y=0., double radar_z=0.,double a=0.,double b=0., double c=0.)\n{\n    Eigen::Isometry3d T=Eigen::Isometry3d::Identity();\n    T.pretranslate ( Eigen::Vector3d (radar_x,radar_y,radar_z ) );\n    Eigen::AngleAxisd rotation_vector_x (a, Eigen::Vector3d ( 1,0,0 ));  \n    Eigen::AngleAxisd rotation_vector_y (b, Eigen::Vector3d ( 0,1,0 ));\n    Eigen::AngleAxisd rotation_vector_z (c, Eigen::Vector3d ( 0,0,1 ));\n    T.rotate(rotation_vector_x);\n    T.rotate(rotation_vector_y);\n    T.rotate(rotation_vector_z);\n    Eigen::Vector3d vtmp(x,y,0.0);\n    vtmp = T*vtmp;\n    return vtmp;\n}",
            "title": "\u6beb\u7c73\u6ce2\u5750\u6807\u7cfb"
        },
        {
            "location": "/\u6beb\u7c73\u6ce2\u548c\u6444\u50cf\u673a\u878d\u5408/",
            "text": "\u73af\u5883\u8981\u6c42\n\n\n\n\nUbuntu 18.04\n\n\nCuda 10.0\n\n\nROS melodic\n\n\nQt5\n\n\n\n\n\u8f6f\u4ef6\u73af\u5883\u90e8\u7f72\n\n\nQT\u5b89\u88c5\n\n\n\n\nsudo apt-get install qttools5-dev-tools libqt5svg5-dev qtmultimedia5-dev\n\n\n\n\nROS melodic \u5b89\u88c5\n\n\nROS\u5b98\u7f51\n\n\nCuda10.0 \uff0ccudnn\u5b89\u88c5\n\n\n\u5b89\u88c5\u6307\u5357\n\n\n\u628a\u7a0b\u5e8f\u5305\u91cc\u7684\nradar\n\u6587\u4ef6\u5939\u62f7\u8d1d\u5230\u672c\u5730\n\n\n\n\n\u5728\u6b64\u76ee\u5f55\u4e0b\uff0c\u6267\u884c\ncatkin_make\n \u6307\u4ee4\n\n\n\n\n\u7f16\u8bd1\u5b8c\u6210\u540e\uff0c\u6307\u4ee4\u8f93\u5165\n\n\nsudo ip link set can0 up type can bitrate 500000\nroslaunch ars_40X ars_40X.launch\n\n\n\n\u5728\u8bbe\u5907\u94fe\u63a5\u6b63\u5e38\u7684\u60c5\u51b5\u4e0b\u5c31\u53ef\u770b\u5230\u8f93\u51fa\n\n\n\n\nARS404\u7684\u5e94\u7528\n\n\ncansend can0 200#F8000000089C0000 // Objects detection with all extended properties\ncansend can0 200#F8000000109C0000 // Clusters detection with all extended properties\n\n\n\n\u8fd9\u4e24\u6761\u6307\u4ee4\u5206\u522b\u542f\u52a8radar\u7684 Cluster\u6a21\u5f0f\u548cObject\u6a21\u5f0f\uff0c\u6211\u4eec\u4f7f\u7528Object\u6a21\u5f0f\uff0c\uff08cluster\u6a21\u5f0f\uff09\n\n\n\n\n\u6beb\u7c73\u6ce2\u96f7\u8fbe\u6570\u636e\u662f\u4e8c\u7ef4\u6570\u636e\uff0c\u53ea\u6709x, y \u5750\u6807\u4fe1\u606f\uff0c \u663e\u793a\u5982\u4e0a\u56fe\n\n\n\u6beb\u7c73\u6ce2\u96f7\u8fbe\u6570\u636e\u8bfb\u53d6\uff0c\u6211\u4eec\u4f7f\u7528\nars_40X\n\u7a0b\u5e8f\n\n\n\u6570\u636e\u663e\u793a\u548c\u56fe\u50cf\u663e\u793a\uff0c\u6211\u4eec\u4f7f\u7528\nros2qt\n \u7a0b\u5e8f\n\n\n\u56fe\u50cf\u7269\u4f53\u8bc6\u522b\uff0c\u6211\u4eec\u4f7f\u7528\nvision_darknet_detect\n \u7a0b\u5e8f\n\n\n\u6beb\u7c73\u6ce2\u548c\u56fe\u50cf\u7684\u878d\u5408\n\n\n\n\n\u9996\u5148\u5bf9\u6444\u50cf\u5934\u8fdb\u884c\u6807\u5b9a\n\n\n\u83b7\u53d6\u5185\u53c2\u77e9\u9635\n\n\n\u901a\u8fc7\u5185\u53c2\u77e9\u9635\u628a\u6beb\u7c73\u6ce2\u6570\u636e\u6620\u5c04\u5230\u56fe\u50cf\u4e0a\n\n\n\u901a\u8fc7yolo\u8bc6\u522b\uff0c\u5e76\u63d0\u53d6\u51fa\u7269\u4f53\u8ddd\u79bb\uff0c\u5982\u4e0b\u56fe\n\n\n\n\n\n\n\u6444\u50cf\u5934\u6807\u5b9a\n\n\n\u6444\u50cf\u5934\u6807\u5b9a\u53ef\u4ee5\u4f7f\u7528opencv\u63d0\u4f9b\u7684\u6807\u5b9a\u65b9\u6cd5\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528matlab\u63d0\u4f9b\u7684\u6444\u50cf\u5934\u6807\u5b9a\u63d2\u4ef6\u3002\n\n\n\u6211\u4eec\u4ecb\u7ecd\u4e0bmatlab\u6444\u50cf\u5934\u6807\u5b9a\u65b9\u6cd5\u3002\u4e3a\u4e86\u65b9\u4fbf\u6807\u5b9a\uff0c\u6211\u4eec\u5236\u4f5c\u4e86\u4e24\u4e2a\u201d\u6807\u5b9a\u76d2\u5b50\u201d\n\n\n\n\n\n\n\u901a\u8fc7\u8be5\u6444\u50cf\u5934\u4f9d\u6b21\u91c7\u96c6\u5982\u4e0b\u4f4d\u7f6e\u56fe\u7247\u4f4d\u4e8eimage\u6587\u4ef6\u5939\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u5728matlab \u5e94\u7528\u7a0b\u5e8f\u4e0b\u627e\u5230Camera Calibration\u5de5\u5177\u7bb1\n\n\n\n\n\u52a0\u8f7d\u5f85\u6807\u5b9a\u7684\u56fe\u50cf\n\n\n\n\n\u586b\u5199\u68cb\u76d8\u683c\u6bcf\u4e2a\u683c\u5b50\u8fb9\u957f\u7684\u771f\u5b9e\u503c\n\n\n\n\n\u53ef\u4ee5\u9884\u89c8\u6210\u529f\u68c0\u6d4b\u51fa\u68cb\u76d8\u683c\u7684\u56fe\u50cf\uff0c\u7136\u540e\u5f00\u59cb\u6807\u5b9a\uff0c\u70b9\u51fbCalibrate\n\n\n\n\n\u5e73\u5747\u8bef\u5dee\u5c0f\u4e8e0.5\u5373\u53ef\n\n\n\n\n\u5bfc\u51fa\u76f8\u673a\u6807\u5b9a\u53c2\u6570\n\n\n\n\n\u6211\u4eec\u53ea\u9700\u8981\u7528\u5230 \nIntrinsicMatrix\n\n\ncv::Point World2Image(Eigen::Vector3d Pw)\n{\n  static Eigen::Matrix3d intric = (Eigen::Matrix3d() << 465.2203, 0, 351.1325, 0, 463.8023, 256.4198, 0, 0, 1).finished();\n  static Eigen::Matrix3d mi = (Eigen::Matrix3d() << 1, 0, 0, 0, -1,0, 0, 0, 1).finished();\n  Pw = mi*Pw;\n  Eigen::VectorXd result(2);\n  result = intric*Pw/Pw.z();\n  cv::Point P (result(0),result(1));\n  return P;\n}\n\n\n\n\u901a\u8fc7world2image\u51fd\u6570\u628a\u6beb\u7c73\u6ce2\u5750\u6807\u7cfb\u8f6c\u6362\u5230\u56fe\u50cf\u5750\u6807\u7cfb\n\n\n\u8fd9\u6837\u5728\u56fe\u50cf\u4e2d\uff0c\u6211\u4eec\u5c31\u6709\u4e86\u6df1\u5ea6\u4fe1\u606f\u3002\n\n\n\u7136\u540e\u518d\u901a\u8fc7yolo\u7684darknet\u8fdb\u884c\u7269\u4f53\u8bc6\u522b\uff0c\u5c31\u53ef\u5f97\u5230\uff0c\u969c\u788d\u7269\u7684\u7c7b\u522b\u548c\u8ddd\u79bb\u4fe1\u606f\u3002\n\n\n\u8fd9\u91cc\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u6444\u50cf\u5934\u548c\u6beb\u7c73\u6ce2\u5b89\u88c5\u4f4d\u7f6e\u5173\u7cfb\uff0c\u7531\u4e8e\u6beb\u7c73\u6ce2\u53ea\u6709x\uff0cy\u4fe1\u606f\uff0c\u6ca1\u6709\u9ad8\u5ea6\u4fe1\u606f\uff0c\u6240\u4ee5\u6211\u4eec\u5c3d\u91cf\u4fdd\u6301\u6444\u50cf\u5934\u548c\u6beb\u7c73\u6ce2\u7ad6\u76f4\u65b9\u5411\u4e0a\u4e2d\u5fc3\u7ebf\u91cd\u5408\u3002\n\n\n\u5907\u6ce8\uff0c\u5982\u679c\u60f3\u8981\u66f4\u6539\u6444\u50cf\u5934\u548c\u6beb\u7c73\u6ce2\u7684\u76f8\u5bf9\u4f4d\u7f6e\uff0c\u53ea\u9700\u901a\u8fc7\u521a\u6027\u53d8\u6362\u77e9\u9635\uff0c\u8fdb\u884c\u5750\u6807\u7cfb\u8f6c\u6362\n\n\n$\n\\begin{bmatrix}cos(\\theta)&-sin(\\theta)&0.0&x\\\\sin(\\theta)&cos(\\theta)&0.0&y\\\\0.0&0.0&1.0&z\\\\0.0&0.0&0.0&1.0\\end{bmatrix}\n$\n\n\n\u53d8\u6362\u77e9\u9635\u91cc$\n\\begin{bmatrix}cos(\\theta)&-sin(\\theta)&0.0\\\\sin(\\theta)&cos(\\theta)&0.0\\\\0.0&0.0&1.0\\end{bmatrix}\n$\u662f3x3\u65cb\u8f6c\u77e9\u9635\uff0cxyz\u662f\u5e73\u79fb\u77e9\u9635\u3002\n\n\n\u53ea\u9700\u66f4\u6539\u5bf9\u5e94\u7684\u65cb\u8f6c\u5e73\u79fb\u53c2\u6570\uff0c\u5373\u53ef\u8fdb\u884c\u5750\u6807\u7cfb\u8f6c\u6362\u3002\n\n\ndarknet\n\n\nYolo\u7269\u4f53\u68c0\u6d4b\n\n\n\u6211\u4eec\u5728\u7a0b\u5e8f\u91cc\u4f7f\u7528\u4e86yolo v3\u8fdb\u884c\u7269\u4f53\u8bc6\u522b\n\n\n\n\n\u5982\u4f55\u63d0\u53d6\u51fa\u8ddd\u79bb\u4fe1\u606f\n\n\n\u6211\u4eec\u5df2\u7ecf\u628a\u6beb\u7c73\u6ce2\u7684\u8ddd\u79bb\u4fe1\u606f\u6295\u5f71\u5230\u4e86\u56fe\u7247\u4e0a\uff0c\u540c\u65f6\uff0c\u6211\u4eec\u7528yolo\u5bf9\u969c\u788d\u7269\u8fdb\u884c\u4e86\u8bc6\u522b\u5206\u7c7b\uff0c\u753b\u51fa\u4e86\u8fb9\u754c\u6846\u3002\n\n\n\u6211\u4eec\u53ea\u9700\u8981\u628a\u8fb9\u754c\u6846\u5185\u90e8\u7684\u8ddd\u79bb\u4fe1\u606f\u63d0\u53d6\u51fa\u6765\u5373\u53ef\u3002\u7531\u4e8e\u6beb\u7c73\u6ce2\u6709\u8bef\u5dee\uff0c\u6240\u4ee5\u6211\u4eec\u9700\u8981\u5bf9\u8fd9\u4e9b\u8fb9\u754c\u6846\u91cc\u7684\u4fe1\u606f\u8fdb\u884c\u7b5b\u9009\u3002\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u6444\u50cf\u5934\u53ef\u4ee5\u76f4\u63a5\u68c0\u6d4b\u5230\u7684\u7269\u4f53\uff0c\u8bf4\u660e\u6b64\u7269\u4f53\u524d\u9762\u6ca1\u6709\u969c\u788d\u7269\uff0c\u56e0\u4e3a\u5149\u662f\u76f4\u7ebf\u4f20\u64ad\u7684\u3002\u4e5f\u5c31\u662f\u8bf4\u5982\u679c\u969c\u788d\u7269\u524d\u65b9\u8fd8\u6709\u6709\u969c\u788d\u7269\uff0c\u6444\u50cf\u5934\u5f88\u5927\u53ef\u80fd\u5c31\u770b\u4e0d\u5230\u540e\u9762\u90a3\u4e00\u4e2a\uff0c\u4f46\u662f\u6beb\u7c73\u6ce2\u53ef\u80fd\u901a\u8fc7\u6f2b\u53cd\u5c04\u770b\u5230\u4e86\uff0c\u4ea7\u751f\u591a\u4e2a\u8ddd\u79bb\u4fe1\u606f\u3002\u6240\u4ee5\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u63d0\u53d6\u8fb9\u754c\u6846\u91cc\u7684\u6700\u5c0f\u8ddd\u79bb\u5f53\u505a\u8bc6\u522b\u969c\u788d\u7269\u7684\u8ddd\u79bb\u3002\u8fd9\u540c\u6837\u662f\u51fa\u4e8e\u5b89\u5168\u8003\u8651\uff0c\u9700\u8981\u9009\u62e9\u8ddd\u79bb\u6700\u8fd1\u7684\u8ddd\u79bb\u3002\n\n\n\u6211\u4eec\u53ef\u4ee5\u6839\u636e\u6027\u80fd\u8981\u6c42\uff0c\u63d0\u51fa\u66f4\u597d\u7684\u63d0\u53d6\u6df1\u5ea6\u4fe1\u606f\u65b9\u6cd5\u3002\n\n\n\n\n\u56fe\u4e2d\u7070\u8272\u7684\u70b9\u662f\u6beb\u7c73\u6ce2\u6295\u5c04\u5230\u56fe\u50cf\u4e0a\u7684\u70b9\uff0c\u9ec4\u8272\u7684\u6846\u662fyolo\u8bc6\u522b\u7684\u8fb9\u754c\u6846\n\n\n\u6574\u4e2a\u7b97\u6cd5\u7684\u5177\u4f53\u5b9e\u73b0\u903b\u8f91\u53ef\u4ee5\u9605\u8bfb\u7a0b\u5e8f\u5305\u91cc\u5b8c\u6574\u7684\u7a0b\u5e8f\u3002\n\n\n\u9884\u6d4b\u78b0\u649e\u65f6\u95f4\uff0cudp\u3002\n\n\n\u6beb\u7c73\u6ce2\u5355\u76ee\u6df1\u5ea6\u878d\u5408(\u6df1\u5ea6\u5b66\u4e60)\n\n\n\u73af\u5883\n\n* Ubuntu 18.04\n* Cuda 10.0\n* Pytorch 1.3.1\n* Python 3.6.5\n\n\n\u4f7f\u7528 \nradar_depth\n\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc,\u53c2\u89c1\n\u7f51\u5740\n\n\n\u73af\u5883\u642d\u5efa\n\u6df1\u5ea6\u878d\u5408\u7684\u57fa\u7840\u662f\u6211\u4eec\u4e0a\u9762\u7684\u6570\u636e\u7ea7\u878d\u5408\u3002\n\u7b2c\u4e00\u6b65\uff0c\u628a\u7a0b\u5e8f\u5305\u91cc\u7684\nradar_depth\n \u6587\u4ef6\u5939\u62f7\u8d1d\u5230\u672c\u5730\n\n\ncd radar_depth\n\npip install -r requirement.txt\n\n\n\n\u5b89\u88c5\u8fc7\u7a0b\u4e2d\u5982\u679c\u51fa\u73b0\u8981\u6c42\u7248\u672c\u7684\u5e93\u7f3a\u5931\uff0c\u4f8b\u5982\nnuscenes-devkit 1.1.5\n\n\u53ef\u4ee5\u5355\u72ec\u5b89\u88c5\n\n\npip install nuscenes-devkit==1.1.5\n\n\n\n\u73af\u5883\u642d\u5efa\u5b8c\u540e\uff0c\u66f4\u6539config/config_nuscenes.py. \u5904\u7684\u9879\u76ee\u8bbe\u7f6e\n\n\nPROJECT_ROOT = \"YOUR_PATH/radar_depth\"\nDATASET_ROOT = \"DATASET_PATH\"\n\n\n\n\u53ef\u4ee5\u5bf9\u8be5\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8fdb\u884c\u6df1\u5165\u7814\u7a76 \npdf\n\n\n\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u9700\u8981\u51c6\u5907\u795e\u7ecf\u7f51\u7edc\u8f93\u5165\u6570\u636e\uff08\u56fe\u7247\u548c\u96f7\u8fbe\u6570\u636e\uff09\u8fd0\u884c\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\uff0c\u5f97\u5230\u8f93\u51fa\u6570\u636e\u3002\n\n\n\u8f93\u5165\n\n\n\n+ \u96f7\u8fbe\u6570\u636e \u3002\u3002\u3002\n\n\n\u8f93\u5165\u6570\u636e\u51c6\u5907\uff0c\u9700\u8981\u5229\u7528\u7a0b\u5e8f\u5305\u91cc \nradar/src/ars_40X/src/ros/deal_data4net.cpp\n \u8282\u70b9\u6765\u5236\u9020\u795e\u7ecf\u7f51\u7edc\u8f93\u5165\u6570\u636e\u3002\n\n\n\u5148\u542f\u52a8\u6570\u636e\u91c7\u96c6\n\n\nsudo ip link set can0 up type can bitrate 500000\n\nroslaunch ars_40X ars_40X.launch\n\nrosrun ars_40X deal_data4net\n\n\n\n\n\u8fd0\u884c\u4f1a\u5b58\u6863\u4e00\u5f20\u56fe\u7247\npic.jpg\n \u548c \u96f7\u8fbe\u6570\u636e \nresult.csv\n\n\n\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\n\n\n\u6211\u4eec\u4f7f\u7528 MyFun.py\u811a\u672c\u8fdb\u884c\u63a8\u7406\n\n\nim = Image.open('/home/promote/pic.jpg') # \u4e3a\u56fe\u50cf\u8bfb\u53d6\u63a5\u53e3\n\u3002\u3002\u3002\nwith open('/home/promote/result.csv', newline='') as f: #\u4e3a\u96f7\u8fbe\u8bfb\u53d6\u63a5\u53e3\n\n\n\n\u53ea\u9700\u4fee\u6539\u4e3a\u672c\u5730\u6b63\u786e\u7684\u8def\u5f84\n\n\n\u63a8\u7406\n\n\npython MyFun.py \\\n--evaluate /home/username/radar_depth/pretrained/resnet18_multistage.pth.tar \\\n--data nuscenes \\\n--arch resnet18_multistage_uncertainty_fixs \\\n--modality rgbd \\\n--sparsifier radar \\\n--decoder upproj\n\n\n\n\u8f93\u51fa",
            "title": "\u6beb\u7c73\u6ce2\u548c\u6444\u50cf\u673a\u878d\u5408"
        },
        {
            "location": "/\u6beb\u7c73\u6ce2\u548c\u6444\u50cf\u673a\u878d\u5408/#_1",
            "text": "Ubuntu 18.04  Cuda 10.0  ROS melodic  Qt5",
            "title": "\u73af\u5883\u8981\u6c42"
        },
        {
            "location": "/\u6beb\u7c73\u6ce2\u548c\u6444\u50cf\u673a\u878d\u5408/#_2",
            "text": "",
            "title": "\u8f6f\u4ef6\u73af\u5883\u90e8\u7f72"
        },
        {
            "location": "/\u6beb\u7c73\u6ce2\u548c\u6444\u50cf\u673a\u878d\u5408/#qt",
            "text": "sudo apt-get install qttools5-dev-tools libqt5svg5-dev qtmultimedia5-dev",
            "title": "QT\u5b89\u88c5"
        },
        {
            "location": "/\u6beb\u7c73\u6ce2\u548c\u6444\u50cf\u673a\u878d\u5408/#ros-melodic",
            "text": "ROS\u5b98\u7f51",
            "title": "ROS melodic \u5b89\u88c5"
        },
        {
            "location": "/\u6beb\u7c73\u6ce2\u548c\u6444\u50cf\u673a\u878d\u5408/#cuda100-cudnn",
            "text": "\u5b89\u88c5\u6307\u5357  \u628a\u7a0b\u5e8f\u5305\u91cc\u7684 radar \u6587\u4ef6\u5939\u62f7\u8d1d\u5230\u672c\u5730   \u5728\u6b64\u76ee\u5f55\u4e0b\uff0c\u6267\u884c catkin_make  \u6307\u4ee4   \u7f16\u8bd1\u5b8c\u6210\u540e\uff0c\u6307\u4ee4\u8f93\u5165  sudo ip link set can0 up type can bitrate 500000\nroslaunch ars_40X ars_40X.launch  \u5728\u8bbe\u5907\u94fe\u63a5\u6b63\u5e38\u7684\u60c5\u51b5\u4e0b\u5c31\u53ef\u770b\u5230\u8f93\u51fa",
            "title": "Cuda10.0 \uff0ccudnn\u5b89\u88c5"
        },
        {
            "location": "/\u6beb\u7c73\u6ce2\u548c\u6444\u50cf\u673a\u878d\u5408/#ars404",
            "text": "cansend can0 200#F8000000089C0000 // Objects detection with all extended properties\ncansend can0 200#F8000000109C0000 // Clusters detection with all extended properties  \u8fd9\u4e24\u6761\u6307\u4ee4\u5206\u522b\u542f\u52a8radar\u7684 Cluster\u6a21\u5f0f\u548cObject\u6a21\u5f0f\uff0c\u6211\u4eec\u4f7f\u7528Object\u6a21\u5f0f\uff0c\uff08cluster\u6a21\u5f0f\uff09   \u6beb\u7c73\u6ce2\u96f7\u8fbe\u6570\u636e\u662f\u4e8c\u7ef4\u6570\u636e\uff0c\u53ea\u6709x, y \u5750\u6807\u4fe1\u606f\uff0c \u663e\u793a\u5982\u4e0a\u56fe  \u6beb\u7c73\u6ce2\u96f7\u8fbe\u6570\u636e\u8bfb\u53d6\uff0c\u6211\u4eec\u4f7f\u7528 ars_40X \u7a0b\u5e8f  \u6570\u636e\u663e\u793a\u548c\u56fe\u50cf\u663e\u793a\uff0c\u6211\u4eec\u4f7f\u7528 ros2qt  \u7a0b\u5e8f  \u56fe\u50cf\u7269\u4f53\u8bc6\u522b\uff0c\u6211\u4eec\u4f7f\u7528 vision_darknet_detect  \u7a0b\u5e8f",
            "title": "ARS404\u7684\u5e94\u7528"
        },
        {
            "location": "/\u6beb\u7c73\u6ce2\u548c\u6444\u50cf\u673a\u878d\u5408/#_3",
            "text": "\u9996\u5148\u5bf9\u6444\u50cf\u5934\u8fdb\u884c\u6807\u5b9a  \u83b7\u53d6\u5185\u53c2\u77e9\u9635  \u901a\u8fc7\u5185\u53c2\u77e9\u9635\u628a\u6beb\u7c73\u6ce2\u6570\u636e\u6620\u5c04\u5230\u56fe\u50cf\u4e0a  \u901a\u8fc7yolo\u8bc6\u522b\uff0c\u5e76\u63d0\u53d6\u51fa\u7269\u4f53\u8ddd\u79bb\uff0c\u5982\u4e0b\u56fe",
            "title": "\u6beb\u7c73\u6ce2\u548c\u56fe\u50cf\u7684\u878d\u5408"
        },
        {
            "location": "/\u6beb\u7c73\u6ce2\u548c\u6444\u50cf\u673a\u878d\u5408/#_4",
            "text": "\u6444\u50cf\u5934\u6807\u5b9a\u53ef\u4ee5\u4f7f\u7528opencv\u63d0\u4f9b\u7684\u6807\u5b9a\u65b9\u6cd5\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528matlab\u63d0\u4f9b\u7684\u6444\u50cf\u5934\u6807\u5b9a\u63d2\u4ef6\u3002  \u6211\u4eec\u4ecb\u7ecd\u4e0bmatlab\u6444\u50cf\u5934\u6807\u5b9a\u65b9\u6cd5\u3002\u4e3a\u4e86\u65b9\u4fbf\u6807\u5b9a\uff0c\u6211\u4eec\u5236\u4f5c\u4e86\u4e24\u4e2a\u201d\u6807\u5b9a\u76d2\u5b50\u201d    \u901a\u8fc7\u8be5\u6444\u50cf\u5934\u4f9d\u6b21\u91c7\u96c6\u5982\u4e0b\u4f4d\u7f6e\u56fe\u7247\u4f4d\u4e8eimage\u6587\u4ef6\u5939                                \u5728matlab \u5e94\u7528\u7a0b\u5e8f\u4e0b\u627e\u5230Camera Calibration\u5de5\u5177\u7bb1   \u52a0\u8f7d\u5f85\u6807\u5b9a\u7684\u56fe\u50cf   \u586b\u5199\u68cb\u76d8\u683c\u6bcf\u4e2a\u683c\u5b50\u8fb9\u957f\u7684\u771f\u5b9e\u503c   \u53ef\u4ee5\u9884\u89c8\u6210\u529f\u68c0\u6d4b\u51fa\u68cb\u76d8\u683c\u7684\u56fe\u50cf\uff0c\u7136\u540e\u5f00\u59cb\u6807\u5b9a\uff0c\u70b9\u51fbCalibrate   \u5e73\u5747\u8bef\u5dee\u5c0f\u4e8e0.5\u5373\u53ef   \u5bfc\u51fa\u76f8\u673a\u6807\u5b9a\u53c2\u6570   \u6211\u4eec\u53ea\u9700\u8981\u7528\u5230  IntrinsicMatrix  cv::Point World2Image(Eigen::Vector3d Pw)\n{\n  static Eigen::Matrix3d intric = (Eigen::Matrix3d() << 465.2203, 0, 351.1325, 0, 463.8023, 256.4198, 0, 0, 1).finished();\n  static Eigen::Matrix3d mi = (Eigen::Matrix3d() << 1, 0, 0, 0, -1,0, 0, 0, 1).finished();\n  Pw = mi*Pw;\n  Eigen::VectorXd result(2);\n  result = intric*Pw/Pw.z();\n  cv::Point P (result(0),result(1));\n  return P;\n}  \u901a\u8fc7world2image\u51fd\u6570\u628a\u6beb\u7c73\u6ce2\u5750\u6807\u7cfb\u8f6c\u6362\u5230\u56fe\u50cf\u5750\u6807\u7cfb  \u8fd9\u6837\u5728\u56fe\u50cf\u4e2d\uff0c\u6211\u4eec\u5c31\u6709\u4e86\u6df1\u5ea6\u4fe1\u606f\u3002  \u7136\u540e\u518d\u901a\u8fc7yolo\u7684darknet\u8fdb\u884c\u7269\u4f53\u8bc6\u522b\uff0c\u5c31\u53ef\u5f97\u5230\uff0c\u969c\u788d\u7269\u7684\u7c7b\u522b\u548c\u8ddd\u79bb\u4fe1\u606f\u3002  \u8fd9\u91cc\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u6444\u50cf\u5934\u548c\u6beb\u7c73\u6ce2\u5b89\u88c5\u4f4d\u7f6e\u5173\u7cfb\uff0c\u7531\u4e8e\u6beb\u7c73\u6ce2\u53ea\u6709x\uff0cy\u4fe1\u606f\uff0c\u6ca1\u6709\u9ad8\u5ea6\u4fe1\u606f\uff0c\u6240\u4ee5\u6211\u4eec\u5c3d\u91cf\u4fdd\u6301\u6444\u50cf\u5934\u548c\u6beb\u7c73\u6ce2\u7ad6\u76f4\u65b9\u5411\u4e0a\u4e2d\u5fc3\u7ebf\u91cd\u5408\u3002  \u5907\u6ce8\uff0c\u5982\u679c\u60f3\u8981\u66f4\u6539\u6444\u50cf\u5934\u548c\u6beb\u7c73\u6ce2\u7684\u76f8\u5bf9\u4f4d\u7f6e\uff0c\u53ea\u9700\u901a\u8fc7\u521a\u6027\u53d8\u6362\u77e9\u9635\uff0c\u8fdb\u884c\u5750\u6807\u7cfb\u8f6c\u6362  $ \\begin{bmatrix}cos(\\theta)&-sin(\\theta)&0.0&x\\\\sin(\\theta)&cos(\\theta)&0.0&y\\\\0.0&0.0&1.0&z\\\\0.0&0.0&0.0&1.0\\end{bmatrix} $  \u53d8\u6362\u77e9\u9635\u91cc$ \\begin{bmatrix}cos(\\theta)&-sin(\\theta)&0.0\\\\sin(\\theta)&cos(\\theta)&0.0\\\\0.0&0.0&1.0\\end{bmatrix} $\u662f3x3\u65cb\u8f6c\u77e9\u9635\uff0cxyz\u662f\u5e73\u79fb\u77e9\u9635\u3002  \u53ea\u9700\u66f4\u6539\u5bf9\u5e94\u7684\u65cb\u8f6c\u5e73\u79fb\u53c2\u6570\uff0c\u5373\u53ef\u8fdb\u884c\u5750\u6807\u7cfb\u8f6c\u6362\u3002",
            "title": "\u6444\u50cf\u5934\u6807\u5b9a"
        },
        {
            "location": "/\u6beb\u7c73\u6ce2\u548c\u6444\u50cf\u673a\u878d\u5408/#darknet",
            "text": "Yolo\u7269\u4f53\u68c0\u6d4b  \u6211\u4eec\u5728\u7a0b\u5e8f\u91cc\u4f7f\u7528\u4e86yolo v3\u8fdb\u884c\u7269\u4f53\u8bc6\u522b",
            "title": "darknet"
        },
        {
            "location": "/\u6beb\u7c73\u6ce2\u548c\u6444\u50cf\u673a\u878d\u5408/#_5",
            "text": "\u6211\u4eec\u5df2\u7ecf\u628a\u6beb\u7c73\u6ce2\u7684\u8ddd\u79bb\u4fe1\u606f\u6295\u5f71\u5230\u4e86\u56fe\u7247\u4e0a\uff0c\u540c\u65f6\uff0c\u6211\u4eec\u7528yolo\u5bf9\u969c\u788d\u7269\u8fdb\u884c\u4e86\u8bc6\u522b\u5206\u7c7b\uff0c\u753b\u51fa\u4e86\u8fb9\u754c\u6846\u3002  \u6211\u4eec\u53ea\u9700\u8981\u628a\u8fb9\u754c\u6846\u5185\u90e8\u7684\u8ddd\u79bb\u4fe1\u606f\u63d0\u53d6\u51fa\u6765\u5373\u53ef\u3002\u7531\u4e8e\u6beb\u7c73\u6ce2\u6709\u8bef\u5dee\uff0c\u6240\u4ee5\u6211\u4eec\u9700\u8981\u5bf9\u8fd9\u4e9b\u8fb9\u754c\u6846\u91cc\u7684\u4fe1\u606f\u8fdb\u884c\u7b5b\u9009\u3002\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u6444\u50cf\u5934\u53ef\u4ee5\u76f4\u63a5\u68c0\u6d4b\u5230\u7684\u7269\u4f53\uff0c\u8bf4\u660e\u6b64\u7269\u4f53\u524d\u9762\u6ca1\u6709\u969c\u788d\u7269\uff0c\u56e0\u4e3a\u5149\u662f\u76f4\u7ebf\u4f20\u64ad\u7684\u3002\u4e5f\u5c31\u662f\u8bf4\u5982\u679c\u969c\u788d\u7269\u524d\u65b9\u8fd8\u6709\u6709\u969c\u788d\u7269\uff0c\u6444\u50cf\u5934\u5f88\u5927\u53ef\u80fd\u5c31\u770b\u4e0d\u5230\u540e\u9762\u90a3\u4e00\u4e2a\uff0c\u4f46\u662f\u6beb\u7c73\u6ce2\u53ef\u80fd\u901a\u8fc7\u6f2b\u53cd\u5c04\u770b\u5230\u4e86\uff0c\u4ea7\u751f\u591a\u4e2a\u8ddd\u79bb\u4fe1\u606f\u3002\u6240\u4ee5\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u63d0\u53d6\u8fb9\u754c\u6846\u91cc\u7684\u6700\u5c0f\u8ddd\u79bb\u5f53\u505a\u8bc6\u522b\u969c\u788d\u7269\u7684\u8ddd\u79bb\u3002\u8fd9\u540c\u6837\u662f\u51fa\u4e8e\u5b89\u5168\u8003\u8651\uff0c\u9700\u8981\u9009\u62e9\u8ddd\u79bb\u6700\u8fd1\u7684\u8ddd\u79bb\u3002  \u6211\u4eec\u53ef\u4ee5\u6839\u636e\u6027\u80fd\u8981\u6c42\uff0c\u63d0\u51fa\u66f4\u597d\u7684\u63d0\u53d6\u6df1\u5ea6\u4fe1\u606f\u65b9\u6cd5\u3002   \u56fe\u4e2d\u7070\u8272\u7684\u70b9\u662f\u6beb\u7c73\u6ce2\u6295\u5c04\u5230\u56fe\u50cf\u4e0a\u7684\u70b9\uff0c\u9ec4\u8272\u7684\u6846\u662fyolo\u8bc6\u522b\u7684\u8fb9\u754c\u6846  \u6574\u4e2a\u7b97\u6cd5\u7684\u5177\u4f53\u5b9e\u73b0\u903b\u8f91\u53ef\u4ee5\u9605\u8bfb\u7a0b\u5e8f\u5305\u91cc\u5b8c\u6574\u7684\u7a0b\u5e8f\u3002  \u9884\u6d4b\u78b0\u649e\u65f6\u95f4\uff0cudp\u3002",
            "title": "\u5982\u4f55\u63d0\u53d6\u51fa\u8ddd\u79bb\u4fe1\u606f"
        },
        {
            "location": "/\u6beb\u7c73\u6ce2\u548c\u6444\u50cf\u673a\u878d\u5408/#_6",
            "text": "\u73af\u5883 \n* Ubuntu 18.04\n* Cuda 10.0\n* Pytorch 1.3.1\n* Python 3.6.5  \u4f7f\u7528  radar_depth \u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc,\u53c2\u89c1 \u7f51\u5740  \u73af\u5883\u642d\u5efa\n\u6df1\u5ea6\u878d\u5408\u7684\u57fa\u7840\u662f\u6211\u4eec\u4e0a\u9762\u7684\u6570\u636e\u7ea7\u878d\u5408\u3002\n\u7b2c\u4e00\u6b65\uff0c\u628a\u7a0b\u5e8f\u5305\u91cc\u7684 radar_depth  \u6587\u4ef6\u5939\u62f7\u8d1d\u5230\u672c\u5730  cd radar_depth\n\npip install -r requirement.txt  \u5b89\u88c5\u8fc7\u7a0b\u4e2d\u5982\u679c\u51fa\u73b0\u8981\u6c42\u7248\u672c\u7684\u5e93\u7f3a\u5931\uff0c\u4f8b\u5982 nuscenes-devkit 1.1.5 \n\u53ef\u4ee5\u5355\u72ec\u5b89\u88c5  pip install nuscenes-devkit==1.1.5  \u73af\u5883\u642d\u5efa\u5b8c\u540e\uff0c\u66f4\u6539config/config_nuscenes.py. \u5904\u7684\u9879\u76ee\u8bbe\u7f6e  PROJECT_ROOT = \"YOUR_PATH/radar_depth\"\nDATASET_ROOT = \"DATASET_PATH\"  \u53ef\u4ee5\u5bf9\u8be5\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8fdb\u884c\u6df1\u5165\u7814\u7a76  pdf  \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u9700\u8981\u51c6\u5907\u795e\u7ecf\u7f51\u7edc\u8f93\u5165\u6570\u636e\uff08\u56fe\u7247\u548c\u96f7\u8fbe\u6570\u636e\uff09\u8fd0\u884c\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\uff0c\u5f97\u5230\u8f93\u51fa\u6570\u636e\u3002",
            "title": "\u6beb\u7c73\u6ce2\u5355\u76ee\u6df1\u5ea6\u878d\u5408(\u6df1\u5ea6\u5b66\u4e60)"
        },
        {
            "location": "/\u6beb\u7c73\u6ce2\u548c\u6444\u50cf\u673a\u878d\u5408/#_7",
            "text": "+ \u96f7\u8fbe\u6570\u636e \u3002\u3002\u3002  \u8f93\u5165\u6570\u636e\u51c6\u5907\uff0c\u9700\u8981\u5229\u7528\u7a0b\u5e8f\u5305\u91cc  radar/src/ars_40X/src/ros/deal_data4net.cpp  \u8282\u70b9\u6765\u5236\u9020\u795e\u7ecf\u7f51\u7edc\u8f93\u5165\u6570\u636e\u3002  \u5148\u542f\u52a8\u6570\u636e\u91c7\u96c6  sudo ip link set can0 up type can bitrate 500000\n\nroslaunch ars_40X ars_40X.launch\n\nrosrun ars_40X deal_data4net  \u8fd0\u884c\u4f1a\u5b58\u6863\u4e00\u5f20\u56fe\u7247 pic.jpg  \u548c \u96f7\u8fbe\u6570\u636e  result.csv",
            "title": "\u8f93\u5165"
        },
        {
            "location": "/\u6beb\u7c73\u6ce2\u548c\u6444\u50cf\u673a\u878d\u5408/#_8",
            "text": "\u6211\u4eec\u4f7f\u7528 MyFun.py\u811a\u672c\u8fdb\u884c\u63a8\u7406  im = Image.open('/home/promote/pic.jpg') # \u4e3a\u56fe\u50cf\u8bfb\u53d6\u63a5\u53e3\n\u3002\u3002\u3002\nwith open('/home/promote/result.csv', newline='') as f: #\u4e3a\u96f7\u8fbe\u8bfb\u53d6\u63a5\u53e3  \u53ea\u9700\u4fee\u6539\u4e3a\u672c\u5730\u6b63\u786e\u7684\u8def\u5f84  \u63a8\u7406  python MyFun.py \\\n--evaluate /home/username/radar_depth/pretrained/resnet18_multistage.pth.tar \\\n--data nuscenes \\\n--arch resnet18_multistage_uncertainty_fixs \\\n--modality rgbd \\\n--sparsifier radar \\\n--decoder upproj",
            "title": "\u795e\u7ecf\u7f51\u7edc\u63a8\u7406"
        },
        {
            "location": "/\u6beb\u7c73\u6ce2\u548c\u6444\u50cf\u673a\u878d\u5408/#_9",
            "text": "",
            "title": "\u8f93\u51fa"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/",
            "text": "\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50\u5c01\u88c5\uff561.0\n\n\n\n\n\u786c\u4ef6\u73af\u5883\u7ec4\u6210\n\n\n\u9002\u7528\u4e8eAGX\u63a7\u5236\u5668\n\n\nvelodyne\u6fc0\u5149\u96f7\u8fbe\uff0c\n\u6ce8\uff1a\u5176\u4ed6\u54c1\u724c\u96f7\u8fbe\u53ea\u9700\u66f4\u6362\u9a71\u52a8\u548c\u5bf9\u5e94\u7684\u70b9\u4e91topic\n\n\n\u8f6f\u4ef6\u73af\u5883\u90e8\u7f72\n\n\nubuntu18.04\u64cd\u4f5c\u7cfb\u7edf\n\n\n\u6839\u636eAGX\u63d0\u4f9b\u7684\u7cfb\u7edf\u5305Jetpack\u5b89\u88c5\n\n\nCuda\u548ccudnn\u5b89\u88c5\n\n\n\u901a\u8fc7Jetpack\u5b89\u88c5\n\n\nROS\u5b89\u88c5\n\n\n\n\n\u4e0b\u8f7d\n\n\ngit clone https://github.com/jetsonhacks/installROSXavier.git\n\n\n\u5207\u6362\u76ee\u5f55\n\n\ncd installROSXavier\n\n\n\u5b89\u88c5\n\n\n./installROS.sh -p ros-melodic-desktop -p ros-melodic-rgbd-launch\n\n\n\u63d2\u4ef6\u5b89\u88c5\n\n\nsudo apt-get install ros-melodic-jsk-rviz-plugins\n\n\n\n\ncaffe \u90e8\u7f72\n\n\n\n\n\u4e0b\u8f7d\n\n\ngit clone https://github.com/BVLC/caffe.git\n\n\n\u5b89\u88c5\u4f9d\u8d56\n\n\nsudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler\n\n\nsudo apt-get install --no-install-recommends libboost-all-dev\n\n\nsudo apt-get install libopenblas-dev\n\n\nsudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev\n\n\n\u5207\u6362\u76ee\u5f55\n\n\ncd caffe\n\n\n\u65b0\u5efaMakefile.config\u6587\u4ef6\n,\u5e76\u7c98\u8d34\u4e0b\u9762\u7684\u5185\u5bb9\n\n\n\n\n```\n    \uff41## Refer to http://caffe.berkeleyvision.org/installation.html\n    # Contributions simplifying and improving our build system are welcome!\n\n\ncuDNN acceleration switch (uncomment to build with cuDNN).\n\n\nUSE_CUDNN := 1\n\n\nCPU-only switch (uncomment to build without GPU support).\n\n\nCPU_ONLY := 1\n\n\nuncomment to disable IO dependencies and corresponding data layers\n\n\nUSE_OPENCV := 0\n\n\nUSE_LEVELDB := 0\n\n\nUSE_LMDB := 0\n\n\nuncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)\n\n\nYou should not set this flag if you will be reading LMDBs with any\n\n\npossibility of simultaneous read and write\n\n\nALLOW_LMDB_NOLOCK := 1\n\n\nUncomment if you're using OpenCV 3\n\n\nOPENCV_VERSION := 3\n\n\nTo customize your choice of compiler, uncomment and set the following.\n\n\nN.B. the default for Linux is g++ and the default for OSX is clang++\n\n\nCUSTOM_CXX := g++\n\n\nCUDA directory contains bin/ and lib/ directories that we need.\n\n\nCUDA_DIR := /usr/local/cuda\n\n\nOn Ubuntu 14.04, if cuda tools are installed via\n\n\n\"sudo apt-get install nvidia-cuda-toolkit\" then use this instead:\n\n\nCUDA_DIR := /usr\n\n\nCUDA architecture setting: going with all of them.\n\n\nFor CUDA < 6.0, comment the lines after *_35 for compatibility.\n\n\nCUDA_ARCH := -gencode arch=compute_30,code=sm_30 \\\n             -gencode arch=compute_35,code=sm_35 \\\n             -gencode arch=compute_50,code=sm_50 \\\n             -gencode arch=compute_52,code=sm_52 \\\n             -gencode arch=compute_61,code=sm_61 \\\n             -gencode arch=compute_61,code=compute_61\n\n\n         # -gencode arch=compute_20,code=sm_20 \\\n         # -gencode arch=compute_20,code=sm_21 \\\n\n\n\nBLAS choice:\n\n\natlas for ATLAS (default)\n\n\nmkl for MKL\n\n\nopen for OpenBlas\n\n\nBLAS := atlas\n\n\nBLAS := open\n\n\nCustom (MKL/ATLAS/OpenBLAS) include and lib directories.\n\n\nLeave commented to accept the defaults for your choice of BLAS\n\n\n(which should work)!\n\n\nBLAS_INCLUDE := /path/to/your/blas\n\n\nBLAS_LIB := /path/to/your/blas\n\n\nHomebrew puts openblas in a directory that is not on the standard search path\n\n\nBLAS_INCLUDE := $(shell brew --prefix openblas)/include\n\n\nBLAS_LIB := $(shell brew --prefix openblas)/lib\n\n\nThis is required only if you will compile the matlab interface.\n\n\nMATLAB directory should contain the mex binary in /bin.\n\n\nMATLAB_DIR := /usr/local\n\n\nMATLAB_DIR := /Applications/MATLAB_R2012b.app\n\n\nNOTE: this is required only if you will compile the python interface.\n\n\nWe need to be able to find Python.h and numpy/arrayobject.h.\n\n\nPYTHON_INCLUDE := /usr/include/python2.7 \\\n        /usr/lib/python2.7/dist-packages/numpy/core/include\n\n\nAnaconda Python distribution is quite popular. Include path:\n\n\nVerify anaconda location, sometimes it's in root.\n\n\nANACONDA_HOME := $(HOME)/anaconda2\n\n\n# PYTHON_INCLUDE := $(ANACONDA_HOME)/include \\\n        $(ANACONDA_HOME)/include/python2.7 \\\n        $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include \\\n\n\nUncomment to use Python 3 (default is Python 2)\n\n\nPYTHON_LIBRARIES := boost_python3 python3.5m\n\n\n# PYTHON_INCLUDE := /usr/include/python3.5m \\\n\n\n/usr/lib/python3.5/dist-packages/numpy/core/include\n\n\nWe need to be able to find libpythonX.X.so or .dylib.\n\n\nPYTHON_LIB := /usr/lib\n\n\nPYTHON_LIB := $(ANACONDA_HOME)/lib\n\n\nHomebrew installs numpy in a non standard path (keg only)\n\n\nPYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.\nfile\n)'))/include\n\n\nPYTHON_LIB += $(shell brew --prefix numpy)/lib\n\n\nUncomment to support layers written in Python (will link against Python libs)\n\n\nWITH_PYTHON_LAYER := 1\n\n\nWhatever else you find you need goes here.\n\n\nINCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial/\nLIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib  /usr/lib/aarch64-linux-gnu/hdf5/serial\n\n\nIf Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies\n\n\nINCLUDE_DIRS += $(shell brew --prefix)/include\n\n\nLIBRARY_DIRS += $(shell brew --prefix)/lib\n\n\nUncomment to use \npkg-config\n to specify OpenCV library paths.\n\n\n(Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)\n\n\nUSE_PKG_CONFIG := 1\n\n\nN.B. both build and distribute dirs are cleared on \nmake clean\n\n\nBUILD_DIR := build\nDISTRIBUTE_DIR := distribute\n\n\nUncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171\n\n\nDEBUG := 1\n\n\nThe ID of the GPU that 'make runtest' will use to run unit tests.\n\n\nTEST_GPUID := 0\n\n\nenable pretty build (comment to see full commands)\n\n\nQ ?= @\n```\n* \u7f16\u8bd1\n  * make -j 8\n  * sudo make distribute\n\n\n\n\n\n\nqt\u5b89\u88c5\n\n\n\n\nsudo apt-get install qttools5-dev-tools libqt5svg5-dev qtmultimedia5-dev\n\n\n\n\n\u7b97\u6cd5\u76d2\u5b50\u7ec4\u6210\u90e8\u5206\n\n\n\n\n\u70b9\u4e91\u6807\u5b9a\u529f\u80fd\n\n\n\u70b9\u4e91\u5efa\u56fe\u529f\u80fd\n\n\n\u70b9\u4e91\u805a\u7c7b\u529f\u80fd\n\n\n\u70b9\u4e91\u667a\u80fd\u8bc6\u522b\u529f\u80fd\n\n\n\n\n\u7a0b\u5e8f\u6587\u4ef6\u5939\u76ee\u5f55\u7ed3\u6784\n\n\n\n\n\u7a0b\u5e8f\n\n\ncatkin_ws\n\n\nsrc\n\n\ncommon\n\n\ndetected_objects_visualizer\n\n\nlidar_cnn_seg_detect\n\n\nlidar_demo\n\n\nndt_mapping\n\n\nrockauto_msgs\n\n\nros2qt\n\n\nvelodyne\n\n\nCMakeLists.txt\n\n\n\n\n\n\n\n\n\u6211\u4eec\u9700\u8981\u628acatkin_ws\u6587\u4ef6\u5939\u62f7\u8d1d\u5230\u5de5\u63a7\u673a\u7684home\u76ee\u5f55\u4e0b\u3002\n\n\n\u7f16\u8bd1\u7a0b\u5e8f\n\n\n\n\n\u5207\u6362\u76ee\u5f55\n\n\ncd catkin_ws\n\n\n\u7f16\u8bd1\n\n\ncatkin_make\n\n\nsource \u73af\u5883\n\n\nsource devel/setup.bash\n\n\n\n\n\u7a0b\u5e8f\u542f\u52a8\n\n\n\n\nrosrun ros2qt ros2qt\n\n\n\n\n\u5e94\u7528\u8bb2\u89e3\n\n\n\n\n\u7a0b\u5e8f\u542f\u52a8\u540e\u5f39\u51fa\u4e0b\u56fe\u754c\u9762\n\n\n\n\n\n\n\n\n\u5982\u4e0b\u56fe\u914d\u7f6e\nFixed Frame\n \u4e3a\nvelodyne\n,\u52fe\u9009\npointCloud2\n\u590d\u9009\u6846\uff0c\u8bbe\u7f6e\u5176\nTopic\n\u4e3a\npoint_raw\n\u53ef\u89c2\u770b\u96f7\u8fbe\u6570\u636e\n\n\n\n\n\n\n\n\n\u542f\u7528\uff33\uff2c\uff21\uff2d\u529f\u80fd\uff0c\u5219\u52fe\u9009\nSLAM\n\u590d\u9009\u6846,\u5c06\u5176\nTopic\n\u8bbe\u7f6e\u4e3a\npoint_raw\n\uff0c\u540c\u65f6\u5c06\npointCloud2\n\u7684\nTopic\n\u8bbe\u7f6e\u4e3a\ncloud\n\u3002\n\n\n\n\n\n\n\n\nSlam\u63d0\u4f9b\u4e86\u53ef\u8c03\u53c2\u6570\uff1a\n  Topic:\u8bbe\u7f6e\u6784\u56fe\u4e3b\u9898\n  Size:\u8bbe\u7f6e\u70b9\u4e91\u5730\u56fe\u4e2d\u70b9\u7684\u663e\u793a\u5927\u5c0f\n  Color:\u70b9\u4e91\u5730\u56fe\u7684\u989c\u8272\n  Res:\u5206\u8fa8\u7387\n  Step_size:\u6b65\u5e45\n  Trans_epsilon:\u6536\u655b\u5747\u65b9\u5dee\n  Max_iter:\u6700\u5927\u8fed\u4ee3\u6b21\u6570\n  Voxel_leaf_size:Voxel\u5c3a\u5bf8\n  Min_scan_range:\u6700\u5c0f\u626b\u63cf\u8303\u56f4\n  Max_scan_range:\u6700\u5927\u626b\u63cf\u8303\u56f4\n  Scan_rate:\u626b\u63cf\u5468\u671f\n\n\n\n\n\n\n\n\n\u663e\u793a\u969c\u788d\u7269\uff0c\u5219\u53d6\u6d88\nSLAM\n\u52fe\u9009\uff0c\u5c06\npointCloud2\n\u7684\nTopic\n\u66f4\u6539\u56de\npoint_raw\n.\u52fe\u9009\n\uff22oundingBox\n\u590d\u9009\u6846\u3002\n\n\n\n\n\n\n\u663e\u793a\u4f4d\u7f6e\uff0c\u901f\u5ea6\uff0c\u52a0\u901f\u5ea6\u548c\u8def\u5f84\n\u663e\u793a\u805a\u7c7b\u969c\u788d\u7269\uff0c\u5219\u52fe\u9009'BoundingBox2'\u590d\u9009\u6846\u3002\n\n\n\n\n\u8fd9\u91cc\u67093\u4e2a\u53c2\u6570:\nCluster_D:\u805a\u7c7b\u76f4\u5f84\nCluster_Min:\u6700\u5c0f\u805a\u7c7b\u70b9\u6570\nCluster_Max:\u6700\u5927\u805a\u7c7b\u70b9\u6570\n\n\n\n\n\u70b9\u4e91\u6807\u5b9a\n  \u9996\u5148\u8981\u52fe\u9009\nPointCloud2\n\u663e\u793a\u70b9\u4e91\uff0c\u518d\u52fe\u9009\n\u6807\u5b9a\n\u53ef\u79fb\u52a8xyz\u4e09\u8f74\uff0c\u4e5f\u53ef\u7ed5\u4e09\u8f74\u65cb\u8f6c\u3002\n\n\n\n\n\n\n\u7a0b\u5e8f\u8bb2\u89e3\n\n\n\u70b9\u4e91\u6807\u5b9a\u529f\u80fd\n\n\n\u6807\u5b9a\u7a0b\u5e8f\u529f\u80fd\u4ee3\u7801\u6bb5\u4f4d\u4e8e\"ros2qt\\src\\qnode.cpp\"\u6587\u4ef6\n\n\nvoid QNode::biaodingCallback(const PointCloud::ConstPtr& msg)\n{\n    //\u70b9\u4e91\u65cb\u8f6c\n    Eigen::Affine3f transform_2 = Eigen::Affine3f::Identity();\n    transform_2.translation() << x_pianyi, y_pianyi, z_pianyi;\n    float theta_2 = 0*M_PI/180;\n    transform_2.rotate (Eigen::AngleAxisf (theta_2, Eigen::Vector3f::UnitY()));\n    pcl::PointCloud<pcl::PointXYZ>::Ptr transformed_cloud_2 (new pcl::PointCloud<pcl::PointXYZ> ());\n    pcl::transformPointCloud (*msg, *transformed_cloud_2, transform_2);\n\n    Eigen::Affine3f transform = Eigen::Affine3f::Identity();\n    transform.translation() << 0.0, 0.0, 0.0;\n    float theta = x_xuanzhuan*M_PI/180;\n    transform.rotate (Eigen::AngleAxisf (theta, Eigen::Vector3f::UnitX()));\n    pcl::PointCloud<pcl::PointXYZ>::Ptr transformed_cloud (new pcl::PointCloud<pcl::PointXYZ> ());\n    pcl::transformPointCloud (*transformed_cloud_2, *transformed_cloud, transform);\n\n    Eigen::Affine3f transform_3 = Eigen::Affine3f::Identity();\n    transform_3.translation() << 0.0, 0.0, 0.0;\n    float theta_3 = z_xuanzhuan*M_PI/180;\n    transform_3.rotate (Eigen::AngleAxisf (theta_3, Eigen::Vector3f::UnitZ()));\n    pcl::PointCloud<pcl::PointXYZ>::Ptr transformed_cloud_3 (new pcl::PointCloud<pcl::PointXYZ> ());\n    pcl::transformPointCloud (*transformed_cloud, *transformed_cloud_3, transform_3);\n\n    Eigen::Affine3f transform_4 = Eigen::Affine3f::Identity();\n    transform_4.translation() << 0.0, 0.0, 0.0;\n    float theta_4 = y_xuanzhuan*M_PI/180;\n    transform_4.rotate (Eigen::AngleAxisf (theta_4, Eigen::Vector3f::UnitY()));\n    pcl::PointCloud<pcl::PointXYZ>::Ptr transformed_cloud_4 (new pcl::PointCloud<pcl::PointXYZ> ());\n    pcl::transformPointCloud (*transformed_cloud_3, *transformed_cloud_4, transform_4);\n    transformed_cloud_4->header.frame_id = frame_id;\n    pub.publish(transformed_cloud_4);\n}\n\n\n\n\n\u70b9\u4e91\u7684\u6807\u5b9a\uff0c\u4e3b\u8981\u662f\u70b9\u4e91\u7684\u65cb\u8f6c\u548c\u5e73\u79fb\u529f\u80fd\uff0c\u53ef\u4ee5\u53c2\u8003pcl\uff0c\u8fd9\u91cc\u7b80\u5355\u8bb2\u89e3\u4e00\u4e0b\u65cb\u8f6c\u5e73\u79fb\u529f\u80fd\n\n\n\n\n\u9996\u5148\u5b9a\u4e49\u4e00\u4e2aAffine3f\u7ed3\u6784\u4f53\uff0c\u7528\u6765\u5b58\u50a8\u70b9\u4e91\u65cb\u8f6c\u548c\u5e73\u79fb\u4fe1\u606f\n\n\n\n\nEigen::Affine3f transform_2 = Eigen::Affine3f::Identity();\n\n\n\n\n\u518d\u628a\uff58\uff0c\uff59\uff0c\uff5a\u8f74\u7684\u5e73\u79fb\u91cfx_pianyi, y_pianyi, z_pianyi\u4f20\u9012\u8fdbAffine3f\u7ed3\u6784\u4f53\n\n\n\n\ntransform_2.translation() << x_pianyi, y_pianyi, z_pianyi;\n\n\n\n\n\u5b9a\u4e49\u4e00\u4e2a\u65cb\u8f6c\u89d2\u5ea6\n\n\n\n\nfloat theta_2 = 0*M_PI/180;\n\n\n\n\n\u628a\u7ed5\uff39\u8f74\u65cb\u8f6c\u89d2\u5ea6\uff0c\u4f20\u5165Affin3f\u7ed3\u6784\u4f53\uff0c\u65cb\u8f6c\u8f74Eigen::Vector3f::UnitY()\n\n\n\n\ntransform_2.rotate (Eigen::AngleAxisf (theta_2, Eigen::Vector3f::UnitY()));\n\n\n\n\n\u5b9a\u4e49\u65cb\u8f6c\u540e\u7684\u70b9\u4e91\n\n\n\n\npcl::PointCloudpcl::pointxyz::Ptr transformed_cloud_2 (new pcl::PointCloudpcl::pointxyz ());\n\n\n\n\n\u6267\u884c\u70b9\u4e91\u7684\u5e73\u79fb\u548c\u65cb\u8f6c\uff0c\u5e76\u628a\u7ed3\u679c\u5b58\u5165\ntransformed_cloud_2\n\n\n\n\npcl::transformPointCloud (*msg, *transformed_cloud_2, transform_2);\n\n\n\u70b9\u4e91\u5efa\u56fe\u529f\u80fd\n\n\n\u5efa\u56fe\u7a0b\u5e8f\u5305\u5373ndt_mapping\u6587\u4ef6\u5939,\u91cd\u70b9\u8fd0\u7b97\u903b\u8f91\u5728ndt_mapping.cpp\u6587\u4ef6\u5185\uff0c\u8fd9\u91cc\u89e3\u6790\u4e00\u4e0b\n\n\nndt_mapping::ndt_mapping() \n{\n  transform_pub = nh_.advertise<sensor_msgs::PointCloud2> (\"/cloud\", 1, false);\n  points_sub_ = nh_.subscribe(slam_topic, 100000, &ndt_mapping::points_callback,this);\n  ndt_map_pub_ = nh_.advertise<sensor_msgs::PointCloud2>(\"/ndt_map\", 1000);\n  current_pose_pub_ = nh_.advertise<geometry_msgs::PoseStamped>(\"/current_pose\", 1000);\n\n  max_iter_ = max_iter_1;\n  ndt_res_ = ndt_res_1;\n  step_size_ = step_size_1;\n  trans_eps_ = trans_eps_1;\n  voxel_leaf_size_ = voxel_leaf_size_1;\n  scan_rate_ = scan_rate_1;\n  min_scan_range_ = min_scan_range_1;\n  max_scan_range_ = max_scan_range_1;\n  min_add_scan_shift_ = min_add_scan_shift_1;\n\n\n  initial_scan_loaded = 0;\n  min_add_scan_shift_ = 1.0;\n\n  _tf_x=0.0, _tf_y=0.0, _tf_z=0.0, _tf_roll=0.0, _tf_pitch=0.0, _tf_yaw=0.0;\n\n  Eigen::Translation3f tl_btol(_tf_x, _tf_y, _tf_z);        \n  Eigen::AngleAxisf rot_x_btol(_tf_roll, Eigen::Vector3f::UnitX());  \n  Eigen::AngleAxisf rot_y_btol(_tf_pitch, Eigen::Vector3f::UnitY());\n  Eigen::AngleAxisf rot_z_btol(_tf_yaw, Eigen::Vector3f::UnitZ());\n  tf_btol_ = (tl_btol * rot_z_btol * rot_y_btol * rot_x_btol).matrix();\n  tf_ltob_ = tf_btol_.inverse();\n\n  map_.header.frame_id = \"velodyne\";\n\n  current_pose_.x = current_pose_.y = current_pose_.z = 0.0;current_pose_.roll = current_pose_.pitch = current_pose_.yaw = 0.0;\n  previous_pose_.x = previous_pose_.y = previous_pose_.z = 0.0;previous_pose_.roll = previous_pose_.pitch = previous_pose_.yaw = 0.0;\n\n  voxel_grid_filter_.setLeafSize(voxel_leaf_size_, voxel_leaf_size_, voxel_leaf_size_);  \n\n  ndt.setTransformationEpsilon(trans_eps_);\n  ndt.setStepSize(step_size_);\n  ndt.setResolution(ndt_res_);\n  ndt.setMaximumIterations(max_iter_);\n\n  is_first_map_ = true;\n}; \n\n\n\nndt_mapping::ndt_mapping()\u51fd\u6570\uff0c\u4e3b\u8981\u8fdb\u884c\u4e86\u4e00\u4e9b\u53c2\u6570\u8bbe\u5b9a\uff0c\u548ctopic\u8bbe\u5b9a\u3002\n\n\nvoid ndt_mapping::points_callback(const sensor_msgs::PointCloud2::ConstPtr& input)\n{\n  pcl::PointCloud<pcl::PointXYZI> tmp, scan;\n  pcl::PointCloud<pcl::PointXYZI>::Ptr filtered_scan_ptr(new pcl::PointCloud<pcl::PointXYZI>());\n  pcl::PointCloud<pcl::PointXYZI>::Ptr transformed_scan_ptr(new pcl::PointCloud<pcl::PointXYZI>());\n\n  tf::Quaternion q;\n\n  Eigen::Matrix4f t_localizer(Eigen::Matrix4f::Identity());\n  Eigen::Matrix4f t_base_link(Eigen::Matrix4f::Identity());\n  static tf::TransformBroadcaster br_;\n  tf::Transform transform;\n\n  pcl::fromROSMsg(*input, tmp);\n  double r;\n  Eigen::Vector3d point_pos;\n  pcl::PointXYZI p;\n  for (pcl::PointCloud<pcl::PointXYZI>::const_iterator item = tmp.begin(); item != tmp.end(); item++)\n  {\n    use_imu_ = false;\n    if(use_imu_){\n      // deskew(TODO:inplement of predicting pose by imu)\n      point_pos.x() = (double)item->x;\n      point_pos.y() = (double)item->y;\n      point_pos.z() = (double)item->z;\n      double s = scan_rate_ * (double(item->intensity) - int(item->intensity));\n\n      point_pos.x() -= s * current_pose_msg_.pose.position.x;//current_pose_imu_\n      point_pos.y() -= s * current_pose_msg_.pose.position.y;\n      point_pos.z() -= s * current_pose_msg_.pose.position.z;\n\n      Eigen::Quaterniond start_quat, end_quat, mid_quat;\n      mid_quat.setIdentity();\n      end_quat = Eigen::Quaterniond(\n        current_pose_msg_.pose.orientation.w,\n        current_pose_msg_.pose.orientation.x,\n        current_pose_msg_.pose.orientation.y,\n        current_pose_msg_.pose.orientation.z);\n      start_quat = mid_quat.slerp(s, end_quat);\n\n      point_pos = start_quat.conjugate() * start_quat * point_pos;\n\n      point_pos.x() += current_pose_msg_.pose.position.x;\n      point_pos.y() += current_pose_msg_.pose.position.y;\n      point_pos.z() += current_pose_msg_.pose.position.z;\n\n      p.x = point_pos.x();\n      p.y = point_pos.y();\n      p.z = point_pos.z();\n    }\n    else{\n      p.x = (double)item->x;\n      p.y = (double)item->y;\n      p.z = (double)item->z;\n    }\n    p.intensity = (double)item->intensity;\n    r = sqrt(pow(p.x, 2.0) + pow(p.y, 2.0));\n    if (min_scan_range_ < r && r < max_scan_range_)\n    {\n      scan.push_back(p);\n    }\n  }\n\n  pcl::PointCloud<pcl::PointXYZI>::Ptr scan_ptr(new pcl::PointCloud<pcl::PointXYZI>(scan));\n  if (initial_scan_loaded == 0)\n  {\n    pcl::transformPointCloud(*scan_ptr, *transformed_scan_ptr, tf_btol_);\n    map_ += *transformed_scan_ptr;\n    initial_scan_loaded = 1;\n  }\n\n  voxel_grid_filter_.setInputCloud(scan_ptr);\n  voxel_grid_filter_.filter(*filtered_scan_ptr);\n  ndt.setInputSource(filtered_scan_ptr);\n\n  pcl::PointCloud<pcl::PointXYZI>::Ptr map_ptr(new pcl::PointCloud<pcl::PointXYZI>(map_));\n  if (is_first_map_ == true){\n    ndt.setInputTarget(map_ptr);\n    is_first_map_ = false;\n  }\n\n  Eigen::Translation3f init_translation(current_pose_.x, current_pose_.y, current_pose_.z);\n  Eigen::AngleAxisf init_rotation_x(current_pose_.roll, Eigen::Vector3f::UnitX());\n  Eigen::AngleAxisf init_rotation_y(current_pose_.pitch, Eigen::Vector3f::UnitY());\n  Eigen::AngleAxisf init_rotation_z(current_pose_.yaw, Eigen::Vector3f::UnitZ());\n\n  Eigen::Matrix4f init_guess =\n      (init_translation * init_rotation_z * init_rotation_y * init_rotation_x).matrix() * tf_btol_;\n\n  pcl::PointCloud<pcl::PointXYZI>::Ptr output_cloud(new pcl::PointCloud<pcl::PointXYZI>);\n\n  ndt.align(*output_cloud, init_guess);\n  t_localizer = ndt.getFinalTransformation();\n\n  t_base_link = t_localizer * tf_ltob_;\n\n  pcl::transformPointCloud(*scan_ptr, *transformed_scan_ptr, t_localizer);\n  sensor_msgs::PointCloud2::Ptr tt(new sensor_msgs::PointCloud2);\n  pcl::toROSMsg(*transformed_scan_ptr, *tt);\n  tt->header.frame_id = \"velodyne\";\n  transform_pub.publish(tt);\n  tf::Matrix3x3 mat_b;\n  mat_b.setValue(static_cast<double>(t_base_link(0, 0)), static_cast<double>(t_base_link(0, 1)),\n                 static_cast<double>(t_base_link(0, 2)), static_cast<double>(t_base_link(1, 0)),\n                 static_cast<double>(t_base_link(1, 1)), static_cast<double>(t_base_link(1, 2)),\n                 static_cast<double>(t_base_link(2, 0)), static_cast<double>(t_base_link(2, 1)),\n                 static_cast<double>(t_base_link(2, 2)));\n\n  current_pose_.x = t_base_link(0, 3);current_pose_.y = t_base_link(1, 3);current_pose_.z = t_base_link(2, 3);\n  mat_b.getRPY(current_pose_.roll, current_pose_.pitch, current_pose_.yaw, 1);\n\n  transform.setOrigin(tf::Vector3(current_pose_.x, current_pose_.y, current_pose_.z));\n  q.setRPY(current_pose_.roll, current_pose_.pitch, current_pose_.yaw);\n  transform.setRotation(q);\n\n  br_.sendTransform(tf::StampedTransform(transform, input->header.stamp, \"velodyne\", \"base_link\"));\n\n  double shift = sqrt(pow(current_pose_.x - previous_pose_.x, 2.0) + pow(current_pose_.y - previous_pose_.y, 2.0));\n  if (shift >= min_add_scan_shift_)\n  {\n    map_ += *transformed_scan_ptr;\n    previous_pose_.x = current_pose_.x;previous_pose_.y = current_pose_.y;previous_pose_.z = current_pose_.z;\n    previous_pose_.roll = current_pose_.roll;previous_pose_.pitch = current_pose_.pitch;previous_pose_.yaw = current_pose_.yaw;\n    ndt.setInputTarget(map_ptr);\n\n    sensor_msgs::PointCloud2::Ptr map_msg_ptr(new sensor_msgs::PointCloud2);\n    pcl::toROSMsg(*map_ptr, *map_msg_ptr);\n    ndt_map_pub_.publish(*map_msg_ptr);\n  }\n\n  current_pose_msg_.header.frame_id = \"velodyne\";\n  current_pose_msg_.header.stamp = input->header.stamp;\n  current_pose_msg_.pose.position.x = current_pose_.x;current_pose_msg_.pose.position.y = current_pose_.y;current_pose_msg_.pose.position.z = current_pose_.z;\n  current_pose_msg_.pose.orientation.x = q.x();current_pose_msg_.pose.orientation.y = q.y();current_pose_msg_.pose.orientation.z = q.z();current_pose_msg_.pose.orientation.w = q.w();\n\n  current_pose_pub_.publish(current_pose_msg_);\n\n  std::cout << \"-----------------------------------------------------------------\" << std::endl;\n  std::cout << \"\u6784\u5efa\u5730\u56fe\" << std::endl;\n  std::cout << \"-----------------------------------------------------------------\" << std::endl;\n}\n\n\n\nndt_mapping::points_callback\u51fd\u6570\u5373\u8fdb\u884c\u6784\u56fe\uff0c\u5c06\u521d\u59cb\u5316\u70b9\u4e91\u52a0\u5165\u81f3\u5730\u56fe\uff0c\u82e5\u70b9\u4e91\u5730\u56fe\u6ca1\u6709\u521d\u59cb\u5316\u8f7d\u5165\uff0c\u5219\u5c06\u7b2c\u4e00\u5e27\u56fe\u50cf\u4f5c\u4e3a\u521d\u59cb\u56fe\u50cf\uff0c\u7136\u540e\u5c06\u914d\u51c6\u4e4b\u540e\u7684\u56fe\u50cf\u9010\u5e27\u52a0\u5165map\u3002\u901a\u8fc7tf_btol\u53d8\u6362\u77e9\u9635\u5c06\u539f\u59cb\u70b9\u4e91\u8fdb\u884c\u8f6c\u5316\u3002tf_btol\u662f\u8f66\u8f86\u5728\u8d77\u59cb\u4f4d\u7f6e\u662f\u4e0d\u5728\u5168\u5c40\u5730\u56fe\u539f\u70b9\u65f6\u7684\u53d8\u6362\u77e9\u9635\u3002\u7136\u540e\u5bf9\u539f\u59cb\u8f93\u5165\u70b9\u4e91\u8fdb\u884c\u4f53\u7d20\u8fc7\u6ee4\uff0c\u9009\u62e9\u4e0d\u540c\u7684\u65b9\u6cd5\u8fdb\u884c\u53c2\u6570\u8bbe\u7f6e\n\n\ninit_guess\u662fndt\u914d\u51c6\u65f6\u5019\u7684\u521d\u59cb\u4f4d\u7f6e\uff0c\u8be5\u4f4d\u7f6e\u4e00\u822c\u7531\u524d\u4e00\u5e27\u4f4d\u7f6e\u52a0\u4e0a\u5fae\u5c0f\u65f6\u95f4\u6bb5\u5185\u7684\u53d8\u5316\uff0c\u5f53\u91c7\u7528imu\u6216odom\u65f6\u53ef\u4ee5\u5229\u7528\u5176\u8fdb\u884c\u8f85\u52a9\u7cbe\u786e\u5b9a\u4f4d\u521d\u59cb\u4f4d\u7f6e\u3002\u5982\u679c\u672a\u4f7f\u7528imu\u4ee5\u53caodom\u5219\u4f7f\u7528\u539f\u6765\u7684init_guess\n\n\n\u70b9\u4e91\u805a\u7c7b\u529f\u80fd\n\n\n\u70b9\u4e91\u805a\u7c7b\u529f\u80fd\u5728\"lidar_demo/src/lidar_demo.cpp\"\u6587\u4ef6\n\n\nvoid callback(const boost::shared_ptr<const sensor_msgs::PointCloud2>& msg)\n{\n    ros::NodeHandle n;\n    n.getParam(\"Cluster_D\", Cluster_D);\n    n.getParam(\"Cluster_Max\", Cluster_Max);\n    n.getParam(\"Cluster_Min\", Cluster_Min);\n    pcl::PCLPointCloud2 pcl_pc2;\n    pcl_conversions::toPCL(*msg,pcl_pc2);\n    pcl::PointCloud<pcl::PointXYZ>::Ptr temp_cloud(new pcl::PointCloud<pcl::PointXYZ>);\n    pcl::fromPCLPointCloud2(pcl_pc2,*temp_cloud);\n\n    std::vector<int> mapping;\n    pcl::removeNaNFromPointCloud(*temp_cloud, *temp_cloud, mapping);\n    pub2.publish(temp_cloud);\n    pcl::PointIndices::Ptr inliers (new pcl::PointIndices);\n    pcl::ModelCoefficients::Ptr coefficients (new pcl::ModelCoefficients);\n    pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_plane (new pcl::PointCloud<pcl::PointXYZ> ());\n    pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_f (new pcl::PointCloud<pcl::PointXYZ>);\n\n    if (temp_cloud->points.size() == 0)\n    {\n        std::cout << \"cloud in ROI is empty\" << std::endl;\n        return;\n    }\n    pcl::search::KdTree<pcl::PointXYZ>::Ptr tree (new pcl::search::KdTree<pcl::PointXYZ>);\n    tree->setInputCloud (temp_cloud);\n\n    std::vector<pcl::PointIndices> cluster_indices;\n    pcl::EuclideanClusterExtraction<pcl::PointXYZ> ec;\n    cout << Cluster_D << \":\" << Cluster_Min << \":\" << Cluster_Max<<endl;\n    ec.setClusterTolerance (Cluster_D);\n\n    ec.setMinClusterSize (Cluster_Min);\n\n    ec.setMaxClusterSize (Cluster_Max);\n\n    ec.setSearchMethod (tree);\n    ec.setInputCloud (temp_cloud);\n    ec.extract (cluster_indices);\n\n    jsk_recognition_msgs::BoundingBoxArray BOXS;\n    int j = 0;\n    vector<Eigen::Vector3f> center;\n    for (std::vector<pcl::PointIndices>::const_iterator it = cluster_indices.begin (); it != cluster_indices.end (); ++it)\n    {\n        pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_cluster (new pcl::PointCloud<pcl::PointXYZ>);\n        for (std::vector<int>::const_iterator pit = it->indices.begin (); pit != it->indices.end (); ++pit)\n            cloud_cluster->push_back ((*temp_cloud)[*pit]); \n        cloud_cluster->width = cloud_cluster->size ();\n        cloud_cluster->height = 1;\n        cloud_cluster->is_dense = true;\n\n        std::cout << \"Cluster has : \" << cloud_cluster->size () << \" data points.\" << std::endl;\n        j++;\n        jsk_recognition_msgs::BoundingBox box2;\n\n        Eigen::Vector3f mass_center;\n        pcl::MomentOfInertiaEstimation<pcl::PointXYZ> feature_extractor;\n        feature_extractor.setInputCloud(cloud_cluster);\n        feature_extractor.compute();\n        pcl::PointXYZ min_point_OBB;\n        pcl::PointXYZ max_point_OBB;\n        pcl::PointXYZ position_OBB;\n        Eigen::Matrix3f rotational_matrix_OBB;\n        feature_extractor.getOBB(min_point_OBB, max_point_OBB, position_OBB, rotational_matrix_OBB);\n        Eigen::Quaternionf quat (rotational_matrix_OBB);\n        feature_extractor.getMassCenter (mass_center);\n        center.push_back(mass_center);\n\n        pcl::PointXYZ min;\n        pcl::PointXYZ max;\n        pcl::getMinMax3D(*cloud_cluster,min,max);\n        max.x = max.x;\n\n        box2.label = j+1;\n        box2.pose.position.x = (max.x + min.x)  / 2;\n        box2.pose.position.y = (max.y + min.y)  / 2;\n        box2.pose.position.z = (max.z + min.z)  / 2;\n        box2.dimensions.x = (max.x - min.x);\n        box2.dimensions.y = (max.y - min.y);\n        box2.dimensions.z = (max.z - min.z);\n        box2.header.frame_id = frame_id;\n\n        BOXS.boxes.push_back(box2);   \n    }\n    BOXS.header.frame_id = frame_id;\n    pub.publish(BOXS);\n}\n\n\n\n\n\u70b9\u4e91\u805a\u7c7b\u4e3b\u8981\u4f7f\u7528pcl\u5e93\u805a\u7c7b\u7b97\u6cd5\n\n\npcl::EuclideanClusterExtractionpcl::pointxyz ec;\n\n\n\u53ea\u9700\u8bbe\u5b9a\u53c2\u6570\uff1a\n\n\nCluster_D:\u805a\u7c7b\u76f4\u5f84\nCluster_Min:\u6700\u5c0f\u805a\u7c7b\u70b9\u6570\nCluster_Max:\u6700\u5927\u805a\u7c7b\u70b9\u6570\n\n\n\u70b9\u4e91\u667a\u80fd\u8bc6\u522b\u529f\u80fd\n\n\n\u4e3b\u8981\u4ee3\u7801\u5206\u5e03\u4f4d\u4e8elidar_cnn_seg_detect\uff0cdetected_objects_visualizer\uff0ccommon\uff0crockauto_msgs\u6587\u4ef6\u5939\u5185\n\n\n\u70b9\u4e91\u667a\u80fd\u8bc6\u522b\u529f\u80fd\uff0c\u91c7\u7528\u7684\u662fapolo\u7684\u795e\u7ecf\u7f51\u7edc\u8bc6\u522b\u7b97\u6cd5\uff0c\u53ef\u5230\u8be5\u7b97\u6cd5\u7684\n\u7ef4\u62a4\u5e73\u53f0\n\u8fdb\u884c\u6df1\u5165\u7814\u7a76\uff0c\u8be5\u7b97\u6cd5\u4e0evoxelnet\u7f51\u7edc\u76f8\u4f3c\uff0c\u4e5f\u53ef\u53c2\u8003\ngithub\n\n\n\u6211\u4eec\u5728\u667a\u80fd\u8bc6\u522b\u7684\u57fa\u7840\u4e0a\u6dfb\u52a0\u4e86\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\u663e\u793a\n\n\nVisualizeDetectedObjects::ObjectsToLabels(const rockauto_msgs::DetectedObjectArray &in_objects)\n{\n  visualization_msgs::MarkerArray label_markers;\n  for (auto const &object: in_objects.objects)\n  {\n    if (IsObjectValid(object))\n    {\n      visualization_msgs::Marker label_marker;\n\n      label_marker.lifetime = ros::Duration(marker_display_duration_);\n      label_marker.header = in_objects.header;\n      label_marker.ns = ros_namespace_ + \"/label_markers\";\n      label_marker.action = visualization_msgs::Marker::ADD;\n      label_marker.type = visualization_msgs::Marker::TEXT_VIEW_FACING;\n      label_marker.scale.x = 1.5;\n      label_marker.scale.y = 1.5;\n      label_marker.scale.z = 1.5;\n\n      label_marker.color = label_color_;\n\n      label_marker.id = marker_id_++;\n\n      if(!object.label.empty() && object.label != \"unknown\")\n        label_marker.text = object.label + \" \"; //Object Class if available\n      float velocity_x = 0.0, velocity_y = 0.0, a_x =0.0, a_y = 0.0;\n      if(past_x.size() ==0)\n      {\n        velocity_x = 0.0;\n        velocity_y = 0.0;\n      }\n      else{\n        int  before = 0;\n        float past_d =100.0;\n        for(int i=0;i<past_x.size();i++)\n        {\n          float d = sqrt(pow((past_x[i] - object.pose.position.x),2)+pow((past_y[i] - object.pose.position.y),2));\n          if(d < past_d)\n          {\n            past_d = d;\n            before = i;\n          }\n        }\n        velocity_x = (-past_x[before] + object.pose.position.x)/(0.1);\n        velocity_y = (-past_y[before] + object.pose.position.y)/(0.1);\n        a_x = (-past_vx[before] + velocity_x)/(0.1);\n        a_y = (-past_vy[before] + velocity_y)/(0.1);\n      }\n      std::stringstream distance_stream;\n      distance_stream << std::fixed << std::setprecision(1)\n                      << sqrt((object.pose.position.x * object.pose.position.x) +\n                                (object.pose.position.y * object.pose.position.y));\n\n      std::stringstream velocity_stream;\n      velocity_stream <<std::fixed<<std::setprecision(2)<<\"v_x:\"<<velocity_x <<\"m/s a_x:\"<<a_x<<\"m/s2\\n\" <<\"v_y:\"<<velocity_y<<\"m/s a_y:\"<<a_y<<\"m/s2\";\n      std::string distance_str = distance_stream.str() + \" m\\n\" + velocity_stream.str();\n      label_marker.text += distance_str;\n\n      if (object.velocity_reliable)\n      {\n        double velocity = object.velocity.linear.x;\n        if (velocity < -0.1)\n        {\n          velocity *= -1;\n        }\n\n        if (abs(velocity) < object_speed_threshold_)\n        {\n          velocity = 0.0;\n        }\n\n        tf::Quaternion q(object.pose.orientation.x, object.pose.orientation.y,\n                         object.pose.orientation.z, object.pose.orientation.w);\n\n        double roll, pitch, yaw;\n        tf::Matrix3x3(q).getRPY(roll, pitch, yaw);\n\n        // convert m/s to km/h\n        std::stringstream kmh_velocity_stream;\n        kmh_velocity_stream << std::fixed << std::setprecision(1) << (velocity * 3.6);\n        std::string text = \"\\n<\" + std::to_string(object.id) + \"> \" + kmh_velocity_stream.str() + \" km/h\";\n        label_marker.text += text;\n      }\n\n      label_marker.pose.position.x = object.pose.position.x;\n      label_marker.pose.position.y = object.pose.position.y;\n      label_marker.pose.position.z = label_height_;\n      label_marker.scale.z = 1.0;\n      if (!label_marker.text.empty())\n        label_markers.markers.push_back(label_marker);\n      past_x.push_back(object.pose.position.x);\n      past_y.push_back(object.pose.position.y);\n      past_vx.push_back(velocity_x);\n      past_vy.push_back(velocity_y);\n    }\n  }  // end in_objects.objects loop\n\n  return label_markers;\n}//ObjectsToLabels\n\n\n\n\u6ce8\u610f\u4e8b\u9879\n\n\n\n\n\u7a0b\u5e8f\u8981\u653e\u5728\u82f1\u6587\u8def\u5f84\u4e0b\uff0c\u6700\u597d\u4e0d\u8981\u653e\u5728\u4e2d\u6587\u76ee\u5f55\u4e0b\uff0c\u56e0\u4e3a\u5bb9\u6613\u51fa\u73b0\u4e2d\u6587\u4e71\u7801\u95ee\u9898\u800c\u5f71\u54cd\u7a0b\u5e8f\u8fd0\u884c",
            "title": "\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#v10",
            "text": "",
            "title": "\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50\u5c01\u88c5\uff561.0"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#_1",
            "text": "\u9002\u7528\u4e8eAGX\u63a7\u5236\u5668  velodyne\u6fc0\u5149\u96f7\u8fbe\uff0c \u6ce8\uff1a\u5176\u4ed6\u54c1\u724c\u96f7\u8fbe\u53ea\u9700\u66f4\u6362\u9a71\u52a8\u548c\u5bf9\u5e94\u7684\u70b9\u4e91topic",
            "title": "\u786c\u4ef6\u73af\u5883\u7ec4\u6210"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#_2",
            "text": "",
            "title": "\u8f6f\u4ef6\u73af\u5883\u90e8\u7f72"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#ubuntu1804",
            "text": "\u6839\u636eAGX\u63d0\u4f9b\u7684\u7cfb\u7edf\u5305Jetpack\u5b89\u88c5",
            "title": "ubuntu18.04\u64cd\u4f5c\u7cfb\u7edf"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#cudacudnn",
            "text": "\u901a\u8fc7Jetpack\u5b89\u88c5",
            "title": "Cuda\u548ccudnn\u5b89\u88c5"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#ros",
            "text": "\u4e0b\u8f7d  git clone https://github.com/jetsonhacks/installROSXavier.git  \u5207\u6362\u76ee\u5f55  cd installROSXavier  \u5b89\u88c5  ./installROS.sh -p ros-melodic-desktop -p ros-melodic-rgbd-launch  \u63d2\u4ef6\u5b89\u88c5  sudo apt-get install ros-melodic-jsk-rviz-plugins",
            "title": "ROS\u5b89\u88c5"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#caffe",
            "text": "\u4e0b\u8f7d  git clone https://github.com/BVLC/caffe.git  \u5b89\u88c5\u4f9d\u8d56  sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler  sudo apt-get install --no-install-recommends libboost-all-dev  sudo apt-get install libopenblas-dev  sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev  \u5207\u6362\u76ee\u5f55  cd caffe  \u65b0\u5efaMakefile.config\u6587\u4ef6 ,\u5e76\u7c98\u8d34\u4e0b\u9762\u7684\u5185\u5bb9   ```\n    \uff41## Refer to http://caffe.berkeleyvision.org/installation.html\n    # Contributions simplifying and improving our build system are welcome!",
            "title": "caffe \u90e8\u7f72"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#cudnn-acceleration-switch-uncomment-to-build-with-cudnn",
            "text": "",
            "title": "cuDNN acceleration switch (uncomment to build with cuDNN)."
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#use_cudnn-1",
            "text": "",
            "title": "USE_CUDNN := 1"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#cpu-only-switch-uncomment-to-build-without-gpu-support",
            "text": "",
            "title": "CPU-only switch (uncomment to build without GPU support)."
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#cpu_only-1",
            "text": "",
            "title": "CPU_ONLY := 1"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#uncomment-to-disable-io-dependencies-and-corresponding-data-layers",
            "text": "",
            "title": "uncomment to disable IO dependencies and corresponding data layers"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#use_opencv-0",
            "text": "",
            "title": "USE_OPENCV := 0"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#use_leveldb-0",
            "text": "",
            "title": "USE_LEVELDB := 0"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#use_lmdb-0",
            "text": "",
            "title": "USE_LMDB := 0"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#uncomment-to-allow-mdb_nolock-when-reading-lmdb-files-only-if-necessary",
            "text": "",
            "title": "uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#you-should-not-set-this-flag-if-you-will-be-reading-lmdbs-with-any",
            "text": "",
            "title": "You should not set this flag if you will be reading LMDBs with any"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#possibility-of-simultaneous-read-and-write",
            "text": "",
            "title": "possibility of simultaneous read and write"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#allow_lmdb_nolock-1",
            "text": "",
            "title": "ALLOW_LMDB_NOLOCK := 1"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#uncomment-if-youre-using-opencv-3",
            "text": "OPENCV_VERSION := 3",
            "title": "Uncomment if you're using OpenCV 3"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#to-customize-your-choice-of-compiler-uncomment-and-set-the-following",
            "text": "",
            "title": "To customize your choice of compiler, uncomment and set the following."
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#nb-the-default-for-linux-is-g-and-the-default-for-osx-is-clang",
            "text": "",
            "title": "N.B. the default for Linux is g++ and the default for OSX is clang++"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#custom_cxx-g",
            "text": "",
            "title": "CUSTOM_CXX := g++"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#cuda-directory-contains-bin-and-lib-directories-that-we-need",
            "text": "CUDA_DIR := /usr/local/cuda",
            "title": "CUDA directory contains bin/ and lib/ directories that we need."
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#on-ubuntu-1404-if-cuda-tools-are-installed-via",
            "text": "",
            "title": "On Ubuntu 14.04, if cuda tools are installed via"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#sudo-apt-get-install-nvidia-cuda-toolkit-then-use-this-instead",
            "text": "",
            "title": "\"sudo apt-get install nvidia-cuda-toolkit\" then use this instead:"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#cuda_dir-usr",
            "text": "",
            "title": "CUDA_DIR := /usr"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#cuda-architecture-setting-going-with-all-of-them",
            "text": "",
            "title": "CUDA architecture setting: going with all of them."
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#for-cuda-60-comment-the-lines-after-_35-for-compatibility",
            "text": "CUDA_ARCH := -gencode arch=compute_30,code=sm_30 \\\n             -gencode arch=compute_35,code=sm_35 \\\n             -gencode arch=compute_50,code=sm_50 \\\n             -gencode arch=compute_52,code=sm_52 \\\n             -gencode arch=compute_61,code=sm_61 \\\n             -gencode arch=compute_61,code=compute_61           # -gencode arch=compute_20,code=sm_20 \\\n         # -gencode arch=compute_20,code=sm_21 \\",
            "title": "For CUDA &lt; 6.0, comment the lines after *_35 for compatibility."
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#blas-choice",
            "text": "",
            "title": "BLAS choice:"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#atlas-for-atlas-default",
            "text": "",
            "title": "atlas for ATLAS (default)"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#mkl-for-mkl",
            "text": "",
            "title": "mkl for MKL"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#open-for-openblas",
            "text": "",
            "title": "open for OpenBlas"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#blas-atlas",
            "text": "BLAS := open",
            "title": "BLAS := atlas"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#custom-mklatlasopenblas-include-and-lib-directories",
            "text": "",
            "title": "Custom (MKL/ATLAS/OpenBLAS) include and lib directories."
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#leave-commented-to-accept-the-defaults-for-your-choice-of-blas",
            "text": "",
            "title": "Leave commented to accept the defaults for your choice of BLAS"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#which-should-work",
            "text": "",
            "title": "(which should work)!"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#blas_include-pathtoyourblas",
            "text": "",
            "title": "BLAS_INCLUDE := /path/to/your/blas"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#blas_lib-pathtoyourblas",
            "text": "",
            "title": "BLAS_LIB := /path/to/your/blas"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#homebrew-puts-openblas-in-a-directory-that-is-not-on-the-standard-search-path",
            "text": "",
            "title": "Homebrew puts openblas in a directory that is not on the standard search path"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#blas_include-shell-brew-prefix-openblasinclude",
            "text": "",
            "title": "BLAS_INCLUDE := $(shell brew --prefix openblas)/include"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#blas_lib-shell-brew-prefix-openblaslib",
            "text": "",
            "title": "BLAS_LIB := $(shell brew --prefix openblas)/lib"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#this-is-required-only-if-you-will-compile-the-matlab-interface",
            "text": "",
            "title": "This is required only if you will compile the matlab interface."
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#matlab-directory-should-contain-the-mex-binary-in-bin",
            "text": "",
            "title": "MATLAB directory should contain the mex binary in /bin."
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#matlab_dir-usrlocal",
            "text": "",
            "title": "MATLAB_DIR := /usr/local"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#matlab_dir-applicationsmatlab_r2012bapp",
            "text": "",
            "title": "MATLAB_DIR := /Applications/MATLAB_R2012b.app"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#note-this-is-required-only-if-you-will-compile-the-python-interface",
            "text": "",
            "title": "NOTE: this is required only if you will compile the python interface."
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#we-need-to-be-able-to-find-pythonh-and-numpyarrayobjecth",
            "text": "PYTHON_INCLUDE := /usr/include/python2.7 \\\n        /usr/lib/python2.7/dist-packages/numpy/core/include",
            "title": "We need to be able to find Python.h and numpy/arrayobject.h."
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#anaconda-python-distribution-is-quite-popular-include-path",
            "text": "",
            "title": "Anaconda Python distribution is quite popular. Include path:"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#verify-anaconda-location-sometimes-its-in-root",
            "text": "",
            "title": "Verify anaconda location, sometimes it's in root."
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#anaconda_home-homeanaconda2",
            "text": "# PYTHON_INCLUDE := $(ANACONDA_HOME)/include \\\n        $(ANACONDA_HOME)/include/python2.7 \\\n        $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include \\",
            "title": "ANACONDA_HOME := $(HOME)/anaconda2"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#uncomment-to-use-python-3-default-is-python-2",
            "text": "",
            "title": "Uncomment to use Python 3 (default is Python 2)"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#python_libraries-boost_python3-python35m",
            "text": "# PYTHON_INCLUDE := /usr/include/python3.5m \\",
            "title": "PYTHON_LIBRARIES := boost_python3 python3.5m"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#usrlibpython35dist-packagesnumpycoreinclude",
            "text": "",
            "title": "/usr/lib/python3.5/dist-packages/numpy/core/include"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#we-need-to-be-able-to-find-libpythonxxso-or-dylib",
            "text": "PYTHON_LIB := /usr/lib",
            "title": "We need to be able to find libpythonX.X.so or .dylib."
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#python_lib-anaconda_homelib",
            "text": "",
            "title": "PYTHON_LIB := $(ANACONDA_HOME)/lib"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#homebrew-installs-numpy-in-a-non-standard-path-keg-only",
            "text": "",
            "title": "Homebrew installs numpy in a non standard path (keg only)"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#python_include-dir-shell-python-c-import-numpycore-printnumpycorefileinclude",
            "text": "",
            "title": "PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.file)'))/include"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#python_lib-shell-brew-prefix-numpylib",
            "text": "",
            "title": "PYTHON_LIB += $(shell brew --prefix numpy)/lib"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#uncomment-to-support-layers-written-in-python-will-link-against-python-libs",
            "text": "",
            "title": "Uncomment to support layers written in Python (will link against Python libs)"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#with_python_layer-1",
            "text": "",
            "title": "WITH_PYTHON_LAYER := 1"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#whatever-else-you-find-you-need-goes-here",
            "text": "INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial/\nLIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib  /usr/lib/aarch64-linux-gnu/hdf5/serial",
            "title": "Whatever else you find you need goes here."
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#if-homebrew-is-installed-at-a-non-standard-location-for-example-your-home-directory-and-you-use-it-for-general-dependencies",
            "text": "",
            "title": "If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#include_dirs-shell-brew-prefixinclude",
            "text": "",
            "title": "INCLUDE_DIRS += $(shell brew --prefix)/include"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#library_dirs-shell-brew-prefixlib",
            "text": "",
            "title": "LIBRARY_DIRS += $(shell brew --prefix)/lib"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#uncomment-to-use-pkg-config-to-specify-opencv-library-paths",
            "text": "",
            "title": "Uncomment to use pkg-config to specify OpenCV library paths."
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#usually-not-necessary-opencv-libraries-are-normally-installed-in-one-of-the-above-library_dirs",
            "text": "",
            "title": "(Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#use_pkg_config-1",
            "text": "",
            "title": "USE_PKG_CONFIG := 1"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#nb-both-build-and-distribute-dirs-are-cleared-on-make-clean",
            "text": "BUILD_DIR := build\nDISTRIBUTE_DIR := distribute",
            "title": "N.B. both build and distribute dirs are cleared on make clean"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#uncomment-for-debugging-does-not-work-on-osx-due-to-httpsgithubcombvlccaffeissues171",
            "text": "",
            "title": "Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#debug-1",
            "text": "",
            "title": "DEBUG := 1"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#the-id-of-the-gpu-that-make-runtest-will-use-to-run-unit-tests",
            "text": "TEST_GPUID := 0",
            "title": "The ID of the GPU that 'make runtest' will use to run unit tests."
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#enable-pretty-build-comment-to-see-full-commands",
            "text": "Q ?= @\n```\n* \u7f16\u8bd1\n  * make -j 8\n  * sudo make distribute",
            "title": "enable pretty build (comment to see full commands)"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#qt",
            "text": "sudo apt-get install qttools5-dev-tools libqt5svg5-dev qtmultimedia5-dev",
            "title": "qt\u5b89\u88c5"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#_3",
            "text": "\u70b9\u4e91\u6807\u5b9a\u529f\u80fd  \u70b9\u4e91\u5efa\u56fe\u529f\u80fd  \u70b9\u4e91\u805a\u7c7b\u529f\u80fd  \u70b9\u4e91\u667a\u80fd\u8bc6\u522b\u529f\u80fd   \u7a0b\u5e8f\u6587\u4ef6\u5939\u76ee\u5f55\u7ed3\u6784   \u7a0b\u5e8f  catkin_ws  src  common  detected_objects_visualizer  lidar_cnn_seg_detect  lidar_demo  ndt_mapping  rockauto_msgs  ros2qt  velodyne  CMakeLists.txt     \u6211\u4eec\u9700\u8981\u628acatkin_ws\u6587\u4ef6\u5939\u62f7\u8d1d\u5230\u5de5\u63a7\u673a\u7684home\u76ee\u5f55\u4e0b\u3002",
            "title": "\u7b97\u6cd5\u76d2\u5b50\u7ec4\u6210\u90e8\u5206"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#_4",
            "text": "\u5207\u6362\u76ee\u5f55  cd catkin_ws  \u7f16\u8bd1  catkin_make  source \u73af\u5883  source devel/setup.bash",
            "title": "\u7f16\u8bd1\u7a0b\u5e8f"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#_5",
            "text": "rosrun ros2qt ros2qt",
            "title": "\u7a0b\u5e8f\u542f\u52a8"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#_6",
            "text": "\u7a0b\u5e8f\u542f\u52a8\u540e\u5f39\u51fa\u4e0b\u56fe\u754c\u9762     \u5982\u4e0b\u56fe\u914d\u7f6e Fixed Frame  \u4e3a velodyne ,\u52fe\u9009 pointCloud2 \u590d\u9009\u6846\uff0c\u8bbe\u7f6e\u5176 Topic \u4e3a point_raw \u53ef\u89c2\u770b\u96f7\u8fbe\u6570\u636e     \u542f\u7528\uff33\uff2c\uff21\uff2d\u529f\u80fd\uff0c\u5219\u52fe\u9009 SLAM \u590d\u9009\u6846,\u5c06\u5176 Topic \u8bbe\u7f6e\u4e3a point_raw \uff0c\u540c\u65f6\u5c06 pointCloud2 \u7684 Topic \u8bbe\u7f6e\u4e3a cloud \u3002     Slam\u63d0\u4f9b\u4e86\u53ef\u8c03\u53c2\u6570\uff1a\n  Topic:\u8bbe\u7f6e\u6784\u56fe\u4e3b\u9898\n  Size:\u8bbe\u7f6e\u70b9\u4e91\u5730\u56fe\u4e2d\u70b9\u7684\u663e\u793a\u5927\u5c0f\n  Color:\u70b9\u4e91\u5730\u56fe\u7684\u989c\u8272\n  Res:\u5206\u8fa8\u7387\n  Step_size:\u6b65\u5e45\n  Trans_epsilon:\u6536\u655b\u5747\u65b9\u5dee\n  Max_iter:\u6700\u5927\u8fed\u4ee3\u6b21\u6570\n  Voxel_leaf_size:Voxel\u5c3a\u5bf8\n  Min_scan_range:\u6700\u5c0f\u626b\u63cf\u8303\u56f4\n  Max_scan_range:\u6700\u5927\u626b\u63cf\u8303\u56f4\n  Scan_rate:\u626b\u63cf\u5468\u671f     \u663e\u793a\u969c\u788d\u7269\uff0c\u5219\u53d6\u6d88 SLAM \u52fe\u9009\uff0c\u5c06 pointCloud2 \u7684 Topic \u66f4\u6539\u56de point_raw .\u52fe\u9009 \uff22oundingBox \u590d\u9009\u6846\u3002    \u663e\u793a\u4f4d\u7f6e\uff0c\u901f\u5ea6\uff0c\u52a0\u901f\u5ea6\u548c\u8def\u5f84\n\u663e\u793a\u805a\u7c7b\u969c\u788d\u7269\uff0c\u5219\u52fe\u9009'BoundingBox2'\u590d\u9009\u6846\u3002   \u8fd9\u91cc\u67093\u4e2a\u53c2\u6570:\nCluster_D:\u805a\u7c7b\u76f4\u5f84\nCluster_Min:\u6700\u5c0f\u805a\u7c7b\u70b9\u6570\nCluster_Max:\u6700\u5927\u805a\u7c7b\u70b9\u6570   \u70b9\u4e91\u6807\u5b9a\n  \u9996\u5148\u8981\u52fe\u9009 PointCloud2 \u663e\u793a\u70b9\u4e91\uff0c\u518d\u52fe\u9009 \u6807\u5b9a \u53ef\u79fb\u52a8xyz\u4e09\u8f74\uff0c\u4e5f\u53ef\u7ed5\u4e09\u8f74\u65cb\u8f6c\u3002",
            "title": "\u5e94\u7528\u8bb2\u89e3"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#_7",
            "text": "",
            "title": "\u7a0b\u5e8f\u8bb2\u89e3"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#_8",
            "text": "\u6807\u5b9a\u7a0b\u5e8f\u529f\u80fd\u4ee3\u7801\u6bb5\u4f4d\u4e8e\"ros2qt\\src\\qnode.cpp\"\u6587\u4ef6  void QNode::biaodingCallback(const PointCloud::ConstPtr& msg)\n{\n    //\u70b9\u4e91\u65cb\u8f6c\n    Eigen::Affine3f transform_2 = Eigen::Affine3f::Identity();\n    transform_2.translation() << x_pianyi, y_pianyi, z_pianyi;\n    float theta_2 = 0*M_PI/180;\n    transform_2.rotate (Eigen::AngleAxisf (theta_2, Eigen::Vector3f::UnitY()));\n    pcl::PointCloud<pcl::PointXYZ>::Ptr transformed_cloud_2 (new pcl::PointCloud<pcl::PointXYZ> ());\n    pcl::transformPointCloud (*msg, *transformed_cloud_2, transform_2);\n\n    Eigen::Affine3f transform = Eigen::Affine3f::Identity();\n    transform.translation() << 0.0, 0.0, 0.0;\n    float theta = x_xuanzhuan*M_PI/180;\n    transform.rotate (Eigen::AngleAxisf (theta, Eigen::Vector3f::UnitX()));\n    pcl::PointCloud<pcl::PointXYZ>::Ptr transformed_cloud (new pcl::PointCloud<pcl::PointXYZ> ());\n    pcl::transformPointCloud (*transformed_cloud_2, *transformed_cloud, transform);\n\n    Eigen::Affine3f transform_3 = Eigen::Affine3f::Identity();\n    transform_3.translation() << 0.0, 0.0, 0.0;\n    float theta_3 = z_xuanzhuan*M_PI/180;\n    transform_3.rotate (Eigen::AngleAxisf (theta_3, Eigen::Vector3f::UnitZ()));\n    pcl::PointCloud<pcl::PointXYZ>::Ptr transformed_cloud_3 (new pcl::PointCloud<pcl::PointXYZ> ());\n    pcl::transformPointCloud (*transformed_cloud, *transformed_cloud_3, transform_3);\n\n    Eigen::Affine3f transform_4 = Eigen::Affine3f::Identity();\n    transform_4.translation() << 0.0, 0.0, 0.0;\n    float theta_4 = y_xuanzhuan*M_PI/180;\n    transform_4.rotate (Eigen::AngleAxisf (theta_4, Eigen::Vector3f::UnitY()));\n    pcl::PointCloud<pcl::PointXYZ>::Ptr transformed_cloud_4 (new pcl::PointCloud<pcl::PointXYZ> ());\n    pcl::transformPointCloud (*transformed_cloud_3, *transformed_cloud_4, transform_4);\n    transformed_cloud_4->header.frame_id = frame_id;\n    pub.publish(transformed_cloud_4);\n}  \u70b9\u4e91\u7684\u6807\u5b9a\uff0c\u4e3b\u8981\u662f\u70b9\u4e91\u7684\u65cb\u8f6c\u548c\u5e73\u79fb\u529f\u80fd\uff0c\u53ef\u4ee5\u53c2\u8003pcl\uff0c\u8fd9\u91cc\u7b80\u5355\u8bb2\u89e3\u4e00\u4e0b\u65cb\u8f6c\u5e73\u79fb\u529f\u80fd   \u9996\u5148\u5b9a\u4e49\u4e00\u4e2aAffine3f\u7ed3\u6784\u4f53\uff0c\u7528\u6765\u5b58\u50a8\u70b9\u4e91\u65cb\u8f6c\u548c\u5e73\u79fb\u4fe1\u606f   Eigen::Affine3f transform_2 = Eigen::Affine3f::Identity();   \u518d\u628a\uff58\uff0c\uff59\uff0c\uff5a\u8f74\u7684\u5e73\u79fb\u91cfx_pianyi, y_pianyi, z_pianyi\u4f20\u9012\u8fdbAffine3f\u7ed3\u6784\u4f53   transform_2.translation() << x_pianyi, y_pianyi, z_pianyi;   \u5b9a\u4e49\u4e00\u4e2a\u65cb\u8f6c\u89d2\u5ea6   float theta_2 = 0*M_PI/180;   \u628a\u7ed5\uff39\u8f74\u65cb\u8f6c\u89d2\u5ea6\uff0c\u4f20\u5165Affin3f\u7ed3\u6784\u4f53\uff0c\u65cb\u8f6c\u8f74Eigen::Vector3f::UnitY()   transform_2.rotate (Eigen::AngleAxisf (theta_2, Eigen::Vector3f::UnitY()));   \u5b9a\u4e49\u65cb\u8f6c\u540e\u7684\u70b9\u4e91   pcl::PointCloudpcl::pointxyz::Ptr transformed_cloud_2 (new pcl::PointCloudpcl::pointxyz ());   \u6267\u884c\u70b9\u4e91\u7684\u5e73\u79fb\u548c\u65cb\u8f6c\uff0c\u5e76\u628a\u7ed3\u679c\u5b58\u5165 transformed_cloud_2   pcl::transformPointCloud (*msg, *transformed_cloud_2, transform_2);",
            "title": "\u70b9\u4e91\u6807\u5b9a\u529f\u80fd"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#_9",
            "text": "\u5efa\u56fe\u7a0b\u5e8f\u5305\u5373ndt_mapping\u6587\u4ef6\u5939,\u91cd\u70b9\u8fd0\u7b97\u903b\u8f91\u5728ndt_mapping.cpp\u6587\u4ef6\u5185\uff0c\u8fd9\u91cc\u89e3\u6790\u4e00\u4e0b  ndt_mapping::ndt_mapping() \n{\n  transform_pub = nh_.advertise<sensor_msgs::PointCloud2> (\"/cloud\", 1, false);\n  points_sub_ = nh_.subscribe(slam_topic, 100000, &ndt_mapping::points_callback,this);\n  ndt_map_pub_ = nh_.advertise<sensor_msgs::PointCloud2>(\"/ndt_map\", 1000);\n  current_pose_pub_ = nh_.advertise<geometry_msgs::PoseStamped>(\"/current_pose\", 1000);\n\n  max_iter_ = max_iter_1;\n  ndt_res_ = ndt_res_1;\n  step_size_ = step_size_1;\n  trans_eps_ = trans_eps_1;\n  voxel_leaf_size_ = voxel_leaf_size_1;\n  scan_rate_ = scan_rate_1;\n  min_scan_range_ = min_scan_range_1;\n  max_scan_range_ = max_scan_range_1;\n  min_add_scan_shift_ = min_add_scan_shift_1;\n\n\n  initial_scan_loaded = 0;\n  min_add_scan_shift_ = 1.0;\n\n  _tf_x=0.0, _tf_y=0.0, _tf_z=0.0, _tf_roll=0.0, _tf_pitch=0.0, _tf_yaw=0.0;\n\n  Eigen::Translation3f tl_btol(_tf_x, _tf_y, _tf_z);        \n  Eigen::AngleAxisf rot_x_btol(_tf_roll, Eigen::Vector3f::UnitX());  \n  Eigen::AngleAxisf rot_y_btol(_tf_pitch, Eigen::Vector3f::UnitY());\n  Eigen::AngleAxisf rot_z_btol(_tf_yaw, Eigen::Vector3f::UnitZ());\n  tf_btol_ = (tl_btol * rot_z_btol * rot_y_btol * rot_x_btol).matrix();\n  tf_ltob_ = tf_btol_.inverse();\n\n  map_.header.frame_id = \"velodyne\";\n\n  current_pose_.x = current_pose_.y = current_pose_.z = 0.0;current_pose_.roll = current_pose_.pitch = current_pose_.yaw = 0.0;\n  previous_pose_.x = previous_pose_.y = previous_pose_.z = 0.0;previous_pose_.roll = previous_pose_.pitch = previous_pose_.yaw = 0.0;\n\n  voxel_grid_filter_.setLeafSize(voxel_leaf_size_, voxel_leaf_size_, voxel_leaf_size_);  \n\n  ndt.setTransformationEpsilon(trans_eps_);\n  ndt.setStepSize(step_size_);\n  ndt.setResolution(ndt_res_);\n  ndt.setMaximumIterations(max_iter_);\n\n  is_first_map_ = true;\n};   ndt_mapping::ndt_mapping()\u51fd\u6570\uff0c\u4e3b\u8981\u8fdb\u884c\u4e86\u4e00\u4e9b\u53c2\u6570\u8bbe\u5b9a\uff0c\u548ctopic\u8bbe\u5b9a\u3002  void ndt_mapping::points_callback(const sensor_msgs::PointCloud2::ConstPtr& input)\n{\n  pcl::PointCloud<pcl::PointXYZI> tmp, scan;\n  pcl::PointCloud<pcl::PointXYZI>::Ptr filtered_scan_ptr(new pcl::PointCloud<pcl::PointXYZI>());\n  pcl::PointCloud<pcl::PointXYZI>::Ptr transformed_scan_ptr(new pcl::PointCloud<pcl::PointXYZI>());\n\n  tf::Quaternion q;\n\n  Eigen::Matrix4f t_localizer(Eigen::Matrix4f::Identity());\n  Eigen::Matrix4f t_base_link(Eigen::Matrix4f::Identity());\n  static tf::TransformBroadcaster br_;\n  tf::Transform transform;\n\n  pcl::fromROSMsg(*input, tmp);\n  double r;\n  Eigen::Vector3d point_pos;\n  pcl::PointXYZI p;\n  for (pcl::PointCloud<pcl::PointXYZI>::const_iterator item = tmp.begin(); item != tmp.end(); item++)\n  {\n    use_imu_ = false;\n    if(use_imu_){\n      // deskew(TODO:inplement of predicting pose by imu)\n      point_pos.x() = (double)item->x;\n      point_pos.y() = (double)item->y;\n      point_pos.z() = (double)item->z;\n      double s = scan_rate_ * (double(item->intensity) - int(item->intensity));\n\n      point_pos.x() -= s * current_pose_msg_.pose.position.x;//current_pose_imu_\n      point_pos.y() -= s * current_pose_msg_.pose.position.y;\n      point_pos.z() -= s * current_pose_msg_.pose.position.z;\n\n      Eigen::Quaterniond start_quat, end_quat, mid_quat;\n      mid_quat.setIdentity();\n      end_quat = Eigen::Quaterniond(\n        current_pose_msg_.pose.orientation.w,\n        current_pose_msg_.pose.orientation.x,\n        current_pose_msg_.pose.orientation.y,\n        current_pose_msg_.pose.orientation.z);\n      start_quat = mid_quat.slerp(s, end_quat);\n\n      point_pos = start_quat.conjugate() * start_quat * point_pos;\n\n      point_pos.x() += current_pose_msg_.pose.position.x;\n      point_pos.y() += current_pose_msg_.pose.position.y;\n      point_pos.z() += current_pose_msg_.pose.position.z;\n\n      p.x = point_pos.x();\n      p.y = point_pos.y();\n      p.z = point_pos.z();\n    }\n    else{\n      p.x = (double)item->x;\n      p.y = (double)item->y;\n      p.z = (double)item->z;\n    }\n    p.intensity = (double)item->intensity;\n    r = sqrt(pow(p.x, 2.0) + pow(p.y, 2.0));\n    if (min_scan_range_ < r && r < max_scan_range_)\n    {\n      scan.push_back(p);\n    }\n  }\n\n  pcl::PointCloud<pcl::PointXYZI>::Ptr scan_ptr(new pcl::PointCloud<pcl::PointXYZI>(scan));\n  if (initial_scan_loaded == 0)\n  {\n    pcl::transformPointCloud(*scan_ptr, *transformed_scan_ptr, tf_btol_);\n    map_ += *transformed_scan_ptr;\n    initial_scan_loaded = 1;\n  }\n\n  voxel_grid_filter_.setInputCloud(scan_ptr);\n  voxel_grid_filter_.filter(*filtered_scan_ptr);\n  ndt.setInputSource(filtered_scan_ptr);\n\n  pcl::PointCloud<pcl::PointXYZI>::Ptr map_ptr(new pcl::PointCloud<pcl::PointXYZI>(map_));\n  if (is_first_map_ == true){\n    ndt.setInputTarget(map_ptr);\n    is_first_map_ = false;\n  }\n\n  Eigen::Translation3f init_translation(current_pose_.x, current_pose_.y, current_pose_.z);\n  Eigen::AngleAxisf init_rotation_x(current_pose_.roll, Eigen::Vector3f::UnitX());\n  Eigen::AngleAxisf init_rotation_y(current_pose_.pitch, Eigen::Vector3f::UnitY());\n  Eigen::AngleAxisf init_rotation_z(current_pose_.yaw, Eigen::Vector3f::UnitZ());\n\n  Eigen::Matrix4f init_guess =\n      (init_translation * init_rotation_z * init_rotation_y * init_rotation_x).matrix() * tf_btol_;\n\n  pcl::PointCloud<pcl::PointXYZI>::Ptr output_cloud(new pcl::PointCloud<pcl::PointXYZI>);\n\n  ndt.align(*output_cloud, init_guess);\n  t_localizer = ndt.getFinalTransformation();\n\n  t_base_link = t_localizer * tf_ltob_;\n\n  pcl::transformPointCloud(*scan_ptr, *transformed_scan_ptr, t_localizer);\n  sensor_msgs::PointCloud2::Ptr tt(new sensor_msgs::PointCloud2);\n  pcl::toROSMsg(*transformed_scan_ptr, *tt);\n  tt->header.frame_id = \"velodyne\";\n  transform_pub.publish(tt);\n  tf::Matrix3x3 mat_b;\n  mat_b.setValue(static_cast<double>(t_base_link(0, 0)), static_cast<double>(t_base_link(0, 1)),\n                 static_cast<double>(t_base_link(0, 2)), static_cast<double>(t_base_link(1, 0)),\n                 static_cast<double>(t_base_link(1, 1)), static_cast<double>(t_base_link(1, 2)),\n                 static_cast<double>(t_base_link(2, 0)), static_cast<double>(t_base_link(2, 1)),\n                 static_cast<double>(t_base_link(2, 2)));\n\n  current_pose_.x = t_base_link(0, 3);current_pose_.y = t_base_link(1, 3);current_pose_.z = t_base_link(2, 3);\n  mat_b.getRPY(current_pose_.roll, current_pose_.pitch, current_pose_.yaw, 1);\n\n  transform.setOrigin(tf::Vector3(current_pose_.x, current_pose_.y, current_pose_.z));\n  q.setRPY(current_pose_.roll, current_pose_.pitch, current_pose_.yaw);\n  transform.setRotation(q);\n\n  br_.sendTransform(tf::StampedTransform(transform, input->header.stamp, \"velodyne\", \"base_link\"));\n\n  double shift = sqrt(pow(current_pose_.x - previous_pose_.x, 2.0) + pow(current_pose_.y - previous_pose_.y, 2.0));\n  if (shift >= min_add_scan_shift_)\n  {\n    map_ += *transformed_scan_ptr;\n    previous_pose_.x = current_pose_.x;previous_pose_.y = current_pose_.y;previous_pose_.z = current_pose_.z;\n    previous_pose_.roll = current_pose_.roll;previous_pose_.pitch = current_pose_.pitch;previous_pose_.yaw = current_pose_.yaw;\n    ndt.setInputTarget(map_ptr);\n\n    sensor_msgs::PointCloud2::Ptr map_msg_ptr(new sensor_msgs::PointCloud2);\n    pcl::toROSMsg(*map_ptr, *map_msg_ptr);\n    ndt_map_pub_.publish(*map_msg_ptr);\n  }\n\n  current_pose_msg_.header.frame_id = \"velodyne\";\n  current_pose_msg_.header.stamp = input->header.stamp;\n  current_pose_msg_.pose.position.x = current_pose_.x;current_pose_msg_.pose.position.y = current_pose_.y;current_pose_msg_.pose.position.z = current_pose_.z;\n  current_pose_msg_.pose.orientation.x = q.x();current_pose_msg_.pose.orientation.y = q.y();current_pose_msg_.pose.orientation.z = q.z();current_pose_msg_.pose.orientation.w = q.w();\n\n  current_pose_pub_.publish(current_pose_msg_);\n\n  std::cout << \"-----------------------------------------------------------------\" << std::endl;\n  std::cout << \"\u6784\u5efa\u5730\u56fe\" << std::endl;\n  std::cout << \"-----------------------------------------------------------------\" << std::endl;\n}  ndt_mapping::points_callback\u51fd\u6570\u5373\u8fdb\u884c\u6784\u56fe\uff0c\u5c06\u521d\u59cb\u5316\u70b9\u4e91\u52a0\u5165\u81f3\u5730\u56fe\uff0c\u82e5\u70b9\u4e91\u5730\u56fe\u6ca1\u6709\u521d\u59cb\u5316\u8f7d\u5165\uff0c\u5219\u5c06\u7b2c\u4e00\u5e27\u56fe\u50cf\u4f5c\u4e3a\u521d\u59cb\u56fe\u50cf\uff0c\u7136\u540e\u5c06\u914d\u51c6\u4e4b\u540e\u7684\u56fe\u50cf\u9010\u5e27\u52a0\u5165map\u3002\u901a\u8fc7tf_btol\u53d8\u6362\u77e9\u9635\u5c06\u539f\u59cb\u70b9\u4e91\u8fdb\u884c\u8f6c\u5316\u3002tf_btol\u662f\u8f66\u8f86\u5728\u8d77\u59cb\u4f4d\u7f6e\u662f\u4e0d\u5728\u5168\u5c40\u5730\u56fe\u539f\u70b9\u65f6\u7684\u53d8\u6362\u77e9\u9635\u3002\u7136\u540e\u5bf9\u539f\u59cb\u8f93\u5165\u70b9\u4e91\u8fdb\u884c\u4f53\u7d20\u8fc7\u6ee4\uff0c\u9009\u62e9\u4e0d\u540c\u7684\u65b9\u6cd5\u8fdb\u884c\u53c2\u6570\u8bbe\u7f6e  init_guess\u662fndt\u914d\u51c6\u65f6\u5019\u7684\u521d\u59cb\u4f4d\u7f6e\uff0c\u8be5\u4f4d\u7f6e\u4e00\u822c\u7531\u524d\u4e00\u5e27\u4f4d\u7f6e\u52a0\u4e0a\u5fae\u5c0f\u65f6\u95f4\u6bb5\u5185\u7684\u53d8\u5316\uff0c\u5f53\u91c7\u7528imu\u6216odom\u65f6\u53ef\u4ee5\u5229\u7528\u5176\u8fdb\u884c\u8f85\u52a9\u7cbe\u786e\u5b9a\u4f4d\u521d\u59cb\u4f4d\u7f6e\u3002\u5982\u679c\u672a\u4f7f\u7528imu\u4ee5\u53caodom\u5219\u4f7f\u7528\u539f\u6765\u7684init_guess",
            "title": "\u70b9\u4e91\u5efa\u56fe\u529f\u80fd"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#_10",
            "text": "\u70b9\u4e91\u805a\u7c7b\u529f\u80fd\u5728\"lidar_demo/src/lidar_demo.cpp\"\u6587\u4ef6  void callback(const boost::shared_ptr<const sensor_msgs::PointCloud2>& msg)\n{\n    ros::NodeHandle n;\n    n.getParam(\"Cluster_D\", Cluster_D);\n    n.getParam(\"Cluster_Max\", Cluster_Max);\n    n.getParam(\"Cluster_Min\", Cluster_Min);\n    pcl::PCLPointCloud2 pcl_pc2;\n    pcl_conversions::toPCL(*msg,pcl_pc2);\n    pcl::PointCloud<pcl::PointXYZ>::Ptr temp_cloud(new pcl::PointCloud<pcl::PointXYZ>);\n    pcl::fromPCLPointCloud2(pcl_pc2,*temp_cloud);\n\n    std::vector<int> mapping;\n    pcl::removeNaNFromPointCloud(*temp_cloud, *temp_cloud, mapping);\n    pub2.publish(temp_cloud);\n    pcl::PointIndices::Ptr inliers (new pcl::PointIndices);\n    pcl::ModelCoefficients::Ptr coefficients (new pcl::ModelCoefficients);\n    pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_plane (new pcl::PointCloud<pcl::PointXYZ> ());\n    pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_f (new pcl::PointCloud<pcl::PointXYZ>);\n\n    if (temp_cloud->points.size() == 0)\n    {\n        std::cout << \"cloud in ROI is empty\" << std::endl;\n        return;\n    }\n    pcl::search::KdTree<pcl::PointXYZ>::Ptr tree (new pcl::search::KdTree<pcl::PointXYZ>);\n    tree->setInputCloud (temp_cloud);\n\n    std::vector<pcl::PointIndices> cluster_indices;\n    pcl::EuclideanClusterExtraction<pcl::PointXYZ> ec;\n    cout << Cluster_D << \":\" << Cluster_Min << \":\" << Cluster_Max<<endl;\n    ec.setClusterTolerance (Cluster_D);\n\n    ec.setMinClusterSize (Cluster_Min);\n\n    ec.setMaxClusterSize (Cluster_Max);\n\n    ec.setSearchMethod (tree);\n    ec.setInputCloud (temp_cloud);\n    ec.extract (cluster_indices);\n\n    jsk_recognition_msgs::BoundingBoxArray BOXS;\n    int j = 0;\n    vector<Eigen::Vector3f> center;\n    for (std::vector<pcl::PointIndices>::const_iterator it = cluster_indices.begin (); it != cluster_indices.end (); ++it)\n    {\n        pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_cluster (new pcl::PointCloud<pcl::PointXYZ>);\n        for (std::vector<int>::const_iterator pit = it->indices.begin (); pit != it->indices.end (); ++pit)\n            cloud_cluster->push_back ((*temp_cloud)[*pit]); \n        cloud_cluster->width = cloud_cluster->size ();\n        cloud_cluster->height = 1;\n        cloud_cluster->is_dense = true;\n\n        std::cout << \"Cluster has : \" << cloud_cluster->size () << \" data points.\" << std::endl;\n        j++;\n        jsk_recognition_msgs::BoundingBox box2;\n\n        Eigen::Vector3f mass_center;\n        pcl::MomentOfInertiaEstimation<pcl::PointXYZ> feature_extractor;\n        feature_extractor.setInputCloud(cloud_cluster);\n        feature_extractor.compute();\n        pcl::PointXYZ min_point_OBB;\n        pcl::PointXYZ max_point_OBB;\n        pcl::PointXYZ position_OBB;\n        Eigen::Matrix3f rotational_matrix_OBB;\n        feature_extractor.getOBB(min_point_OBB, max_point_OBB, position_OBB, rotational_matrix_OBB);\n        Eigen::Quaternionf quat (rotational_matrix_OBB);\n        feature_extractor.getMassCenter (mass_center);\n        center.push_back(mass_center);\n\n        pcl::PointXYZ min;\n        pcl::PointXYZ max;\n        pcl::getMinMax3D(*cloud_cluster,min,max);\n        max.x = max.x;\n\n        box2.label = j+1;\n        box2.pose.position.x = (max.x + min.x)  / 2;\n        box2.pose.position.y = (max.y + min.y)  / 2;\n        box2.pose.position.z = (max.z + min.z)  / 2;\n        box2.dimensions.x = (max.x - min.x);\n        box2.dimensions.y = (max.y - min.y);\n        box2.dimensions.z = (max.z - min.z);\n        box2.header.frame_id = frame_id;\n\n        BOXS.boxes.push_back(box2);   \n    }\n    BOXS.header.frame_id = frame_id;\n    pub.publish(BOXS);\n}  \u70b9\u4e91\u805a\u7c7b\u4e3b\u8981\u4f7f\u7528pcl\u5e93\u805a\u7c7b\u7b97\u6cd5  pcl::EuclideanClusterExtractionpcl::pointxyz ec;  \u53ea\u9700\u8bbe\u5b9a\u53c2\u6570\uff1a  Cluster_D:\u805a\u7c7b\u76f4\u5f84\nCluster_Min:\u6700\u5c0f\u805a\u7c7b\u70b9\u6570\nCluster_Max:\u6700\u5927\u805a\u7c7b\u70b9\u6570",
            "title": "\u70b9\u4e91\u805a\u7c7b\u529f\u80fd"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#_11",
            "text": "\u4e3b\u8981\u4ee3\u7801\u5206\u5e03\u4f4d\u4e8elidar_cnn_seg_detect\uff0cdetected_objects_visualizer\uff0ccommon\uff0crockauto_msgs\u6587\u4ef6\u5939\u5185  \u70b9\u4e91\u667a\u80fd\u8bc6\u522b\u529f\u80fd\uff0c\u91c7\u7528\u7684\u662fapolo\u7684\u795e\u7ecf\u7f51\u7edc\u8bc6\u522b\u7b97\u6cd5\uff0c\u53ef\u5230\u8be5\u7b97\u6cd5\u7684 \u7ef4\u62a4\u5e73\u53f0 \u8fdb\u884c\u6df1\u5165\u7814\u7a76\uff0c\u8be5\u7b97\u6cd5\u4e0evoxelnet\u7f51\u7edc\u76f8\u4f3c\uff0c\u4e5f\u53ef\u53c2\u8003 github  \u6211\u4eec\u5728\u667a\u80fd\u8bc6\u522b\u7684\u57fa\u7840\u4e0a\u6dfb\u52a0\u4e86\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\u663e\u793a  VisualizeDetectedObjects::ObjectsToLabels(const rockauto_msgs::DetectedObjectArray &in_objects)\n{\n  visualization_msgs::MarkerArray label_markers;\n  for (auto const &object: in_objects.objects)\n  {\n    if (IsObjectValid(object))\n    {\n      visualization_msgs::Marker label_marker;\n\n      label_marker.lifetime = ros::Duration(marker_display_duration_);\n      label_marker.header = in_objects.header;\n      label_marker.ns = ros_namespace_ + \"/label_markers\";\n      label_marker.action = visualization_msgs::Marker::ADD;\n      label_marker.type = visualization_msgs::Marker::TEXT_VIEW_FACING;\n      label_marker.scale.x = 1.5;\n      label_marker.scale.y = 1.5;\n      label_marker.scale.z = 1.5;\n\n      label_marker.color = label_color_;\n\n      label_marker.id = marker_id_++;\n\n      if(!object.label.empty() && object.label != \"unknown\")\n        label_marker.text = object.label + \" \"; //Object Class if available\n      float velocity_x = 0.0, velocity_y = 0.0, a_x =0.0, a_y = 0.0;\n      if(past_x.size() ==0)\n      {\n        velocity_x = 0.0;\n        velocity_y = 0.0;\n      }\n      else{\n        int  before = 0;\n        float past_d =100.0;\n        for(int i=0;i<past_x.size();i++)\n        {\n          float d = sqrt(pow((past_x[i] - object.pose.position.x),2)+pow((past_y[i] - object.pose.position.y),2));\n          if(d < past_d)\n          {\n            past_d = d;\n            before = i;\n          }\n        }\n        velocity_x = (-past_x[before] + object.pose.position.x)/(0.1);\n        velocity_y = (-past_y[before] + object.pose.position.y)/(0.1);\n        a_x = (-past_vx[before] + velocity_x)/(0.1);\n        a_y = (-past_vy[before] + velocity_y)/(0.1);\n      }\n      std::stringstream distance_stream;\n      distance_stream << std::fixed << std::setprecision(1)\n                      << sqrt((object.pose.position.x * object.pose.position.x) +\n                                (object.pose.position.y * object.pose.position.y));\n\n      std::stringstream velocity_stream;\n      velocity_stream <<std::fixed<<std::setprecision(2)<<\"v_x:\"<<velocity_x <<\"m/s a_x:\"<<a_x<<\"m/s2\\n\" <<\"v_y:\"<<velocity_y<<\"m/s a_y:\"<<a_y<<\"m/s2\";\n      std::string distance_str = distance_stream.str() + \" m\\n\" + velocity_stream.str();\n      label_marker.text += distance_str;\n\n      if (object.velocity_reliable)\n      {\n        double velocity = object.velocity.linear.x;\n        if (velocity < -0.1)\n        {\n          velocity *= -1;\n        }\n\n        if (abs(velocity) < object_speed_threshold_)\n        {\n          velocity = 0.0;\n        }\n\n        tf::Quaternion q(object.pose.orientation.x, object.pose.orientation.y,\n                         object.pose.orientation.z, object.pose.orientation.w);\n\n        double roll, pitch, yaw;\n        tf::Matrix3x3(q).getRPY(roll, pitch, yaw);\n\n        // convert m/s to km/h\n        std::stringstream kmh_velocity_stream;\n        kmh_velocity_stream << std::fixed << std::setprecision(1) << (velocity * 3.6);\n        std::string text = \"\\n<\" + std::to_string(object.id) + \"> \" + kmh_velocity_stream.str() + \" km/h\";\n        label_marker.text += text;\n      }\n\n      label_marker.pose.position.x = object.pose.position.x;\n      label_marker.pose.position.y = object.pose.position.y;\n      label_marker.pose.position.z = label_height_;\n      label_marker.scale.z = 1.0;\n      if (!label_marker.text.empty())\n        label_markers.markers.push_back(label_marker);\n      past_x.push_back(object.pose.position.x);\n      past_y.push_back(object.pose.position.y);\n      past_vx.push_back(velocity_x);\n      past_vy.push_back(velocity_y);\n    }\n  }  // end in_objects.objects loop\n\n  return label_markers;\n}//ObjectsToLabels",
            "title": "\u70b9\u4e91\u667a\u80fd\u8bc6\u522b\u529f\u80fd"
        },
        {
            "location": "/\u6fc0\u5149\u96f7\u8fbe\u7b97\u6cd5\u76d2\u5b50/#_12",
            "text": "\u7a0b\u5e8f\u8981\u653e\u5728\u82f1\u6587\u8def\u5f84\u4e0b\uff0c\u6700\u597d\u4e0d\u8981\u653e\u5728\u4e2d\u6587\u76ee\u5f55\u4e0b\uff0c\u56e0\u4e3a\u5bb9\u6613\u51fa\u73b0\u4e2d\u6587\u4e71\u7801\u95ee\u9898\u800c\u5f71\u54cd\u7a0b\u5e8f\u8fd0\u884c",
            "title": "\u6ce8\u610f\u4e8b\u9879"
        },
        {
            "location": "/\u7b1b\u5361\u5c14\u79ef/",
            "text": "${X_i|i\\in I}$\n\n\n\n\n\n\\prod_{x\\in I}X_i=\\{f:I\\rightarrow\\bigcup_{x\\in I}X_i|(\\forall_i)(f(i)\\in X_i)\\}\n\n\n\n\n\\pi_j(f)=f(j)\n\n\n\n\n\n\u7b2cj\u6295\u5f71\u6620\u5c04\n\n\n$$\n\\pi_j:\\prod_{i\\in I}X_i\\rightarrow X_j\n\n\n$$\n\n\n$$\n\\prod_{n=1}^{\\infty}\\mathbb{R}=\\mathbb{R}^{\\omega}=\\mathbb{R}\\times\\mathbb{R}\\cdots\n\n\n$$\n\n\n\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u5e94\u7528\u4e8e\u7ebf\u6027\u56de\u5f52\n\n\n\u6781\u5927\u4f3c\u7136\u6cd5\u662f\u4e00\u79cd\u7528\u4e8e\u4f30\u8ba1\u6a21\u578b\u53c2\u6570\u7684\u7edf\u8ba1\u65b9\u6cd5\u3002\n\n\n\u5728\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u4e2d\uff0c\u9009\u62e9\u53c2\u6570\u4ee5\u4f7f\u5047\u8bbe\u7684\u6a21\u578b\u4ea7\u751f\u89c2\u6d4b\u6570\u636e\u7684\u53ef\u80fd\u6027\u6700\u5927\u3002\n\n\n\n\n\u5047\u8bbe\u6211\u4eec\u6709\u4e2a\u6a21\u578b\uff0c\u4e5f\u79f0\u4e3a\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u3002\n\n\n\u80fd\u591f\u4e3a\u6211\u4eec\u7684\u6570\u636e\u5bfc\u51fa\u4f3c\u7136\u51fd\u6570\u3002\n\n\n\n\n\u4e00\u65e6\u5bfc\u51fa\u4f3c\u7136\u51fd\u6570\uff0c\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u4ec5\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u4f18\u5316\u95ee\u9898\u3002\n\n\n\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u4f18\u52bf\uff1a\n\n\n\n\n\u5982\u679c\u6b63\u786e\u8bbe\u5b9a\u4e86\u6a21\u578b\uff0c\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5668\u5c06\u662f\u6700\u6709\u6548\u7684\u4f30\u8ba1\u5668\n\n\n\u5b83\u63d0\u4f9b\u4e86\u4e00\u81f4\u4f46\u7075\u6d3b\u7684\u65b9\u6cd5\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u5404\u79cd\u5e94\u7528\u7a0b\u5e8f\uff0c\u5305\u62ec\u5176\u4ed6\u8bbe\u5b9a\u6a21\u578b\u65e0\u6548\u7684\u60c5\u51b5\u3002\n\n\n\u5728\u8f83\u5927\u6a21\u578b\u4e2d\u5f97\u51fa\u65e0\u504f\u4f30\u8ba1\n\n\n\n\n\u6548\u7387\u662f\u4f30\u8ba1\u5668\u8d28\u91cf\u7684\u4e00\u79cd\u5ea6\u91cf\uff0c\u6709\u6548\u7684\u4f30\u8ba1\u5668\u5177\u6709\u8f83\u5c0f\u7684\u65b9\u5dee\u6216\u5747\u65b9\u8bef\u5dee\u3002\n\n\n\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u7f3a\u70b9\uff1a\n\n\n\n\n\u5b83\u4f9d\u8d56\u4e8e\u6a21\u578b\u7684\u8bbe\u5b9a\u4ee5\u53ca\u4f3c\u7136\u51fd\u6570\u7684\u63a8\u5bfc\uff0c\u800c\u8fd9\u79cd\u63a8\u5bfc\u5e76\u4e0d\u603b\u662f\u90a3\u4e48\u5bb9\u6613\n\n\n\u50cf\u5176\u4ed6\u4f18\u5316\u95ee\u9898\u4e00\u6837\uff0c\u6700\u5927 \u4f3c\u7136\u4f30\u8ba1\u53ef\u80fd\u5bf9\u521d\u59cb\u503c\u7684\u9009\u62e9\u5f88\u654f\u611f\u3002\n\n\n\u53d6\u51b3\u4e8e\u4f3c\u7136\u51fd\u6570\u7684\u590d\u6742\u5ea6\uff0c\u6570\u503c\u4f30\u8ba1\u5728\u8ba1\u7b97\u4e0a\u53ef\u80fd\u662f\u6602\u8d35\u7684\u3002\n\n\n\u5c0f\u6837\u672c\u7684\u4f30\u8ba1\u503c\u53ef\u80fd\u6709\u504f\u5dee\u3002\n\n\n\n\n\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u53d6\u51b3\u4e8e\u4f3c\u7136\u51fd\u6570\u7684\u63a8\u5bfc\u3002\u56e0\u6b64\u91cd\u8981\u7684\u662f\u8981\u5bf9\u4f3c\u7136\u51fd\u6570\u662f\u4ec0\u4e48\u4ee5\u53ca\u5b83\u6765\u81ea\u4f55\u5904\u6709\u4e00\u4e2a\u5f88\u597d\u7684\u4e86\u89e3\u3002\n\n\n\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u7b2c\u4e00\u6b65\u662f\u5047\u8bbe\u6570\u636e\u7684\u6982\u7387\u5206\u5e03\uff0c\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u5728\u7ed9\u5b9a\u4e00\u7ec4\u57fa\u7840\u6a21\u578b\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u6d4b\u91cf\u89c2\u5bdf\u6570\u636e\u7684\u6982\u7387\u3002\n\n\n\u6211\u4eec\u5047\u8bbe\u6211\u4eec\u7684\u6570\u636e\u5177\u6709\u6f5c\u5728\u7684\u6cca\u677e\u5206\u5e03\uff0c\u8fd9\u662f\u4e00\u4e2a\u666e\u904d\u7684\u5047\u8bbe\uff0c\u5c24\u5176\u5bf9\u4e8e\u975e\u8d1f\u8ba1\u6570\u6570\u636e\u3002\n\n\n\u5355\u4e2a\u89c2\u6d4b\u503c$y_i$\u7684\u6cca\u677e\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u7531\u4e0b\u5f0f\u7ed9\u51fa\uff1a\n\n\n$$\nf(y_i|\\theta)=\\frac{e^{-\\theta}\\theta^{y_i}}{y_i!}\n\n\n$$\n\n\n\u7531\u4e8e\u6211\u4eec\u6837\u672c\u4e2d\u7684\u89c2\u6d4b\u503c\u662f\u72ec\u7acb\u7684\uff0c\u56e0\u6b64\u53ef\u4ee5\u901a\u8fc7\u53d6\u5404\u4e2a\u89c2\u6d4b\u503c\u7684\u6982\u7387\u4e58\u79ef\u6765\u6c42\u51fa\u6211\u4eec\u89c2\u6d4b\u6837\u672c\u7684 \u6982\u7387\u5bc6\u5ea6\uff1a\n\n\n$$\nf(y_1,y_2,\\cdots,y_{10}|\\theta)=\\prod_{i=1}^{10}\\frac{e^{-\\theta}\\theta^{y_i}}{y_i!}=\\frac{e^{-10\\theta}\\theta\\sum_{i=1}^{10}y_i}{\\prod_{i=1}^{10}y_i!}\n\n\n$$\n\n\n\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u6982\u7387\u5bc6\u5ea6\u6765\u56de\u7b54\u5728\u7ed9\u5b9a\u7279\u5b9a\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u6570\u636e\u53d1\u751f\u7684\u53ef\u80fd\u6027\u3002\n\n\n\u4f3c\u7136\u51fd\u6570\u548c\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u7684\u4e0d\u540c\u662f\u7ec6\u5fae\u7684\u4f46\u662f\u662f\u91cd\u8981\u7684\n\n\n\n\n\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u5f3a\u8c03\u5728\u7ed9\u5b9a\u57fa\u7840\u5206\u5e03\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u89c2\u5bdf\u6211\u4eec\u6570\u636e\u7684\u6982\u7387\uff0c\u524d\u63d0\u662f\u5047\u8bbe\u53c2\u6570\u5df2\u77e5\u3002\n\n\n\u4f3c\u7136\u51fd\u6570\u5f3a\u8c03\u89c2\u6d4b\u6570\u636e\u65f6\u51fa\u73b0\u53c2\u6570\u503c\u7684\u53ef\u80fd\u6027\uff0c\u8fd9\u91cc\u53c2\u6570\u662f\u672a\u77e5\u7684\u3002\n\n\n\n\n\u6570\u5b66\u4e0a\u4f3c\u7136\u51fd\u6570\u548c\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u5f88\u50cf\n\n\n$$\nL(\\theta|y_1,y_2,\\cdots,y_{10})=f(y_1,y_2,\\cdots,y_{10}|\\theta)\n\n\n$$\n\n\n\u5bf9\u4e8e\u6211\u4eec\u7684\u6cca\u677e\u5206\u5e03\u6570\u636e\uff0c\u6211\u4eec\u76f8\u5f53\u5bb9\u6613\u7684\u5f97\u5230\u4f3c\u7136\u51fd\u6570\n\n\n$$\nL(\\theta|y_1,y_2,\\cdots,y_{10})=\\frac{e^{-10\\theta}\\theta\\sum_{i=1}^{10}y_i}{\\prod_{i=1}^{10}y_i!}=\\frac{e^{-10\\theta}\\theta^{20}}{207360}\n\n\n$$\n\n\n$$\n\\prod_{k=3}^{7}k=3\\times4\\times5\\times6\\times7\n\n\n$$\n\n\n\u672a\u77e5\u53c2\u6570$\\theta$\u7684\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\uff0c\u662f\u4f7f\u8fd9\u79cd\u53ef\u80fd\u6027\u6700\u5927\u5316\u7684\u503c\u3002\n\n\n\u5bf9\u6570\u4f3c\u7136\u51fd\u6570\n\n\n\u5728\u5b9e\u9645\u4e2d\uff0c\u8054\u5408\u5206\u914d\u51fd\u6570\u96be\u4ee5\u4f7f\u7528\uff0c\u4e00\u822c\u7528$ln$\u4f3c\u7136\u51fd\u6570\u4ee3\u66ff\u3002\u5728\u6211\u4eec\u7684\u6cca\u677e\u6570\u636e\u91cc\u5bf9\u6570\u4f3c\u7136\u51fd\u6570\u662f\uff1a\n\nln(L(\\theta|y)=-n\\theta+ln\\sum_{i=1}^{n}y_i-ln\\theta\\sum_{i=1}^{n}y_i!=-10\\theta+20ln(\\theta)-ln(207360)\n\n\n\n\n\u5728\u7ebf\u6027\u56de\u5f52\uff0c\u6211\u4eec\u5047\u8bbe\u6a21\u578b\u7684\u6b8b\u5dee\u662f\u72ec\u7279\u7684\u5b8c\u5168\u6b63\u6001\u5206\u5e03\u3002\n\n\\epsilon=y-\\hat{\\beta}x\\quad\\sim N(0,\\delta^2)\n\n\u5bf9\u4e8e\u8fd9\u79cd\u60c5\u51b5\uff0c\u672a\u77e5\u53c2\u6570\u5411\u91cf$\\theta={\\beta,\\delta^2}$,\u89c2\u5bdf\u6570\u636e\u7684\u6761\u4ef6y\u548cx\u5df2\u77e5\uff0c\u5bf9\u6570\u4f3c\u7136\u51fd\u6570\u662f\uff1a\n\nlnL(\\theta|y,x)=-\\frac{1}{2}\\sum_{i=1}^{n}\\left[ln\\delta^2+ln(2\\pi)+\\frac{y-\\hat{\\beta}x}{\\delta^2}\\right]\n\n$\\beta$\u548c$\\delta^2$\u7684\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u5c31\u662f\u4f7f\u53ef\u80fd\u6700\u5927\u5316\u7684\u503c\u3002\n\n\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u548c\u6982\u7387\u6a21\u578b\n\n\ny^*=x\\theta+\\epsilon\\quad\\epsilon\\sim N(0,1)\n\n\ny_i=\\left\\{\\begin{matrix}0\\quad if\\quad y^{*}_{i}\\leqslant0 &\\\\1\\quad if\\quad y^{*}_i>0\\end{matrix}\\right.\n\n\nP(y_i=1|X_i)=P(y^*_i>0|X_i)=P(x\\theta+\\epsilon>0|X_i)=P(\\epsilon>-x\\theta|X_i)=1-\\Phi(-\\theta x)=\\Phi(x\\theta)\n\n$\\Phi$\u4ee3\u8868\u6b63\u6001\u7d2f\u8ba1\u5206\u5e03\u3002\n\u6bcf\u4e2a$y_i$\u662f\u72ec\u7acb\u7684\u6240\u4ee5\n\nf(y=(y_1,y_2,\\cdots,y_n|w)=f_1(y_1|w)f_2(y_2|w)\\cdots f_n(y_n|w)\n\n\nlog\\pi_\\theta(\\alpha|s)=log[P_\\theta(s)]_\\alpha\n\n\n\u91c7\u6837\n\n\u5df2\u77e5mean action $\\mu_\\theta(s)$\u548c\u6807\u51c6\u504f\u5dee$\\delta_\\theta(s)$\u548c\u7403\u5f62\u9ad8\u65af\u566a\u97f3\u7684\u5411\u91cfz\uff08$z\\sim N(0,1)$\uff09\u53ef\u4ee5\u8ba1\u7b97\u51fa\u4e00\u4e2aaction sample\n\n\\alpha=\\mu_\\theta(s)+\\delta_\\theta(s)\\bigodot z\n\n$\\bigodot$\u4ee3\u8868\u4e24\u4e2a\u5411\u91cf\u5143\u7d20\u4e58\u79ef\u3002\n\n\n\u5bf9\u4e8e\u5747\u503c$\\mu=\\mu_\\theta(s),\u6807\u51c6\u504f\u5dee$$\\delta=\\delta_\\theta(s)$\u7684\u5bf9\u89d2\u9ad8\u65af\uff0c$k$\u7ef4action $\\alpha$ \u7684\u5bf9\u6570\u4f3c\u7136\u7531\u4ee5\u4e0b\u7ed9\u51fa\uff1a\n\nlog\\pi_\\theta(\\alpha|s)=-\\frac{1}{2}\\left(\\sum_{i=1}^{k}\\left(\\frac{(\\alpha_i-\\mu_i)^2}{\\delta^2_i}+2log\\delta_i\\right)+klog2\\pi\\right)\n\n\nTrajectories\n\n\u4e00\u4e2a\u8f68\u8ff9$\\tau$\u662fworld\u91cc\u4e00\u7cfb\u5217\u72b6\u6001\u548caction\u3002\n\n\\tau=(s_0,\\alpha_0,s_1,\\alpha_1,\\cdots)\n\n\ns_{t+1}=f(s_t,\\alpha_t)\n\nOr \n\ns_{t+1}\\sim P(\\cdot|s_t,\\alpha_t)\n\nactions\u662f\u4ee3\u7406\u901a\u8fc7\u7b56\u7565\u4ea7\u751f\u7684\n\n\u5956\u52b1\u548c\u53cd\u9988\n\n\u5956\u52b1\u53d6\u51b3\u4e8e\u5f53\u524dworld\u7684\u72b6\u6001\uff0c\u521a\u521a\u91c7\u53d6\u7684action\u548cworld\u7684\u4e0b\u4e00\u4e2a\u72b6\u6001\n\nr_t=R(s_t,a_t,s_{t+1})\n\n\u6709\u65f6\u4f1a\u7b80\u5316\u4e3a$r_t=R(s_t)$\u6216\u8005state-action $r_t=R(s_t,a_t)$\n\u4ee3\u7406\u7684\u76ee\u6807\u662f\u4f7f\u67d0\u4e2a\u8f68\u8ff9\u4e0a\u7684\u67d0\u4e2a\u6982\u5ff5\u7684\u7d2f\u8ba1\u5956\u52b1\u6700\u5927\u5316\u3002\n\u6211\u4eec\u5c06\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u8bb0\u5f55\u6240\u6709\u7684\u60c5\u51b5$R(\\tau)$\n\n\n\u4e00\u79cd\u53cd\u9988\u662f\u6709\u9650\u6c34\u5e73\u7684\u65e0\u6298\u73b0\u53cd\u9988\uff0c\u5c31\u662f\u6240\u6709\u5956\u52b1\u7684\u548c\n\nR(\\tau)=\\sum_{t=0}^{T}r_t\n\n\u53e6\u4e00\u79cd\u53cd\u9988\u662f\u6709\u9650\u6c34\u5e73\u7684\u6298\u73b0\u53cd\u9988\uff0c\u4ee3\u7406\u83b7\u5f97\u7684\u5956\u52b1\u4e4b\u548c\uff0c\u6298\u73b0\u5c06\u6765\u4ee3\u7406\u83b7\u5f97\u7684how far off.\u8fd9\u4e2a\u65b9\u7a0b\u5f0f\u7684\u5956\u52b1\u5305\u542b\u4e00\u4e2a\u6298\u73b0\u56e0\u5b50$\\gamma\\in(0,1)$\n\nR(\\tau)=\\sum_{t=o}^{\\infty}\\gamma^t\\tau_t\n\n\u4e3a\u4ec0\u4e48\u6211\u4eec\u60f3\u8981\u4e00\u4e2a\u6298\u73b0\u7684\u53cd\u9988\uff0c\u800c\u4e0d\u662f\u6240\u6709\u7684\u53cd\u9988\uff1f\u6211\u4eec\u7684\u786e\u60f3\u8981\u6240\u6709\u53cd\u9988\uff0c\u4f46\u662f\u6298\u73b0\u56e0\u5b50\u5728\u76f4\u89c2\u4e0a\u5373\u5438\u5f15\u4eba\uff0c\u6570\u5b66\u5b9e\u73b0\u8d77\u6765\u53c8\u65b9\u4fbf\u3002\u4e00\u79cd\u76f4\u89c9\uff1a\u73b0\u5728\u7684\u73b0\u91d1\u6bd4\u4ee5\u540e\u7684\u73b0\u91d1\u597d\u3002\u6570\u5b66\u4e0a\uff1a\u65e0\u9650\u6c34\u5e73\u7684\u5956\u52b1\u4e4b\u548c\u53ef\u80fd\u4e0d\u4f1a\u6536\u655b\u5230\u6709\u9650\u7684\u503c\uff0c\u5e76\u4e14\u5f88\u96be\u7528\u65b9\u7a0b\u5f0f\u5904\u7406\u3002\u4f46\u662f\u5728\u6709\u6298\u73b0\u56e0\u5b50\u7684\u60c5\u51b5\u4e0b\uff0c\u5e76\u5728\u5408\u7406\u7684\u6761\u4ef6\u4e0b\uff0c\u65e0\u7a77\u5927\u662f\u6536\u655b\u7684\u3002\n\n\nDeep RL \u7ec3\u4e60\u5f80\u5f80\u4f1a\u4f7f\u8fd9\u6761\u7ebf\u6a21\u7cca\u4e0d\u6e05\uff0c\u4f8b\u5982\uff0c\u6211\u4eec\u7ecf\u5e38\u8bbe\u7f6e\u7b97\u6cd5\u4ee5\u4f18\u5316\u672a\u6298\u73b0\u7684\u53cd\u9988\uff0c\u4f46\u5728\u8bc4\u4ef7value function \u7684\u65f6\u5019\u4f7f\u7528\u6298\u73b0\u56e0\u5b50\u3002\n\n\n\u65e0\u8bba\u9009\u62e9\u54ea\u79cd\u53cd\u9988\u65b9\u6cd5\uff0c\u9009\u62e9\u54ea\u79cd\u7b56\u7565\uff0cRL\u7684\u76ee\u6807\u662f\u9009\u62e9\u4e00\u4e2a\u7b56\u7565\u4f7f\u4ee3\u7406\u8fd0\u884c\u6b64\u7b56\u7565\u4f7f\u671f\u671b\u7684\u53cd\u9988\u6700\u5927\u5316\u3002\n\u8981\u8ba8\u8bba\u671f\u671b\u7684\u53cd\u9988\uff0c\u6211\u4eec\u5148\u6765\u8ba8\u8bba\u4e00\u4e0b\u8f68\u8ff9\u4e0a\u7684\u6982\u7387\u5206\u5e03\u3002\n\n\n\u8ba9\u6211\u5047\u8bbe\u73af\u5883\u8f6c\u53d8\u548c\u7b56\u7565\u90fd\u662f\u968f\u673a\u7684\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u8f68\u8ff9T-step\u7684\u6982\u7387\u662f\uff1a\n\nP(T|\\pi)=\\rho_0(s_0)\\prod_{t=0}^{T-1}P(s_{t+1}|s_t,\\alpha_t)\\pi(s_t,\\alpha_t)\n\n\u671f\u671b\u7684\u53cd\u9988\u8868\u793a\u4e3a\n\nJ(\\pi)=\\int_{r}P(\\tau|\\pi)R(\\tau)=\\underset{\\tau\\sim\\pi}{E}[R(\\tau)]\n\nRL\u4e2d\u7684\u96c6\u4e2d\u4f18\u5316\u95ee\u9898\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a\n\n\\pi^*=arg\\quad\\underset{\\pi}{max}J(\\pi)\n\n$\\pi^\n$\u662f\u6700\u4f73\u7b56\u7565\u3002\n\nValue Function\n\n\u77e5\u9053value of state \u6216\u8005 state-action \u901a\u5e38\u662f\u6709\u7528\u7684\u3002\n\u8fd9\u91cc\u6709\u56db\u79cdvalue function.\n1. \u57fa\u4e8e\u7b56\u7565\u7684value function.$V^\\pi(s)$,\u5f53state s,\u6267\u884c\u7b56\u7565$\\pi$\u65f6\uff0c\n\nV^\\pi(s)=\\underset{\\tau\\in\\pi}{E}[R(\\tau)|s_0=s]\n\n2. \u57fa\u4e8e\u7b56\u7565\u7684 action-value function. $Q^\\pi(s,\\alpha)$,\n\nQ^\\pi(s,\\alpha)=\\underset{\\tau\\in\\pi}{E}[R(\\tau)|s_0=s,a_0=a]\n\n3. \u6700\u4f18value function\uff0c $V^\n(s)$\n\nV^*(s)=\\underset{\\pi}{max}\\underset{\\tau\\in\\pi}{E}[R(\\tau)|s_0=s]\n\n4. \u6700\u4f73action-value function, $Q^*(s,\\alpha)$\n\n\n\n\nQ^*(s,\\alpha)=\\underset{\\pi}{max}\\underset{\\tau\\in\\pi}{E}[R(\\tau)|s_0=s,\\alpha_0=\\alpha]\n\n\u5728value function \u548c action-value function \u4e4b\u95f4\u6709\u4e24\u4e2a\u5173\u952e\u7684\u94fe\u63a5\n\nV^\\pi(s)=\\underset{\\alpha\\sim\\pi}{E}[Q^\\pi(s,\\alpha)]\n\n\u548c\n\nV^*(s)=\\underset{\\alpha}{max}Q^*(s,\\alpha)\n\n\n\\alpha^*(s)=arg\\quad\\underset{\\alpha}{max}Q^*(s,\\alpha)\n\n\n\u8d1d\u5c14\u66fc\u65b9\u7a0b\n\n\u8fd94\u4e2avalue function\u90fd\u9075\u5faa\u7684\u7279\u6b8a\u81ea\u6cbb\u65b9\u7a0b,\u79f0\u4e3aBellman equations\nBellman equation\u540e\u9762\u7684\u57fa\u672c\u601d\u60f3\u662f\uff1a\n    \u8d77\u70b9\u5904\u7684value\uff0c\u662f\u4f60\u671f\u671b\u4ece\u90a3\u91cc\u5f97\u5230\u7684reward\uff0c\u518d\u52a0\u4e0a\u4e0b\u4e00\u6b65\u5230\u8fbe\u5730\u65b9\u7684value\u3002\n\n\n\u57fa\u4e8e\u7b56\u7565\u7684value function \u7684Bellman equations\n\nV^\\pi(s)=\\underset{s'\\sim P}{\\underset{\\alpha\\sim\\pi}{E}}[\\tau(s,\\alpha)+\\gamma V^\\pi(s')]\n\n\nQ^\\pi(s,\\alpha)=\\underset{s'\\sim P}{E}\\left[\\tau(s,\\alpha)+\\gamma\\underset{a'\\sim\\pi}{E}[Q^\\pi(s',\\alpha')]\\right]\n\n$s'\\sim P$\u662f$s'\\sim P(\\cdot|s,\\alpha)$\u7684\u7f29\u5199\uff0c\u6307\u793a\u4e0b\u4e00\u72b6\u6001$s'$\u4ece\u73af\u5883\u7684\u8fc7\u5ea6\u89c4\u5219\u4e2d\u91c7\u6837\uff1b$\\alpha\\sim\\pi$\u662f$\\alpha\\sim\\pi(\\cdot|s)$\u7684\u7f29\u5199\uff1b$\\alpha'\\sim\\pi$\u662f$\\alpha'\\sim\\pi(\\cdot|s')$\u7684\u7f29\u5199\u3002\n\n\n\u6700\u4f73value function\u7684Bellman equation\u662f\n\nV^*(s)=\\underset{\\alpha}{max}\\quad\\underset{s'\\sim P}{E}[\\tau(s,\\alpha)+\\gamma V^*(s')]\n\n\nQ^*(s,\\alpha)=\\underset{s'\\sim P}{E}\\left[\\tau(s,\\alpha)+\\gamma\\underset{\\alpha'}{max}Q^*(s',\\alpha')\\right]\n\nBellman equation:  reward-plus-next-value\n\n\nAdvantage Functions\n\n\nA^\\pi(s,\\alpha)=Q^\\pi(s,\\alpha)-V^\\pi(s)\n\n\nMarkov Decision Processes\n\nAn MDP is a 5-tuple,$(S,A,R,P,\\rho_0)$\nS is the set of all valid states.\nA is the set of all valid actions.\nR:$S\\times A\\times S\\rightarrow \\mathbb{R}$ is the reward function , with $\\tau_t=R(s_t,\\alpha_t,s_{t+1})$\nP:$S\\times A \\rightarrow P(S)$ is the transition probability function, with $P(s'|s,\\alpha)$ being the probability of transitioning into state $s'$,if you start in state s and take action $\\alpha$\n$\\rho_0$ is the starting state distribution.",
            "title": "\u7b1b\u5361\u5c14\u79ef"
        }
    ]
}