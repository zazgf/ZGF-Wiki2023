<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>voxelnet - ZGF Wiki</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../css/theme.css" />
  <link rel="stylesheet" href="../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "voxelnet";
    var mkdocs_page_input_path = "voxelnet.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> ZGF Wiki</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../c%2B%2B/">C++</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../python/">Python</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../ubuntu18/">Ubuntu18</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../ubuntu20/">Ubuntu20</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../Ros/">Ros</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../Ros%E5%BB%BA%E6%A8%A1/">Ros建模</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../tf_document/">tf_document</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../caffe/">Caffe</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../deeplearning/">deeplearning</a>
                    </li>
                </ul>
                <ul class="current">
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">voxelnet</a>
    <ul class="current">
    </ul>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../radar_camera_fusion/">radar_camera</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../python%E6%9C%89%E7%94%A8%E7%9A%84%E5%87%BD%E6%95%B0/">python有用的函数(一)</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">ZGF Wiki</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>voxelnet</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h4 id="voxelnet">VoxelNet</h4>
<p>To interface a highly sparse稀疏 LIDAR point cloud with a region proposal network(RPN区域建议网络),most existing efforts have focused on hand-crafted手工制作 feature representations特征表示，for example, a bird's eye view projectiong鸟瞰图.</p>
<p>In this work,we remove the need of manual feature engineering for 3D point clouds and purpose VoxelNet, a generic 3D detection network that unifies feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network.</p>
<p>Specially,VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a unified feature representation through the newly introduced voxel feature encoding(VFE) layer.In this way,the point cloud is encoded as a descriptive volumetric representation, which is then connected to a RPN to generate detections.</p>
<p>Our network learns an effective discriminative representation区分性表示 of objects with various geometrics, leading to encouraging results in 3D detection of pedestrains and cyclists, based on only Lidar.</p>
<p><img alt="image" src="../images/Screenshot%202021-01-11%2016%3A20%3A43.png" />\
VoxelNet architecture. The feature learning network takes a raw point cloud as input, partitions the space into voxels, and transforms points within each voxel to a vector representation characterizing the shape information.The space is represented as a sparse 4D tensor.The convolutional middle layers processes the 4D tensor to aggregate spatial context 聚合空间语境.Finally,a RPN generates the 3D detection.</p>
<p>Scalling up 3D feature learning networks to orders of magnitude more points and to 3D detection tasks are the main challenges that we address in this paper.</p>
<p>We present VoxelNet, a generic 3D detection framework that simultaneously同时 learns a discriminative判别性 feature representation from point clouds and predicts accurate 3D bounding boxes, in an end-to-end fashion方式.</p>
<p>We design a novel新颖的 voxel feature encoding(VFE) layer,which enables inter-point interaction点间交互　within a voxel,by combining point-wise features逐点特征 with a locally aggregated feature聚合特征.</p>
<p>Stacking 堆叠 multiple VFE layers allows learning complex features for characterizing表征 local 3D shape information.</p>
<p>Specially, VoxelNet divides the point cloud into equally spaced 3D voxel等距的３Ｄ体素 ,encodes each voxel via stacked VFE layers, and then 3D convolution furture进一步 aggregate聚合 local voxel features, transforming the pointcloud into a high-dimensional volumetric体积 representation.</p>
<p>Finally,a RPN consumes the volumetric represetation and yields产生 the detection result.</p>
<p>This efficient algorithm benefits both from the sparse稀疏的 point structure and efficient parallel processing on the voxel grid体素网格的高效并行处理.</p>
<h3 id="voxelnet-architecture">VoxelNet Architecture</h3>
<p>The proposed VoxelNet consists of three functional blocks:(1) Feature learning network,(2) Convolutional middle layers, and (3) Region proposal network</p>
<h3 id="feature-learning-network">Feature Learning Network</h3>
<p><strong>Voxel Partition</strong> Given a point cloud, we subdivide the 3D space into equally spaced voxels as shown in Figure 2.Suppose the point cloud encompasses包含 3D space with range D,H,W（点云尺寸） along the Z,Y,X axes respectively.We define each voxel of size vD,vH,and vW（体素尺寸） accordingly.The resulting所生成的 3D voxel grid is of size D'=D/vD, H'=H/vH, W'=W/vW（点云包含体素个数）.Here, for simplicity,we assume D,H,W are a multiple of vD,vH,vW.我们假设D,H,W是vD, vH, vW的倍数。</p>
<p><strong>Grouping</strong> We group the points according to the voxel they reside in我们根据它们所在的体素对点进行分组.Due to factors such as distance,occlusion遮挡,object's relative pose物体的相对姿势,and non-uniform不均匀 sampling,the LiDAR point cloud is sparse and highly variable高度可变的 point density点密度 throughout the space.Therefore, after gouping, a voxel will contain a variable number of points.</p>
<p><strong>Random Sampling</strong> Typically a high-definition LiDAR point cloud is composed of ~100k points. Directly processing all the points not only imposes加强 increased memory/efficiency burdens负担　on the computing platform, but also highly variable point density  throughout the space might bias the detection.To this end因此,we randomly sample a fixed number, T,of points from those voxels containing more than T points.This sampling strategy战略 has two purpose,(1)computational saving;and (2)decrease the imbalance of points between the voxels which reduces the sampling bias, and adds more variation变化 to training.</p>
<p><strong>Stacked Voxel Feature Encoding</strong>\
<img alt="image" src="../images/Screenshot%202021-01-12%2009%3A18%3A52.png" />\
The key innovation革新 is the chain链 of VFE layers.For simplicity, Figure 2 illustrates the hierachical阶级式 feature encoding process for one voxel.Without loss of generality概要,(在不失一般性的前提下)we use VFE Layer-1 to describe the details in the following paragraph段落.Figure 3 shows the architecture for VFE Layer-1.\
Denote表示 <img alt="image" src="../images/Screenshot%202021-01-12%2010%3A02%3A35.png" /> as a non-empty voxel containing t≤Ｔ LiDAR points,where <img alt="image" src="../images/Screenshot%202021-01-12%2010%3A03%3A35.png" /> contains XYZ coordinates for the i-th point and <img alt="image" src="../images/Screenshot%202021-01-12%2010%3A04%3A35.png" /> is the received reflectance反射率.We first compute the local mean局部均值 as the centroid质心 of all the points in V, denoted as <img alt="image" src="../images/Screenshot%202021-01-12%2010%3A16%3A13.png" />.Then we augment增加 each point <img alt="image" src="../images/Screenshot%202021-01-12%2010%3A03%3A35.png" /> with the relative offset w.r.t. the centriod and obtain the input feature set <img alt="image" src="../images/Screenshot%202021-01-12%2010%3A48%3A19.png" /><img alt="image" src="../images/Screenshot%202021-01-12%2010%3A48%3A18.png" />, Next, each <img alt="image" src="../images/Screenshot%202021-01-12%2010%3A48%3A20.png" /> is transformed through the fully connected network(FCN) into a feature space,where we can aggregate汇总 information from the point features <img alt="image" src="../images/Screenshot%202021-01-12%2011%3A02%3A17.png" /> to ecode the shape of the surface contained within the voxel.The FCN is composed of a linear layer, a batch normalization(BN批量标准化) layer and a rectified纠正的 linear unit(ReLU) layer.After obtaining point-wise feature representations, we use element-wise MaxPooling across all <img alt="image" src="../images/Screenshot%202021-01-12%2011%3A02%3A27.png" /> associated to V to get the locally aggregated feature <img alt="image" src="../images/Screenshot%202021-01-12%2011%3A27%3A35.png" /> 
Finally, we augment each <img alt="image" src="../images/Screenshot%202021-01-12%2011%3A02%3A27.png" /> with <img alt="image" src="../images/Screenshot%202021-01-12%2011%3A27%3A45.png" /> to form the point-wise concatenated级联的 feature as <img alt="image" src="../images/Screenshot%202021-01-12%2011%3A32%3A52.png" /> Thus we obtain the output feature set <img alt="image" src="../images/Screenshot%202021-01-12%2011%3A32%3A53.png" />.All non-empty voxels are ecoded in the same way and they share the same set of parameters in FCN.</p>
<p>We use <img alt="image" src="../images/Screenshot%202021-01-12%2011%3A32%3A56.png" /> to represent the i-th VFE layer that transforms input features of dimension <img alt="image" src="../images/Screenshot%202021-01-12%2011%3A32%3A57.png" /> into output features of dimension <img alt="image" src="../images/Screenshot%202021-01-12%2011%3A32%3A58.png" />. The linear layer learns a matrix of size <img alt="image" src="../images/Screenshot%202021-01-12%2011%3A32%3A60.png" />, and the point-wise concatenation yields the output of dimension <img alt="image" src="../images/Screenshot%202021-01-12%2011%3A32%3A58.png" />.</p>
<p>Because the output feature combines both point-wise features and locally aggregated feature, stacking VFE layers encodes point interactions互动　within a voxel and enables the final feature representation to learn descriptive shape information. The voxel-wise feature is obtained by transforming the output of VFE-n into <img alt="i" src="../images/Screenshot%202021-01-12%2014%3A01%3A27.png" /> via FCN and applying element-wise Maxpool where C is the dimension of the voxel-wise feature, as shown in Figure2.</p>
<p><strong>Saparse Tensor Representation</strong>稀疏张量表示　By processing only the non-empty voxels, we obtain a list of voxel features,each uniquely独特的 associated to the spatial空间 coordinates of a pictular non-empty voxel.The obtained list of voxel-wise features can be represented as a sparse 4D tensor, of size C x D' x H' x W' as shown in Figure2.Although the point cloud contains ~100k points, more than 90% of voxels typically are empty.Representing non-empty voxel features as a sparse tensor greatly reduce the memory usage and computation cost during backpropagation反向传播, and it is a critical step in our efficient implementation.</p>
<h3 id="convolutional-middle-layers">Convolutional Middle Layers</h3>
<p>We use ConvMD<img alt="i" src="../images/Screenshot%202021-01-12%2014%3A01%3A28.png" /> to represent an M-dimensional  convolution operator where <img alt="image" src="../images/Screenshot%202021-01-12%2011%3A32%3A57.png" /> and <img alt="image" src="../images/Screenshot%202021-01-12%2011%3A32%3A58.png" /> are the number of input and output channels, k,s, and p are the M-dimensional vectors correspoinding to kernel size, stride size and padding填充 size respectively.When the size across the M-dimensions are the same, we use a scalar to represent the size e.g. k for k=(k,k,k).</p>
<p>Each convolutional middle layer applies 3D convolution,BN layer, and ReLU layer sequentially依次.The convolutional middle layers aggregate voxel-wise features within a progressively逐步 expanding扩大的 receptive接收 field，adding more context to the shape description.The detialed sizes of the filters in the convolutional middle layers are explained in Section 3.</p>
<h3 id="region-proposal-network">Region Proposal Network</h3>
<p><img alt="image" src="../images/Screenshot%202021-01-12%2011%3A32%3A55.png" />\
Recently, region proposal提案 networks have become an important building block of top-performing object detection frameworks. In this work,we make servel key modifications to the RPN architecture proposed in [34], and combine it with the feature learning network and convolutional middle layers to form an end-to-end trainable pipeline管道.</p>
<p>The input to our RPN is the feature map provided by the convolutional middle layers.The architecture of this network is illustrate in Figure 4.The network has three blocks of fully convolutional layers.The first layer of each block downsamples the feature map by half via a convolution with a stride size of 2,followed by a sequence of convolutions of stride 1(xq means q applications of the filter).After each convolution layer,BN and ReLU operations are applied.We then upsample the output of every block to a fixed size and concatanate to construct the high resolution feature map. Finally, this feature map is mapped to the desired learning targets:(1) a probability score map and (2) a regression map.ＲＰＮ的输入是卷积中间层提供的特征图。该网络的体系结构如图４所示。该网络具有三个完全卷基层的块。每个块的第一层通过步幅为２的卷积对特征图进行一半下采样，然后是步幅１的卷积序列（xq 表示滤波器的ｑ个应用）。在每个卷基层之后，BN 和ReLU 操作被应用。然后，我们将每个块的输出上采样到固定大小，并汇总以构建高分辨率特征图。最后，将此特征图映射到所需的学习目标：（１）概率分数图和（２）回归图。</p>
<h3 id="loss-function">Loss Function</h3>
<p>Let <img alt="image" src="../images/Screenshot%202021-01-12%2015%3A39%3A61.png" /> be the set of <img alt="image" src="../images/Screenshot%202021-01-12%2015%3A39%3A63.png" /> positive anchors锚点 and <img alt="image" src="../images/Screenshot%202021-01-12%2015%3A39%3A64.png" /> be the set of <img alt="image" src="../images/Screenshot%202021-01-12%2015%3A39%3A65.png" /> negative anchors.
We parameterize a 3D ground truth box as <img alt="image" src="../images/Screenshot%202021-01-12%2015%3A39%3A55.png" />,where <img alt="image" src="../images/Screenshot%202021-01-12%2015%3A39%3A56.png" /> represent the center location,<img alt="image" src="../images/Screenshot%202021-01-12%2015%3A39%3A57.png" /> are length ,width,height of the box, and <img alt="image" src="../images/Screenshot%202021-01-12%2015%3A39%3A60png.png" /> is the yaw rotation around Z-axis. To retrieve找回 the ground truth box from a matching positive anchorparameterized as <img alt="image" src="../images/Screenshot%202021-01-12%2015%3A39%3A74.png" />,we define the residual vector残差向量 <img alt="image" src="../images/Screenshot%202021-01-12%2015%3A39%3A84.png" /> containing the 7 regression targets corresponding to center location △x,△y,△z,three dimensions △l, △w, △h, and the rotation △Θ, which are computed as :\
<img alt="image" src="../images/Screenshot%202021-01-12%2016%3A31%3A41.png" />\
where <img alt="image" src="../images/Screenshot%202021-01-12%2016%3A31%3A42.png" /> is the diagonal对角线 of the base of the anchor box.Here, we aim to directly estimate the oriented定向的 3D box and normallize Δx and Δy homogeneously均匀的 with the diagonal对角线 <img alt="image" src="../images/Screenshot%202021-01-12%2016%3A31%3A44.png" />,We define the loss function as follows:\
<img alt="image" src="../images/Screenshot%202021-01-12%2016%3A31%3A45.png" />\
where <img alt="image" src="../images/Screenshot%202021-01-12%2016%3A31%3A46.png" /> and <img alt="image" src="../images/Screenshot%202021-01-12%2016%3A31%3A47.png" /> represent the softmax output for positive anchor <img alt="image" src="../images/Screenshot%202021-01-12%2016%3A31%3A48.png" /> and negative anchor <img alt="image" src="../images/Screenshot%202021-01-12%2016%3A31%3A49.png" /> respectively,while <img alt="image" src="../images/Screenshot%202021-01-12%2016%3A31%3A50.png" /> and <img alt="image" src="../images/Screenshot%202021-01-12%2015%3A39%3A84.png" /> are the regression回归 output and ground truth for positive anchor <img alt="image" src="../images/Screenshot%202021-01-12%2016%3A31%3A48.png" />.The first two terms are the normalized classification loss for <img alt="image" src="../images/Screenshot%202021-01-12%2015%3A39%3A61.png" /> and <img alt="image" src="../images/Screenshot%202021-01-12%2015%3A39%3A64.png" />, where the <img alt="image" src="../images/Screenshot%202021-01-13%2011%3A26%3A10png.png" /> stands for binary cross entropy loss二元交叉熵损失 and α，β are postive constants balancing the relative importance. The last term <img alt="image" src="../images/Screenshot%202021-01-13%2011%3A26%3A11.png" /> is the regression loss, where we use the SmoothL1 function.</p>
<h3 id="efficient-implementation">Efficient Implementation</h3>
<p>Gpus are optimized优化 for processing dense稠密 tensor structures．The problem with working directly with the point cloud is that the points are sparsely distributed across sapce and each voxel has a variable number of points.We devised设计的 
a method that converts the point cloud into a dense tensor structure where stacked VFE operations can be processed in parallel across points and voxels.\
<img alt="image" src="../images/Screenshot%202021-01-13%2011%3A43%3A06.png" />\
The method is summarized in Figure 5. We initialize a KxTx7 dimensional tensor structure to store the voxel input feature where K is the maximum number of non-empty voxels, T is the maximum number of points per voxel, and 7 is the input encoding dimension for each point.The points are randomized随机的 before processing.For each point in pointcloud, we check if the corresponding voxel already exists. This lookup operation is done efficiently in O(1) using a hash table where the voxel coordinate is used as the hash key.If the voxel is already initialized we insert the point to voxel location if there are less than T points, otherwise the point is ignored.If the voxel is not initialized, we initialize a new voxel, store its coordinate in the voxel coordinate buffer, and insert the point to this voxel location.The voxel input feature and coordinate buffers can be constructed建 via a signle pass over the point list, therefore its complexity is O(n).体素输入特征和坐标缓冲区可以通过对点列表的一次遍历来构造，因此其复杂度为O（n）。To further improve the memory/compute efficiency it is possible to only store a limited number of voxels(K) and ignore points coming from voxels with few points.</p>
<p>After the voxel input buffer is constructed,the stacked VFE only involves涉及 point level and voxel level dense operations which can be computed on a GPU in parallel.Note that, after concatenation级联 operations in VFE, we reset the features corresponding to empty points to zero such that they do not affect the computed voxel features.Finally, using the stored coordinate buffer we reorganize改组 the computed sparse voxel-wise structures to the dense voxel grid.最后，使用存储的坐标缓冲区，我们将计算的稀疏体素结构重组为密集体素网格.The following convolutional middle layers and RPN operations work on a dense voxel grid which can be efficiently implemented on a GPU.</p>
<h2 id="training-details">Training Details</h2>
<h3 id="network-details">Network Details</h3>
<p>Our experimental setup实验设置 is based on the LiDAR specifications of the KITTI dataset.\
<strong>Car Detection</strong> For this task,we consider point clouds within the range of [-3,1]x[-40,40]x[0,70.4]meters along Z,Y,X axis respectively.Points taht are Points that are projected outside of image boundaries are removed.We choose a voxel size of <img alt="image" src="../images/Screenshot%202021-01-13%2014%3A36%3A22.png" /> meters,which leads to D'=10,H'=400,W'=352. We set T = 35 as the maximum number of randomly sampled points in each non-empty voxel.We use two VFE layers VFE-1(7,32) and VFE-2(32,128).The final FCN maps VFE-2 output to <img alt="image" src="../images/Screenshot%202021-01-13%2014%3A36%3A23.png" />.Thus our feature learning net generate a sparse tensor of shape 128x10x400x352.To aggregate voxel-wise features, we employ three convolution middle layers sequentially as Conv3D(128,64,3,(2,1,1),(1,1,1)),Conv3D(64,64,3,(1,1,1),(0,1,1)),and Conv3D(64,64,3,(2,1,1),(1,1,1)), which yields a 4D tensor of size 64x2x400x352.After reshaping, the input to RPN is a feature map of size 128x400x352,where the dimensions correspond to channel,height, and width of the 3D tensor.Figure 4 illustrates the detailed network architecture for this task.Unlike, we use only one anchor size,<img alt="i" src="../images/Screenshot%202021-01-13%2014%3A52%3A18.png" /> meters, centered at集中于 <img alt="i" src="../images/Screenshot%202021-01-13%2014%3A52%3A19.png" /> meters with two rotations, 0 and 90 degrees.Our anchor matching criteria匹配标准 as follows: An anchor is considered as positive if it has the highest Intersection over Union(IoU) with a ground truth or its IoU with ground truth is above 0.6(in bird's eye view).An anchor is considered as negative if the IoU between it and all ground true boxes is less than 0.45. We treat anchors as don't care if they have 0.45≤IoU≤0.6 with any ground truth.We set α＝1.5 and β=1 in Eqn.2.</p>
<p><strong>Pedestrain and Cyclist Detection</strong> The input range is [-3,1]x[-20,20]x[0,48] meters along Z,Y,X axis respectively.We use the same voxel size as for car detection,which yields D=10, H=200, W=240.We set T=45 in order to obtain more LiDAR points for better capturing shape information.The feature learning network and convolutional middle layers ate identical相同 to the networks used in car detection task.For the RPN, we make one modification to block 1 in Figure 4 by changing the stride size in the first 2D convolution from 2 to 1. This allows finer resolution in anchor matching, which is necessary for detecting pedestrains and cyclists.We use anchor size <img alt="1" src="../images/Screenshot%202021-01-13%2015%3A15%3A22.png" /> <img alt="1" src="../images/Screenshot%202021-01-13%2015%3A15%3A23.png" /> meters centered at <img alt="1" src="../images/Screenshot%202021-01-13%2015%3A15%3A24.png" /> with 0 and 90 degrees rotation for pedestrain detection and use anchor size <img alt="1" src="../images/Screenshot%202021-01-13%2015%3A15%3A21.png" /> meters centered at <img alt="1" src="../images/Screenshot%202021-01-13%2015%3A15%3A24.png" /> with 0 and 90 degrees rotation for cyclist detecction.The specific anchor matching criteria is as follows:We assign an anchor as positive if it has the highest IoU with a ground truth, or its IoU with ground truth is above 0.5. An anchor is considered as negative if its IoU with every ground truth is less than 0.35.For anchors having 0.35≤IoU≤0.5 with any ground truth,we treat them as don't care.</p>
<p>During training, we use stochastic随机 gradient descent(SGD) with learning rate 0.01 for the first 150 epochs and decrease the learning rate to 0.001 for the last 10 epochs.We use a batchsize of 16 point clouds.</p>
<h3 id="data-augmentation">Data Augmentation 数据扩展</h3>
<p>Ｗith less than 4000 training point clouds, training our network from scratch will inevitably 不可避免 suffer from overfitting. To reduce this issue, we introduce three different forms of data augmentation. The augmented training data are generated on-the-fly即时　without the need to be stored on disk.</p>
<p>Define set <img alt="1" src="../images/Screenshot%202021-01-13%2014%3A53%3A17.png" /> as the whole point cloud, consisting of N points.We parameterize a 3D bounding box <img alt="1" src="../images/Screenshot%202021-01-13%2014%3A54%3A17.png" />,where <img alt="1" src="../images/Screenshot%202021-01-13%2014%3A55%3A17.png" /> are center locations, l,w,h are length, width, height, and θ is the yaw rotation around Z-axis.We define <img alt="1" src="../images/Screenshot%202021-01-13%2014%3A56%3A17.png" /> <img alt="1" src="../images/Screenshot%202021-01-13%2014%3A57%3A17.png" /> as the set containing all LiDAR points within <img alt="1" src="../images/Screenshot%202021-01-13%2014%3A58%3A17.png" />, where p=[x,y,z,r] denotes a particular LiDAR point in the whole set M.</p>
<p>The first form of data augmentation applies perturbation摄动 independently to each ground truth 3D bounding box together with those LiDAR points within the box.Specifically,around Z-axis we rotate <img alt="1" src="../images/Screenshot%202021-01-13%2014%3A58%3A17.png" /> and the associated <img alt="1" src="../images/Screenshot%202021-01-13%2014%3A62%3A17.png" /> with respect to (<img alt="1" src="../images/Screenshot%202021-01-13%2014%3A55%3A17.png" />) by a uniformally统一的 distributed random variable <img alt="1" src="../images/Screenshot%202021-01-13%2014%3A82%3A17.png" />. Then we add a translation (Δx,Δy,Δz) to the XYZ components of <img alt="1" src="../images/Screenshot%202021-01-13%2014%3A58%3A17.png" /> and to each point in <img alt="1" src="../images/Screenshot%202021-01-13%2014%3A62%3A17.png" />,where Δx,Δy,Δz are drawn independently from a Gaussian distribution with mean zero and standard deviation 1.0. To avoid physically impossible outcomes, we perform a collision碰撞 test between any two boxes after the perturbation and revert还原 to orignal if a collision is detected.Since the perturbation is applied to each ground truth box and the associated LiDAR points independently, the network is able to learn from substantially实质上 more variations than from the orignal training data.</p>
<p>Secondly,we apply global scaling to all ground truth boxes <img alt="1" src="../images/Screenshot%202021-01-13%2014%3A58%3A17.png" /> and to whole point cloud M. Specifically, we multiply the XYZ coordinates and the three dimensions of each <img alt="1" src="../images/Screenshot%202021-01-13%2014%3A58%3A17.png" />, and the XYZ coordinates of all points in M with a random variable drawn from uniform distribution [0.95,1.05]. Introducing global scale augementation improves robustness of the network for detecting objects with various sizes and distances as shown in image-based classification and detection tasks.</p>
<p>Finally, we apply global rotation to all ground truth boxes <img alt="1" src="../images/Screenshot%202021-01-13%2014%3A58%3A17.png" /> and to the whole point cloud M. The rotation is applied along Z-axis and around(0,0,0). The global rotation offset is determined by sampling from uniform distribution [-π/4,π/4].By rotating the entire point cloud, we simulate the vehicle making a turn.</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../radar_camera_fusion/" class="btn btn-neutral float-right" title="radar_camera">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../deeplearning/" class="btn btn-neutral" title="deeplearning"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../deeplearning/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../radar_camera_fusion/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../mathjaxhelper.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
