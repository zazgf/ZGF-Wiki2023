<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Pytorch - ZGF Wiki</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../css/theme.css" />
  <link rel="stylesheet" href="../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "Pytorch";
    var mkdocs_page_input_path = "pytorch.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> ZGF Wiki</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../c%2B%2B/">C++</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../python/">Python</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../ubuntu18/">Ubuntu18</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../ubuntu20/">Ubuntu20</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../Ros/">Ros</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../Ros%E5%BB%BA%E6%A8%A1/">Ros建模</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../tf_document/">tf_document</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../caffe/">Caffe</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../deeplearning/">deeplearning</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../voxelnet/">voxelnet</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../radar_camera_fusion/">radar_camera</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../python%E6%9C%89%E7%94%A8%E7%9A%84%E5%87%BD%E6%95%B0/">python有用的函数(一)</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">ZGF Wiki</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Pytorch</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h3 id="pytorch-exception-in-thread-valueerror-signal-number-32-out-of-range">Pytorch Exception in Thread: ValueError: signal number 32 out of range</h3>
<p>设置 <code>num_workers=0</code></p>
<h3 id="unable-to-create-file-unable-to-lock-file">Unable to create file (unable to lock file</h3>
<p>由于有应用程序正在使用该文件
<code>ps aux | grep main.py</code></p>
<p><code>sudo kill</code></p>
<h3 id="hdf5">HDF5</h3>
<p>HDF5 simplifies the file structure to include only two major types of object:
* DataSets, which are multimensional arrays of homogeneous type
* Groups, which are container structures which can hold datasets and other groups.</p>
<p>This results in a truly hierarchical等级制，filesystem-like data format.In fact ,resources in an HDF5 file can be accessed using the POSIX-like syntax <code>/path/to/resource</code>. Metadata is stored in the form of user-defined, named attributes属性 attached to groups and datasets. More complex storage APIs representing images and tables can then be built up using datasets, groups and attributs.</p>
<p>In addition to these advances in the file format, HDF5 includes an improved type system, and dataspace objects which represent selections over dataset regions. The API is also object-oriented面向对象的 with respect to datasets, groups, attributes, types, dataspaces and property lists.</p>
<p>Because it uses B-trees to index table objects, HDF5 works well for time series data such as stock股票 price series, network monitoring data, and 3D meteorological气象 data. The bulk of the data goes into straightforward arrays (the table objects) that can be accessed much more quickly than the rows of an SQL database, but B-tree access is available for non-array data. The HDF5 data storage mechanism can be simpler and faster than an SQL star schema.</p>
<h3 id="jupyter-lab-show-img">jupyter-lab show IMG</h3>
<pre><code class="language-python">from matplotlib.pyplot import imshow
import numpy as np
from PIL import Image

%matplotlib inline
pil_im = Image.open('data/empire.jpg', 'r')
imshow(np.asarray(pil_im))
</code></pre>
<h3 id="how-to-create-and-use-a-pytorch-dataloader">How to create and Use a Pytorch DataLoader</h3>
<p>In the early days of pytroch , you had to write completely custom code for data loading. Now however, the vast majority of PyTorch systems I've seen (and created myself) use the PyTorch DataSet and DataLoader interfaces to serve up training or test data.Briefly a Dataset object loads training or test data into memory , and a DataLoader object fetches获取 data from a Dataset and serves the data up in batches.</p>
<p><img alt="" src="../images/120731VSMCarrUpdateDelivery.jpg" /></p>
<p>You must write code to create a DataSet that matches your data and problem scenario设想；No two Dataset implementations are exactly the same.On the other hand,a DataLoader object is used mostly the same no matter which Dataset object it's associated with . For example:</p>
<pre><code class="language-python">class MyDataSet(T.utils.data.Dataset):
    #implent custom code to load data here
my_ds = MyDataSet(&quot;my_train_data.txt&quot;)
my_ldr = torch.utils.data.DataLoader(my_ds, 10, True)
for (idx, batch) in enumerate(my_ldr):
</code></pre>
<p>The code fragment shows you must implement a Dataset class yourself. Then you create a Dataset instance and pass it to a DataLoader constructor. The DataLoader object serves up batches of data, in this case with batch size = 10 training items in a random (True) order.</p>
<p>This article explains how to create and use PyTorch Dataset and DataLoader objects. A good way to see where this article is headed is to take a look at the screenshot of a demo program in <strong>Figure 1</strong> . The source data is a tiny 8-item file. Each line represents a person: sex(male = 1 0, female = 0 1), normalized age, region (east= 1 0 0, west = 0 1 0, central = 0 0 1), normalized income, and political政治的 learning (conservative = 0, moderate = 1, liberal = 2).The goal of the demo is to serve up data in batches where the dependent variable to predict is political learning and the other variables are the predictors.</p>
<p>The 8-item source data is stored in a tab-delimited file named people_train.txt and looks like:</p>
<pre><code class="language-python">1 0 0.171429 1 0 0 0.966805 0
0 1  0.085714  0 1 0  0.188797  1
1 0  0.000000  0 0 1  0.690871  2
1 0  0.057143  0 1 0  1.000000  1
0 1  1.000000  0 0 1  0.016598  2
1 0  0.171429  1 0 0  0.802905  0
0 1  0.171429  1 0 0  0.966805  1
1 0  0.257143  0 1 0  0.329876  0
</code></pre>
<p>Behind the scenes, the demo loads data into memory using a custom Dataset object ,and then serves the data up in randomly selected batches of size 3 rows/items. Because the source data has 8 lines, the first two batches have 3 data items , but the last batch has 2 items. The demo processes the source data twice, in other words,two epochs.</p>
<p>This artical assumes you have intermediate or better skill with a C-family programming language. The demo program is coded using Python.which is used by PyTorch and which is essentially the primary language for deep neural networks.The complete source code for the demo program is presented in this article. The source code and source data are also available  in the file download that accompanies this article.</p>
<h3 id="the-demo-program">The demo program</h3>
<p>The demo program , with a few minor edits to save space, is presented in <strong>Listing 1</strong> . I indent my Python programs using two spaces, rather than the more common four spaces or a tab character,as a matter of personal preference.</p>
<p><strong>Listing 1:</strong> DataLoader Demo Program</p>
<pre><code class="language-python">import numpy as np
import Torch as T
device = T.device(&quot;cpu&quot;)

#predictors and label in same file 
#data has been normialized and encoded like:
# sex   age        region   income      politic
# [0]   [2]        [3]      [6]         [7]
# 1 0   0.057143   0 1 0    0.690871    2

class PeopleDataset(T.utils.data.Dataset):
    def __init__(self, src_file, num_rows= None):
        x_tmp = np.loadtxt(src_file, max_rows=num_rows, usecols = rane(0, 7), delimiter=&quot;\t&quot;, skiprows=0, dtype=np.float32)
        y_tmp = np.loadtxt(src_file, max_rows=num_rows, usecols =7, delimiter=&quot;\t&quot;, skiprows=0, dtype= np.long)

        self.x_data = T.tensor(x_tmp, dtype=T.float32).to(device)
        self.y_data = T.tensor(y_tmp, dtype = T.long).to(device)

    def __len__ (self):
        return len(self.x_data) #required

    def __getitem__(self, idx):
        if T.is_tensor(idx):
            idx = idx.tolist()
        preds = self.x_data[idx, 0:7]
        pol = self.y_data[idx]
        sample =\
            {'predictors': preds, 'political':pol }
        return sample

def main():
    print(&quot;\nBegin PyTorch DataLoader demo&quot; )
    # 0. miscellaneous prep
    T.manual_seed(0)
    np.random.seed(0)

    print(&quot;\nSource data looks like: &quot;)
    print(&quot;1 0  0.171429  1 0 0  0.966805  0&quot;)
    print(&quot;0 1  0.085714  0 1 0  0.188797  1&quot;)
    print(&quot; . . . &quot;)

    # 1. create Dataset and DataLoader object
    print(&quot;\nCreating Dataset and DataLoader &quot;)

    train_file = &quot;./people_train.txt&quot;
    train_ds = PeopleDataset(train_file, num_rows = 8)

    bat_size = 3 
    train_dir = T.utils.data.DataLoader(train_ds, batch_size=bat_size, shuffle=True)

    #2. iterate thru training data twice
    for epoch in range(2):
        print(&quot;\n===========================&quot;)
        print(&quot;Epoch = &quot;  + str(epoch))
        for (batch_idx, batch) in enumerate(train_ldr):
            print(&quot;\nBatch = &quot; + str(batch_idx))
            X = batch['predictors']
            Y = batch['political']
            print(X)
            print(Y)
    print(&quot;\n============================&quot;)
    print(&quot;\nEnd demo&quot;)

if __name__ = &quot;__main__&quot;:
    main()
</code></pre>
<p>The excution of the demo program begins with:</p>
<pre><code class="language-python">def main():
    print(&quot;\nBegin PyTorch DataLoader demo&quot; )
    # 0. miscellaneous prep
    T.manual_seed(0)
    np.random.seed(0)
    . . .
</code></pre>
<p>In almost all PyTorch programs, it's a good idea to set the system random number generator seed values so that your results will be reproducible. Unfortunately,because of execution across multiple processes, sometimes your results are not reproducible even if you set the random generator seeds. But if you don't set the seeds, your results will almost certainly not be reproducible.</p>
<p>Next a Dataset and a DataLoader object are created:</p>
<pre><code class="language-python">train_file = &quot;.\\people_train.txt&quot;
train_ds = PeopleDataset(train_file, num_rows=8)

bat_size = 3
train_ldr = T.utils.data.DataLoader(train_ds,
batch_size=bat_size, shuffle=True)
</code></pre>
<p>The custom PeopleDataset object constructor accepts a path to the training data, and a num_rows parameter in case you want to load just part of a very large data file during system development.</p>
<p>The built-in DataLoader class definition is housed in the torch.utils.data module. The class constructor has one required parameter, the Dataset that holds the data. There are 10 optional parameters . The demo specifies values for just the batch_size and shuffle parameters, and therefore uses the default values for the other 8 optional parameters.</p>
<p>The demo concudes by using the DataLoader to iterate through the source data:</p>
<pre><code class="language-python">  for epoch in range(2):
    print(&quot;\n==============================\n&quot;)
    print(&quot;Epoch = &quot; + str(epoch))
    for (batch_idx, batch) in enumerate(train_ldr):
      print(&quot;\nBatch = &quot; + str(batch_idx))
      X = batch['predictors']  # [3,7]
      Y = batch['political']   # [3]
      print(X)
      print(Y)
</code></pre>
<p>In neural network terminology术语，an epoch is one pass through all source data. The DataLoader class is designed so that it can be iterated using the enumerate() function, which returns a tuple with the current batch zero-based index value, and the actual batch of data.There is a tight coupling between a Dataset and its associated DataLoader, meaning you have to know the names of the keys used for the predictor values and the dependent variable values. In this case the two keys are "predictors" and "political".</p>
<p><strong>Implementing a Dataset Class</strong></p>
<p>You have a lot of flexibility when implementing a Dataset class. You are required to implement three methods and you can optionally add other methods depending on your source data The required methods are <strong>init</strong>(), <strong>len</strong>(),and <strong>getitem</strong>(). The demo PeopleDataset defines its <strong>init</strong>() method as</p>
<pre><code class="language-python">def __init__(self, src_file, num_rows=None):
    x_tmp = np.loadtxt(src_file, max_rows=num_rows, usecols= range(0,7), delimiter=&quot;\t&quot;, skiprows=0, dtype=np.float32)
    y_tmp = np.loadtxt(src_file, max_rows= num_rows, usecols=7, delimiter=&quot;\t&quot;, skiprows = 0, dtype =np.long)

    self.x_data = T.tensor(x_tmp, dtype=T.float32).to(device)
    self.y_data = T.tensor(y_tmp, dtype=T.long).to(device)

</code></pre>
<p>In situations where your source data is too large to fit into memory, you will have to read data into a buffer and refill the buffer when the buffer has been emptied . This is a fairly difficult task.</p>
<p>The demo data stores both the predictor values and dependent values-to-predict in the same file. In situation where the predictor values and dependent variables values are in separate files, you'd have to pass in two source file names instead of just one. Another common alternative is to pass in just a single source directory and then use hard-coded file names for the training and test data.</p>
<p>The demo <strong>init</strong>() method bulk-converts大量转换 all Numpy array data to PyTorch tensors. An alternative is to leave the data in memory as Numpy arrays and then convert to batches of data to tensors in the <strong>getitem</strong>() method.Conversion from Numpy array data to PyTorch tensor data is an expensive operation so it's usually better to convert just once rather than repeatedly converting batches of data.</p>
<p>The <strong>len</strong>() method is defined as:</p>
<pre><code class="language-python">def __len__(self):
    return len(self.x_data)
</code></pre>
<p>A Dataset object has to know how much data there is so that an associated DataLoader object knows how to iterate through all data in batches.</p>
<p>The <strong>getitem</strong>() method is defined as :</p>
<pre><code class="language-python">def __getitem__(self, idx):
    if T.is_tensor(idx):
        idx = idx.tolist()
    preds = self.x_data[idx, 0:7]
    pol = self.y_data[idx]
    sample = {'prediction':preds, 'political': pol}
    return sample
</code></pre>
<p>It's common practice to name the parameter which specifies which data to fetch as "idx" but this is somewhat misleading because the idx parameter is usually a Python list of several idexes.The <strong>getitem</strong>() method checks to see if the idx parameter is a PyTorch tensor instead of a Python list, and if so, converts the tensor to a list. The method return value, sample, is a Python Dictionary object and so you must specify names for dictionary keys("predictors" in the demo) and the dictionary values ("political" in the demo).</p>
<p><strong> Using a Dataset in DataLoader </strong>
The demo program creates a relatively simple DataLoader object using just the Dataset object plus the batch_size and shuffle parameters.</p>
<pre><code class="language-python">train_file = &quot;./people_train.txt&quot;
train_ds = PeopleDataset(train_file, num_rows=8)

bat_size = 3
train_ldr = T.utils.data.DataLoader(train_ds, batch_size=bat_size, shuffle = True)

</code></pre>
<p>In some situations, instead of using a DataLoader to consume the data in a Dataset, it's useful to iterate through a Dataset directly. For example:</p>
<pre><code class="language-python">def process_ds(model , ds):
    #ds is an iterable Dataset of tensors
    for i in range(len(ds)):
        inpts = ds[i]['predictors']
        trgt = ds[i]['target']
        oupt = model(inpts)
        #do something
    return somevalue
</code></pre>
<p><strong>Using other DataLoaders</strong>
Once you understand how to create a custom Dataset and use it in a DataLoader, many of the built-in PyTorch library Dataset objects make more sense than they might otherwise. For example, the TorchVision module has data and functions that are useful for image processing . One of the Dataset classes in TorchVision holds the MNIST image data. There are 70,000 MNIST images. Each image is a handwritten digit from '0' to '9'. Each image has size 28x28 pixels and pixels are grayscale values from 0 to 255.</p>
<p>A Dataset class for the MNIST images is defined in the torchvison.datasets package and is named MNIST. You can create a Dataset for MNIST training images like so:</p>
<pre><code class="language-python">import torchvision as tv
tform = tv.transforms.Compose([tv.transforms.ToTensor()])
mnist_train_ds = tv.datasets.MNIST(root=&quot;./MNIST_Data&quot;, train=True, transform=tform, target_transform=None, download=True)
</code></pre>
<p>After an MNIST Dataset object has been created, it can be used in a DataLoader as normal, for example </p>
<pre><code class="language-python">mnist_train_dataldr = T.utils.data.DataLoader(mnist_train_ds,batch_size =2, shuffle = True)

for (batch_idx, batch) in enumerate(mnist_train_dataldr):
    print(&quot;&quot;)
    print(batch_idx)
    print(batch)
    input()
</code></pre>
<p>To recap回顾， there are many built-in Dataset classes defined in various PyTorch packages. They have different calling signatures, but they all read in data from some source (often a hard-coded URL), and have a way to convert their data to PyTorch tensors.After a built-in Dataset has been created, it can be processed by a DataLoader object using the enumerate() function.</p>
              
            </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../mathjaxhelper.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
