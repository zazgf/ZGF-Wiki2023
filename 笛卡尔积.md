$\{X_i|i\in I\}$

$$
\prod_{x\in I}X_i=\{f:I\rightarrow\bigcup_{x\in I}X_i|(\forall_i)(f(i)\in X_i)\}
$$
$$
\pi_j(f)=f(j)
$$

第j投影映射

$$
\pi_j:\prod_{i\in I}X_i\rightarrow X_j

$$

$$
\prod_{n=1}^{\infty}\mathbb{R}=\mathbb{R}^{\omega}=\mathbb{R}\times\mathbb{R}\cdots

$$

极大似然估计应用于线性回归

极大似然法是一种用于估计模型参数的统计方法。

在最大似然估计中，选择参数以使假设的模型产生观测数据的可能性最大。

1. 假设我们有个模型，也称为数据生成过程。
2. 能够为我们的数据导出似然函数。

一旦导出似然函数，最大似然估计仅是一个简单的优化问题。

极大似然估计的优势：

* 如果正确设定了模型，最大似然估计器将是最有效的估计器
* 它提供了一致但灵活的方法，使其适用于各种应用程序，包括其他设定模型无效的情况。
* 在较大模型中得出无偏估计

效率是估计器质量的一种度量，有效的估计器具有较小的方差或均方误差。

极大似然估计的缺点：

* 它依赖于模型的设定以及似然函数的推导，而这种推导并不总是那么容易
* 像其他优化问题一样，最大 似然估计可能对初始值的选择很敏感。
* 取决于似然函数的复杂度，数值估计在计算上可能是昂贵的。
* 小样本的估计值可能有偏差。

极大似然估计取决于似然函数的推导。因此重要的是要对似然函数是什么以及它来自何处有一个很好的了解。

极大似然估计的第一步是假设数据的概率分布，概率密度函数在给定一组基础模型参数的情况下测量观察数据的概率。

我们假设我们的数据具有潜在的泊松分布，这是一个普遍的假设，尤其对于非负计数数据。

单个观测值$y_i$的泊松概率密度函数由下式给出：

$$
f(y_i|\theta)=\frac{e^{-\theta}\theta^{y_i}}{y_i!}

$$

由于我们样本中的观测值是独立的，因此可以通过取各个观测值的概率乘积来求出我们观测样本的 概率密度：

$$
f(y_1,y_2,\cdots,y_{10}|\theta)=\prod_{i=1}^{10}\frac{e^{-\theta}\theta^{y_i}}{y_i!}=\frac{e^{-10\theta}\theta\sum_{i=1}^{10}y_i}{\prod_{i=1}^{10}y_i!}

$$

我们可以使用概率密度来回答在给定特定参数的情况下数据发生的可能性。

似然函数和概率密度函数的不同是细微的但是是重要的

* 概率密度函数强调在给定基础分布参数的情况下观察我们数据的概率，前提是假设参数已知。
* 似然函数强调观测数据时出现参数值的可能性，这里参数是未知的。

数学上似然函数和概率密度函数很像

$$
L(\theta|y_1,y_2,\cdots,y_{10})=f(y_1,y_2,\cdots,y_{10}|\theta)

$$

对于我们的泊松分布数据，我们相当容易的得到似然函数

$$
L(\theta|y_1,y_2,\cdots,y_{10})=\frac{e^{-10\theta}\theta\sum_{i=1}^{10}y_i}{\prod_{i=1}^{10}y_i!}=\frac{e^{-10\theta}\theta^{20}}{207360}

$$

$$
\prod_{k=3}^{7}k=3\times4\times5\times6\times7

$$

未知参数$\theta$的极大似然估计，是使这种可能性最大化的值。

**对数似然函数**

在实际中，联合分配函数难以使用，一般用$ln$似然函数代替。在我们的泊松数据里对数似然函数是：
$$ln(L(\theta|y)=-n\theta+ln\sum_{i=1}^{n}y_i-ln\theta\sum_{i=1}^{n}y_i!=-10\theta+20ln(\theta)-ln(207360)$$

在线性回归，我们假设模型的残差是独特的完全正态分布。
$$\epsilon=y-\hat{\beta}x\quad\sim N(0,\delta^2)$$
对于这种情况，未知参数向量$\theta=\{\beta,\delta^2\}$,观察数据的条件y和x已知，对数似然函数是：
$$lnL(\theta|y,x)=-\frac{1}{2}\sum_{i=1}^{n}\left[ln\delta^2+ln(2\pi)+\frac{y-\hat{\beta}x}{\delta^2}\right]$$
$\beta$和$\delta^2$的极大似然估计就是使可能最大化的值。
**极大似然估计和概率模型**
$$y^*=x\theta+\epsilon\quad\epsilon\sim N(0,1)$$
$$y_i=\left\{\begin{matrix}0\quad if\quad y^{*}_{i}\leqslant0 &\\1\quad if\quad y^{*}_i>0\end{matrix}\right.$$
$$P(y_i=1|X_i)=P(y^*_i>0|X_i)=P(x\theta+\epsilon>0|X_i)=P(\epsilon>-x\theta|X_i)=1-\Phi(-\theta x)=\Phi(x\theta)$$
$\Phi$代表正态累计分布。
每个$y_i$是独立的所以
$$f(y=(y_1,y_2,\cdots,y_n|w)=f_1(y_1|w)f_2(y_2|w)\cdots f_n(y_n|w)$$
$$log\pi_\theta(\alpha|s)=log[P_\theta(s)]_\alpha$$
**采样**
已知mean action $\mu_\theta(s)$和标准偏差$\delta_\theta(s)$和球形高斯噪音的向量z（$z\sim N(0,1)$）可以计算出一个action sample
$$\alpha=\mu_\theta(s)+\delta_\theta(s)\bigodot z$$
$\bigodot$代表两个向量元素乘积。


对于均值$\mu=\mu_\theta(s),标准偏差$$\delta=\delta_\theta(s)$的对角高斯，$k$维action $\alpha$ 的对数似然由以下给出：
$$log\pi_\theta(\alpha|s)=-\frac{1}{2}\left(\sum_{i=1}^{k}\left(\frac{(\alpha_i-\mu_i)^2}{\delta^2_i}+2log\delta_i\right)+klog2\pi\right)$$
**Trajectories**
一个轨迹$\tau$是world里一系列状态和action。
$$\tau=(s_0,\alpha_0,s_1,\alpha_1,\cdots)$$
$$s_{t+1}=f(s_t,\alpha_t)$$
Or 
$$s_{t+1}\sim P(\cdot|s_t,\alpha_t)$$
actions是代理通过策略产生的
**奖励和反馈**
奖励取决于当前world的状态，刚刚采取的action和world的下一个状态
$$r_t=R(s_t,a_t,s_{t+1})$$
有时会简化为$r_t=R(s_t)$或者state-action $r_t=R(s_t,a_t)$
代理的目标是使某个轨迹上的某个概念的累计奖励最大化。
我们将通过以下方式记录所有的情况$R(\tau)$

一种反馈是有限水平的无折现反馈，就是所有奖励的和
$$R(\tau)=\sum_{t=0}^{T}r_t$$
另一种反馈是有限水平的折现反馈，代理获得的奖励之和，折现将来代理获得的how far off.这个方程式的奖励包含一个折现因子$\gamma\in(0,1)$
$$R(\tau)=\sum_{t=o}^{\infty}\gamma^t\tau_t$$
为什么我们想要一个折现的反馈，而不是所有的反馈？我们的确想要所有反馈，但是折现因子在直观上即吸引人，数学实现起来又方便。一种直觉：现在的现金比以后的现金好。数学上：无限水平的奖励之和可能不会收敛到有限的值，并且很难用方程式处理。但是在有折现因子的情况下，并在合理的条件下，无穷大是收敛的。

Deep RL 练习往往会使这条线模糊不清，例如，我们经常设置算法以优化未折现的反馈，但在评价value function 的时候使用折现因子。

无论选择哪种反馈方法，选择哪种策略，RL的目标是选择一个策略使代理运行此策略使期望的反馈最大化。
要讨论期望的反馈，我们先来讨论一下轨迹上的概率分布。

让我假设环境转变和策略都是随机的。在这种情况下，轨迹T-step的概率是：
$$P(T|\pi)=\rho_0(s_0)\prod_{t=0}^{T-1}P(s_{t+1}|s_t,\alpha_t)\pi(s_t,\alpha_t)$$
期望的反馈表示为
$$J(\pi)=\int_{r}P(\tau|\pi)R(\tau)=\underset{\tau\sim\pi}{E}[R(\tau)]$$
RL中的集中优化问题可以表示为：
$$\pi^*=arg\quad\underset{\pi}{max}J(\pi)$$
$\pi^*$是最佳策略。
**Value Function**
知道value of state 或者 state-action 通常是有用的。
这里有四种value function.
1. 基于策略的value function.$V^\pi(s)$,当state s,执行策略$\pi$时，
$$V^\pi(s)=\underset{\tau\in\pi}{E}[R(\tau)|s_0=s]$$
2. 基于策略的 action-value function. $Q^\pi(s,\alpha)$,
$$Q^\pi(s,\alpha)=\underset{\tau\in\pi}{E}[R(\tau)|s_0=s,a_0=a]$$
3. 最优value function， $V^*(s)$
$$V^*(s)=\underset{\pi}{max}\underset{\tau\in\pi}{E}[R(\tau)|s_0=s]$$
4. 最佳action-value function, $Q^*(s,\alpha)$

$$Q^*(s,\alpha)=\underset{\pi}{max}\underset{\tau\in\pi}{E}[R(\tau)|s_0=s,\alpha_0=\alpha]$$
在value function 和 action-value function 之间有两个关键的链接
$$V^\pi(s)=\underset{\alpha\sim\pi}{E}[Q^\pi(s,\alpha)]$$
和
$$V^*(s)=\underset{\alpha}{max}Q^*(s,\alpha)$$
$$\alpha^*(s)=arg\quad\underset{\alpha}{max}Q^*(s,\alpha)$$
**贝尔曼方程**
这4个value function都遵循的特殊自治方程,称为Bellman equations
Bellman equation后面的基本思想是：
    起点处的value，是你期望从那里得到的reward，再加上下一步到达地方的value。
 
 基于策略的value function 的Bellman equations
$$V^\pi(s)=\underset{s'\sim P}{\underset{\alpha\sim\pi}{E}}[\tau(s,\alpha)+\gamma V^\pi(s')]$$
$$Q^\pi(s,\alpha)=\underset{s'\sim P}{E}\left[\tau(s,\alpha)+\gamma\underset{a'\sim\pi}{E}[Q^\pi(s',\alpha')]\right]$$
$s'\sim P$是$s'\sim P(\cdot|s,\alpha)$的缩写，指示下一状态$s'$从环境的过度规则中采样；$\alpha\sim\pi$是$\alpha\sim\pi(\cdot|s)$的缩写；$\alpha'\sim\pi$是$\alpha'\sim\pi(\cdot|s')$的缩写。

最佳value function的Bellman equation是
$$V^*(s)=\underset{\alpha}{max}\quad\underset{s'\sim P}{E}[\tau(s,\alpha)+\gamma V^*(s')]$$
$$Q^*(s,\alpha)=\underset{s'\sim P}{E}\left[\tau(s,\alpha)+\gamma\underset{\alpha'}{max}Q^*(s',\alpha')\right]$$
Bellman equation:  reward-plus-next-value

**Advantage Functions**
$$A^\pi(s,\alpha)=Q^\pi(s,\alpha)-V^\pi(s)$$
**Markov Decision Processes**
An MDP is a 5-tuple,$(S,A,R,P,\rho_0)$
S is the set of all valid states.
A is the set of all valid actions.
R:$S\times A\times S\rightarrow \mathbb{R}$ is the reward function , with $\tau_t=R(s_t,\alpha_t,s_{t+1})$
P:$S\times A \rightarrow P(S)$ is the transition probability function, with $P(s'|s,\alpha)$ being the probability of transitioning into state $s'$,if you start in state s and take action $\alpha$
$\rho_0$ is the starting state distribution.